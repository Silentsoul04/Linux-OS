[TOC]

# 1.6. An Overview of Unix Kernels

Unix kernels provide an **execution environment** in which applications may run. Therefore, the kernel
must implement a set of **services** and corresponding **interfaces**. Applications use those **interfaces** and
do not usually interact directly with **hardware resources**.



## 1.6.1. The Process/Kernel Model

When a program is executed in **User Mode**, it cannot directly access the **kernel data structures** or the
**kernel programs**. When an application executes in **Kernel Mode**, however, these restrictions no
longer apply. Each CPU model provides special **instructions** to switch from **User Mode** to **Kernel Mode**
and vice versa. A program usually executes in User Mode and switches to Kernel Mode only when
requesting a service provided by the kernel. When the kernel has satisfied the program's request, it
puts the program back in **User Mode**.

> NOTE: See also
>
> - [User space](https://en.wikipedia.org/wiki/User_space)
> - [CPU modes](https://en.wikipedia.org/wiki/CPU_modes)
> - [Protection ring](https://en.wikipedia.org/wiki/Protection_ring)

Processes are dynamic entities that usually have a limited life span within the system. The task of
creating, eliminating, and synchronizing the existing processes is delegated to a group of routines in
the kernel.

**The kernel itself is not a process but a process manager**. The process/kernel model assumes that
processes that require a **kernel service** use specific programming constructs called **system calls** .
Each **system call** sets up the group of parameters that identifies the **process request** and then
executes the hardware-dependent CPU **instruction** to switch from **User Mode** to **Kernel Mode**.

> NOTE: See also
>
> - [System call](https://en.wikipedia.org/wiki/System_call)

Besides user processes, Unix systems include a few **privileged processes** called **kernel threads** with
the following characteristics:

- They run in Kernel Mode in the kernel address space.
- They do not interact with users, and thus do not require terminal devices.
- They are usually created during system startup and remain alive until the system is shut down.

> NOTE : 
> - [What is a Kernel thread?](https://stackoverflow.com/questions/9481055/what-is-a-kernel-thread)
> - [Understanding Kernel Threads](https://www.ibm.com/support/knowledgecenter/en/ssw_aix_72/kernelextension/kern_threads.html)

On a uniprocessor system, only one process is running at a time, and it may run either in User or in Kernel Mode. If it runs in **Kernel Mode**, the processor is executing some **kernel routine**. Figure 1-2 illustrates examples of transitions between User and Kernel Mode. Process 1 in **User Mode** issues a **system call**, after which the process switches to **Kernel Mode**, and the **system call** is serviced. Process 1 then resumes execution in **User Mode** until a **timer interrupt** occurs, and the **scheduler** is activated in **Kernel Mode**. A **process switch** takes place, and Process 2 starts its execution in **User Mode** until a hardware device raises an interrupt. As a consequence of the interrupt, Process 2 switches to **Kernel Mode** and services the interrupt.

![](./Figure1-2Transitions-between-User-and-Kernel-Mode.JPG)



**Unix kernels** do much more than handle **system calls**; in fact, **kernel routines** can be activated in
several ways:

- A process invokes a **system call**.
- The CPU executing the process signals an **exception**, which is an unusual condition such as an
  invalid instruction. The kernel handles the exception on behalf of the process that caused it.
- A peripheral device issues an **interrupt signal** to the CPU to notify it of an event such as a
  request for attention, a status change, or the completion of an I/O operation. Each **interrupt
  signal** is dealt by a kernel program called an **interrupt handler**. Because peripheral devices
  operate asynchronously with respect to the CPU, interrupts occur at unpredictable times.
- A **kernel thread** is executed. Because it runs in Kernel Mode, the corresponding program must
  be considered part of the kernel.



## 1.6.2. Process Implementation

To let the kernel manage processes, each process is represented by a *process descriptor* that
includes information about the current state of the process.

> NOTE: *process descriptor*在3.2. Process Descriptor中进行专门介绍
>
> 本节中的process所指为lightweight process，而不是标准的[Process (computing)](https://en.wikipedia.org/wiki/Process_(computing))

When the kernel stops the execution of a process, it saves the current contents of several processor
registers in the process descriptor. These include:

- The program counter (PC) and stack pointer (SP) registers
- The general purpose registers
- The floating point registers
- The processor control registers (Processor Status Word) containing information about the CPU
  state
- The memory management registers used to keep track of the RAM accessed by the process

> NOTE: See also
>
> - [Program counter](https://en.wikipedia.org/wiki/Program_counter)
> - [Stack register](https://en.wikipedia.org/wiki/Stack_register)
> - [Processor register](https://en.wikipedia.org/wiki/Processor_register)

When the kernel decides to resume executing a process, it uses the proper **process descriptor fields**
to load the CPU registers. Because the stored value of the **program counter** points to the instruction
following the last instruction executed, the process resumes execution at the point where it was
stopped.

When a process is not executing on the CPU, it is waiting for some event. Unix kernels distinguish many **wait states**, which are usually implemented by **queues of process descriptors** ; each (possibly
empty) queue corresponds to the set of processes waiting for a specific event.

> NOTE:  参见3.2.4. How Processes Are Organized



## 1.6.3. Reentrant Kernels

> NOTE: 从一个内核设计者的角度来思考本节的内容，将更加容易掌握作者所要传达的思想。内核的设计者会追求系统能够快速地响应用户的请求，系统能够高效地运行，系统需要尽可能的压缩CPU的空闲时间，让CPU更多地进行运转。所以，它就需要在某个请求暂时无法完成的情况下，将它挂起并转向另外一个请求；当该请求的执行条件满足的时候再将它重启；另外，kernel还需要处理无法预测何时会出现的各种interrupt和exception，挂起当前的请求转去执行相应的handler。这种能力就是本节所述的*reentrant*。显然这种设计能够最大程度地保证系统的高效。这种设计也不可避免地导致系统的复杂，正如在本节后面所述的， 系统是在多个*kernel control path*中交错运行的，这种设计会派生出一系列的问题，比如将在1.6.5. Synchronization and Critical Regions中介绍的race condition，所以它kernel的实现提出了更高的要求。当然可以预期的是，系统是在这样的交错中不断向前进的。
>
> 如何来实现reentrant kernel呢？这是一个需要系统地进行设计才能够解决的问题，下面总结了和这个问题相关的一些章节：
>
> - 1.6.4. Process Address Space
>
>   Kernel control path refers to its own private kernel stack.
>
> - 1.6.5. Synchronization and Critical Regions
>
>   描述了kernel control path的Synchronization

All Unix kernels are *reentrant*. This means that several processes（指的是lightweight process） may be executing in **Kernel Mode** at the same time. Of course, on uniprocessor systems, only one process can progress, but **many** can be blocked in **Kernel Mode** when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the **disk controller** handle it and resumes executing other processes. An **interrupt** notifies the kernel when the device has satisfied the read, so the former process can resume the execution.

One way to provide **reentrancy** is to write functions so that they modify only **local variables** and do not alter **global data structures**. Such functions are called *reentrant functions* . But a **reentrant kernel** is not limited only to such **reentrant functions** (although that is how some **real-time kernels** are implemented). Instead, the kernel can include **nonreentrant functions** and use **locking mechanisms** to ensure that only one process can execute a **nonreentrant function** at a time.

If a **hardware interrupt** occurs, a **reentrant kernel** is able to suspend the current running process even if that process is in **Kernel Mode**. This capability is very important, because it improves the  throughput of the **device controllers** that issue interrupts. Once a device has issued an interrupt, it waits until the **CPU acknowledges** it. If the kernel is able to answer quickly, the **device controller** will be able to perform other tasks while the CPU handles the interrupt.

Now let's look at **kernel reentrancy** and its impact on the organization of the kernel. A *kernel control path* denotes the sequence of instructions executed by the kernel to handle a **system call**, an **exception**, or an **interrupt**.

In the simplest case, the CPU executes a **kernel control path** sequentially from the first instruction to
the last. When one of the following events occurs, however, the CPU interleaves the **kernel control paths** :

- A process executing in User Mode invokes a **system call**, and the corresponding **kernel control path** verifies that the request cannot be satisfied immediately; it then invokes the **scheduler** to select a new process to run. As a result, a **process switch** occurs. The first **kernel control path** is left unfinished, and the CPU resumes the execution of some other **kernel control path**. In this case, the two **control paths** are executed on behalf of two different processes.
  
- The CPU detects an exception for example, access to a page not present in RAM while running a **kernel control path**. The first control path is suspended, and the CPU starts the execution of a suitable procedure. In our example, this type of procedure can allocate a new page for the process and read its contents from disk. When the procedure terminates, the first control path can be resumed. In this case, the two control paths are executed on behalf of the same process.
  
  > NOTE: 从这段可以看出，**kernel control path**和process不是一一对应的关系。
  
- A hardware interrupt occurs while the CPU is running a kernel control path with the interrupts enabled. The first kernel control path is left unfinished, and the CPU starts processing another kernel control path to handle the interrupt. The first kernel control path resumes when the interrupt handler terminates. In this case, the two kernel control paths run in the execution context of the same process, and the total system CPU time is accounted to it. However, the interrupt handler doesn't necessarily operate on behalf of the process.
  
- An interrupt occurs while the CPU is running with kernel preemption enabled, and a higher
  priority process is runnable. In this case, the first kernel control path is left unfinished, and the
  CPU resumes executing another kernel control path on behalf of the higher priority process.
  This occurs only if the kernel has been compiled with kernel preemption support.

Figure 1-3 illustrates a few examples of noninterleaved and interleaved kernel control paths. Three different CPU states are considered:

- Running a process in User Mode (`User`)
- Running an exception or a system call handler (`Excp`)
- Running an interrupt handler (`Intr`)

![](./Figure-1-3-Interleaving-of-kernel-control-paths.jpg)



## 1.6.4. Process Address Space

Each process runs in its private address space. A process running in User Mode refers to private stack, data, and code areas. When running in Kernel Mode, the process addresses the kernel data and code areas and uses another private stack.

> NOTE: 不同的mode，使用不同的address space

Because the kernel is reentrant, several **kernel control paths** each related to a different **process** may be executed in turn. In this case, each **kernel control path** refers to its own **private kernel stack**.

> NOTE : 关于kernel stack参见3.2.2.1. Process descriptors handling

While it appears to each process that it has access to a **private address space**, there are times when part of the address space is **shared** among processes. In some cases, this sharing is explicitly requested by processes; in others, it is done automatically by the kernel to reduce memory usage.

If the same program, say an editor, is needed simultaneously by several users, the program is loaded into memory only once, and its instructions can be shared by all of the users who need it. Its data, of course, must not be shared, because each user will have separate data. This kind of shared address space is done automatically by the kernel to save memory.

Processes also can share parts of their address space as a kind of **interprocess communication**, using the "shared memory" technique introduced in System V and supported by Linux.

Finally, Linux supports the  [`mmap( )`](http://man7.org/linux/man-pages/man2/mmap.2.html) system call, which allows part of a file or the information stored on a block device to be mapped into a part of a process address space. Memory mapping can provide an alternative to normal reads and writes for transferring data. If the same file is shared by several processes, its memory mapping is included in the address space of each of the processes that share it.

## 1.6.5. Synchronization and Critical Regions

> NOTE: 虽然本节所描述的是kernel的synchronization，但是其中所描述的方法、思路可以广泛应用于其他领域。

Implementing a **reentrant kernel** requires the use of **synchronization** . If a **kernel control path** is suspended while acting on a kernel data structure, no other **kernel control path** should be allowed to act on the same data structure unless it has been reset to a **consistent state**. Otherwise, the interaction of the two control paths could corrupt the stored information.

For example, suppose a global variable `V` contains the number of available items of some system resource. The first kernel control path, `A`, reads the variable and determines that there is just one available item. At this point, another kernel control path, `B`, is activated and reads the same variable, which still contains the value `1`. Thus, `B` decreases `V` and starts using the resource item. Then `A` resumes the execution; because it has already read the value of `V`, it assumes that it can decrease `V` and take the resource item, which `B` already uses. As a final result, V contains -1, and two kernel control paths use the same resource item with potentially disastrous effects.

When the outcome of a computation depends on how two or more processes are scheduled, the code is incorrect. We say that there is a *race condition*.

In general, safe access to a **global variable** is ensured by using **atomic operations** . In the previous example, data corruption is not possible if the two control paths **read and decrease** `V` with a single, **noninterruptible operation**. However, kernels contain many data structures that cannot be accessed with a single operation. For example, it usually isn't possible to remove an element from a linked list with a single operation, because the kernel needs to access at least two pointers at once. Any section of code that should be finished by each process that begins it before another process can enter it is called a **critical region**. `[*]`

> `[*]` Synchronization problems have been fully described in other works; we refer the interested reader to books on the Unix operating systems (see the Bibliography).

These problems occur not only among **kernel control paths** but also among processes sharing common data. Several synchronization techniques have been adopted. The following section concentrates on how to synchronize **kernel control paths**.

### 1.6.5.1. Kernel preemption disabling

To provide a drastically simple solution to synchronization problems, some traditional Unix kernels are nonpreemptive: when a process executes in Kernel Mode, it cannot be arbitrarily suspended and substituted with another process. Therefore, on a uniprocessor system, all kernel data structures that are not updated by interrupts or exception handlers are safe for the kernel to access.

Of course, a process in Kernel Mode can voluntarily relinquish the CPU, but in this case, it must ensure that all data structures are left in a consistent state. Moreover, when it resumes its execution, it must recheck the value of any previously accessed data structures that could be changed.

A synchronization mechanism applicable to preemptive kernels consists of disabling kernel preemption before entering a critical region and reenabling it right after leaving the region. Nonpreemptability is not enough for multiprocessor systems, because two kernel control paths running on different CPUs can concurrently access the same data structure.

### 1.6.5.2. Interrupt disabling

Another **synchronization mechanism** for **uniprocessor systems** consists of disabling all hardware interrupts before entering a critical region and reenabling them right after leaving it. This mechanism, while simple, is far from optimal. If the critical region is large, interrupts can remain disabled for a relatively long time, potentially causing all hardware activities to freeze.

### 1.6.5.3. Semaphores

A widely used mechanism, effective in both uniprocessor and multiprocessor systems, relies on the use of *semaphores* . A semaphore is simply a counter associated with a data structure; it is checked by all kernel threads before they try to access the data structure. Each semaphore may be viewed as an object composed of:

- An integer variable
- A list of waiting processes
- Two atomic methods:  `down( )` and  `up( )`

The  `down( )` method decreases the value of the semaphore. If the new value is less than 0, the method adds the running process to the `semaphore list` and then blocks (i.e., invokes the **scheduler**). The  `up( )` method increases the value of the semaphore and, if its new value is greater than or equal to 0, reactivates one or more processes in the semaphore list.

Each data structure to be protected has its own semaphore, which is initialized to 1. When a kernel control path wishes to access the data structure, it executes the  `down( )` method on the proper semaphore. If the value of the new semaphore isn't negative, access to the data structure is granted. Otherwise, the process that is executing the kernel control path is added to the semaphore list and blocked. When another process executes the  `up( )` method on that semaphore, one of the processes in the semaphore list is allowed to proceed.

### 1.6.5.4. Spin locks

In multiprocessor systems, semaphores are not always the best solution to the **synchronization** problems. Some kernel data structures should be protected from being concurrently accessed by **kernel control paths** that run on different CPUs. In this case, if the time required to update the data structure is short, a semaphore could be very inefficient. To check a semaphore, the kernel must insert a process in the semaphore list and then suspend it. Because both operations are relatively expensive, in the time it takes to complete them, the other kernel control path could have already released the semaphore.

In these cases, multiprocessor operating systems use **spin locks** . A **spin lock** is very similar to a semaphore, but it has no **process list**; when a process finds the lock closed by another process, it "spins" around repeatedly, executing a tight instruction loop until the lock becomes open.

Of course, **spin locks** are useless in a uniprocessor environment. When a kernel control path tries to access a locked data structure, it starts an endless loop. Therefore, the kernel control path that is updating the protected data structure would not have a chance to continue the execution and release the spin lock. The final result would be that the system hangs.



### 1.6.5.5. Avoiding deadlocks

Processes or kernel control paths that synchronize with other control paths may easily enter a deadlock state. The simplest case of deadlock occurs when process `p1` gains access to data structure `a` and process `p2` gains access to `b`, but `p1` then waits for `b` and `p2` waits for `a`. Other more complex cyclic waits among groups of processes also may occur. Of course, a deadlock condition causes a complete freeze of the affected processes or kernel control paths.

As far as kernel design is concerned, deadlocks become an issue when the number of kernel locks used is high. In this case, it may be quite difficult to ensure that no deadlock state will ever be reached for all possible ways to interleave kernel control paths. Several operating systems, including Linux, avoid this problem by requesting locks in a predefined order.





## 1.6.6. Signals and Interprocess Communication

Unix signals provide a mechanism for notifying processes of system events. Each event has its own signal number, which is usually referred to by a symbolic constant such as  `SIGTERM` . There are two kinds of system events:

*Asynchronous notifications*

For instance, a user can send the interrupt signal  `SIGINT` to a foreground process by pressing the interrupt keycode (usually Ctrl-C) at the terminal.

*Synchronous notifications*

For instance, the kernel sends the signal  `SIGSEGV` to a process when it accesses a memory location at an invalid address.



## 1.6.7. Process Management



### 1.6.7.1. Zombie processes



### 1.6.7.2. Process groups and login sessions



## 1.6.8. Memory Management



### 1.6.8.1. Virtual memory



### 1.6.8.2. Random access memory usage



### 1.6.8.3. Kernel Memory Allocator



### 1.6.8.4. Process virtual address space handling



### 1.6.8.5. Caching



## 1.6.9. Device Drivers

The kernel interacts with I/O devices by means of device drivers . Device drivers are included in the kernel and consist of data structures and functions that control one or more devices, such as hard disks, keyboards, mouses, monitors, network interfaces, and devices connected to an SCSI bus. Each driver interacts with the remaining part of the kernel (even with other drivers) through a specific interface. This approach has the following advantages:

- Device-specific code can be encapsulated in a specific module.
- Vendors can add new devices without knowing the kernel source code; only the interface specifications must be known.
- The kernel deals with all devices in a uniform way and accesses them through the same interface.
- It is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring the system to be rebooted. It is also possible to dynamically unload a module that is no longer needed, therefore minimizing the size of the kernel image stored in RAM.



Figure 1-4 illustrates how **device drivers** interface with the rest of the kernel and with the processes.

Figure 1-4. Device driver interface

![](./Figure-1-4-Device-driver-interface.jpg)

Some user programs (P) wish to operate on hardware devices. They make requests to the kernel using the usual file-related system calls and the device files normally found in the `/dev` directory. Actually, the device files are the user-visible portion of the device driver interface. Each device file refers to a specific device driver, which is invoked by the kernel to perform the requested operation on the hardware component.

At the time Unix was introduced, graphical terminals were uncommon and expensive, so only alphanumeric terminals were handled directly by Unix kernels. When graphical terminals became widespread, ad hoc applications such as the X Window System were introduced that ran as standard processes and accessed the I/O ports of the graphics interface and the RAM video area directly. Some recent Unix kernels, such as Linux 2.6, provide an abstraction for the frame buffer of the graphic card and allow application software to access them without needing to know anything about the I/O ports of the graphics interface (see the section "Levels of Kernel Support" in Chapter 13.)