[TOC]


## 3.2.2. Identifying a Process

As a general rule, each **execution context** that can be independently scheduled must have its own
**process descriptor**; therefore, even **lightweight processes**, which share a large portion of their kernel
data structures, have their own  `task_struct` structures.

***SUMMARY*** : **lightweight processes**是thread；选用注意的是，不要将**process descriptor**和process ID混淆；

The strict one-to-one correspondence between the **process** and **process descriptor** makes the 32-bit
address [ ] of the  `task_struct` structure a useful means for the kernel to identify processes. These
addresses are referred to as *process descriptor pointers*. Most of the references to processes that the
kernel makes are through **process descriptor pointers**.

[ ] As already noted in the section "Segmentation in Linux" in Chapter 2, although technically these 32 bits are only the offset component of a logical address, they coincide with the **linear address**



On the other hand, Unix-like operating systems allow users to identify processes by means of a
number called the *Process ID* (or `PID`), which is stored in the  `pid` field of the **process descriptor**. PIDs
are numbered sequentially: the `PID` of a newly created process is normally the `PID` of the previously
created process increased by one. Of course, there is an upper limit on the `PID` values; when the
kernel reaches such limit, it must start recycling the lower, unused PIDs. By default, the maximum
`PID` number is 32,767 ( `PID_MAX_DEFAULT` - 1 ); the system administrator may reduce this limit by
writing a smaller value into the `/proc/sys/kernel/pid_max` file `(/proc` is the mount point of a special
filesystem, see the section "Special Filesystems" in Chapter 12). In 64-bit architectures, the system
administrator can enlarge the maximum PID number up to 4,194,303.

When recycling PID numbers, the kernel must manage a  `pidmap_array` bitmap that denotes which
are the PIDs currently assigned and which are the free ones. Because a page frame contains 32,768（`4K`）
bits, in 32-bit architectures the  `pidmap_array` bitmap is stored in a single page. In 64-bit
architectures, however, additional pages can be added to the bitmap when the kernel assigns a PID
number too large for the current bitmap size. These pages are never released.

***SUMMARY*** : 关于`pidmap_array`，参见

- https://elixir.bootlin.com/linux/v2.6.17.7/source/kernel/pid.c#L46
- https://blog.csdn.net/Jay14/article/details/54863073



Linux associates a different `PID` with each **process** or **lightweight process** in the system. (As we shall
see later in this chapter, there is a tiny exception on multiprocessor systems.) This approach allows
the maximum flexibility, because every **execution context** in the system can be uniquely identified.

On the other hand, Unix programmers expect threads in the same **group** to have a common `PID`. For
instance, it should be possible to a send a signal specifying a `PID` that affects all threads in the
group. In fact, the `POSIX 1003.1c` standard states that all threads of a multithreaded application
must have the same `PID`.

To comply with this standard, Linux makes use of **thread groups**. The identifier shared by the
threads is the `PID` of the **thread group leader** , that is, the `PID` of the first **lightweight process** in the
group; it is stored in the  `tgid` field of the **process descriptors**. The  `getpid( )` system call returns the
value of  `tgid` relative to the current process instead of the value of  `pid` , so all the threads of a
multithreaded application share the same identifier. Most processes belong to a **thread group**
consisting of a single member; as thread group leaders, they have the  `tgid` field equal to the  `pid`
field, thus the  `getpid( )` system call works as usual for this kind of process.

***SUMMARY*** : 在同一个**thread group**中的每个`task_struct`都有一个唯一的`pid`，它们共用同一个`tgid`。

***SUMMARY*** : 通过`getpid( )`获得到的值是`tgid`字段，而不是`pid`字段；这和我们的直观理解是相悖的；

Later, we'll show you how it is possible to derive a true process descriptor pointer efficiently from its
respective `PID`. Efficiency is important because many system calls such as  `kill( )` use the `PID` to
denote the affected process.



#### 3.2.2.1. Process descriptors handling



Processes are dynamic entities whose lifetimes range from a few milliseconds to months. Thus, the
kernel must be able to handle many processes at the same time, and **process descriptors** are stored
in **dynamic memory** rather than in the memory area permanently assigned to the kernel. For each
process, Linux **packs** two different data structures in a single per-process memory area: a small data
structure linked to the **process descriptor**, namely the  `thread_info` structure, and the **Kernel Mode process stack**. The length of this **memory area** is usually 8,192 bytes (two page frames). For reasons
of efficiency the kernel stores the 8-KB memory area in two consecutive page frames with the first
page frame aligned to a multiple of $2^{13}$ ; this may turn out to be a problem when little dynamic
memory is available, because the free memory may become highly fragmented (see the section "The
Buddy System Algorithm" in Chapter 8). Therefore, in the 80x86 architecture the kernel can be
configured at **compilation time** so that **the memory area** including **stack** and  `thread_info` structure
spans a single page frame (4,096 bytes).

***SUMMARY*** : 1.6.3. Reentrant Kernels中**Kernel Mode process stack**是为kernel control path而准备的，kernel control path的执行是Reentrant的，应该不需要太多的space；

***SUMMARY*** : 上面这段话中介绍的**process descriptors**的存储位置非常重要： **process descriptors** are stored
in **dynamic memory** rather than in the memory area permanently assigned to the kernel;还有就是`thread_union`的存储位置：**per-process memory area**；需要注意per-process的含义；



***SUMMARY*** : https://elixir.bootlin.com/linux/v2.6.11/source/include/linux/sched.h

```c
union thread_union {
	struct thread_info thread_info;
	unsigned long stack[THREAD_SIZE/sizeof(long)];
};
```

***SUMMARY*** : `thread_union`就是上述的memory area；

In the section "Segmentation in Linux" in Chapter 2, we learned that a process in **Kernel Mode**
accesses a **stack** contained in the **kernel data segment**, which is different from the **stack** used by the
process in **User Mode**. Because kernel control paths make little use of the stack, only a few thousand
bytes of **kernel stack** are required. Therefore, 8 KB is ample space for the stack and the  `thread_info`
structure. However, when stack and  `thread_info` structure are contained in a single page frame, the
kernel uses a few additional stacks to avoid the overflows caused by deeply nested interrupts and
exceptions (see Chapter 4).



***SUMMARY*** : 根据上上一段中提及的内容可以推测：`thread_union`是保存在per-process memory area，这也就意味着： **Kernel Mode process stack**也保存在per-process memory area中；而这一段中又提及：a process in **Kernel Mode**  accesses a **stack** contained in the **kernel data segment**；那kernel data segment是存放在何处呢？基于这个问题，我进行了Google：`is kernel data segment in process address space`；目前所有的和这个问题相关的内容都在《`virtual-memory-address-space-thinking.md`》中；在阅读这一段的时候，一个关键点是要知道本书的基于`i386`架构来进行描述的，在`i386`中，使用了segmentation，但是在后来这种方式被取代了；所以很多架构中压根可能就没有 **Kernel Mode stack** contained in the **kernel data segment**的这种结构；如在[How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux?](https://reverseengineering.stackexchange.com/questions/2006/how-are-the-segment-registers-fs-gs-cs-ss-ds-es-used-in-linux)中所述的；





Figure 3-2 shows how the two data structures are stored in the 2-page (8 KB) memory area. The
`thread_info` structure resides at the beginning of the memory area, and the stack grows downward
from the end. The figure also shows that the  `thread_info` structure and the  `task_struct` structure
are mutually linked by means of the fields  `task` and  `thread_info` , respectively.

***SUMMARY*** ： 要理解上面这段话，需要搞清楚`struct thread_info`的定义，一下是[i386的`struct thread_info`](https://elixir.bootlin.com/linux/v2.6.11/source/include/asm-i386/thread_info.h#L28)

```c
struct thread_info {
	struct task_struct	*task;		/* main task structure */
	struct exec_domain	*exec_domain;	/* execution domain */
	unsigned long		flags;		/* low level flags */
	unsigned long		status;		/* thread-synchronous flags */
	__u32			cpu;		/* current CPU */
	__s32			preempt_count; /* 0 => preemptable, <0 => BUG */


	mm_segment_t		addr_limit;	/* thread address space:
					 	   0-0xBFFFFFFF for user-thead
						   0-0xFFFFFFFF for kernel-thread
						*/
	struct restart_block    restart_block;

	unsigned long           previous_esp;   /* ESP of the previous stack in case
						   of nested (IRQ) stacks
						*/
	__u8			supervisor_stack[0];
};
```



可以看到`struct thread_info`有成员变量`struct task_struct	*task`，而在`struct task_struct`中，有成员变量`struct thread_info *thread_info;`，这就是上面这段话的最后一句所描述的：

> The figure also shows that the  `thread_info` structure and the  `task_struct` structure
> are mutually linked by means of the fields  task and  `tHRead_info` , respectively.





The  `esp` register is the CPU stack pointer, which is used to address the stack's top location. On 80x86
systems, the stack starts at the end and grows toward the beginning of the memory area. Right
after switching from **User Mode** to **Kernel Mode**, the kernel stack of a process is always empty, and
therefore the  `esp` register points to the byte immediately following the stack.

The value of the  `esp` is decreased as soon as data is written into the stack. Because the  `thread_info`
structure is 52 bytes long, the **kernel stack** can expand up to 8,140 bytes.

The C language allows the  `thread_info` structure and the **kernel stack** of a process to be
conveniently represented by means of the following union construct:

```c
union thread_union {
struct thread_info thread_info;
unsigned long stack[2048]; /* 1024 for 4KB stacks */
};
```

The  `thread_info` structure shown in Figure 3-2 is stored starting at address  `0x015fa000` , and the
stack is stored starting at address  `0x015fc000` . The value of the  `esp` register points to the current **top**
of the stack at  `0x015fa878` .

The kernel uses the  `alloc_thread_info` and  `free_thread_info` macros to allocate and release the
memory area storing a  `thread_info` structure and a **kernel stack**.



#### 3.2.2.2. Identifying the current process

The close association between the  `thread_info` structure and the **Kernel Mode stack** just described
offers a key benefit in terms of efficiency: the **kernel** can easily obtain the address of the
`thread_info` structure of the process currently running on a CPU from the value of the  `esp` register.
In fact, if the  `thread_union` structure is 8 KB ($2^{13}$ bytes) long, the kernel masks out the 13 least
significant bits of  `esp` to obtain the base address of the  `thread_info` structure; on the other hand, if
the  `thread_union` structure is 4 KB long, the kernel masks out the 12 least significant bits of  `esp` .
This is done by the  `current_thread_info( )` function, which produces assembly language
instructions like the following:

```assembly
movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */
andl %esp,%ecx
movl %ecx,p
```

After executing these three instructions,  p contains the  `thread_info` structure pointer of the process
running on the CPU that executes the instruction.

Most often the kernel needs the address of the **process descriptor** rather than the address of the
`thread_info` structure. To get the **process descriptor pointer** of the process currently running on a
CPU, the kernel makes use of the  `current` macro, which is essentially equivalent to
`current_thread_info( )->task` and produces assembly language instructions like the following:

```c
movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */
andl %esp,%ecx
movl (%ecx),p
```

Because the  `task` field is at offset 0 in the  `thread_info` structure, after executing these three
instructions  `p` contains the **process descriptor pointer** of the process running on the CPU.

The  `current` macro often appears in kernel code as a prefix to fields of the **process descriptor**. For
example,  `current->pid` returns the **process ID** of the process currently running on the CPU.

Another advantage of storing the **process descriptor** with the **stack** emerges on multiprocessor
systems: the correct current process for each hardware processor can be derived just by checking
the stack, as shown previously. Earlier versions of Linux did not store the **kernel stack** and the
**process descriptor** together. Instead, they were forced to introduce a global static variable called
`current` to identify the **process descriptor** of the running process. On multiprocessor systems, it was
necessary to define  current as an array one element for each available CPU.



***SUMMARY*** : 所有的**process descriptor** 和 **kernel stack** 都是位于kernel中；由kernel来执行调度；当CPU需要执行某个**process descriptor**的时候，它需要读取这个**process descriptor**的一些数据，比如之前保存的register数据等以便resume；从上面的描述可以看出，CPU是根据`esp`的值来获得**process descriptor**的地址，并且，从前面的描述来看，每个`thread_union`都有一个自己的 **kernel stack** ，而从上面的描述来看，是根据`esp`的值来获得**process descriptor**的地址，所以CPU是在某个`thread_union的 **kernel stack**中执行，然后得到对应的**process descriptor**；

***SUMMARY*** : 因为scheduler在调度一个task开始运行之前会将这个task的所有的register都恢复到CPU中，所以必然会包含`esp`，所以它就可以根据`esp`快速地定位到process descriptor；

#### 3.2.2.3. Doubly linked lists

Before moving on and describing how the kernel keeps track of the various processes in the system,
we would like to emphasize the role of special data structures that implement doubly linked lists.



For each list, a set of primitive operations must be implemented: initializing the list, inserting and
deleting an element, scanning the list, and so on. It would be both a waste of programmers' efforts
and a waste of memory to replicate the primitive operations for each different list.



Therefore, the **Linux kernel** defines the  `list_head` data structure, whose only fields  `next` and  `prev`
represent the forward and back pointers of a generic doubly linked list element, respectively. It is
important to note, however, that the pointers in a  `list_head` field store the addresses of other
`list_head` fields rather than the addresses of the whole data structures in which the  `list_head`
structure is included; see Figure 3-3 (a).

***SUMMARY*** : [list_head](https://elixir.bootlin.com/linux/v2.6.11/source/include/linux/list.h#L28) 

A new list is created by using the  `LIST_HEAD(list_name)` macro. It declares a new variable named
`list_name` of type  `list_head` , which is a dummy first element that acts as a placeholder for the head
of the new list, and initializes the  `prev` and  `next` fields of the  `list_head` data structure so as to point
to the  `list_name` variable itself; see Figure 3-3 (b).

Several functions and macros implement the primitives, including those shown in Table Table 3-1.





The Linux kernel 2.6 sports another kind of doubly linked list, which mainly differs from a  `list_head`
list because it is not circular; it is mainly used for **hash tables**, where space is important, and finding
the the last element in constant time is not. The list head is stored in an  `hlist_head` data structure,
which is simply a pointer to the first element in the list ( NULL if the list is empty). Each element is represented by an  `hlist_node` data structure, which includes a pointer  `next` to the next element, and
a pointer  `pprev` to the  `next` field of the previous element. Because the list is not circular, the  `pprev`
field of the first element and the  `next` field of the last element are set to  `NULL` . The list can be
handled by means of several helper functions and macros similar to those listed in Table 3-1:
`hlist_add_head( )` ,  `hlist_del( )` ,  `hlist_empty( )` ,  `hlist_entry` ,  `hlist_for_each_entry` , and so on.

#### 3.2.2.4. The process list

The first example of a doubly linked list we will examine is the *process list*, a list that links together
all existing process descriptors. Each  `task_struct` structure includes a  `tasks` field of type  `list_head`
whose  `prev` and  `next` fields point, respectively, to the previous and to the next  `task_struct` element.



The **head** of the **process list** is the  `init_task task_struct` descriptor; it is the **process descriptor** of
the so-called *process 0* or *swapper* (see the section "Kernel Threads" later in this chapter). The
`tasks->prev` field of  `init_task` points to the  `tasks` field of the process descriptor inserted last in the
list.

The  `SET_LINKS` and  `REMOVE_LINKS` macros are used to insert and to remove a process descriptor in the
**process list**, respectively. These macros also take care of the parenthood relationship of the process
(see the section "How Processes Are Organized" later in this chapter).



Another useful macro, called  `for_each_process` , scans the whole process list. It is defined as:

```assembly
#define for_each_process(p) \
for (p=&init_task; (p=list_entry((p)->tasks.next, \
struct task_struct, tasks) \
) != &init_task; )
```

The macro is the loop control statement after which the kernel programmer supplies the loop. Notice
how the  `init_task` process descriptor just plays the role of **list header**. The macro starts by moving
past  `init_task` to the next task and continues until it reaches  `init_task` again (thanks to the
**circularity** of the list). At each iteration, the variable passed as the argument of the macro contains
the address of the currently scanned process descriptor, as returned by the  `list_entry` macro.

***SUMMARY*** : 在multiprocessor中，是否是每个processor都有一个process list，还是说所有的process descriptor都放在一个process list中？



#### 3.2.2.5. The lists of `TASK_RUNNING` processes

When looking for a new process to run on a CPU, the kernel has to consider only the runnable processes (that is, the processes in the  `TASK_RUNNING` state).

Earlier Linux versions put all runnable processes in the same list called *`runqueue`*. Because it would
be too costly to maintain the list ordered according to process priorities, the earlier schedulers were
compelled to scan the whole list in order to select the "best" runnable process.

Linux 2.6 implements the `runqueue` differently. The aim is to allow the scheduler to select the best
runnable process in constant time, independently of the number of runnable processes. We'll defer
to Chapter 7 a detailed description of this new kind of `runqueue`, and we'll provide here only some
basic information.

The trick used to achieve the scheduler speedup consists of splitting the `runqueue` in many lists of
runnable processes, one list per **process priority**. Each  `task_struct` descriptor includes a  `run_list`
field of type  `list_head` . If the **process priority** is equal to `k` (a value ranging between 0 and 139), the
`run_list` field links the **process descriptor** into the list of runnable processes having priority `k`.
Furthermore, on a multiprocessor system, each CPU has its own `runqueue`, that is, its own set of
lists of processes. This is a classic example of making a data structures more complex to improve
performance: to make scheduler operations more efficient, the `runqueue` list has been split into 140 different lists!

As we'll see, the kernel must preserve a lot of data for every `runqueue` in the system; however, the
main data structures of a `runqueue` are the lists of process descriptors belonging to the `runqueue`;
all these lists are implemented by a single  `prio_array_t` data structure, whose fields are shown in
Table 3-2.

***SUMMARY*** : [prio_array](https://elixir.bootlin.com/linux/v2.6.11/source/kernel/sched.c#L185)

The  `enqueue_task(p,array)` function inserts a process descriptor into a `runqueue` list; its code is
essentially equivalent to:

```c
list_add_tail(&p->run_list, &array->queue[p->prio]);
__set_bit(p->prio, array->bitmap);
array->nr_active++;
p->array = array;
```

The  `prio` field of the process descriptor stores the dynamic priority of the process, while the  `array`
field is a pointer to the  `prio_array_t` data structure of its current `runqueue`. Similarly, the
`dequeue_task(p,array)` function removes a process descriptor from a `runqueue` list.

