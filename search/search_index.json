{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u9879\u76ee # \u4ece\u591a\u4e2a\u65b9\u9762\u5bf9Linux operating system\u8fdb\u884c\u5206\u6790\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4e0b\u51e0\u4e2a\u65b9\u9762: Architecture # \u63cf\u8ff0Computing system\u7684architecture\u3002 Kernel # \u5305\u542b kernel \u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u4e3b\u8981\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 linux kernel \u7684\u5b9e\u73b0\uff0c\u4e3b\u8981\u53c2\u8003\u4e66\u7c4d\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b Programming # \u5305\u542bLinux operating system\u4e2d\u8fdb\u884cprogramming\u65f6\u9700\u8981\u638c\u63e1\u7684\u6240\u6709\u77e5\u8bc6\uff0c\u5305\u62ec\uff1a system call interface philosophy \u4e3b\u8981\u53c2\u8003\uff1a Advanced Programming in the UNIX\u00ae Environment, Third Edition The Linux Programming Interface man7.org Linux man pages die.net Linux man pages Shell # Linux operating system\u4e2d\u7684\u5e38\u89c1\u547d\u4ee4 \u4e3b\u8981\u53c2\u8003\uff1a Linux Documentation man7.org Linux man pages die.net Linux man pages","title":"Home"},{"location":"#_1","text":"\u4ece\u591a\u4e2a\u65b9\u9762\u5bf9Linux operating system\u8fdb\u884c\u5206\u6790\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4e0b\u51e0\u4e2a\u65b9\u9762:","title":"\u5173\u4e8e\u672c\u9879\u76ee"},{"location":"#architecture","text":"\u63cf\u8ff0Computing system\u7684architecture\u3002","title":"Architecture"},{"location":"#kernel","text":"\u5305\u542b kernel \u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u4e3b\u8981\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 linux kernel \u7684\u5b9e\u73b0\uff0c\u4e3b\u8981\u53c2\u8003\u4e66\u7c4d\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b","title":"Kernel"},{"location":"#programming","text":"\u5305\u542bLinux operating system\u4e2d\u8fdb\u884cprogramming\u65f6\u9700\u8981\u638c\u63e1\u7684\u6240\u6709\u77e5\u8bc6\uff0c\u5305\u62ec\uff1a system call interface philosophy \u4e3b\u8981\u53c2\u8003\uff1a Advanced Programming in the UNIX\u00ae Environment, Third Edition The Linux Programming Interface man7.org Linux man pages die.net Linux man pages","title":"Programming"},{"location":"#shell","text":"Linux operating system\u4e2d\u7684\u5e38\u89c1\u547d\u4ee4 \u4e3b\u8981\u53c2\u8003\uff1a Linux Documentation man7.org Linux man pages die.net Linux man pages","title":"Shell"},{"location":"Operating-system/","text":"Operating system\u6982\u8ff0 # Operating system\uff08\u7b80\u79f0OS\uff09\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u6d89\u53ca\u7684\u5185\u5bb9\u975e\u5e38\u591a\u3002\u4f5c\u4e3a\u4e00\u540dsoftware engineer\uff0c\u638c\u63e1operating system\u7684\u57fa\u7840\u77e5\u8bc6\u662f\u5fc5\u987b\u7684\u3002\u672c\u8282\u662f\u5bf9OS\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Operating system \u3002 An operating system ( OS ) is system software that manages computer hardware and software resources and provides common services for computer programs . Types of operating systems # NOTE: \u4e0b\u9762\u7f57\u5217\u4e86\u591a\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u6bcf\u79cd\u5206\u7c7b\u65b9\u6cd5\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cd\u7279\u6027\u3002\u663e\u7136\u6709\u5f88\u591a\u7684operating system\u53ef\u4ee5\u517c\u5177\u591a\u79cd\u7279\u6027\uff0c\u6bd4\u5982linux\uff0c\u5b83\u5177\u5907multi-tasking\u3001 multi-user\u7b49\u7279\u6027 Single-tasking and multi-tasking # single-tasking operating system multi-tasking operating system Single- and multi-user # single-user operating system multi-user operating system Distributed # distributed operating system Templated # templating Embedded # embedded operating systems Real-time # real-time operating system Examples # Unix and Unix-like operating systems # Main article: Unix Unix Unix-like System V BSD Linux . Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants. BSD and its descendants # Main article: Berkeley Software Distribution FreeBSD NetBSD OpenBSD macOS # Main article: macOS Linux # Main articles: Linux and Linux kernel The Linux kernel is used in some popular distributions, such as Red Hat , Debian , Ubuntu , Linux Mint and Google 's Android , Chrome OS , and Chromium OS . Microsoft Windows # Main article: Microsoft Windows Components # The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component. Kernel # A kernel connects the application software to the hardware of a computer. Main article: Kernel (computing) Program execution # Main article: Process (computing) Interrupts # Main article: Interrupt Modes # Main articles: User mode and Supervisor mode Memory management # Main article: Memory management Virtual memory # Main article: Virtual memory Further information: Page fault Multitasking # Main articles: Computer multitasking and Process management (computing) Further information: Context switch , Preemptive multitasking , and Cooperative multitasking Disk access and file systems # Main article: Virtual file system Device drivers # Main article: Device driver Networking # Main article: Computer network Security # Main article: Computer security User interface # Main article: Operating system user interface Diversity of operating systems and portability # Market share # Further information: Usage share of operating systems","title":"Operating-system-\u6982\u8ff0"},{"location":"Operating-system/#operating-system","text":"Operating system\uff08\u7b80\u79f0OS\uff09\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u6d89\u53ca\u7684\u5185\u5bb9\u975e\u5e38\u591a\u3002\u4f5c\u4e3a\u4e00\u540dsoftware engineer\uff0c\u638c\u63e1operating system\u7684\u57fa\u7840\u77e5\u8bc6\u662f\u5fc5\u987b\u7684\u3002\u672c\u8282\u662f\u5bf9OS\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Operating system \u3002 An operating system ( OS ) is system software that manages computer hardware and software resources and provides common services for computer programs .","title":"Operating system\u6982\u8ff0"},{"location":"Operating-system/#types-of-operating-systems","text":"NOTE: \u4e0b\u9762\u7f57\u5217\u4e86\u591a\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u6bcf\u79cd\u5206\u7c7b\u65b9\u6cd5\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cd\u7279\u6027\u3002\u663e\u7136\u6709\u5f88\u591a\u7684operating system\u53ef\u4ee5\u517c\u5177\u591a\u79cd\u7279\u6027\uff0c\u6bd4\u5982linux\uff0c\u5b83\u5177\u5907multi-tasking\u3001 multi-user\u7b49\u7279\u6027","title":"Types of operating systems"},{"location":"Operating-system/#single-tasking-and-multi-tasking","text":"single-tasking operating system multi-tasking operating system","title":"Single-tasking and multi-tasking"},{"location":"Operating-system/#single-and-multi-user","text":"single-user operating system multi-user operating system","title":"Single- and multi-user"},{"location":"Operating-system/#distributed","text":"distributed operating system","title":"Distributed"},{"location":"Operating-system/#templated","text":"templating","title":"Templated"},{"location":"Operating-system/#embedded","text":"embedded operating systems","title":"Embedded"},{"location":"Operating-system/#real-time","text":"real-time operating system","title":"Real-time"},{"location":"Operating-system/#examples","text":"","title":"Examples"},{"location":"Operating-system/#unix-and-unix-like-operating-systems","text":"Main article: Unix Unix Unix-like System V BSD Linux . Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.","title":"Unix and Unix-like operating systems"},{"location":"Operating-system/#bsd-and-its-descendants","text":"Main article: Berkeley Software Distribution FreeBSD NetBSD OpenBSD","title":"BSD and its descendants"},{"location":"Operating-system/#macos","text":"Main article: macOS","title":"macOS"},{"location":"Operating-system/#linux","text":"Main articles: Linux and Linux kernel The Linux kernel is used in some popular distributions, such as Red Hat , Debian , Ubuntu , Linux Mint and Google 's Android , Chrome OS , and Chromium OS .","title":"Linux"},{"location":"Operating-system/#microsoft-windows","text":"Main article: Microsoft Windows","title":"Microsoft Windows"},{"location":"Operating-system/#components","text":"The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.","title":"Components"},{"location":"Operating-system/#kernel","text":"A kernel connects the application software to the hardware of a computer. Main article: Kernel (computing)","title":"Kernel"},{"location":"Operating-system/#program-execution","text":"Main article: Process (computing)","title":"Program execution"},{"location":"Operating-system/#interrupts","text":"Main article: Interrupt","title":"Interrupts"},{"location":"Operating-system/#modes","text":"Main articles: User mode and Supervisor mode","title":"Modes"},{"location":"Operating-system/#memory-management","text":"Main article: Memory management","title":"Memory management"},{"location":"Operating-system/#virtual-memory","text":"Main article: Virtual memory Further information: Page fault","title":"Virtual memory"},{"location":"Operating-system/#multitasking","text":"Main articles: Computer multitasking and Process management (computing) Further information: Context switch , Preemptive multitasking , and Cooperative multitasking","title":"Multitasking"},{"location":"Operating-system/#disk-access-and-file-systems","text":"Main article: Virtual file system","title":"Disk access and file systems"},{"location":"Operating-system/#device-drivers","text":"Main article: Device driver","title":"Device drivers"},{"location":"Operating-system/#networking","text":"Main article: Computer network","title":"Networking"},{"location":"Operating-system/#security","text":"Main article: Computer security","title":"Security"},{"location":"Operating-system/#user-interface","text":"Main article: Operating system user interface","title":"User interface"},{"location":"Operating-system/#diversity-of-operating-systems-and-portability","text":"","title":"Diversity of operating systems and portability"},{"location":"Operating-system/#market-share","text":"Further information: Usage share of operating systems","title":"Market share"},{"location":"TODO/","text":"20190817 # \u4eca\u5929\u5728\u9605\u8bfb\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b\u76846.2.1.2. The jiffies variable\u7ae0\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u5bf9\u539f\u5b50\u6027\u7684\u601d\u8003\uff1a You might wonder why jiffies has not been directly declared as a 64-bit unsigned long long integer on the 80 x 86 architecture. The answer is that accesses to 64-bit variables in 32-bit architectures cannot be done atomically . Therefore, every read operation on the whole 64 bits requires some synchronization technique to ensure that the counter is not updated while the two 32-bit half-counters are read; as a consequence, every 64-bit read operation is significantly slower than a 32-bit read operation. \u572832\u4f4d\u7684\u673a\u5668\u4e2d\uff0c\u4e00\u6b21\u80fd\u591f\u8bfb\u53d6\u7684\u6570\u636e\u7684\u957f\u5ea6\u4e3a32\u4f4d\uff0c\u6240\u4ee5\u8bfb\u53d6\u8d85\u8fc732\u7684\u6570\u636e\u5c31\u9700\u8981\u591a\u6761\u6307\u4ee4\uff0c\u663e\u7136\uff0c\u8fd9\u5c31\u4e0d\u662f\u539f\u5b50\u6027\u7684\u4e86\uff1b \u8054\u60f3\u5230\u4eca\u5929\u5728\u9605\u8bfb\u300a\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\u300b\u76841.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807\uff0c\u6211\u6240\u505a\u7684\u7b14\u8bb0\u5982\u4e0b\uff1a \u6307\u5904\u7406\u673a \u8fd0\u7b97\u5668 \u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684 \u4f4d\u6570 \uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b SUMMARY : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right SUMMARY : \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f \u73b0\u5728\u8054\u7cfb\u5230\u539f\u5b50\u6027\uff0c\u663e\u7136\uff0c\u4e0a\u8ff0\u8fd9\u6bb5\u8bdd\u4e2d\u7684 \u4e00\u6b21 \u7684\u542b\u4e49\u662f\u975e\u5e38\u6df1\u523b\u7684\uff1a\u5b83\u8574\u542b\u7740\u539f\u5b50\u6027\u7684\u4fdd\u8bc1\uff1b \u5176\u5b9e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u5728\u4e4b\u524d\u5c31\u5df2\u7ecf\u9047\u5230\u8fc7\u7684\uff0c\u8bb0\u5f97\u5f53\u65f6\u9605\u8bfb\u7684\u6587\u7ae0\u662f\uff1a Atomic vs. Non-Atomic Operations \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6211\u5df2\u7ecf\u6536\u5f55\u4e86\uff1b \u7efc\u4e0a\u6240\u8ff0\uff0c\u5176\u5b9e\u6211\u7684\u95ee\u9898\u53ef\u4ee5\u5f52\u7eb3\u4e3a\uff1awhy read 64 bit data in 32 bit is not atomic Google\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a How to Customize Serialization in Java Using the Externalizable Interface Are 64 bit operations atomic for a 32 bit app on 64 bit Windows 20190829 # epoll_wait \u3001 epoll_pwait \u548c\u4fe1\u53f7\u4e4b\u95f4\u7684\u5173\u7cfb # http://man7.org/linux/man-pages/man2/epoll_wait.2.html epoll and nonblocking # \u4f7f\u7528epoll\u7684\u65f6\u5019\uff0c\u662f\u5426\u4e00\u5b9a\u8981\u4f7f\u7528nonblocking IO\uff1f 20190905 # passing file descriptor between process # https://docs.python.org/3.5/library/multiprocessing.html \u5728python\u7684multiprocessing\u6587\u6863\u4e2d\u770b\u5230\u4e86\u5b83\u63d0\u51fa\u7684\u8fd9\u4e2a\u95ee\u9898\uff1a forkserver When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited. Available on Unix platforms which support passing file descriptors over Unix pipes. http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch17lev1sec4.html https://openforums.wordpress.com/2016/08/07/open-file-descriptor-passing-over-unix-domain-sockets/ semaphore tracker process # https://docs.python.org/3.5/library/multiprocessing.html Synchronization between processes # https://stackoverflow.com/questions/248911/how-do-i-synchronize-two-processes https://en.wikipedia.org/wiki/Semaphore_%28programming%29 http://sce2.umkc.edu/csee/cotterr/cs431_sp13/CS431_Linux_Process_Sync_12_bw.ppt 20190906 # https://stackoverflow.com/questions/11129212/tcp-can-two-different-sockets-share-a-port https://lwn.net/Articles/542629/ https://stackoverflow.com/questions/1694144/can-two-applications-listen-to-the-same-port/25033226 20190909 # TCP backlog # https://stackoverflow.com/questions/36594400/what-is-backlog-in-tcp-connections http://www.linuxjournal.com/files/linuxjournal.com/linuxjournal/articles/023/2333/2333s2.html https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html 20190913 # atomic & race condition & lock & consistent model \u4e13\u9898 # \u4eca\u5929\u5728\u9605\u8bfbredis source code\u7684\u65f6\u5019\uff0c\u603b\u7ed3\u4e86redis\u4e2dsocket file descriptor\u90fd\u662fnon blocking\u7684\uff0c\u7136\u540e\u6211\u5c31\u67e5\u9605APUE\u4e2d\u5173\u4e8e\u6587\u4ef6\u63cf\u8ff0\u7b26\u6807\u5fd7\uff0c\u6587\u4ef6\u6807\u5fd7\u7684\u5185\u5bb9\uff1b\u53c8\u91cd\u65b0\u770b\u4e86\u4e00\u904dAPUE 3.3\u8282\u4e2d\u5173\u4e8eTOCTTOU\u7684\u63cf\u8ff0\uff0c\u4ee5\u53caAPUE 3.11 \u539f\u5b50\u64cd\u4f5c\u7684\u63cf\u8ff0\uff0c\u6211\u624d\u610f\u8bc6\u5230\u539f\u7406linux\u7684system call\u4e5f\u662f\u80fd\u591f\u4fdd\u8bc1atomic\u7684\uff08\u4ece\u5e95\u5c42\u5b9e\u73b0\u6765\u770b\uff0c\u662f\u56e0\u4e3abig kernel lock \uff09\uff1b\u539f\u6765\u6211\u4ec5\u4ec5\u8ba4\u77e5\u5230\u4e00\u4e2ainstruction\u662fatomic\uff0c\u5176\u5b9e\u8fd9\u79cd\u8ba4\u77e5\u662f\u6bd4\u8f83\u5c40\u9650\u7684\uff1b\u8054\u60f3\u5230\u6211\u4e4b\u524d\u603b\u7ed3\u8fc7\u5173\u4e8eatomic\u7684\u5185\u5bb9\uff0c\u5982\u4e0b\uff1a everything\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u627e\u6240\u6709atomic\u76f8\u5173\u7684\u5185\u5bb9\uff1a nowholeword:c:\\users\\dengkai17334\\appdata\\local\\ynote\\data\\ content:atomic \u73b0\u5728\u662f\u6709\u5fc5\u8981\u6574\u7406\u4e00\u756a\u4e86\u3002 linux system call atomic # network # data integrity in network # \u5728 2.1. Two Types of Internet Sockets \u4e2d\u63d0\u53caTCP\u80fd\u591f\u4fdd\u8bc1 data integrity \uff0c\u53ef\u89c1 data integrity \u5e76\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u4e00\u65b9\u9762\uff1b What's the difference between 127.0.0.1 and 0.0.0.0? # What's the difference between 127.0.0.1 and 0.0.0.0? interface address # \u5728\u5f88\u591a\u5730\u65b9\u90fd\u89c1\u5230\u4e86\u8fd9\u4e2a\u8bcd\uff1a - getaddrinfo(3) - Linux man page # \u5b83\u5230\u5e95\u662f\u4ec0\u4e48\u542b\u4e49 stream of bytes and bitstream # Bitstream \u5df2\u7ecf\u9605\u8bfb Maximum segment lifetime # https://en.wikipedia.org/wiki/Maximum_segment_lifetime 20190909 # epoll\u4e2d\u7684file descripor\u662f\u5426\u4e00\u5b9a\u8981\u8bbe\u7f6e\u4e3anon blocking SO_RCVLOWAT and SO_SNDLOWAT \u7684\u503c\u90fd\u662f1\uff0c\u90a3\u4e48\u5982\u4f55\u786e\u5b9amessage boundary\uff1f shell # getconf echo \"2^12\" | bc How to check if port is in use on Linux or Unix https://unix.stackexchange.com/a/185767 how to get all threads of a process process # https://www.cyberciti.biz/faq/show-all-running-processes-in-linux/ https://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes How to print out a variable in makefile # https://stackoverflow.com/questions/16467718/how-to-print-out-a-variable-in-makefile Process control block # https://www.tldp.org/LDP/lki/lki-2.html \u4e0a\u9762\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u8bb2\u8ff0\u7684\u662flinux kernel\u7684\u5b9e\u73b0 How to Find Out Which Windows Process is Using a File # user the software:Process Explorer How find out which process is using a file in Linux? # fuser file_name what will happen if a process exceed its resource limits # \u8bb0\u5f97\u5728redis in action\u8fd9\u672c\u4e66\u4e2d\u6709\u63d0\u53ca\u8fc7\u7c7b\u4f3c\u7684\u95ee\u9898\uff1b C POSIX library # https://en.wikipedia.org/wiki/C_POSIX_library http://pubs.opengroup.org/onlinepubs/9699919799/idx/head.html C standard library # https://en.wikipedia.org/wiki/C_standard_library process id # APUE 4.4 Set-User-ID and Set-Group-ID https://en.wikipedia.org/wiki/User_identifier Time of check to time of use # https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use User identifier # https://en.wikipedia.org/wiki/User_identifier Secure Programming HOWTO # https://dwheeler.com/secure-programs/Secure-Programs-HOWTO/index.html thread-local # thread-local\u548creentry\u4e4b\u95f4\u7684\u5173\u7cfb hole in file # hole\u5bf9\u6587\u4ef6\u5927\u5c0f\u7684\u5f71\u54cd\uff1b\u4e0a\u5468\u5728\u67e5\u770bhttps://liftoff.github.io/pyminifier/\u6587\u6863\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709minifying\u529f\u80fd\uff0c\u662f\u548chole in file\u6709\u5173\u7684\uff1b \u540c\u65f6binary mode\u6765\u4fdd\u5b58\u6587\u4ef6\u4e5f\u80fd\u591f\u964d\u4f4e\u6587\u4ef6\u5927\u5c0f\uff1b linux memory # memory usage memory available virtual process space # https://www.tutorialspoint.com/where-are-static-variables-stored-in-c-cplusplus https://cs61.seas.harvard.edu/wiki/2016/Kernel2X \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d https://www.cs.utexas.edu/~lorenzo/corsi/cs372/06F/hw/3sol.html https://cs.stackexchange.com/questions/56825/how-to-calculate-virtual-address-space-from-page-size-virtual-address-size-and process environment # 7.12\u4e0d\u540c\u8bed\u8a00\u7684environment\u662f\u5426\u4e0d\u540c file IO\uff1apython VS linux # Python\u4e0eLinux\u7c7b\u4f3c\uff0c\u4e0e\u6587\u4ef6\u76f8\u5173\u7684\u64cd\u4f5c\u90fd\u662f\u4eceopen\u51fd\u6570\u5f00\u59cb\u7684 Fragmentation (computing) # https://en.wikipedia.org/wiki/Fragmentation_(computing) page table size # https://www.cs.cornell.edu/courses/cs4410/2015su/lectures/lec14-pagetables.html http://www.cs.cornell.edu/courses/cs4410/2016su/slides/lecture11.pdf http://www.cs.cornell.edu/courses/cs4410/2016su/schedule.html Data segment # https://en.wikipedia.org/wiki/Data_segment ENOENT # Why does ENOENT mean \u201cNo such file or directory\u201d? EACCES # cron # https://en.wikipedia.org/wiki/Cron https://www.adminschoice.com/crontab-quick-reference \u5728shell\u4e2d\u5220\u9664\u6389\u8fdb\u7a0b\u4f7f\u7528\u7684\u6587\u4ef6 # \u4eca\u5929\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u53d1\u73b0\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u95ee\u9898\uff1a\u8fdb\u7a0b\u8fd0\u884c\u4e2d\uff0c\u8fd9\u4e2a\u8fdb\u7a0b\u4f1a\u4e0d\u65ad\u5730\u5411\u5176\u65e5\u5fd7\u6587\u4ef6\u4e2d\u5199\u5165\u65e5\u5fd7\uff1b\u7136\u540e\u6211\u5728shell\u4e2d\u5c06\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\u7ed9\u5220\u9664\u4e86\uff0c\u53d1\u73b0\u8fdb\u7a0b\u5e76\u6ca1\u6709\u53d1\u73b0\u5b83\u7684\u65e5\u5fd7\u6587\u4ef6\u88ab\u5220\u4e86\uff0c\u4e5f\u6ca1\u6709\u51fa\u73b0\u521b\u5efa\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\uff0c\u7a0b\u5e8f\u4e5f\u6ca1\u6709\u505c\u6b62\u4e0b\u6765\uff1b \u78c1\u76d8\u7a7a\u95f4\u6ee1\u540e\uff0c\u4e5f\u4f1a\u5bfc\u81f4process\u65e0\u6cd5\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff1b\u4f46\u662f\u5f53\u91ca\u653e\u4e00\u90e8\u5206\u7a7a\u95f4\u540e\uff0c\u53d1\u73b0process\u4ecd\u7136\u4e0d\u4f1a\u5199\u5165\uff0c\u5c31\u50cf\u662f\u653e\u5f03\u4e86\u4e00\u6837\uff1b Segmentation fault # hiredis\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u4eca\u5929\u5728\u591a\u7ebf\u7a0b\u73af\u5883\u4e0b\u6d4b\u8bd5\u51fa\u5b83\u4f1a\u5bfc\u81f4process core dump\uff0cdump\u7684\u539f\u56e0\u662f Program terminated with signal 11, Segmentation fault. https://kb.iu.edu/d/aqsj https://en.wikipedia.org/wiki/Segmentation_fault thread unsafe and core dump # how to test # \u5982\u679c\u662f\u652f\u6301\u7f51\u7edc\uff0c\u9700\u8981\u6d4b\u8bd5\u591a\u4e2aclient\u8fde\u63a5\uff1b \u9700\u8981\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5 \u9700\u8981\u8fdb\u884c\u5e76\u53d1\u6d4b\u8bd5 process and its thread # Is there a way to see details of all the threads that a process has in Linux? google:how to get all threads of a process https://www.unix.com/aix/154772-how-list-all-threads-running-process.html http://ask.xmodulo.com/view-threads-process-linux.html how OS know process use a illegal memory location # https://en.wikipedia.org/wiki/Memory_protection https://stackoverflow.com/questions/41172563/how-os-catches-illegal-memory-references-at-paging-scheme https://unix.stackexchange.com/questions/511963/how-linux-finds-out-about-illegal-memory-access-error https://www.kernel.org/ https://www.kernel.org/doc/gorman/html/understand/index.html https://www.kernel.org/doc/gorman/ https://www.kernel.org/doc/ dev file # https://unix.stackexchange.com/questions/93531/what-is-stored-in-dev-pts-files-and-can-we-open-them Unix memory usage # https://utcc.utoronto.ca/~cks/space/blog/linux/LinuxMemoryStats https://utcc.utoronto.ca/~cks/space/blog/unix/UnderstandingRSS https://stackoverflow.com/questions/131303/how-to-measure-actual-memory-usage-of-an-application-or-process https://unix.stackexchange.com/questions/554/how-to-monitor-cpu-memory-usage-of-a-single-process How can I kill a process by name instead of PID? # https://stackoverflow.com/questions/160924/how-can-i-kill-a-process-by-name-instead-of-pid http://osxdaily.com/2017/01/12/kill-process-by-name-command-line/ APUE\u7684\u300aUnix-interruption-and-atom\u300b\u8fd8\u6ca1\u6709wanch # \u5176\u4e2d\u4e3b\u8981\u8ba8\u8bba\u4e86Atomicity Consistency models # \u5728parallel computing\u4e2d\u7684Consistency models\u8fd8\u6ca1\u6709\u5b8c\u6210 https://en.wikipedia.org/wiki/Category:Consistency_models epoll # https://en.wikipedia.org/wiki/Epoll youdao Unix-abort # init of linux # init\u8fdb\u7a0b # ch8.2\u4e2d\u6709\u5bf9init\u8fdb\u7a0b\u7684\u4e00\u4e2a\u4ecb\u7ecd ch9.2\u4ecb\u7ecd\u5230\uff0cinit\u4f1a\u8bfb\u53d6\u6587\u4ef6 /etc/ttys how to know what init system linux use https://unix.stackexchange.com/questions/18209/detect-init-system-using-the-shell https://fedoramagazine.org/what-is-an-init-system/ https://en.wikipedia.org/wiki/Init https://en.wikipedia.org/wiki/Systemd Convert between Unix and Windows text files # https://kb.iu.edu/d/acux https://stackoverflow.com/questions/16239551/eol-conversion-in-notepad Memory management algorithms # https://en.wikipedia.org/wiki/Category:Memory_management_algorithms Slab allocation interrupt in Unix # interrupt vector table http://www.cis.upenn.edu/~lee/03cse380/lectures/ln2-process-v4.pdf \u5728 Context switch \u4e2d\u4e5f\u5bf9interrupt\u8fdb\u884c\u4e86\u4ecb\u7ecd signal # signal\u4e5f\u662f\u4e00\u79cdinterrupt\uff0c\u6545\u5c06\u5b83\u653e\u5728interrupt\u4e4b\u4e0b Google unix signals and threads # https://stackoverflow.com/questions/2575106/posix-threads-and-signals https://en.wikipedia.org/wiki/Signal_(IPC) Scheduling (computing) # https://en.wikipedia.org/wiki/Scheduling_(computing) thread join and detach # wait # https://linux.die.net/man/2/waitpid","title":"20190817"},{"location":"TODO/#20190817","text":"\u4eca\u5929\u5728\u9605\u8bfb\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b\u76846.2.1.2. The jiffies variable\u7ae0\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u5bf9\u539f\u5b50\u6027\u7684\u601d\u8003\uff1a You might wonder why jiffies has not been directly declared as a 64-bit unsigned long long integer on the 80 x 86 architecture. The answer is that accesses to 64-bit variables in 32-bit architectures cannot be done atomically . Therefore, every read operation on the whole 64 bits requires some synchronization technique to ensure that the counter is not updated while the two 32-bit half-counters are read; as a consequence, every 64-bit read operation is significantly slower than a 32-bit read operation. \u572832\u4f4d\u7684\u673a\u5668\u4e2d\uff0c\u4e00\u6b21\u80fd\u591f\u8bfb\u53d6\u7684\u6570\u636e\u7684\u957f\u5ea6\u4e3a32\u4f4d\uff0c\u6240\u4ee5\u8bfb\u53d6\u8d85\u8fc732\u7684\u6570\u636e\u5c31\u9700\u8981\u591a\u6761\u6307\u4ee4\uff0c\u663e\u7136\uff0c\u8fd9\u5c31\u4e0d\u662f\u539f\u5b50\u6027\u7684\u4e86\uff1b \u8054\u60f3\u5230\u4eca\u5929\u5728\u9605\u8bfb\u300a\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\u300b\u76841.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807\uff0c\u6211\u6240\u505a\u7684\u7b14\u8bb0\u5982\u4e0b\uff1a \u6307\u5904\u7406\u673a \u8fd0\u7b97\u5668 \u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684 \u4f4d\u6570 \uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b SUMMARY : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right SUMMARY : \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f \u73b0\u5728\u8054\u7cfb\u5230\u539f\u5b50\u6027\uff0c\u663e\u7136\uff0c\u4e0a\u8ff0\u8fd9\u6bb5\u8bdd\u4e2d\u7684 \u4e00\u6b21 \u7684\u542b\u4e49\u662f\u975e\u5e38\u6df1\u523b\u7684\uff1a\u5b83\u8574\u542b\u7740\u539f\u5b50\u6027\u7684\u4fdd\u8bc1\uff1b \u5176\u5b9e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u5728\u4e4b\u524d\u5c31\u5df2\u7ecf\u9047\u5230\u8fc7\u7684\uff0c\u8bb0\u5f97\u5f53\u65f6\u9605\u8bfb\u7684\u6587\u7ae0\u662f\uff1a Atomic vs. Non-Atomic Operations \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6211\u5df2\u7ecf\u6536\u5f55\u4e86\uff1b \u7efc\u4e0a\u6240\u8ff0\uff0c\u5176\u5b9e\u6211\u7684\u95ee\u9898\u53ef\u4ee5\u5f52\u7eb3\u4e3a\uff1awhy read 64 bit data in 32 bit is not atomic Google\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a How to Customize Serialization in Java Using the Externalizable Interface Are 64 bit operations atomic for a 32 bit app on 64 bit Windows","title":"20190817"},{"location":"TODO/#20190829","text":"","title":"20190829"},{"location":"TODO/#epoll_wait-epoll_pwait","text":"http://man7.org/linux/man-pages/man2/epoll_wait.2.html","title":"epoll_wait \u3001 epoll_pwait\u548c\u4fe1\u53f7\u4e4b\u95f4\u7684\u5173\u7cfb"},{"location":"TODO/#epoll-and-nonblocking","text":"\u4f7f\u7528epoll\u7684\u65f6\u5019\uff0c\u662f\u5426\u4e00\u5b9a\u8981\u4f7f\u7528nonblocking IO\uff1f","title":"epoll and nonblocking"},{"location":"TODO/#20190905","text":"","title":"20190905"},{"location":"TODO/#passing-file-descriptor-between-process","text":"https://docs.python.org/3.5/library/multiprocessing.html \u5728python\u7684multiprocessing\u6587\u6863\u4e2d\u770b\u5230\u4e86\u5b83\u63d0\u51fa\u7684\u8fd9\u4e2a\u95ee\u9898\uff1a forkserver When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited. Available on Unix platforms which support passing file descriptors over Unix pipes. http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch17lev1sec4.html https://openforums.wordpress.com/2016/08/07/open-file-descriptor-passing-over-unix-domain-sockets/","title":"passing file descriptor between process"},{"location":"TODO/#semaphore-tracker-process","text":"https://docs.python.org/3.5/library/multiprocessing.html","title":"semaphore tracker process"},{"location":"TODO/#synchronization-between-processes","text":"https://stackoverflow.com/questions/248911/how-do-i-synchronize-two-processes https://en.wikipedia.org/wiki/Semaphore_%28programming%29 http://sce2.umkc.edu/csee/cotterr/cs431_sp13/CS431_Linux_Process_Sync_12_bw.ppt","title":"Synchronization between processes"},{"location":"TODO/#20190906","text":"https://stackoverflow.com/questions/11129212/tcp-can-two-different-sockets-share-a-port https://lwn.net/Articles/542629/ https://stackoverflow.com/questions/1694144/can-two-applications-listen-to-the-same-port/25033226","title":"20190906"},{"location":"TODO/#20190909","text":"","title":"20190909"},{"location":"TODO/#tcp-backlog","text":"https://stackoverflow.com/questions/36594400/what-is-backlog-in-tcp-connections http://www.linuxjournal.com/files/linuxjournal.com/linuxjournal/articles/023/2333/2333s2.html https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html","title":"TCP backlog"},{"location":"TODO/#20190913","text":"","title":"20190913"},{"location":"TODO/#atomic-race-condition-lock-consistent-model","text":"\u4eca\u5929\u5728\u9605\u8bfbredis source code\u7684\u65f6\u5019\uff0c\u603b\u7ed3\u4e86redis\u4e2dsocket file descriptor\u90fd\u662fnon blocking\u7684\uff0c\u7136\u540e\u6211\u5c31\u67e5\u9605APUE\u4e2d\u5173\u4e8e\u6587\u4ef6\u63cf\u8ff0\u7b26\u6807\u5fd7\uff0c\u6587\u4ef6\u6807\u5fd7\u7684\u5185\u5bb9\uff1b\u53c8\u91cd\u65b0\u770b\u4e86\u4e00\u904dAPUE 3.3\u8282\u4e2d\u5173\u4e8eTOCTTOU\u7684\u63cf\u8ff0\uff0c\u4ee5\u53caAPUE 3.11 \u539f\u5b50\u64cd\u4f5c\u7684\u63cf\u8ff0\uff0c\u6211\u624d\u610f\u8bc6\u5230\u539f\u7406linux\u7684system call\u4e5f\u662f\u80fd\u591f\u4fdd\u8bc1atomic\u7684\uff08\u4ece\u5e95\u5c42\u5b9e\u73b0\u6765\u770b\uff0c\u662f\u56e0\u4e3abig kernel lock \uff09\uff1b\u539f\u6765\u6211\u4ec5\u4ec5\u8ba4\u77e5\u5230\u4e00\u4e2ainstruction\u662fatomic\uff0c\u5176\u5b9e\u8fd9\u79cd\u8ba4\u77e5\u662f\u6bd4\u8f83\u5c40\u9650\u7684\uff1b\u8054\u60f3\u5230\u6211\u4e4b\u524d\u603b\u7ed3\u8fc7\u5173\u4e8eatomic\u7684\u5185\u5bb9\uff0c\u5982\u4e0b\uff1a everything\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u627e\u6240\u6709atomic\u76f8\u5173\u7684\u5185\u5bb9\uff1a nowholeword:c:\\users\\dengkai17334\\appdata\\local\\ynote\\data\\ content:atomic \u73b0\u5728\u662f\u6709\u5fc5\u8981\u6574\u7406\u4e00\u756a\u4e86\u3002","title":"atomic &amp; race condition &amp; lock &amp; consistent model \u4e13\u9898"},{"location":"TODO/#linux-system-call-atomic","text":"","title":"linux system call atomic"},{"location":"TODO/#network","text":"","title":"network"},{"location":"TODO/#data-integrity-in-network","text":"\u5728 2.1. Two Types of Internet Sockets \u4e2d\u63d0\u53caTCP\u80fd\u591f\u4fdd\u8bc1 data integrity \uff0c\u53ef\u89c1 data integrity \u5e76\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u4e00\u65b9\u9762\uff1b","title":"data integrity in network"},{"location":"TODO/#whats-the-difference-between-127001-and-0000","text":"What's the difference between 127.0.0.1 and 0.0.0.0?","title":"What's the difference between 127.0.0.1 and 0.0.0.0?"},{"location":"TODO/#interface-address","text":"\u5728\u5f88\u591a\u5730\u65b9\u90fd\u89c1\u5230\u4e86\u8fd9\u4e2a\u8bcd\uff1a","title":"interface address"},{"location":"TODO/#-getaddrinfo3-linux-man-page","text":"\u5b83\u5230\u5e95\u662f\u4ec0\u4e48\u542b\u4e49","title":"- getaddrinfo(3) - Linux man page"},{"location":"TODO/#stream-of-bytes-and-bitstream","text":"Bitstream \u5df2\u7ecf\u9605\u8bfb","title":"stream of bytes and bitstream"},{"location":"TODO/#maximum-segment-lifetime","text":"https://en.wikipedia.org/wiki/Maximum_segment_lifetime","title":"Maximum segment lifetime"},{"location":"TODO/#20190909_1","text":"epoll\u4e2d\u7684file descripor\u662f\u5426\u4e00\u5b9a\u8981\u8bbe\u7f6e\u4e3anon blocking SO_RCVLOWAT and SO_SNDLOWAT \u7684\u503c\u90fd\u662f1\uff0c\u90a3\u4e48\u5982\u4f55\u786e\u5b9amessage boundary\uff1f","title":"20190909"},{"location":"TODO/#shell","text":"getconf echo \"2^12\" | bc How to check if port is in use on Linux or Unix https://unix.stackexchange.com/a/185767 how to get all threads of a process","title":"shell"},{"location":"TODO/#process","text":"https://www.cyberciti.biz/faq/show-all-running-processes-in-linux/ https://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes","title":"process"},{"location":"TODO/#how-to-print-out-a-variable-in-makefile","text":"https://stackoverflow.com/questions/16467718/how-to-print-out-a-variable-in-makefile","title":"How to print out a variable in makefile"},{"location":"TODO/#process-control-block","text":"https://www.tldp.org/LDP/lki/lki-2.html \u4e0a\u9762\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u8bb2\u8ff0\u7684\u662flinux kernel\u7684\u5b9e\u73b0","title":"Process control block"},{"location":"TODO/#how-to-find-out-which-windows-process-is-using-a-file","text":"user the software:Process Explorer","title":"How to Find Out Which Windows Process is Using a File"},{"location":"TODO/#how-find-out-which-process-is-using-a-file-in-linux","text":"fuser file_name","title":"How find out which process is using a file in Linux?"},{"location":"TODO/#what-will-happen-if-a-process-exceed-its-resource-limits","text":"\u8bb0\u5f97\u5728redis in action\u8fd9\u672c\u4e66\u4e2d\u6709\u63d0\u53ca\u8fc7\u7c7b\u4f3c\u7684\u95ee\u9898\uff1b","title":"what will happen if a process exceed its resource limits"},{"location":"TODO/#c-posix-library","text":"https://en.wikipedia.org/wiki/C_POSIX_library http://pubs.opengroup.org/onlinepubs/9699919799/idx/head.html","title":"C POSIX library"},{"location":"TODO/#c-standard-library","text":"https://en.wikipedia.org/wiki/C_standard_library","title":"C standard library"},{"location":"TODO/#process-id","text":"APUE 4.4 Set-User-ID and Set-Group-ID https://en.wikipedia.org/wiki/User_identifier","title":"process id"},{"location":"TODO/#time-of-check-to-time-of-use","text":"https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use","title":"Time of check to time of use"},{"location":"TODO/#user-identifier","text":"https://en.wikipedia.org/wiki/User_identifier","title":"User identifier"},{"location":"TODO/#secure-programming-howto","text":"https://dwheeler.com/secure-programs/Secure-Programs-HOWTO/index.html","title":"Secure Programming HOWTO"},{"location":"TODO/#thread-local","text":"thread-local\u548creentry\u4e4b\u95f4\u7684\u5173\u7cfb","title":"thread-local"},{"location":"TODO/#hole-in-file","text":"hole\u5bf9\u6587\u4ef6\u5927\u5c0f\u7684\u5f71\u54cd\uff1b\u4e0a\u5468\u5728\u67e5\u770bhttps://liftoff.github.io/pyminifier/\u6587\u6863\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709minifying\u529f\u80fd\uff0c\u662f\u548chole in file\u6709\u5173\u7684\uff1b \u540c\u65f6binary mode\u6765\u4fdd\u5b58\u6587\u4ef6\u4e5f\u80fd\u591f\u964d\u4f4e\u6587\u4ef6\u5927\u5c0f\uff1b","title":"hole in file"},{"location":"TODO/#linux-memory","text":"memory usage memory available","title":"linux memory"},{"location":"TODO/#virtual-process-space","text":"https://www.tutorialspoint.com/where-are-static-variables-stored-in-c-cplusplus https://cs61.seas.harvard.edu/wiki/2016/Kernel2X \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d https://www.cs.utexas.edu/~lorenzo/corsi/cs372/06F/hw/3sol.html https://cs.stackexchange.com/questions/56825/how-to-calculate-virtual-address-space-from-page-size-virtual-address-size-and","title":"virtual process space"},{"location":"TODO/#process-environment","text":"7.12\u4e0d\u540c\u8bed\u8a00\u7684environment\u662f\u5426\u4e0d\u540c","title":"process environment"},{"location":"TODO/#file-iopython-vs-linux","text":"Python\u4e0eLinux\u7c7b\u4f3c\uff0c\u4e0e\u6587\u4ef6\u76f8\u5173\u7684\u64cd\u4f5c\u90fd\u662f\u4eceopen\u51fd\u6570\u5f00\u59cb\u7684","title":"file IO\uff1apython VS linux"},{"location":"TODO/#fragmentation-computing","text":"https://en.wikipedia.org/wiki/Fragmentation_(computing)","title":"Fragmentation (computing)"},{"location":"TODO/#page-table-size","text":"https://www.cs.cornell.edu/courses/cs4410/2015su/lectures/lec14-pagetables.html http://www.cs.cornell.edu/courses/cs4410/2016su/slides/lecture11.pdf http://www.cs.cornell.edu/courses/cs4410/2016su/schedule.html","title":"page table size"},{"location":"TODO/#data-segment","text":"https://en.wikipedia.org/wiki/Data_segment","title":"Data segment"},{"location":"TODO/#enoent","text":"Why does ENOENT mean \u201cNo such file or directory\u201d?","title":"ENOENT"},{"location":"TODO/#eacces","text":"","title":"EACCES"},{"location":"TODO/#cron","text":"https://en.wikipedia.org/wiki/Cron https://www.adminschoice.com/crontab-quick-reference","title":"cron"},{"location":"TODO/#shell_1","text":"\u4eca\u5929\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u53d1\u73b0\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u95ee\u9898\uff1a\u8fdb\u7a0b\u8fd0\u884c\u4e2d\uff0c\u8fd9\u4e2a\u8fdb\u7a0b\u4f1a\u4e0d\u65ad\u5730\u5411\u5176\u65e5\u5fd7\u6587\u4ef6\u4e2d\u5199\u5165\u65e5\u5fd7\uff1b\u7136\u540e\u6211\u5728shell\u4e2d\u5c06\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\u7ed9\u5220\u9664\u4e86\uff0c\u53d1\u73b0\u8fdb\u7a0b\u5e76\u6ca1\u6709\u53d1\u73b0\u5b83\u7684\u65e5\u5fd7\u6587\u4ef6\u88ab\u5220\u4e86\uff0c\u4e5f\u6ca1\u6709\u51fa\u73b0\u521b\u5efa\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\uff0c\u7a0b\u5e8f\u4e5f\u6ca1\u6709\u505c\u6b62\u4e0b\u6765\uff1b \u78c1\u76d8\u7a7a\u95f4\u6ee1\u540e\uff0c\u4e5f\u4f1a\u5bfc\u81f4process\u65e0\u6cd5\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff1b\u4f46\u662f\u5f53\u91ca\u653e\u4e00\u90e8\u5206\u7a7a\u95f4\u540e\uff0c\u53d1\u73b0process\u4ecd\u7136\u4e0d\u4f1a\u5199\u5165\uff0c\u5c31\u50cf\u662f\u653e\u5f03\u4e86\u4e00\u6837\uff1b","title":"\u5728shell\u4e2d\u5220\u9664\u6389\u8fdb\u7a0b\u4f7f\u7528\u7684\u6587\u4ef6"},{"location":"TODO/#segmentation-fault","text":"hiredis\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u4eca\u5929\u5728\u591a\u7ebf\u7a0b\u73af\u5883\u4e0b\u6d4b\u8bd5\u51fa\u5b83\u4f1a\u5bfc\u81f4process core dump\uff0cdump\u7684\u539f\u56e0\u662f Program terminated with signal 11, Segmentation fault. https://kb.iu.edu/d/aqsj https://en.wikipedia.org/wiki/Segmentation_fault","title":"Segmentation fault"},{"location":"TODO/#thread-unsafe-and-core-dump","text":"","title":"thread unsafe and core dump"},{"location":"TODO/#how-to-test","text":"\u5982\u679c\u662f\u652f\u6301\u7f51\u7edc\uff0c\u9700\u8981\u6d4b\u8bd5\u591a\u4e2aclient\u8fde\u63a5\uff1b \u9700\u8981\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5 \u9700\u8981\u8fdb\u884c\u5e76\u53d1\u6d4b\u8bd5","title":"how to test"},{"location":"TODO/#process-and-its-thread","text":"Is there a way to see details of all the threads that a process has in Linux? google:how to get all threads of a process https://www.unix.com/aix/154772-how-list-all-threads-running-process.html http://ask.xmodulo.com/view-threads-process-linux.html","title":"process and its thread"},{"location":"TODO/#how-os-know-process-use-a-illegal-memory-location","text":"https://en.wikipedia.org/wiki/Memory_protection https://stackoverflow.com/questions/41172563/how-os-catches-illegal-memory-references-at-paging-scheme https://unix.stackexchange.com/questions/511963/how-linux-finds-out-about-illegal-memory-access-error https://www.kernel.org/ https://www.kernel.org/doc/gorman/html/understand/index.html https://www.kernel.org/doc/gorman/ https://www.kernel.org/doc/","title":"how OS know process use a illegal memory location"},{"location":"TODO/#dev-file","text":"https://unix.stackexchange.com/questions/93531/what-is-stored-in-dev-pts-files-and-can-we-open-them","title":"dev file"},{"location":"TODO/#unix-memory-usage","text":"https://utcc.utoronto.ca/~cks/space/blog/linux/LinuxMemoryStats https://utcc.utoronto.ca/~cks/space/blog/unix/UnderstandingRSS https://stackoverflow.com/questions/131303/how-to-measure-actual-memory-usage-of-an-application-or-process https://unix.stackexchange.com/questions/554/how-to-monitor-cpu-memory-usage-of-a-single-process","title":"Unix memory usage"},{"location":"TODO/#how-can-i-kill-a-process-by-name-instead-of-pid","text":"https://stackoverflow.com/questions/160924/how-can-i-kill-a-process-by-name-instead-of-pid http://osxdaily.com/2017/01/12/kill-process-by-name-command-line/","title":"How can I kill a process by name instead of PID?"},{"location":"TODO/#apueunix-interruption-and-atomwanch","text":"\u5176\u4e2d\u4e3b\u8981\u8ba8\u8bba\u4e86Atomicity","title":"APUE\u7684\u300aUnix-interruption-and-atom\u300b\u8fd8\u6ca1\u6709wanch"},{"location":"TODO/#consistency-models","text":"\u5728parallel computing\u4e2d\u7684Consistency models\u8fd8\u6ca1\u6709\u5b8c\u6210 https://en.wikipedia.org/wiki/Category:Consistency_models","title":"Consistency models"},{"location":"TODO/#epoll","text":"https://en.wikipedia.org/wiki/Epoll","title":"epoll"},{"location":"TODO/#youdao-unix-abort","text":"","title":"youdao Unix-abort"},{"location":"TODO/#init-of-linux","text":"","title":"init of linux"},{"location":"TODO/#init","text":"ch8.2\u4e2d\u6709\u5bf9init\u8fdb\u7a0b\u7684\u4e00\u4e2a\u4ecb\u7ecd ch9.2\u4ecb\u7ecd\u5230\uff0cinit\u4f1a\u8bfb\u53d6\u6587\u4ef6 /etc/ttys how to know what init system linux use https://unix.stackexchange.com/questions/18209/detect-init-system-using-the-shell https://fedoramagazine.org/what-is-an-init-system/ https://en.wikipedia.org/wiki/Init https://en.wikipedia.org/wiki/Systemd","title":"init\u8fdb\u7a0b"},{"location":"TODO/#convert-between-unix-and-windows-text-files","text":"https://kb.iu.edu/d/acux https://stackoverflow.com/questions/16239551/eol-conversion-in-notepad","title":"Convert between Unix and Windows text files"},{"location":"TODO/#memory-management-algorithms","text":"https://en.wikipedia.org/wiki/Category:Memory_management_algorithms Slab allocation","title":"Memory management algorithms"},{"location":"TODO/#interrupt-in-unix","text":"interrupt vector table http://www.cis.upenn.edu/~lee/03cse380/lectures/ln2-process-v4.pdf \u5728 Context switch \u4e2d\u4e5f\u5bf9interrupt\u8fdb\u884c\u4e86\u4ecb\u7ecd","title":"interrupt in Unix"},{"location":"TODO/#signal","text":"signal\u4e5f\u662f\u4e00\u79cdinterrupt\uff0c\u6545\u5c06\u5b83\u653e\u5728interrupt\u4e4b\u4e0b","title":"signal"},{"location":"TODO/#google-unix-signals-and-threads","text":"https://stackoverflow.com/questions/2575106/posix-threads-and-signals https://en.wikipedia.org/wiki/Signal_(IPC)","title":"Google unix signals and threads"},{"location":"TODO/#scheduling-computing","text":"https://en.wikipedia.org/wiki/Scheduling_(computing)","title":"Scheduling (computing)"},{"location":"TODO/#thread-join-and-detach","text":"","title":"thread join and detach"},{"location":"TODO/#wait","text":"https://linux.die.net/man/2/waitpid","title":"wait"},{"location":"Unix-&-Unix-like-&-Linux/","text":"Unix & Unix like & Linux # \u672c\u8282\u533a\u5206\u8fd9\u51e0\u4e2a\u6982\u5ff5\uff0c\u5e76\u8bb0\u5f55\u4e00\u4e9b\u6709\u7528\u8d44\u6e90\uff1a Unix # wikipedia Unix Unix-like # wikipedia Unix-like Linux # \u5982\u4e0b\u662f\u5173\u4e8elinux OS\u7684\u4e00\u4e9b\u8d44\u6e90\uff1a wikipedia Linux NOTE: linux\u662fUnix-like wikipedia Linux kernel The Linux Kernel Archives The Linux Kernel documentation The Linux man-pages project LWN.net","title":"Unix-&-Unix-like-&-Linux"},{"location":"Unix-&-Unix-like-&-Linux/#unix-unix-like-linux","text":"\u672c\u8282\u533a\u5206\u8fd9\u51e0\u4e2a\u6982\u5ff5\uff0c\u5e76\u8bb0\u5f55\u4e00\u4e9b\u6709\u7528\u8d44\u6e90\uff1a","title":"Unix &amp; Unix like &amp; Linux"},{"location":"Unix-&-Unix-like-&-Linux/#unix","text":"wikipedia Unix","title":"Unix"},{"location":"Unix-&-Unix-like-&-Linux/#unix-like","text":"wikipedia Unix-like","title":"Unix-like"},{"location":"Unix-&-Unix-like-&-Linux/#linux","text":"\u5982\u4e0b\u662f\u5173\u4e8elinux OS\u7684\u4e00\u4e9b\u8d44\u6e90\uff1a wikipedia Linux NOTE: linux\u662fUnix-like wikipedia Linux kernel The Linux Kernel Archives The Linux Kernel documentation The Linux man-pages project LWN.net","title":"Linux"},{"location":"Unix-standardization-and-implementation/","text":"UNIX standardization and implementations # \u672c\u8282\u68b3\u7406 Unix operating system \u548c Unix-like operating system \u4e4b\u95f4\u7684\u6807\u51c6\u3001\u6f14\u8fdb\u3001\u5173\u7cfb\u7b49\u3002\u5bf9\u4e8e\u8fd9\u4e9b\u6807\u51c6\u6709\u5fc5\u8981\u4e86\u89e3\u4e00\u4e0b\uff0c\u56e0\u4e3a\u5728\u5f88\u591a\u4e66\u7c4d\uff0c\u6587\u7ae0\u4e2d\u90fd\u4f1a\u89c1\u5230\u8fd9\u4e9b\u6807\u51c6\u3002\u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81ea\u4e8e Advanced Programming in the UNIX\u00ae Environment, Third Edition \u7684chapter 2 UNIX Standardization and Implementations Introduction # Much work has gone into standardizing the UNIX programming environment and the C programming language. In this chapter we first look at the various standardization efforts that have been under way over the past two and a half decades. We then discuss the effects of these UNIX programming standards on the operating system implementations that are described in this book. An important part of all the standardization efforts is the specification of various limits that each implementation must define, so we look at these limits and the various ways to determine their values. UNIX Standardization # ISO C # NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86ISO C standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eISO C standard\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia C (programming language) The Standard wikipedia ANSI C This standard defines not only the syntax and semantics of the programming language but also a standard library . The ISO C library can be divided into 24 areas, based on the headers defined by the standard (see Figure 2.1). The POSIX.1 standard includes these headers, as well as others. As Figure 2.1 shows, all of these headers are supported by the four implementations (FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris 10) that are described later in this chapter. NOTE: POSIX.1 standard \u662fISO C standard\u7684\u8d85\u96c6\u3002 The ISO C headers depend on which version of the C compiler is used with the operating system. FreeBSD 8.0 ships with version 4.2.1 of gcc, Solaris 10 ships with version 3.4.3 of gcc (in addition to its own C compiler in Sun Studio), Ubuntu 12.04 (Linux 3.2.0) ships with version 4.6.3 of gcc, and Mac OS X 10.6.8 ships with both versions 4.0.1 and 4.2.1 of gcc. Headers defined by the ISO C standard # C standard library IEEE POSIX # NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86IEEE POSIX standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eIEEE POSIX\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia POSIX POSIX official site POSIX is a family of standards initially developed by the IEEE (Institute of Electrical and Electronics Engineers). POSIX stands for Portable Operating System Interface. It originally referred only to the IEEE Standard 1003.1-1988 \u2014 the operating system interface \u2014 but was later extended to include many of the standards and draft standards with the 1003 designation, including the shell and utilities (1003.2). Because the 1003.1 standard specifies an interface and not an implementation , no distinction is made between system calls and library functions . All the routines in the standard are called functions . POSIX Threads # POSIX Threads The Open Group Base Specifications Issue 7, IEEE Std 1003.1 C POSIX library header files # C POSIX library C POSIX library header files Official List of headers in the POSIX library on opengroup.org The Single UNIX Specification # The Single UNIX Specification, a superset of the POSIX.1 standard, specifies additional interfaces that extend the functionality provided by the POSIX.1 specification. POSIX.1 is equivalent to the Base Specifications portion of the Single UNIX Specification. NOTE: wikipedia The Open Group wikipedia Single UNIX Specification The X/Open System Interfaces (XSI) option in POSIX.1 describes optional interfaces and defines which optional portions of POSIX.1 must be supported for an implementation to be deemed XSI conforming . These include file synchronization, thread stack address and size attributes, thread process-shared synchronization, and the _XOPEN_UNIX symbolic constant (marked \u2018\u2018SUS mandatory\u2019\u2019 in Figure 2.5). Only XSI-conforming implementations can be called UNIX systems . NOTE: X/Open The Single UNIX Specification is a publication of The Open Group, which was formed in 1996 as a merger of X/Open and the Open Software Foundation (OSF), both industry consortia. X/Open used to publish the X/Open Portability Guide, which adopted specific standards and filled in the gaps where functionality was missing. The goal of these guides was to improve application portability beyond what was possible by merely conforming to published standards. NOTE: \u4e0b\u9762\u89e3\u91ca\u4e00\u4e9b\u7b80\u79f0\u7684\u542b\u4e49 SUSv3\uff1a the third version of the Single UNIX Specification SUSv4\uff1a the forth version of the Single UNIX Specification UNIX System Implementations # The previous section described ISO C, IEEE POSIX, and the Single UNIX Specification \u2014 three standards originally created by independent organizations. Standards, however, are interface specifications. How do these standards relate to the real world? These standards are taken by vendors and turned into actual implementations. In this book, we are interested in both these standards and their implementation. UNIX System V Release 4 # UNIX System V Release 4 (SVR4) was a product of AT&T\u2019s UNIX System Laboratories (USL, formerly AT&T\u2019s UNIX Software Operation). SVR4 merged functionality from AT&T UNIX System V Release 3.2 (SVR3.2), the SunOS operating system from Sun Microsystems, the 4.3BSD release from the University of California, and the Xenix system from Microsoft into one coherent operating system. (Xenix was originally developed from Version 7, with many features later taken from System V.) The SVR4 source code was released in late 1989, with the first end-user copies becoming available during 1990. SVR4 conformed to both the POSIX 1003.1 standard and the X/Open Portability Guide, Issue 3 (XPG3). 4.4BSD # FreeBSD # Linux # Mac OS X # Solaris # Relationship of Standards and Implementations # The standards that we\u2019ve mentioned define a subset of any actual system. The focus of this book is on four real systems: FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris Although only Mac OS X and Solaris can call themselves UNIX systems, all four provide a similar programming environment. Because all four are POSIX compliant to varying degrees, we will also concentrate on the features required by the POSIX.1 standard, noting any differences between POSIX and the actual implementations of these four systems. Those features and routines that are specific to only a particular implementation are clearly marked. We\u2019ll also note any features that are required on UNIX systems but are optional on other POSIX-conforming systems. Be aware that the implementations provide backward compatibility for features in earlier releases, such as SVR3.2 and 4.3BSD. For example, Solaris supports both the POSIX.1 specification for nonblocking I/O (O_NONBLOCK) and the traditional System V method ( O_NDELAY ). In this text, we\u2019ll use only the POSIX.1 feature, although we\u2019ll mention the nonstandard feature that it replaces. Similarly, both SVR3.2 and 4.3BSD provided reliable signals in a way that differs from the POSIX.1 standard. In Chapter 10 we describe only the POSIX.1 signal mechanism. man STANDARDS(7) # NOTE: \u8fd9\u662flinux\u6587\u6863\u4e2d\u5bf9standard\u7684\u4ecb\u7ecd\u3002","title":"Unix-standardization-and-implementation"},{"location":"Unix-standardization-and-implementation/#unix-standardization-and-implementations","text":"\u672c\u8282\u68b3\u7406 Unix operating system \u548c Unix-like operating system \u4e4b\u95f4\u7684\u6807\u51c6\u3001\u6f14\u8fdb\u3001\u5173\u7cfb\u7b49\u3002\u5bf9\u4e8e\u8fd9\u4e9b\u6807\u51c6\u6709\u5fc5\u8981\u4e86\u89e3\u4e00\u4e0b\uff0c\u56e0\u4e3a\u5728\u5f88\u591a\u4e66\u7c4d\uff0c\u6587\u7ae0\u4e2d\u90fd\u4f1a\u89c1\u5230\u8fd9\u4e9b\u6807\u51c6\u3002\u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81ea\u4e8e Advanced Programming in the UNIX\u00ae Environment, Third Edition \u7684chapter 2 UNIX Standardization and Implementations","title":"UNIX standardization and implementations"},{"location":"Unix-standardization-and-implementation/#introduction","text":"Much work has gone into standardizing the UNIX programming environment and the C programming language. In this chapter we first look at the various standardization efforts that have been under way over the past two and a half decades. We then discuss the effects of these UNIX programming standards on the operating system implementations that are described in this book. An important part of all the standardization efforts is the specification of various limits that each implementation must define, so we look at these limits and the various ways to determine their values.","title":"Introduction"},{"location":"Unix-standardization-and-implementation/#unix-standardization","text":"","title":"UNIX Standardization"},{"location":"Unix-standardization-and-implementation/#iso-c","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86ISO C standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eISO C standard\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia C (programming language) The Standard wikipedia ANSI C This standard defines not only the syntax and semantics of the programming language but also a standard library . The ISO C library can be divided into 24 areas, based on the headers defined by the standard (see Figure 2.1). The POSIX.1 standard includes these headers, as well as others. As Figure 2.1 shows, all of these headers are supported by the four implementations (FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris 10) that are described later in this chapter. NOTE: POSIX.1 standard \u662fISO C standard\u7684\u8d85\u96c6\u3002 The ISO C headers depend on which version of the C compiler is used with the operating system. FreeBSD 8.0 ships with version 4.2.1 of gcc, Solaris 10 ships with version 3.4.3 of gcc (in addition to its own C compiler in Sun Studio), Ubuntu 12.04 (Linux 3.2.0) ships with version 4.6.3 of gcc, and Mac OS X 10.6.8 ships with both versions 4.0.1 and 4.2.1 of gcc.","title":"ISO C"},{"location":"Unix-standardization-and-implementation/#headers-defined-by-the-iso-c-standard","text":"C standard library","title":"Headers defined by the ISO C standard"},{"location":"Unix-standardization-and-implementation/#ieee-posix","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86IEEE POSIX standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eIEEE POSIX\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia POSIX POSIX official site POSIX is a family of standards initially developed by the IEEE (Institute of Electrical and Electronics Engineers). POSIX stands for Portable Operating System Interface. It originally referred only to the IEEE Standard 1003.1-1988 \u2014 the operating system interface \u2014 but was later extended to include many of the standards and draft standards with the 1003 designation, including the shell and utilities (1003.2). Because the 1003.1 standard specifies an interface and not an implementation , no distinction is made between system calls and library functions . All the routines in the standard are called functions .","title":"IEEE POSIX"},{"location":"Unix-standardization-and-implementation/#posix-threads","text":"POSIX Threads The Open Group Base Specifications Issue 7, IEEE Std 1003.1","title":"POSIX Threads"},{"location":"Unix-standardization-and-implementation/#c-posix-library-header-files","text":"C POSIX library C POSIX library header files Official List of headers in the POSIX library on opengroup.org","title":"C POSIX library header files"},{"location":"Unix-standardization-and-implementation/#the-single-unix-specification","text":"The Single UNIX Specification, a superset of the POSIX.1 standard, specifies additional interfaces that extend the functionality provided by the POSIX.1 specification. POSIX.1 is equivalent to the Base Specifications portion of the Single UNIX Specification. NOTE: wikipedia The Open Group wikipedia Single UNIX Specification The X/Open System Interfaces (XSI) option in POSIX.1 describes optional interfaces and defines which optional portions of POSIX.1 must be supported for an implementation to be deemed XSI conforming . These include file synchronization, thread stack address and size attributes, thread process-shared synchronization, and the _XOPEN_UNIX symbolic constant (marked \u2018\u2018SUS mandatory\u2019\u2019 in Figure 2.5). Only XSI-conforming implementations can be called UNIX systems . NOTE: X/Open The Single UNIX Specification is a publication of The Open Group, which was formed in 1996 as a merger of X/Open and the Open Software Foundation (OSF), both industry consortia. X/Open used to publish the X/Open Portability Guide, which adopted specific standards and filled in the gaps where functionality was missing. The goal of these guides was to improve application portability beyond what was possible by merely conforming to published standards. NOTE: \u4e0b\u9762\u89e3\u91ca\u4e00\u4e9b\u7b80\u79f0\u7684\u542b\u4e49 SUSv3\uff1a the third version of the Single UNIX Specification SUSv4\uff1a the forth version of the Single UNIX Specification","title":"The Single UNIX Specification"},{"location":"Unix-standardization-and-implementation/#unix-system-implementations","text":"The previous section described ISO C, IEEE POSIX, and the Single UNIX Specification \u2014 three standards originally created by independent organizations. Standards, however, are interface specifications. How do these standards relate to the real world? These standards are taken by vendors and turned into actual implementations. In this book, we are interested in both these standards and their implementation.","title":"UNIX System Implementations"},{"location":"Unix-standardization-and-implementation/#unix-system-v-release-4","text":"UNIX System V Release 4 (SVR4) was a product of AT&T\u2019s UNIX System Laboratories (USL, formerly AT&T\u2019s UNIX Software Operation). SVR4 merged functionality from AT&T UNIX System V Release 3.2 (SVR3.2), the SunOS operating system from Sun Microsystems, the 4.3BSD release from the University of California, and the Xenix system from Microsoft into one coherent operating system. (Xenix was originally developed from Version 7, with many features later taken from System V.) The SVR4 source code was released in late 1989, with the first end-user copies becoming available during 1990. SVR4 conformed to both the POSIX 1003.1 standard and the X/Open Portability Guide, Issue 3 (XPG3).","title":"UNIX System V Release 4"},{"location":"Unix-standardization-and-implementation/#44bsd","text":"","title":"4.4BSD"},{"location":"Unix-standardization-and-implementation/#freebsd","text":"","title":"FreeBSD"},{"location":"Unix-standardization-and-implementation/#linux","text":"","title":"Linux"},{"location":"Unix-standardization-and-implementation/#mac-os-x","text":"","title":"Mac OS X"},{"location":"Unix-standardization-and-implementation/#solaris","text":"","title":"Solaris"},{"location":"Unix-standardization-and-implementation/#relationship-of-standards-and-implementations","text":"The standards that we\u2019ve mentioned define a subset of any actual system. The focus of this book is on four real systems: FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris Although only Mac OS X and Solaris can call themselves UNIX systems, all four provide a similar programming environment. Because all four are POSIX compliant to varying degrees, we will also concentrate on the features required by the POSIX.1 standard, noting any differences between POSIX and the actual implementations of these four systems. Those features and routines that are specific to only a particular implementation are clearly marked. We\u2019ll also note any features that are required on UNIX systems but are optional on other POSIX-conforming systems. Be aware that the implementations provide backward compatibility for features in earlier releases, such as SVR3.2 and 4.3BSD. For example, Solaris supports both the POSIX.1 specification for nonblocking I/O (O_NONBLOCK) and the traditional System V method ( O_NDELAY ). In this text, we\u2019ll use only the POSIX.1 feature, although we\u2019ll mention the nonstandard feature that it replaces. Similarly, both SVR3.2 and 4.3BSD provided reliable signals in a way that differs from the POSIX.1 standard. In Chapter 10 we describe only the POSIX.1 signal mechanism.","title":"Relationship of Standards and Implementations"},{"location":"Unix-standardization-and-implementation/#man-standards7","text":"NOTE: \u8fd9\u662flinux\u6587\u6863\u4e2d\u5bf9standard\u7684\u4ecb\u7ecd\u3002","title":"man STANDARDS(7)"},{"location":"Architecture/Architecture-of-computing-system/","text":"Architecture of computing system # \u672c\u7ae0\u6240\u8981\u63a2\u8ba8\u7684\u662farchitecture of computing system \uff0c\u800c\u4e0d\u662f\u5bfb\u5e38\u6240\u8bf4\u7684 computer architecture \uff0c\u4e00\u822c computer architecture \u6240\u6307\u7684\u662f\u8bf8\u5982 Von Neumann architecture \u3001 Harvard architecture \u7b49\u63cf\u8ff0computer\u786c\u4ef6\u7684\u67b6\u6784\u3002\u672c\u8282\u6240\u8981\u63cf\u8ff0\u7684\u5185\u5bb9\u662f\u4ece\u4e00\u4e2a\u66f4\u52a0\u9ad8\u7684\u89d2\u5ea6\u6765\u770b\u5f85 computing system \uff0c\u5305\u62ec\u6700\u5e95\u5c42\u7684hardware\uff0coperating system\uff0capplication software\u3002 \u6b63\u5982 Computer hardware \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a The progression from levels of \"hardness\" to \"softness\" in computer systems parallels a progression of layers of abstraction in computing. A combination of hardware and software forms a usable computing system. \u73b0\u4ee3 computing system \u7684\u6574\u4f53\u67b6\u6784\u7684\u53d1\u5c55\u662f\u53d7\u5230\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u7684 layers of abstraction \u601d\u60f3\uff08\u5206\u5c42\u601d\u60f3\uff09\u7684\u5f71\u54cd\u7684\uff0c\u4e00\u4e2a computing system \u53ef\u4ee5\u8ba4\u4e3a\u7531\u4e24\u5c42\u6784\u6210\uff1a software hardware Instruction set \u662fsoftware\u548chardware\u4e4b\u95f4\u7684\u63a5\u53e3\u3002 \u73b0\u4ee3 computing system \u7684\u8fd0\u884c\u662f\u79bb\u4e0d\u5f00operating system\u7684\uff0coperating system\u6240\u5c5e\u7684\u662fsoftware\u8fd9\u4e00\u5c42\uff0c\u4e0b\u9762\u5bf9operating system\u6765\u8fdb\u884c\u66f4\u52a0\u7cbe\u7ec6\u7684\u5206\u5c42\u3002 Architecture of operating system # \u5728 Operating system \u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5178\u578b\u7684operating system\u7684architecture\u5982\u4e0b\uff1a Operating systems \u4e0a\u56fe\u4e2dUser\u8868\u793a\u7684\u662f\u7528\u6237\uff0c\u4e0d\u5c5e\u4e8eOS\u4e2d\uff0c\u540e\u9762\u7684\u8ba8\u8bba\u4f1a\u5c06\u5176\u5ffd\u89c6\u3002 \u5728 Advanced Programming in the UNIX\u00ae Environment, Third Edition \u76841.2 UNIX Architecture\u8282\u4e2d\u7ed9\u51fa\u4e86Architecture of the UNIX operating system\uff0c\u5982\u4e0b\uff1a \u4e0a\u8ff0\u4e24\u4e2a\u67b6\u6784\u56fe\u90fd\u5927\u4f53\u5c55\u793aOS\u7684architecture\uff0c\u4e24\u8005\u90fd\u5404\u6709\u5229\u5f0a\uff0c\u56fe\u4e00\u5305\u542b\u4e86hardware\uff0c\u4f46\u662f\u5ffd\u89c6\u4e86\u5404\u5c42\u4e4b\u95f4\u7684interface\u3002\u56fe\u4e8c\u5219\u6b63\u597d\u76f8\u53cd\u3002\u6240\u4ee5\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u5219\u6b63\u597d\uff0c\u540e\u9762\u4f1a\u6309\u7167\u56fe\u4e8c\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5c06interface\u4e5f\u770b\u505a\u662f\u4e00\u5c42\u3002\u5145\u5f53interface\u7684layer\u4f5c\u4e3a\u5b83\u7684\u4e0a\u4e0b\u4e24\u5c42\u4e4b\u95f4\u7684interface\u3002 upper layer layer role software application system calls &library routines interface kernel Instruction set interface hardware hardware \u901a\u8fc7architecture\u6765\u5206\u6790OS\u7684\u4f5c\u7528 # \u901a\u8fc7\u4e0a\u8ff0\u7684OS\u7684architecture\uff0c\u6211\u4eec\u80fd\u591f\u66f4\u52a0\u6df1\u523b\u5730\u4e86\u89e3OS\u7684\u4f5c\u7528\u4e86\uff0c\u4e0b\u9762\u662f\u6458\u6284\u81ea Understanding.The.Linux.kernel.3rd.Edition \u7684Chapter 1.4. Basic Operating System Concepts\u4e2d\u5173\u4e8eOS\u7684\u4f5c\u7528\u7684\u63cf\u8ff0\uff1a The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). \u8fd9\u4e24\u4e2aobjective\uff08\u5176\u5b9e\u5c31\u662fOS\u7684\u4f5c\u7528\u3001\u4f7f\u547d\uff09\u76f8\u5f53\u4e8e\u4e24\u6761\u7ebf\uff0c\u540e\u9762\u6211\u4eec\u5c06\u6cbf\u7740\u8fd9\u4e24\u6761\u7ebf\u6df1\u5165\u5bf9linux OS\u7684\u5b66\u4e60\u3002 \u6211\u4eec\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7b80\u5316\uff1aOS\u7ba1\u7406\u7740hardware\u548cprocess\uff0c\u5b83\u4f5c\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u6865\u6881\u3002 The Process/Kernel Model # \u5728 Understanding.The.Linux.kernel.3rd.Edition \u7684chapter 1.6.1. The Process/Kernel Model\u4e2d\u6240\u63cf\u8ff0Process/Kernel Model\u4e0e\u4e0a\u9762\u6240\u63cf\u8ff0\u7684architecture\u76f8\u540c\uff0c\u8fd9\u4e2a\u540d\u79f0\u66f4\u52a0\u7cbe\u7b80\uff0c\u4fbf\u4e8e\u8868\u8fbe\u3002\u5bf9\u4e8e\u4e00\u4e2alinux OS programmer\uff0c\u5fc3\u4e2d\u5e94\u8be5\u7262\u8bb0\u8fd9\u4e2amodel\u3002 \u603b\u7ed3 # \u4e0a\u9762\u6211\u4f7f\u7528\u4e86 \u5c42\u6b21\u5316\u7684\u7ed3\u6784 \u6765\u63cf\u8ff0 computing system \u7684\u67b6\u6784\uff0c\u81f3\u6b64\uff0c\u5df2\u7ecf\u5efa\u7acb\u4e86operating system\u7684\u6574\u4f53architecture\uff08model\uff09\u3002","title":"Architecture-of-computing-system"},{"location":"Architecture/Architecture-of-computing-system/#architecture-of-computing-system","text":"\u672c\u7ae0\u6240\u8981\u63a2\u8ba8\u7684\u662farchitecture of computing system \uff0c\u800c\u4e0d\u662f\u5bfb\u5e38\u6240\u8bf4\u7684 computer architecture \uff0c\u4e00\u822c computer architecture \u6240\u6307\u7684\u662f\u8bf8\u5982 Von Neumann architecture \u3001 Harvard architecture \u7b49\u63cf\u8ff0computer\u786c\u4ef6\u7684\u67b6\u6784\u3002\u672c\u8282\u6240\u8981\u63cf\u8ff0\u7684\u5185\u5bb9\u662f\u4ece\u4e00\u4e2a\u66f4\u52a0\u9ad8\u7684\u89d2\u5ea6\u6765\u770b\u5f85 computing system \uff0c\u5305\u62ec\u6700\u5e95\u5c42\u7684hardware\uff0coperating system\uff0capplication software\u3002 \u6b63\u5982 Computer hardware \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a The progression from levels of \"hardness\" to \"softness\" in computer systems parallels a progression of layers of abstraction in computing. A combination of hardware and software forms a usable computing system. \u73b0\u4ee3 computing system \u7684\u6574\u4f53\u67b6\u6784\u7684\u53d1\u5c55\u662f\u53d7\u5230\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u7684 layers of abstraction \u601d\u60f3\uff08\u5206\u5c42\u601d\u60f3\uff09\u7684\u5f71\u54cd\u7684\uff0c\u4e00\u4e2a computing system \u53ef\u4ee5\u8ba4\u4e3a\u7531\u4e24\u5c42\u6784\u6210\uff1a software hardware Instruction set \u662fsoftware\u548chardware\u4e4b\u95f4\u7684\u63a5\u53e3\u3002 \u73b0\u4ee3 computing system \u7684\u8fd0\u884c\u662f\u79bb\u4e0d\u5f00operating system\u7684\uff0coperating system\u6240\u5c5e\u7684\u662fsoftware\u8fd9\u4e00\u5c42\uff0c\u4e0b\u9762\u5bf9operating system\u6765\u8fdb\u884c\u66f4\u52a0\u7cbe\u7ec6\u7684\u5206\u5c42\u3002","title":"Architecture of computing system"},{"location":"Architecture/Architecture-of-computing-system/#architecture-of-operating-system","text":"\u5728 Operating system \u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5178\u578b\u7684operating system\u7684architecture\u5982\u4e0b\uff1a Operating systems \u4e0a\u56fe\u4e2dUser\u8868\u793a\u7684\u662f\u7528\u6237\uff0c\u4e0d\u5c5e\u4e8eOS\u4e2d\uff0c\u540e\u9762\u7684\u8ba8\u8bba\u4f1a\u5c06\u5176\u5ffd\u89c6\u3002 \u5728 Advanced Programming in the UNIX\u00ae Environment, Third Edition \u76841.2 UNIX Architecture\u8282\u4e2d\u7ed9\u51fa\u4e86Architecture of the UNIX operating system\uff0c\u5982\u4e0b\uff1a \u4e0a\u8ff0\u4e24\u4e2a\u67b6\u6784\u56fe\u90fd\u5927\u4f53\u5c55\u793aOS\u7684architecture\uff0c\u4e24\u8005\u90fd\u5404\u6709\u5229\u5f0a\uff0c\u56fe\u4e00\u5305\u542b\u4e86hardware\uff0c\u4f46\u662f\u5ffd\u89c6\u4e86\u5404\u5c42\u4e4b\u95f4\u7684interface\u3002\u56fe\u4e8c\u5219\u6b63\u597d\u76f8\u53cd\u3002\u6240\u4ee5\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u5219\u6b63\u597d\uff0c\u540e\u9762\u4f1a\u6309\u7167\u56fe\u4e8c\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5c06interface\u4e5f\u770b\u505a\u662f\u4e00\u5c42\u3002\u5145\u5f53interface\u7684layer\u4f5c\u4e3a\u5b83\u7684\u4e0a\u4e0b\u4e24\u5c42\u4e4b\u95f4\u7684interface\u3002 upper layer layer role software application system calls &library routines interface kernel Instruction set interface hardware hardware","title":"Architecture of operating system"},{"location":"Architecture/Architecture-of-computing-system/#architectureos","text":"\u901a\u8fc7\u4e0a\u8ff0\u7684OS\u7684architecture\uff0c\u6211\u4eec\u80fd\u591f\u66f4\u52a0\u6df1\u523b\u5730\u4e86\u89e3OS\u7684\u4f5c\u7528\u4e86\uff0c\u4e0b\u9762\u662f\u6458\u6284\u81ea Understanding.The.Linux.kernel.3rd.Edition \u7684Chapter 1.4. Basic Operating System Concepts\u4e2d\u5173\u4e8eOS\u7684\u4f5c\u7528\u7684\u63cf\u8ff0\uff1a The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). \u8fd9\u4e24\u4e2aobjective\uff08\u5176\u5b9e\u5c31\u662fOS\u7684\u4f5c\u7528\u3001\u4f7f\u547d\uff09\u76f8\u5f53\u4e8e\u4e24\u6761\u7ebf\uff0c\u540e\u9762\u6211\u4eec\u5c06\u6cbf\u7740\u8fd9\u4e24\u6761\u7ebf\u6df1\u5165\u5bf9linux OS\u7684\u5b66\u4e60\u3002 \u6211\u4eec\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7b80\u5316\uff1aOS\u7ba1\u7406\u7740hardware\u548cprocess\uff0c\u5b83\u4f5c\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u6865\u6881\u3002","title":"\u901a\u8fc7architecture\u6765\u5206\u6790OS\u7684\u4f5c\u7528"},{"location":"Architecture/Architecture-of-computing-system/#the-processkernel-model","text":"\u5728 Understanding.The.Linux.kernel.3rd.Edition \u7684chapter 1.6.1. The Process/Kernel Model\u4e2d\u6240\u63cf\u8ff0Process/Kernel Model\u4e0e\u4e0a\u9762\u6240\u63cf\u8ff0\u7684architecture\u76f8\u540c\uff0c\u8fd9\u4e2a\u540d\u79f0\u66f4\u52a0\u7cbe\u7b80\uff0c\u4fbf\u4e8e\u8868\u8fbe\u3002\u5bf9\u4e8e\u4e00\u4e2alinux OS programmer\uff0c\u5fc3\u4e2d\u5e94\u8be5\u7262\u8bb0\u8fd9\u4e2amodel\u3002","title":"The Process/Kernel Model"},{"location":"Architecture/Architecture-of-computing-system/#_1","text":"\u4e0a\u9762\u6211\u4f7f\u7528\u4e86 \u5c42\u6b21\u5316\u7684\u7ed3\u6784 \u6765\u63cf\u8ff0 computing system \u7684\u67b6\u6784\uff0c\u81f3\u6b64\uff0c\u5df2\u7ecf\u5efa\u7acb\u4e86operating system\u7684\u6574\u4f53architecture\uff08model\uff09\u3002","title":"\u603b\u7ed3"},{"location":"Kernel/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a Kernel(operating-system)\uff1a\u8ba8\u8bbaOS kernel\u7684\u57fa\u7840\u77e5\u8bc6 Guide\uff1a\u57fa\u4e8eBook-Understanding-the-Linux-Kernel\u505a\u7684\u4e00\u4e9b\u603b\u7ed3\uff0c\u53ef\u4f5c\u4e3a\u9605\u8bfb\u672c\u4e66\u7684\u6307\u5bfc\u3002 Book-Understanding-the-Linux-Kernel\uff1a\u8ba8\u8bba linux kernel \u7684\u5b9e\u73b0\uff0c\u4ee5 Understanding.The.Linux.kernel.3rd.Edition \u4e3a\u4e3b\u8981\u53c2\u8003\u3002","title":"Introduction"},{"location":"Kernel/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a Kernel(operating-system)\uff1a\u8ba8\u8bbaOS kernel\u7684\u57fa\u7840\u77e5\u8bc6 Guide\uff1a\u57fa\u4e8eBook-Understanding-the-Linux-Kernel\u505a\u7684\u4e00\u4e9b\u603b\u7ed3\uff0c\u53ef\u4f5c\u4e3a\u9605\u8bfb\u672c\u4e66\u7684\u6307\u5bfc\u3002 Book-Understanding-the-Linux-Kernel\uff1a\u8ba8\u8bba linux kernel \u7684\u5b9e\u73b0\uff0c\u4ee5 Understanding.The.Linux.kernel.3rd.Edition \u4e3a\u4e3b\u8981\u53c2\u8003\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Kernel(operating-system)/","text":"Kernel (operating system)\u6982\u8ff0 # \u672c\u8282\u8ba8\u8bbaOS\u7684kernel\uff0c\u672c\u8282\u662f\u5bf9Kernel\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Kernel (operating system) \u3002 The kernel is a computer program that is the core of a computer's operating system , with complete control over everything in the system. On most systems, it is one of the first programs loaded on start-up (after the bootloader ). It handles the rest of start-up as well as input/output requests from software , translating them into data-processing instructions for the central processing unit . It handles memory and peripherals (\u5916\u8bbe) like keyboards, monitors, printers, and speakers. The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application programs or other, less critical parts of the operating system. The kernel performs its tasks, such as running processes, managing hardware devices such as the hard disk , and handling interrupts, in this protected kernel space . In contrast, everything a user does is in user space : writing text in a text editor, running programs in a GUI , etc. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness, as well as preventing malfunctioning application programs from crashing the entire operating system. NOTE:\u9694\u79bb\u5e26\u6765\u5b89\u5168 The kernel's interface is a low-level abstraction layer . When a process makes requests of the kernel, it is called a system call . Kernel designs differ in how they manage these system calls and resources . A monolithic kernel runs all the operating system instructions in the same address space for speed. A microkernel runs most processes in user space, for modularity . A kernel connects the application software to the hardware of a computer. Function of Kernel # \u5173\u4e8e\u5185\u6838\u7684\u529f\u80fd\uff0c\u5728\u672c\u7bc7\u6587\u7ae0\u4e2d\u6ca1\u6709\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u7c7b\u4ecb\u7ecd\uff0c\u6240\u4ee5\u6b64\u5904\u8fdb\u884c\u7701\u7565\uff1b\u5728 Operating system \u7684 Kernel \u7ae0\u8282\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u63a8\u8350\u53bb\u9605\u8bfb\u3002 Kernel design decisions # \u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5728\u8bbe\u8ba1\u4e00\u4e2akernel\u7684\u65f6\u5019\u9700\u8981\u8003\u8651\u54ea\u4e9b\u95ee\u9898\uff0c\u8bfb\u8005\u5e94\u8be5\u5bf9\u8fd9\u4e9b\u95ee\u9898\u6709\u4e9b\u4e86\u89e3\uff0c\u8fd9\u4e9b\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u4e86kernel\u7684\u5b9e\u73b0\u3002 Kernel-wide design approaches # \u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5b9e\u73b0kernel\uff08\u672c\u8d28\u4e0akernel\u662f\u4e00\u4e2asoftware\uff09\u65f6\uff0c\u91c7\u53d6\u600e\u6837\u7684\u8f6f\u4ef6\u67b6\u6784\u3002\u76ee\u524d\u4e3b\u6d41\u7684\u7684\u67b6\u6784\u6709\u4e24\u79cd\u3002\u539f\u6587\u4e2d\u8fd8\u5bf9\u8fd9\u4e24\u79cd\u8f6f\u4ef6\u67b6\u6784\u80cc\u540e\u7684philosophy\u5373 separation of mechanism and policy \u8fdb\u884c\u4e86\u5206\u6790\uff0c\u6211\u8bfb\u5b8c\u4ecd\u7136\u4e00\u5934\u96fe\u6c34\u3002 While monolithic kernels execute all of their code in the same address space ( kernel space ), microkernels try to run most of their services in user space, aiming to improve maintainability and modularity of the codebase. Most kernels do not fit exactly into one of these categories, but are rather found in between these two designs. These are called hybrid kernels . More exotic designs such as nanokernels and exokernels are available, but are seldom used for production systems. The Xen hypervisor, for example, is an exokernel. Monolithic kernels # Main article: Monolithic kernel Diagram of a monolithic kernel In a monolithic kernel, all OS services run along with the main kernel thread, thus also residing in the same memory area. This approach provides rich and powerful hardware access. Some developers, such as UNIX developer Ken Thompson , maintain that it is \"easier to implement a monolithic kernel\" than microkernels. The main disadvantages of monolithic kernels are the dependencies between system components \u2013 a bug in a device driver might crash the entire system \u2013 and the fact that large kernels can become very difficult to maintain. Microkernels # Main article: Microkernel Microkernel (also abbreviated \u03bcK or uK) is the term describing an approach to operating system design by which the functionality of the system is moved out of the traditional \"kernel\", into a set of \"servers\" that communicate through a \"minimal\" kernel, leaving as little as possible in \"system space\" and as much as possible in \"user space\". A microkernel that is designed for a specific platform or device is only ever going to have what it needs to operate. The microkernel approach consists of defining a simple abstraction over the hardware, with a set of primitives or system calls to implement minimal OS services such as memory management , multitasking , and inter-process communication . Other services, including those normally provided by the kernel, such as networking , are implemented in user-space programs, referred to as servers . Microkernels are easier to maintain than monolithic kernels, but the large number of system calls and context switches might slow down the system because they typically generate more overhead than plain function calls.","title":"Kernel"},{"location":"Kernel/Kernel(operating-system)/#kernel-operating-system","text":"\u672c\u8282\u8ba8\u8bbaOS\u7684kernel\uff0c\u672c\u8282\u662f\u5bf9Kernel\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Kernel (operating system) \u3002 The kernel is a computer program that is the core of a computer's operating system , with complete control over everything in the system. On most systems, it is one of the first programs loaded on start-up (after the bootloader ). It handles the rest of start-up as well as input/output requests from software , translating them into data-processing instructions for the central processing unit . It handles memory and peripherals (\u5916\u8bbe) like keyboards, monitors, printers, and speakers. The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application programs or other, less critical parts of the operating system. The kernel performs its tasks, such as running processes, managing hardware devices such as the hard disk , and handling interrupts, in this protected kernel space . In contrast, everything a user does is in user space : writing text in a text editor, running programs in a GUI , etc. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness, as well as preventing malfunctioning application programs from crashing the entire operating system. NOTE:\u9694\u79bb\u5e26\u6765\u5b89\u5168 The kernel's interface is a low-level abstraction layer . When a process makes requests of the kernel, it is called a system call . Kernel designs differ in how they manage these system calls and resources . A monolithic kernel runs all the operating system instructions in the same address space for speed. A microkernel runs most processes in user space, for modularity . A kernel connects the application software to the hardware of a computer.","title":"Kernel (operating system)\u6982\u8ff0"},{"location":"Kernel/Kernel(operating-system)/#function-of-kernel","text":"\u5173\u4e8e\u5185\u6838\u7684\u529f\u80fd\uff0c\u5728\u672c\u7bc7\u6587\u7ae0\u4e2d\u6ca1\u6709\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u7c7b\u4ecb\u7ecd\uff0c\u6240\u4ee5\u6b64\u5904\u8fdb\u884c\u7701\u7565\uff1b\u5728 Operating system \u7684 Kernel \u7ae0\u8282\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u63a8\u8350\u53bb\u9605\u8bfb\u3002","title":"Function of Kernel"},{"location":"Kernel/Kernel(operating-system)/#kernel-design-decisions","text":"\u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5728\u8bbe\u8ba1\u4e00\u4e2akernel\u7684\u65f6\u5019\u9700\u8981\u8003\u8651\u54ea\u4e9b\u95ee\u9898\uff0c\u8bfb\u8005\u5e94\u8be5\u5bf9\u8fd9\u4e9b\u95ee\u9898\u6709\u4e9b\u4e86\u89e3\uff0c\u8fd9\u4e9b\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u4e86kernel\u7684\u5b9e\u73b0\u3002","title":"Kernel design decisions"},{"location":"Kernel/Kernel(operating-system)/#kernel-wide-design-approaches","text":"\u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5b9e\u73b0kernel\uff08\u672c\u8d28\u4e0akernel\u662f\u4e00\u4e2asoftware\uff09\u65f6\uff0c\u91c7\u53d6\u600e\u6837\u7684\u8f6f\u4ef6\u67b6\u6784\u3002\u76ee\u524d\u4e3b\u6d41\u7684\u7684\u67b6\u6784\u6709\u4e24\u79cd\u3002\u539f\u6587\u4e2d\u8fd8\u5bf9\u8fd9\u4e24\u79cd\u8f6f\u4ef6\u67b6\u6784\u80cc\u540e\u7684philosophy\u5373 separation of mechanism and policy \u8fdb\u884c\u4e86\u5206\u6790\uff0c\u6211\u8bfb\u5b8c\u4ecd\u7136\u4e00\u5934\u96fe\u6c34\u3002 While monolithic kernels execute all of their code in the same address space ( kernel space ), microkernels try to run most of their services in user space, aiming to improve maintainability and modularity of the codebase. Most kernels do not fit exactly into one of these categories, but are rather found in between these two designs. These are called hybrid kernels . More exotic designs such as nanokernels and exokernels are available, but are seldom used for production systems. The Xen hypervisor, for example, is an exokernel.","title":"Kernel-wide design approaches"},{"location":"Kernel/Kernel(operating-system)/#monolithic-kernels","text":"Main article: Monolithic kernel Diagram of a monolithic kernel In a monolithic kernel, all OS services run along with the main kernel thread, thus also residing in the same memory area. This approach provides rich and powerful hardware access. Some developers, such as UNIX developer Ken Thompson , maintain that it is \"easier to implement a monolithic kernel\" than microkernels. The main disadvantages of monolithic kernels are the dependencies between system components \u2013 a bug in a device driver might crash the entire system \u2013 and the fact that large kernels can become very difficult to maintain.","title":"Monolithic kernels"},{"location":"Kernel/Kernel(operating-system)/#microkernels","text":"Main article: Microkernel Microkernel (also abbreviated \u03bcK or uK) is the term describing an approach to operating system design by which the functionality of the system is moved out of the traditional \"kernel\", into a set of \"servers\" that communicate through a \"minimal\" kernel, leaving as little as possible in \"system space\" and as much as possible in \"user space\". A microkernel that is designed for a specific platform or device is only ever going to have what it needs to operate. The microkernel approach consists of defining a simple abstraction over the hardware, with a set of primitives or system calls to implement minimal OS services such as memory management , multitasking , and inter-process communication . Other services, including those normally provided by the kernel, such as networking , are implemented in user-space programs, referred to as servers . Microkernels are easier to maintain than monolithic kernels, but the large number of system calls and context switches might slow down the system because they typically generate more overhead than plain function calls.","title":"Microkernels"},{"location":"Kernel/TODO/","text":"20190126linux\u5185\u6838\u662f\u5982\u4f55\u6765\u7ec4\u7ec7process\u7684 # \u4eca\u5929\u5728\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \u8fd9\u7bc7\u6587\u7ae0\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u4e00\u6bb5\u8bdd\uff1a Historically, the Linux kernel has maintained a single process tree. The tree contains a reference to every process currently running in a parent-child hierarchy. A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. \u8fd9\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u7684\u601d\u8003\uff1a\u5728linux\u7684\u5185\u6838\u4e2d\uff0c\u662f\u4f7f\u7528tree\u6765\u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\u7684\u5417\uff1f \u521a\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff0c\u56e0\u4e3a ptree \uff0c\u663e\u7136\u6309\u7167parent-child\u5173\u7cfb\u662f\u53ef\u4ee5\u6309\u7167tree\u7ed3\u6784\u6765\u7ec4\u7ec7process\u7684\uff0c\u4f46\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\u8981\u8fdc\u6bd4\u8fd9\u590d\u6742\uff0c\u56e0\u4e3a\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u6709\uff1a schedule\uff0c\u5373\u8c03\u5ea6\u95ee\u9898\uff0c\u5185\u6838\u9700\u8981\u8fdb\u884c\u9ad8\u6548\u5730\u8c03\u5ea6\uff0c\u6240\u4ee5\u5bf9process\u7684\u7ec4\u7ec7\u5c31\u975e\u5e38\u91cd\u8981 \u4e0b\u9762\u662f\u4e00\u4e9b\u53c2\u8003\u5185\u5bb9\uff1a The Linux Kernel/Processing Traverse the Process Tree Process management Scheduling (computing) Completely Fair Scheduler The Linux process tree","title":"20190126linux\u5185\u6838\u662f\u5982\u4f55\u6765\u7ec4\u7ec7process\u7684"},{"location":"Kernel/TODO/#20190126linuxprocess","text":"\u4eca\u5929\u5728\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \u8fd9\u7bc7\u6587\u7ae0\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u4e00\u6bb5\u8bdd\uff1a Historically, the Linux kernel has maintained a single process tree. The tree contains a reference to every process currently running in a parent-child hierarchy. A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. \u8fd9\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u7684\u601d\u8003\uff1a\u5728linux\u7684\u5185\u6838\u4e2d\uff0c\u662f\u4f7f\u7528tree\u6765\u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\u7684\u5417\uff1f \u521a\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff0c\u56e0\u4e3a ptree \uff0c\u663e\u7136\u6309\u7167parent-child\u5173\u7cfb\u662f\u53ef\u4ee5\u6309\u7167tree\u7ed3\u6784\u6765\u7ec4\u7ec7process\u7684\uff0c\u4f46\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\u8981\u8fdc\u6bd4\u8fd9\u590d\u6742\uff0c\u56e0\u4e3a\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u6709\uff1a schedule\uff0c\u5373\u8c03\u5ea6\u95ee\u9898\uff0c\u5185\u6838\u9700\u8981\u8fdb\u884c\u9ad8\u6548\u5730\u8c03\u5ea6\uff0c\u6240\u4ee5\u5bf9process\u7684\u7ec4\u7ec7\u5c31\u975e\u5e38\u91cd\u8981 \u4e0b\u9762\u662f\u4e00\u4e9b\u53c2\u8003\u5185\u5bb9\uff1a The Linux Kernel/Processing Traverse the Process Tree Process management Scheduling (computing) Completely Fair Scheduler The Linux process tree","title":"20190126linux\u5185\u6838\u662f\u5982\u4f55\u6765\u7ec4\u7ec7process\u7684"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/","text":"\u5173\u4e8e\u672c\u4e66 # Understanding.The.Linux.kernel.3rd.Edition Understanding the Linux Kernel \u5728Architecture\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5efa\u7acb\u8d77\u6765linux OS\u7684\u6574\u4f53architecture\u548c\u5b83\u7684\u4f7f\u547d\u4e86\uff08\u5373\u5b83\u8981\u505a\u54ea\u4e9b\u4e8b\u60c5\uff09\uff0c\u672c\u4e66\u6240\u8981\u505a\u7684\u5c31\u662f\u544a\u8bc9\u8bfb\u8005linux OS\u5982\u4f55\u6765\u5b9e\u73b0\u5b83\u7684\u4f7f\u547d\u7684\u3002","title":"Introduction"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/#_1","text":"Understanding.The.Linux.kernel.3rd.Edition Understanding the Linux Kernel \u5728Architecture\u7ae0\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5efa\u7acb\u8d77\u6765linux OS\u7684\u6574\u4f53architecture\u548c\u5b83\u7684\u4f7f\u547d\u4e86\uff08\u5373\u5b83\u8981\u505a\u54ea\u4e9b\u4e8b\u60c5\uff09\uff0c\u672c\u4e66\u6240\u8981\u505a\u7684\u5c31\u662f\u544a\u8bc9\u8bfb\u8005linux OS\u5982\u4f55\u6765\u5b9e\u73b0\u5b83\u7684\u4f7f\u547d\u7684\u3002","title":"\u5173\u4e8e\u672c\u4e66"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/","text":"1.1. Linux Versus Other Unix-Like Kernels # The various Unix-like systems on the market, some of which have a long history and show signs of archaic\uff08\u53e4\u8001\u7684\uff0c\u9648\u65e7\u7684\uff09 practices, differ in many important respects. All commercial variants were derived from either SVR4 or 4.4BSD, and all tend to agree on some common standards like IEEE's Portable Operating Systems based on Unix (POSIX) and X/Open's Common Applications Environment (CAE). The current standards specify only an application programming interface (API)that is, a well-defined environment in which user programs should run. Therefore, the standards do not impose any restriction on internal design choices of a compliant kernel . [*] [*] As a matter of fact, several non-Unix operating systems, such as Windows NT and its descendents, are POSIX-compliant. NOTE: \u5173\u4e8eUnix-like system\u7684standard\uff0c\u6f14\u8fdb\u5386\u7a0b\uff0c\u53c2\u89c1\u300a Unix-standardization-and-implementation.md \u300b To define a common user interface, Unix-like kernels often share fundamental design ideas and features . In this respect, Linux is comparable with the other Unix-like operating systems. Reading this book and studying the Linux kernel, therefore, may help you understand the other Unix variants, too. The 2.6 version of the Linux kernel aims to be compliant with the IEEE POSIX standard. This, of course, means that most existing Unix programs can be compiled and executed on a Linux system with very little effort or even without the need for patches to the source code. Moreover, Linux includes all the features of a modern Unix operating system, such as virtual memory , a virtual filesystem , lightweight processes , Unix signals , SVR4 interprocess communications , support for Symmetric Multiprocessor (SMP) systems , and so on. When Linus Torvalds wrote the first kernel, he referred to some classical books on Unix internals, like Maurice Bach's The Design of the Unix Operating System (Prentice Hall, 1986). Actually, Linux still has some bias toward the Unix baseline described in Bach's book (i.e., SVR2). However, Linux doesn't stick to any particular variant. Instead, it tries to adopt the best features and design choices of several different Unix kernels. The following list describes how Linux competes against some well-known commercial Unix kernels: Monolithic kernel # It is a large, complex do-it-yourself program, composed of several logically different components. In this, it is quite conventional; most commercial Unix variants are monolithic. (Notable exceptions are the Apple Mac OS X and the GNU Hurd operating systems, both derived from the Carnegie-Mellon's Mach, which follow a microkernel approach.) NOTE: See also Monolithic kernel Compiled and statically linked traditional Unix kernels # Most modern kernels can dynamically load and unload some portions of the kernel code (typically, device drivers ), which are usually called modules . Linux's support for modules is very good, because it is able to automatically load and unload modules on demand. Among the main commercial Unix variants, only the SVR4.2 and Solaris kernels have a similar feature. NOTE: \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbamodule\u7684\u7ae0\u8282\uff1a 1.4.4. Kernel Architecture \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbadevice driver\u7684\u7ae0\u8282\uff1a 1.6.9. Device Drivers See also Loadable kernel module Kernel threading # Some Unix kernels, such as Solaris and SVR4.2/MP, are organized as a set of kernel threads .A kernel thread is an execution context that can be independently scheduled; it may be associated with a user program, or it may run only some kernel functions. Context switches between kernel threads are usually much less expensive than context switches between ordinary processes, because the former usually operate on a common address space. Linux uses kernel threads in a very limited way to execute a few kernel functions periodically; however, they do not represent the basic execution context abstraction. (That's the topic of the next item.) NOTE: \u672c\u4e66\u8ba8\u8bbakernel thread\u7684\u7ae0\u8282\uff1a 3.4.2. Kernel Threads Multithreaded application support # Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes, which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads , Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. NOTE: See also Light-weight process \u9605\u8bfb\u4ee5\u4e0b clone( ) system call\u7684\u6587\u6863\u6709\u52a9\u4e8e\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u7684\u542b\u4e49\u3002 NOTE: LWP VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy Preemptive kernel # When compiled with the \"Preemptible Kernel\" option, Linux 2.6 can arbitrarily interleave execution flows while they are in privileged mode . Besides Linux 2.6, a few other conventional, general-purpose Unix systems, such as Solaris and Mach 3.0 , are fully preemptive kernels . SVR4.2/MP introduces some fixed preemption points as a method to get limited preemption capability. NOTE: See also Preemption (computing) Kernel preemption Multiprocessor support # Several Unix kernel variants take advantage of multiprocessor systems. Linux 2.6 supports symmetric multiprocessing (SMP ) for different memory models, including NUMA: the system can use multiple processors and each processor can handle any task there is no discrimination among them. Although a few parts of the kernel code are still serialized by means of a single \" big kernel lock ,\" it is fair to say that Linux 2.6 makes a near optimal use of SMP. NOTE: Giant-lock Filesystem # Linux's standard filesystems come in many flavors. You can use the plain old Ext2 filesystem if you don't have specific needs. You might switch to Ext3 if you want to avoid lengthy filesystem checks after a system crash. If you'll have to deal with many small files, the ReiserFS filesystem is likely to be the best choice. Besides Ext3 and ReiserFS, several other journaling filesystems can be used in Linux; they include IBM AIX's Journaling File System (JFS ) and Silicon Graphics IRIX 's XFS filesystem. Thanks to a powerful object-oriented Virtual File System technology (inspired by Solaris and SVR4), porting a foreign filesystem to Linux is generally easier than porting to other kernels. STREAMS # Linux has no analog to the STREAMS I/O subsystem introduced in SVR4, although it is included now in most Unix kernels and has become the preferred interface for writing device drivers, terminal drivers, and network protocols.","title":"1.1-Linux-Versus-Other-Unix-Like-Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#11-linux-versus-other-unix-like-kernels","text":"The various Unix-like systems on the market, some of which have a long history and show signs of archaic\uff08\u53e4\u8001\u7684\uff0c\u9648\u65e7\u7684\uff09 practices, differ in many important respects. All commercial variants were derived from either SVR4 or 4.4BSD, and all tend to agree on some common standards like IEEE's Portable Operating Systems based on Unix (POSIX) and X/Open's Common Applications Environment (CAE). The current standards specify only an application programming interface (API)that is, a well-defined environment in which user programs should run. Therefore, the standards do not impose any restriction on internal design choices of a compliant kernel . [*] [*] As a matter of fact, several non-Unix operating systems, such as Windows NT and its descendents, are POSIX-compliant. NOTE: \u5173\u4e8eUnix-like system\u7684standard\uff0c\u6f14\u8fdb\u5386\u7a0b\uff0c\u53c2\u89c1\u300a Unix-standardization-and-implementation.md \u300b To define a common user interface, Unix-like kernels often share fundamental design ideas and features . In this respect, Linux is comparable with the other Unix-like operating systems. Reading this book and studying the Linux kernel, therefore, may help you understand the other Unix variants, too. The 2.6 version of the Linux kernel aims to be compliant with the IEEE POSIX standard. This, of course, means that most existing Unix programs can be compiled and executed on a Linux system with very little effort or even without the need for patches to the source code. Moreover, Linux includes all the features of a modern Unix operating system, such as virtual memory , a virtual filesystem , lightweight processes , Unix signals , SVR4 interprocess communications , support for Symmetric Multiprocessor (SMP) systems , and so on. When Linus Torvalds wrote the first kernel, he referred to some classical books on Unix internals, like Maurice Bach's The Design of the Unix Operating System (Prentice Hall, 1986). Actually, Linux still has some bias toward the Unix baseline described in Bach's book (i.e., SVR2). However, Linux doesn't stick to any particular variant. Instead, it tries to adopt the best features and design choices of several different Unix kernels. The following list describes how Linux competes against some well-known commercial Unix kernels:","title":"1.1. Linux Versus Other Unix-Like Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#monolithic-kernel","text":"It is a large, complex do-it-yourself program, composed of several logically different components. In this, it is quite conventional; most commercial Unix variants are monolithic. (Notable exceptions are the Apple Mac OS X and the GNU Hurd operating systems, both derived from the Carnegie-Mellon's Mach, which follow a microkernel approach.) NOTE: See also Monolithic kernel","title":"Monolithic kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#compiled-and-statically-linked-traditional-unix-kernels","text":"Most modern kernels can dynamically load and unload some portions of the kernel code (typically, device drivers ), which are usually called modules . Linux's support for modules is very good, because it is able to automatically load and unload modules on demand. Among the main commercial Unix variants, only the SVR4.2 and Solaris kernels have a similar feature. NOTE: \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbamodule\u7684\u7ae0\u8282\uff1a 1.4.4. Kernel Architecture \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbadevice driver\u7684\u7ae0\u8282\uff1a 1.6.9. Device Drivers See also Loadable kernel module","title":"Compiled and statically linked traditional Unix kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#kernel-threading","text":"Some Unix kernels, such as Solaris and SVR4.2/MP, are organized as a set of kernel threads .A kernel thread is an execution context that can be independently scheduled; it may be associated with a user program, or it may run only some kernel functions. Context switches between kernel threads are usually much less expensive than context switches between ordinary processes, because the former usually operate on a common address space. Linux uses kernel threads in a very limited way to execute a few kernel functions periodically; however, they do not represent the basic execution context abstraction. (That's the topic of the next item.) NOTE: \u672c\u4e66\u8ba8\u8bbakernel thread\u7684\u7ae0\u8282\uff1a 3.4.2. Kernel Threads","title":"Kernel threading"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#multithreaded-application-support","text":"Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes, which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads , Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. NOTE: See also Light-weight process \u9605\u8bfb\u4ee5\u4e0b clone( ) system call\u7684\u6587\u6863\u6709\u52a9\u4e8e\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u7684\u542b\u4e49\u3002 NOTE: LWP VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy","title":"Multithreaded application support"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#preemptive-kernel","text":"When compiled with the \"Preemptible Kernel\" option, Linux 2.6 can arbitrarily interleave execution flows while they are in privileged mode . Besides Linux 2.6, a few other conventional, general-purpose Unix systems, such as Solaris and Mach 3.0 , are fully preemptive kernels . SVR4.2/MP introduces some fixed preemption points as a method to get limited preemption capability. NOTE: See also Preemption (computing) Kernel preemption","title":"Preemptive kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#multiprocessor-support","text":"Several Unix kernel variants take advantage of multiprocessor systems. Linux 2.6 supports symmetric multiprocessing (SMP ) for different memory models, including NUMA: the system can use multiple processors and each processor can handle any task there is no discrimination among them. Although a few parts of the kernel code are still serialized by means of a single \" big kernel lock ,\" it is fair to say that Linux 2.6 makes a near optimal use of SMP. NOTE: Giant-lock","title":"Multiprocessor support"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#filesystem","text":"Linux's standard filesystems come in many flavors. You can use the plain old Ext2 filesystem if you don't have specific needs. You might switch to Ext3 if you want to avoid lengthy filesystem checks after a system crash. If you'll have to deal with many small files, the ReiserFS filesystem is likely to be the best choice. Besides Ext3 and ReiserFS, several other journaling filesystems can be used in Linux; they include IBM AIX's Journaling File System (JFS ) and Silicon Graphics IRIX 's XFS filesystem. Thanks to a powerful object-oriented Virtual File System technology (inspired by Solaris and SVR4), porting a foreign filesystem to Linux is generally easier than porting to other kernels.","title":"Filesystem"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#streams","text":"Linux has no analog to the STREAMS I/O subsystem introduced in SVR4, although it is included now in most Unix kernels and has become the preferred interface for writing device drivers, terminal drivers, and network protocols.","title":"STREAMS"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.2-Hardware-Dependency/","text":"1.2. Hardware Dependency # Linux tries to maintain a neat distinction between hardware-dependent and hardware-independent source code. To that end, both the arch and the include directories include 23 subdirectories that correspond to the different types of hardware platforms supported. The standard names of the platforms are: arm, arm26 ARM processor-based computers such as PDAs and embedded devices i386 IBM-compatible personal computers based on 80x86 microprocessors ia64 Workstations based on the Intel 64-bit Itanium microprocessor mips Workstations based on MIPS microprocessors, such as those marketed by Silicon Graphics x86_64 Workstations based on the AMD's 64-bit microprocessorssuch Athlon and Opteron and Intel's ia32e/EM64T 64-bit microprocessors","title":"1.2-Hardware-Dependency"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.2-Hardware-Dependency/#12-hardware-dependency","text":"Linux tries to maintain a neat distinction between hardware-dependent and hardware-independent source code. To that end, both the arch and the include directories include 23 subdirectories that correspond to the different types of hardware platforms supported. The standard names of the platforms are: arm, arm26 ARM processor-based computers such as PDAs and embedded devices i386 IBM-compatible personal computers based on 80x86 microprocessors ia64 Workstations based on the Intel 64-bit Itanium microprocessor mips Workstations based on MIPS microprocessors, such as those marketed by Silicon Graphics x86_64 Workstations based on the AMD's 64-bit microprocessorssuch Athlon and Opteron and Intel's ia32e/EM64T 64-bit microprocessors","title":"1.2. Hardware Dependency"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/","text":"1.4. Basic Operating System Concepts # Each computer system includes a basic set of programs called the operating system . The most important program in the set is called the kernel . It is loaded into RAM when the system boots and contains many critical procedures that are needed for the system to operate. The other programs are less crucial utilities; they can provide a wide variety of interactive experiences for the user as well as doing all the jobs the user bought the computer for but the essential shape and capabilities of the system are determined by the kernel . The kernel provides key facilities to everything else on the system and determines many of the characteristics of higher software. Hence, we often use the term \"operating system\" as a synonym for \"kernel.\" NOTE: See also Operating system Kernel (operating system) The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). Some operating systems allow all user programs to directly play with the hardware components (a typical example is MS-DOS ). In contrast, a Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. When a program wants to use a hardware resource, it must issue a request to the operating system. The kernel evaluates the request and, if it chooses to grant the resource, interacts with the proper hardware components on behalf of the user program. To enforce this mechanism, modern operating systems rely on the availability of specific hardware features that forbid user programs to directly interact with low-level hardware components or to access arbitrary memory locations. In particular, the hardware introduces at least two different execution modes for the CPU: a nonprivileged mode for user programs and a privileged mode for the kernel. Unix calls these User Mode and Kernel Mode , respectively. NOTE: See also User space CPU modes Protection ring In the rest of this chapter, we introduce the basic concepts that have motivated the design of Unix over the past two decades, as well as Linux and other operating systems. While the concepts are probably familiar to you as a Linux user, these sections try to delve into them a bit more deeply than usual to explain the requirements they place on an operating system kernel. These broad considerations refer to virtually all Unix-like systems. The other chapters of this book will hopefully help you understand the Linux kernel internals. 1.4.1. Multiuser Systems # A multiuser system is a computer that is able to concurrently and independently execute several applications belonging to two or more users . Concurrently means that applications can be active at the same time and contend for the various resources such as CPU, memory, hard disks, and so on. Independently means that each application can perform its task with no concern for what the applications of the other users are doing. Switching from one application to another, of course, slows down each of them and affects the response time seen by the users. Many of the complexities of modern operating system kernels, which we will examine in this book, are present to minimize the delays enforced on each program and to provide the user with responses that are as fast as possible. Multiuser operating systems must include several features: An authentication mechanism for verifying the user's identity A protection mechanism against buggy user programs that could block other applications running in the system A protection mechanism against malicious user programs that could interfere with or spy on the activity of other users An accounting mechanism that limits the amount of resource units assigned to each user To ensure safe protection mechanisms , operating systems must use the hardware protection associated with the CPU privileged mode . Otherwise, a user program would be able to directly access the system circuitry and overcome the imposed bounds. Unix is a multiuser system that enforces the hardware protection of system resources . 1.4.2. Users and Groups # In a multiuser system , each user has a private space on the machine; typically, he owns some quota of the disk space to store files, receives private mail messages, and so on. The operating system must ensure that the private portion of a user space is visible only to its owner. In particular, it must ensure that no user can exploit a system application for the purpose of violating the private space of another user. All users are identified by a unique number called the User ID , or UID . Usually only a restricted number of persons are allowed to make use of a computer system. When one of these users starts a working session, the system asks for a login name and a password. If the user does not input a valid pair, the system denies access. Because the password is assumed to be secret, the user's privacy is ensured. To selectively share material with other users, each user is a member of one or more user groups , which are identified by a unique number called a user group ID . Each file is associated with exactly one group. For example, access can be set so the user owning the file has read and write privileges, the group has read-only privileges, and other users on the system are denied access to the file. Any Unix-like operating system has a special user called root or superuser . The system administrator must log in as root to handle user accounts, perform maintenance tasks such as system backups and program upgrades, and so on. The root user can do almost everything, because the operating system does not apply the usual protection mechanisms to her. In particular, the root user can access every file on the system and can manipulate every running user program. 1.4.3. Processes # NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process All operating systems use one fundamental abstraction: the process . A process can be defined either as \"an instance of a program in execution\" or as the \"execution context\" of a running program. In traditional operating systems, a process executes a single sequence of instructions in an address space ; the address space is the set of memory addresses that the process is allowed to reference. Modern operating systems allow processes with multiple execution flows that is, multiple sequences of instructions executed in the same address space . NOTE: \u6bcf\u4e2aexecution flow\u5bf9\u5e94\u7684\u662f\u4e00\u4e2athread Multiuser systems must enforce an execution environment in which several processes can be active concurrently and contend for system resources, mainly the CPU. Systems that allow concurrent active processes are said to be multiprogramming or multiprocessing . [*] It is important to distinguish programs from processes; several processes can execute the same program concurrently, while the same process can execute several programs sequentially. [*] Some multiprocessing operating systems are not multiuser; an example is Microsoft Windows 98. On uniprocessor systems, just one process can hold the CPU, and hence just one execution flow can progress at a time. In general, the number of CPUs is always restricted, and therefore only a few processes can progress at once. An operating system component called the scheduler chooses the process that can progress. Some operating systems allow only nonpreemptable processes, which means that the scheduler is invoked only when a process voluntarily relinquishes the CPU. But processes of a multiuser system must be preemptable; the operating system tracks how long each process holds the CPU and periodically activates the scheduler. NOTE: See also Single-tasking and multi-tasking Preemption (computing) Unix is a multiprocessing operating system with preemptable processes . Unix-like operating systems adopt a process/kernel model . Each process has the illusion that it's the only process on the machine, and it has exclusive access to the operating system services. Whenever a process makes a system call (i.e., a request to the kernel, see Chapter 10), the hardware changes the privilege mode from User Mode to Kernel Mode , and the process starts the execution of a kernel procedure with a strictly limited purpose. In this way, the operating system acts within the execution context of the process in order to satisfy its request. Whenever the request is fully satisfied, the kernel procedure forces the hardware to return to User Mode and the process continues its execution from the instruction following the system call. NOTE: process/kernel model \u4f1a\u57281.6.1. The Process/Kernel Model\u8282\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002 1.4.4. Kernel Architecture # As stated before, most Unix kernels are monolithic: each kernel layer is integrated into the whole kernel program and runs in Kernel Mode on behalf of the current process . In contrast, microkernel operating systems demand a very small set of functions from the kernel, generally including a few synchronization primitives , a simple scheduler , and an interprocess communication mechanism . Several system processes that run on top of the microkernel implement other operating system-layer functions , like memory allocators , device drivers , and system call handlers . Although academic research on operating systems is oriented toward microkernels , such operating systems are generally slower than monolithic ones, because the explicit message passing between the different layers of the operating system has a cost. However, microkernel operating systems might have some theoretical advantages over monolithic ones. Microkernels force the system programmers to adopt a modularized approach, because each operating system layer is a relatively independent program that must interact with the other layers through well-defined and clean software interfaces. Moreover, an existing microkernel operating system can be easily ported to other architectures fairly easily, because all hardware-dependent components are generally encapsulated in the microkernel code. Finally, microkernel operating systems tend to make better use of random access memory (RAM) than monolithic ones, because system processes that aren't implementing needed functionalities might be swapped out or destroyed. To achieve many of the theoretical advantages of microkernels without introducing performance penalties, the Linux kernel offers modules . A module is an object file whose code can be linked to (and unlinked from) the kernel at runtime. The object code usually consists of a set of functions that implements a filesystem, a device driver, or other features at the kernel's upper layer. The module, unlike the external layers of microkernel operating systems, does not run as a specific process. Instead, it is executed in Kernel Mode on behalf of the current process, like any other statically linked kernel function. The main advantages of using modules include: modularized approach Because any module can be linked and unlinked at runtime, system programmers must introduce well-defined software interfaces to access the data structures handled by modules. This makes it easy to develop new modules. Platform independence Even if it may rely on some specific hardware features, a module doesn't depend on a fixed hardware platform. For example, a disk driver module that relies on the SCSI standard works as well on an IBM-compatible PC as it does on Hewlett-Packard's Alpha. Frugal main memory usage A module can be linked to the running kernel when its functionality is required and unlinked when it is no longer useful; this is quite useful for small embedded systems. No performance penalty Once linked in, the object code of a module is equivalent to the object code of the statically linked kernel. Therefore, no explicit message passing is required when the functions of the module are invoked. [*] [*] A small performance penalty occurs when the module is linked and unlinked. However, this penalty can be compared to the penalty caused by the creation and deletion of system processes in microkernel operating systems.","title":"1.4-Basic-Operating-System-Concepts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#14-basic-operating-system-concepts","text":"Each computer system includes a basic set of programs called the operating system . The most important program in the set is called the kernel . It is loaded into RAM when the system boots and contains many critical procedures that are needed for the system to operate. The other programs are less crucial utilities; they can provide a wide variety of interactive experiences for the user as well as doing all the jobs the user bought the computer for but the essential shape and capabilities of the system are determined by the kernel . The kernel provides key facilities to everything else on the system and determines many of the characteristics of higher software. Hence, we often use the term \"operating system\" as a synonym for \"kernel.\" NOTE: See also Operating system Kernel (operating system) The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). Some operating systems allow all user programs to directly play with the hardware components (a typical example is MS-DOS ). In contrast, a Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. When a program wants to use a hardware resource, it must issue a request to the operating system. The kernel evaluates the request and, if it chooses to grant the resource, interacts with the proper hardware components on behalf of the user program. To enforce this mechanism, modern operating systems rely on the availability of specific hardware features that forbid user programs to directly interact with low-level hardware components or to access arbitrary memory locations. In particular, the hardware introduces at least two different execution modes for the CPU: a nonprivileged mode for user programs and a privileged mode for the kernel. Unix calls these User Mode and Kernel Mode , respectively. NOTE: See also User space CPU modes Protection ring In the rest of this chapter, we introduce the basic concepts that have motivated the design of Unix over the past two decades, as well as Linux and other operating systems. While the concepts are probably familiar to you as a Linux user, these sections try to delve into them a bit more deeply than usual to explain the requirements they place on an operating system kernel. These broad considerations refer to virtually all Unix-like systems. The other chapters of this book will hopefully help you understand the Linux kernel internals.","title":"1.4. Basic Operating System Concepts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#141-multiuser-systems","text":"A multiuser system is a computer that is able to concurrently and independently execute several applications belonging to two or more users . Concurrently means that applications can be active at the same time and contend for the various resources such as CPU, memory, hard disks, and so on. Independently means that each application can perform its task with no concern for what the applications of the other users are doing. Switching from one application to another, of course, slows down each of them and affects the response time seen by the users. Many of the complexities of modern operating system kernels, which we will examine in this book, are present to minimize the delays enforced on each program and to provide the user with responses that are as fast as possible. Multiuser operating systems must include several features: An authentication mechanism for verifying the user's identity A protection mechanism against buggy user programs that could block other applications running in the system A protection mechanism against malicious user programs that could interfere with or spy on the activity of other users An accounting mechanism that limits the amount of resource units assigned to each user To ensure safe protection mechanisms , operating systems must use the hardware protection associated with the CPU privileged mode . Otherwise, a user program would be able to directly access the system circuitry and overcome the imposed bounds. Unix is a multiuser system that enforces the hardware protection of system resources .","title":"1.4.1. Multiuser Systems"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#142-users-and-groups","text":"In a multiuser system , each user has a private space on the machine; typically, he owns some quota of the disk space to store files, receives private mail messages, and so on. The operating system must ensure that the private portion of a user space is visible only to its owner. In particular, it must ensure that no user can exploit a system application for the purpose of violating the private space of another user. All users are identified by a unique number called the User ID , or UID . Usually only a restricted number of persons are allowed to make use of a computer system. When one of these users starts a working session, the system asks for a login name and a password. If the user does not input a valid pair, the system denies access. Because the password is assumed to be secret, the user's privacy is ensured. To selectively share material with other users, each user is a member of one or more user groups , which are identified by a unique number called a user group ID . Each file is associated with exactly one group. For example, access can be set so the user owning the file has read and write privileges, the group has read-only privileges, and other users on the system are denied access to the file. Any Unix-like operating system has a special user called root or superuser . The system administrator must log in as root to handle user accounts, perform maintenance tasks such as system backups and program upgrades, and so on. The root user can do almost everything, because the operating system does not apply the usual protection mechanisms to her. In particular, the root user can access every file on the system and can manipulate every running user program.","title":"1.4.2. Users and Groups"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#143-processes","text":"NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process All operating systems use one fundamental abstraction: the process . A process can be defined either as \"an instance of a program in execution\" or as the \"execution context\" of a running program. In traditional operating systems, a process executes a single sequence of instructions in an address space ; the address space is the set of memory addresses that the process is allowed to reference. Modern operating systems allow processes with multiple execution flows that is, multiple sequences of instructions executed in the same address space . NOTE: \u6bcf\u4e2aexecution flow\u5bf9\u5e94\u7684\u662f\u4e00\u4e2athread Multiuser systems must enforce an execution environment in which several processes can be active concurrently and contend for system resources, mainly the CPU. Systems that allow concurrent active processes are said to be multiprogramming or multiprocessing . [*] It is important to distinguish programs from processes; several processes can execute the same program concurrently, while the same process can execute several programs sequentially. [*] Some multiprocessing operating systems are not multiuser; an example is Microsoft Windows 98. On uniprocessor systems, just one process can hold the CPU, and hence just one execution flow can progress at a time. In general, the number of CPUs is always restricted, and therefore only a few processes can progress at once. An operating system component called the scheduler chooses the process that can progress. Some operating systems allow only nonpreemptable processes, which means that the scheduler is invoked only when a process voluntarily relinquishes the CPU. But processes of a multiuser system must be preemptable; the operating system tracks how long each process holds the CPU and periodically activates the scheduler. NOTE: See also Single-tasking and multi-tasking Preemption (computing) Unix is a multiprocessing operating system with preemptable processes . Unix-like operating systems adopt a process/kernel model . Each process has the illusion that it's the only process on the machine, and it has exclusive access to the operating system services. Whenever a process makes a system call (i.e., a request to the kernel, see Chapter 10), the hardware changes the privilege mode from User Mode to Kernel Mode , and the process starts the execution of a kernel procedure with a strictly limited purpose. In this way, the operating system acts within the execution context of the process in order to satisfy its request. Whenever the request is fully satisfied, the kernel procedure forces the hardware to return to User Mode and the process continues its execution from the instruction following the system call. NOTE: process/kernel model \u4f1a\u57281.6.1. The Process/Kernel Model\u8282\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"1.4.3. Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#144-kernel-architecture","text":"As stated before, most Unix kernels are monolithic: each kernel layer is integrated into the whole kernel program and runs in Kernel Mode on behalf of the current process . In contrast, microkernel operating systems demand a very small set of functions from the kernel, generally including a few synchronization primitives , a simple scheduler , and an interprocess communication mechanism . Several system processes that run on top of the microkernel implement other operating system-layer functions , like memory allocators , device drivers , and system call handlers . Although academic research on operating systems is oriented toward microkernels , such operating systems are generally slower than monolithic ones, because the explicit message passing between the different layers of the operating system has a cost. However, microkernel operating systems might have some theoretical advantages over monolithic ones. Microkernels force the system programmers to adopt a modularized approach, because each operating system layer is a relatively independent program that must interact with the other layers through well-defined and clean software interfaces. Moreover, an existing microkernel operating system can be easily ported to other architectures fairly easily, because all hardware-dependent components are generally encapsulated in the microkernel code. Finally, microkernel operating systems tend to make better use of random access memory (RAM) than monolithic ones, because system processes that aren't implementing needed functionalities might be swapped out or destroyed. To achieve many of the theoretical advantages of microkernels without introducing performance penalties, the Linux kernel offers modules . A module is an object file whose code can be linked to (and unlinked from) the kernel at runtime. The object code usually consists of a set of functions that implements a filesystem, a device driver, or other features at the kernel's upper layer. The module, unlike the external layers of microkernel operating systems, does not run as a specific process. Instead, it is executed in Kernel Mode on behalf of the current process, like any other statically linked kernel function. The main advantages of using modules include: modularized approach Because any module can be linked and unlinked at runtime, system programmers must introduce well-defined software interfaces to access the data structures handled by modules. This makes it easy to develop new modules. Platform independence Even if it may rely on some specific hardware features, a module doesn't depend on a fixed hardware platform. For example, a disk driver module that relies on the SCSI standard works as well on an IBM-compatible PC as it does on Hewlett-Packard's Alpha. Frugal main memory usage A module can be linked to the running kernel when its functionality is required and unlinked when it is no longer useful; this is quite useful for small embedded systems. No performance penalty Once linked in, the object code of a module is equivalent to the object code of the statically linked kernel. Therefore, no explicit message passing is required when the functions of the module are invoked. [*] [*] A small performance penalty occurs when the module is linked and unlinked. However, this penalty can be compared to the penalty caused by the creation and deletion of system processes in microkernel operating systems.","title":"1.4.4. Kernel Architecture"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6-An-Overview-of-Unix-Kernels/","text":"1.6. An Overview of Unix Kernels 1.6. An Overview of Unix Kernels # Unix kernels provide an execution environment in which applications may run. Therefore, the kernel must implement a set of services and corresponding interfaces . Applications use those interfaces and do not usually interact directly with hardware resources .","title":"1.6-An-Overview-of-Unix-Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6-An-Overview-of-Unix-Kernels/#16-an-overview-of-unix-kernels","text":"Unix kernels provide an execution environment in which applications may run. Therefore, the kernel must implement a set of services and corresponding interfaces . Applications use those interfaces and do not usually interact directly with hardware resources .","title":"1.6. An Overview of Unix Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.1-The-Process-Kernel-Model/","text":"1.6.1. The Process/Kernel Model # When a program is executed in User Mode , it cannot directly access the kernel data structures or the kernel programs . When an application executes in Kernel Mode , however, these restrictions no longer apply. Each CPU model provides special instructions to switch from User Mode to Kernel Mode and vice versa. A program usually executes in User Mode and switches to Kernel Mode only when requesting a service provided by the kernel. When the kernel has satisfied the program's request, it puts the program back in User Mode . NOTE: See also User space CPU modes Protection ring Processes are dynamic entities that usually have a limited life span within the system. The task of creating, eliminating, and synchronizing the existing processes is delegated to a group of routines in the kernel. The kernel itself is not a process but a process manager . The process/kernel model assumes that processes that require a kernel service use specific programming constructs called system calls . Each system call sets up the group of parameters that identifies the process request and then executes the hardware-dependent CPU instruction to switch from User Mode to Kernel Mode . NOTE: See also System call Besides user processes, Unix systems include a few privileged processes called kernel threads with the following characteristics: They run in Kernel Mode in the kernel address space. They do not interact with users, and thus do not require terminal devices. They are usually created during system startup and remain alive until the system is shut down. NOTE : What is a Kernel thread? Understanding Kernel Threads On a uniprocessor system, only one process is running at a time, and it may run either in User or in Kernel Mode. If it runs in Kernel Mode , the processor is executing some kernel routine . Figure 1-2 illustrates examples of transitions between User and Kernel Mode. Process 1 in User Mode issues a system call , after which the process switches to Kernel Mode , and the system call is serviced. Process 1 then resumes execution in User Mode until a timer interrupt occurs, and the scheduler is activated in Kernel Mode . A process switch takes place, and Process 2 starts its execution in User Mode until a hardware device raises an interrupt. As a consequence of the interrupt, Process 2 switches to Kernel Mode and services the interrupt. Unix kernels do much more than handle system calls ; in fact, kernel routines can be activated in several ways: A process invokes a system call . The CPU executing the process signals an exception , which is an unusual condition such as an invalid instruction. The kernel handles the exception on behalf of the process that caused it. A peripheral device issues an interrupt signal to the CPU to notify it of an event such as a request for attention, a status change, or the completion of an I/O operation. Each interrupt signal is dealt by a kernel program called an interrupt handler . Because peripheral devices operate asynchronously with respect to the CPU, interrupts occur at unpredictable times. A kernel thread is executed. Because it runs in Kernel Mode, the corresponding program must be considered part of the kernel.","title":"1.6.1-The-Process-Kernel-Model"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.1-The-Process-Kernel-Model/#161-the-processkernel-model","text":"When a program is executed in User Mode , it cannot directly access the kernel data structures or the kernel programs . When an application executes in Kernel Mode , however, these restrictions no longer apply. Each CPU model provides special instructions to switch from User Mode to Kernel Mode and vice versa. A program usually executes in User Mode and switches to Kernel Mode only when requesting a service provided by the kernel. When the kernel has satisfied the program's request, it puts the program back in User Mode . NOTE: See also User space CPU modes Protection ring Processes are dynamic entities that usually have a limited life span within the system. The task of creating, eliminating, and synchronizing the existing processes is delegated to a group of routines in the kernel. The kernel itself is not a process but a process manager . The process/kernel model assumes that processes that require a kernel service use specific programming constructs called system calls . Each system call sets up the group of parameters that identifies the process request and then executes the hardware-dependent CPU instruction to switch from User Mode to Kernel Mode . NOTE: See also System call Besides user processes, Unix systems include a few privileged processes called kernel threads with the following characteristics: They run in Kernel Mode in the kernel address space. They do not interact with users, and thus do not require terminal devices. They are usually created during system startup and remain alive until the system is shut down. NOTE : What is a Kernel thread? Understanding Kernel Threads On a uniprocessor system, only one process is running at a time, and it may run either in User or in Kernel Mode. If it runs in Kernel Mode , the processor is executing some kernel routine . Figure 1-2 illustrates examples of transitions between User and Kernel Mode. Process 1 in User Mode issues a system call , after which the process switches to Kernel Mode , and the system call is serviced. Process 1 then resumes execution in User Mode until a timer interrupt occurs, and the scheduler is activated in Kernel Mode . A process switch takes place, and Process 2 starts its execution in User Mode until a hardware device raises an interrupt. As a consequence of the interrupt, Process 2 switches to Kernel Mode and services the interrupt. Unix kernels do much more than handle system calls ; in fact, kernel routines can be activated in several ways: A process invokes a system call . The CPU executing the process signals an exception , which is an unusual condition such as an invalid instruction. The kernel handles the exception on behalf of the process that caused it. A peripheral device issues an interrupt signal to the CPU to notify it of an event such as a request for attention, a status change, or the completion of an I/O operation. Each interrupt signal is dealt by a kernel program called an interrupt handler . Because peripheral devices operate asynchronously with respect to the CPU, interrupts occur at unpredictable times. A kernel thread is executed. Because it runs in Kernel Mode, the corresponding program must be considered part of the kernel.","title":"1.6.1. The Process/Kernel Model"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.2-Process-Implementation/","text":"1.6.2. Process Implementation # To let the kernel manage processes, each process is represented by a process descriptor that includes information about the current state of the process. NOTE: process descriptor \u57283.2. Process Descriptor\u4e2d\u8fdb\u884c\u4e13\u95e8\u4ecb\u7ecd \u672c\u8282\u4e2d\u7684process\u6240\u6307\u4e3alightweight process\uff0c\u800c\u4e0d\u662f\u6807\u51c6\u7684 Process (computing) When the kernel stops the execution of a process, it saves the current contents of several processor registers in the process descriptor. These include: The program counter (PC) and stack pointer (SP) registers The general purpose registers The floating point registers The processor control registers (Processor Status Word) containing information about the CPU state The memory management registers used to keep track of the RAM accessed by the process NOTE: See also Program counter Stack register Processor register When the kernel decides to resume executing a process, it uses the proper process descriptor fields to load the CPU registers. Because the stored value of the program counter points to the instruction following the last instruction executed, the process resumes execution at the point where it was stopped. When a process is not executing on the CPU, it is waiting for some event. Unix kernels distinguish many wait states , which are usually implemented by queues of process descriptors ; each (possibly empty) queue corresponds to the set of processes waiting for a specific event. NOTE: \u53c2\u89c13.2.4. How Processes Are Organized","title":"1.6.2-Process-Implementation"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.2-Process-Implementation/#162-process-implementation","text":"To let the kernel manage processes, each process is represented by a process descriptor that includes information about the current state of the process. NOTE: process descriptor \u57283.2. Process Descriptor\u4e2d\u8fdb\u884c\u4e13\u95e8\u4ecb\u7ecd \u672c\u8282\u4e2d\u7684process\u6240\u6307\u4e3alightweight process\uff0c\u800c\u4e0d\u662f\u6807\u51c6\u7684 Process (computing) When the kernel stops the execution of a process, it saves the current contents of several processor registers in the process descriptor. These include: The program counter (PC) and stack pointer (SP) registers The general purpose registers The floating point registers The processor control registers (Processor Status Word) containing information about the CPU state The memory management registers used to keep track of the RAM accessed by the process NOTE: See also Program counter Stack register Processor register When the kernel decides to resume executing a process, it uses the proper process descriptor fields to load the CPU registers. Because the stored value of the program counter points to the instruction following the last instruction executed, the process resumes execution at the point where it was stopped. When a process is not executing on the CPU, it is waiting for some event. Unix kernels distinguish many wait states , which are usually implemented by queues of process descriptors ; each (possibly empty) queue corresponds to the set of processes waiting for a specific event. NOTE: \u53c2\u89c13.2.4. How Processes Are Organized","title":"1.6.2. Process Implementation"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.3-Reentrant-Kernels/","text":"1.6.3. Reentrant Kernels # NOTE: \u4ece\u4e00\u4e2a\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u672c\u8282\u7684\u5185\u5bb9\uff0c\u5c06\u66f4\u52a0\u5bb9\u6613\u638c\u63e1\u4f5c\u8005\u6240\u8981\u4f20\u8fbe\u7684\u601d\u60f3\u3002\u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2a\u8bf7\u6c42\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2a\u8bf7\u6c42\uff1b\u5f53\u8be5\u8bf7\u6c42\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u6302\u8d77\u5f53\u524d\u7684\u8bf7\u6c42\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684handler\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662f\u672c\u8282\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728\u672c\u8282\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization \u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\uff0c\u672c\u6bb5\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u8fd9\u4e2a\u6982\u5ff5\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684\u6d3b\u52a8\uff0c\u4e3b\u8981\u5305\u62ec\u5982\u4e0b\u4e24\u79cd\u60c5\u51b5\uff1a system call interrupt and exception \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff0cresume\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u8bf4\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u5173\u4e8e\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u7684\u4e00\u4e9b\u6267\u884c\u7ec6\u8282\uff0c\u6bd4\u5982kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662fkernel control path runs on behalf of process\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff1a Kernel Control Path Definition All Unix kernels are reentrant . This means that several processes\uff08\u6307\u7684\u662flightweight process\uff09 may be executing in Kernel Mode at the same time. Of course, on uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes. An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution. One way to provide reentrancy is to write functions so that they modify only local variables and do not alter global data structures . Such functions are called reentrant functions . But a reentrant kernel is not limited only to such reentrant functions (although that is how some real-time kernels are implemented). Instead, the kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process can execute a nonreentrant function at a time. If a hardware interrupt occurs, a reentrant kernel is able to suspend the current running process even if that process is in Kernel Mode . This capability is very important, because it improves the throughput of the device controllers that issue interrupts. Once a device has issued an interrupt, it waits until the CPU acknowledges it. If the kernel is able to answer quickly, the device controller will be able to perform other tasks while the CPU handles the interrupt. Now let's look at kernel reentrancy and its impact on the organization of the kernel. A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call , an exception , or an interrupt . In the simplest case, the CPU executes a kernel control path sequentially from the first instruction to the last. When one of the following events occurs, however, the CPU interleaves the kernel control paths : A process executing in User Mode invokes a system call , and the corresponding kernel control path verifies that the request cannot be satisfied immediately; it then invokes the scheduler to select a new process to run. As a result, a process switch occurs. The first kernel control path is left unfinished, and the CPU resumes the execution of some other kernel control path . In this case, the two control paths are executed on behalf of two different processes. The CPU detects an exception for example, access to a page not present in RAM while running a kernel control path . The first control path is suspended, and the CPU starts the execution of a suitable procedure. In our example, this type of procedure can allocate a new page for the process and read its contents from disk. When the procedure terminates, the first control path can be resumed. In this case, the two control paths are executed on behalf of the same process. A hardware interrupt occurs while the CPU is running a kernel control path with the interrupts enabled. The first kernel control path is left unfinished, and the CPU starts processing another kernel control path to handle the interrupt. The first kernel control path resumes when the interrupt handler terminates. In this case, the two kernel control paths run in the execution context of the same process, and the total system CPU time is accounted to it. However, the interrupt handler doesn't necessarily operate on behalf of the process. An interrupt occurs while the CPU is running with kernel preemption enabled, and a higher priority process is runnable. In this case, the first kernel control path is left unfinished, and the CPU resumes executing another kernel control path on behalf of the higher priority process. This occurs only if the kernel has been compiled with kernel preemption support. Figure 1-3 illustrates a few examples of noninterleaved and interleaved kernel control paths. Three different CPU states are considered: Running a process in User Mode ( User ) Running an exception or a system call handler ( Excp ) Running an interrupt handler ( Intr )","title":"1.6.3-Reentrant-Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.3-Reentrant-Kernels/#163-reentrant-kernels","text":"NOTE: \u4ece\u4e00\u4e2a\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u672c\u8282\u7684\u5185\u5bb9\uff0c\u5c06\u66f4\u52a0\u5bb9\u6613\u638c\u63e1\u4f5c\u8005\u6240\u8981\u4f20\u8fbe\u7684\u601d\u60f3\u3002\u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2a\u8bf7\u6c42\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2a\u8bf7\u6c42\uff1b\u5f53\u8be5\u8bf7\u6c42\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u6302\u8d77\u5f53\u524d\u7684\u8bf7\u6c42\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684handler\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662f\u672c\u8282\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728\u672c\u8282\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization \u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\uff0c\u672c\u6bb5\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u8fd9\u4e2a\u6982\u5ff5\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684\u6d3b\u52a8\uff0c\u4e3b\u8981\u5305\u62ec\u5982\u4e0b\u4e24\u79cd\u60c5\u51b5\uff1a system call interrupt and exception \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff0cresume\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u8bf4\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u5173\u4e8e\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u7684\u4e00\u4e9b\u6267\u884c\u7ec6\u8282\uff0c\u6bd4\u5982kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662fkernel control path runs on behalf of process\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff1a Kernel Control Path Definition All Unix kernels are reentrant . This means that several processes\uff08\u6307\u7684\u662flightweight process\uff09 may be executing in Kernel Mode at the same time. Of course, on uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes. An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution. One way to provide reentrancy is to write functions so that they modify only local variables and do not alter global data structures . Such functions are called reentrant functions . But a reentrant kernel is not limited only to such reentrant functions (although that is how some real-time kernels are implemented). Instead, the kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process can execute a nonreentrant function at a time. If a hardware interrupt occurs, a reentrant kernel is able to suspend the current running process even if that process is in Kernel Mode . This capability is very important, because it improves the throughput of the device controllers that issue interrupts. Once a device has issued an interrupt, it waits until the CPU acknowledges it. If the kernel is able to answer quickly, the device controller will be able to perform other tasks while the CPU handles the interrupt. Now let's look at kernel reentrancy and its impact on the organization of the kernel. A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call , an exception , or an interrupt . In the simplest case, the CPU executes a kernel control path sequentially from the first instruction to the last. When one of the following events occurs, however, the CPU interleaves the kernel control paths : A process executing in User Mode invokes a system call , and the corresponding kernel control path verifies that the request cannot be satisfied immediately; it then invokes the scheduler to select a new process to run. As a result, a process switch occurs. The first kernel control path is left unfinished, and the CPU resumes the execution of some other kernel control path . In this case, the two control paths are executed on behalf of two different processes. The CPU detects an exception for example, access to a page not present in RAM while running a kernel control path . The first control path is suspended, and the CPU starts the execution of a suitable procedure. In our example, this type of procedure can allocate a new page for the process and read its contents from disk. When the procedure terminates, the first control path can be resumed. In this case, the two control paths are executed on behalf of the same process. A hardware interrupt occurs while the CPU is running a kernel control path with the interrupts enabled. The first kernel control path is left unfinished, and the CPU starts processing another kernel control path to handle the interrupt. The first kernel control path resumes when the interrupt handler terminates. In this case, the two kernel control paths run in the execution context of the same process, and the total system CPU time is accounted to it. However, the interrupt handler doesn't necessarily operate on behalf of the process. An interrupt occurs while the CPU is running with kernel preemption enabled, and a higher priority process is runnable. In this case, the first kernel control path is left unfinished, and the CPU resumes executing another kernel control path on behalf of the higher priority process. This occurs only if the kernel has been compiled with kernel preemption support. Figure 1-3 illustrates a few examples of noninterleaved and interleaved kernel control paths. Three different CPU states are considered: Running a process in User Mode ( User ) Running an exception or a system call handler ( Excp ) Running an interrupt handler ( Intr )","title":"1.6.3. Reentrant Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.4-Process-Address-Space/","text":"1.6.4. Process Address Space # Each process runs in its private address space. A process running in User Mode refers to private stack, data, and code areas. When running in Kernel Mode, the process addresses the kernel data and code areas and uses another private stack. NOTE: \u4e0d\u540c\u7684mode\uff0c\u4f7f\u7528\u4e0d\u540c\u7684address space Because the kernel is reentrant, several kernel control paths each related to a different process may be executed in turn. In this case, each kernel control path refers to its own private kernel stack . NOTE : \u5173\u4e8ekernel stack\u53c2\u89c13.2.2.1. Process descriptors handling While it appears to each process that it has access to a private address space , there are times when part of the address space is shared among processes. In some cases, this sharing is explicitly requested by processes; in others, it is done automatically by the kernel to reduce memory usage. If the same program, say an editor, is needed simultaneously by several users, the program is loaded into memory only once, and its instructions can be shared by all of the users who need it. Its data, of course, must not be shared, because each user will have separate data. This kind of shared address space is done automatically by the kernel to save memory. Processes also can share parts of their address space as a kind of interprocess communication , using the \"shared memory\" technique introduced in System V and supported by Linux. Finally, Linux supports the mmap( ) system call, which allows part of a file or the information stored on a block device to be mapped into a part of a process address space. Memory mapping can provide an alternative to normal reads and writes for transferring data. If the same file is shared by several processes, its memory mapping is included in the address space of each of the processes that share it.","title":"1.6.4-Process-Address-Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.4-Process-Address-Space/#164-process-address-space","text":"Each process runs in its private address space. A process running in User Mode refers to private stack, data, and code areas. When running in Kernel Mode, the process addresses the kernel data and code areas and uses another private stack. NOTE: \u4e0d\u540c\u7684mode\uff0c\u4f7f\u7528\u4e0d\u540c\u7684address space Because the kernel is reentrant, several kernel control paths each related to a different process may be executed in turn. In this case, each kernel control path refers to its own private kernel stack . NOTE : \u5173\u4e8ekernel stack\u53c2\u89c13.2.2.1. Process descriptors handling While it appears to each process that it has access to a private address space , there are times when part of the address space is shared among processes. In some cases, this sharing is explicitly requested by processes; in others, it is done automatically by the kernel to reduce memory usage. If the same program, say an editor, is needed simultaneously by several users, the program is loaded into memory only once, and its instructions can be shared by all of the users who need it. Its data, of course, must not be shared, because each user will have separate data. This kind of shared address space is done automatically by the kernel to save memory. Processes also can share parts of their address space as a kind of interprocess communication , using the \"shared memory\" technique introduced in System V and supported by Linux. Finally, Linux supports the mmap( ) system call, which allows part of a file or the information stored on a block device to be mapped into a part of a process address space. Memory mapping can provide an alternative to normal reads and writes for transferring data. If the same file is shared by several processes, its memory mapping is included in the address space of each of the processes that share it.","title":"1.6.4. Process Address Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/","text":"1.6.5. Synchronization and Critical Regions # NOTE: \u867d\u7136\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662fkernel\u7684synchronization\uff0c\u4f46\u662f\u5176\u4e2d\u6240\u63cf\u8ff0\u7684\u65b9\u6cd5\u3001\u601d\u8def\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u3002 Implementing a reentrant kernel requires the use of synchronization . If a kernel control path is suspended while acting on a kernel data structure, no other kernel control path should be allowed to act on the same data structure unless it has been reset to a consistent state . Otherwise, the interaction of the two control paths could corrupt the stored information. For example, suppose a global variable V contains the number of available items of some system resource. The first kernel control path, A , reads the variable and determines that there is just one available item. At this point, another kernel control path, B , is activated and reads the same variable, which still contains the value 1 . Thus, B decreases V and starts using the resource item. Then A resumes the execution; because it has already read the value of V , it assumes that it can decrease V and take the resource item, which B already uses. As a final result, V contains -1, and two kernel control paths use the same resource item with potentially disastrous effects. When the outcome of a computation depends on how two or more processes are scheduled, the code is incorrect. We say that there is a race condition . In general, safe access to a global variable is ensured by using atomic operations . In the previous example, data corruption is not possible if the two control paths read and decrease V with a single, noninterruptible operation . However, kernels contain many data structures that cannot be accessed with a single operation. For example, it usually isn't possible to remove an element from a linked list with a single operation, because the kernel needs to access at least two pointers at once. Any section of code that should be finished by each process that begins it before another process can enter it is called a critical region . [*] [*] Synchronization problems have been fully described in other works; we refer the interested reader to books on the Unix operating systems (see the Bibliography). These problems occur not only among kernel control paths but also among processes sharing common data. Several synchronization techniques have been adopted. The following section concentrates on how to synchronize kernel control paths . 1.6.5.1. Kernel preemption disabling # To provide a drastically simple solution to synchronization problems, some traditional Unix kernels are nonpreemptive: when a process executes in Kernel Mode, it cannot be arbitrarily suspended and substituted with another process. Therefore, on a uniprocessor system, all kernel data structures that are not updated by interrupts or exception handlers are safe for the kernel to access. Of course, a process in Kernel Mode can voluntarily relinquish the CPU, but in this case, it must ensure that all data structures are left in a consistent state. Moreover, when it resumes its execution, it must recheck the value of any previously accessed data structures that could be changed. A synchronization mechanism applicable to preemptive kernels consists of disabling kernel preemption before entering a critical region and reenabling it right after leaving the region. Nonpreemptability is not enough for multiprocessor systems, because two kernel control paths running on different CPUs can concurrently access the same data structure. 1.6.5.2. Interrupt disabling # Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts before entering a critical region and reenabling them right after leaving it. This mechanism, while simple, is far from optimal. If the critical region is large, interrupts can remain disabled for a relatively long time, potentially causing all hardware activities to freeze. 1.6.5.3. Semaphores # A widely used mechanism, effective in both uniprocessor and multiprocessor systems, relies on the use of semaphores . A semaphore is simply a counter associated with a data structure; it is checked by all kernel threads before they try to access the data structure. Each semaphore may be viewed as an object composed of: An integer variable A list of waiting processes Two atomic methods: down( ) and up( ) The down( ) method decreases the value of the semaphore. If the new value is less than 0, the method adds the running process to the semaphore list and then blocks (i.e., invokes the scheduler ). The up( ) method increases the value of the semaphore and, if its new value is greater than or equal to 0, reactivates one or more processes in the semaphore list. Each data structure to be protected has its own semaphore, which is initialized to 1. When a kernel control path wishes to access the data structure, it executes the down( ) method on the proper semaphore. If the value of the new semaphore isn't negative, access to the data structure is granted. Otherwise, the process that is executing the kernel control path is added to the semaphore list and blocked. When another process executes the up( ) method on that semaphore, one of the processes in the semaphore list is allowed to proceed. 1.6.5.4. Spin locks # In multiprocessor systems, semaphores are not always the best solution to the synchronization problems. Some kernel data structures should be protected from being concurrently accessed by kernel control paths that run on different CPUs. In this case, if the time required to update the data structure is short, a semaphore could be very inefficient. To check a semaphore, the kernel must insert a process in the semaphore list and then suspend it. Because both operations are relatively expensive, in the time it takes to complete them, the other kernel control path could have already released the semaphore. In these cases, multiprocessor operating systems use spin locks . A spin lock is very similar to a semaphore, but it has no process list ; when a process finds the lock closed by another process, it \"spins\" around repeatedly, executing a tight instruction loop until the lock becomes open. Of course, spin locks are useless in a uniprocessor environment. When a kernel control path tries to access a locked data structure, it starts an endless loop. Therefore, the kernel control path that is updating the protected data structure would not have a chance to continue the execution and release the spin lock. The final result would be that the system hangs. 1.6.5.5. Avoiding deadlocks # Processes or kernel control paths that synchronize with other control paths may easily enter a deadlock state. The simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains access to b , but p1 then waits for b and p2 waits for a . Other more complex cyclic waits among groups of processes also may occur. Of course, a deadlock condition causes a complete freeze of the affected processes or kernel control paths. As far as kernel design is concerned, deadlocks become an issue when the number of kernel locks used is high. In this case, it may be quite difficult to ensure that no deadlock state will ever be reached for all possible ways to interleave kernel control paths. Several operating systems, including Linux, avoid this problem by requesting locks in a predefined order.","title":"1.6.5-Synchronization-and-Critical-Regions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#165-synchronization-and-critical-regions","text":"NOTE: \u867d\u7136\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662fkernel\u7684synchronization\uff0c\u4f46\u662f\u5176\u4e2d\u6240\u63cf\u8ff0\u7684\u65b9\u6cd5\u3001\u601d\u8def\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u3002 Implementing a reentrant kernel requires the use of synchronization . If a kernel control path is suspended while acting on a kernel data structure, no other kernel control path should be allowed to act on the same data structure unless it has been reset to a consistent state . Otherwise, the interaction of the two control paths could corrupt the stored information. For example, suppose a global variable V contains the number of available items of some system resource. The first kernel control path, A , reads the variable and determines that there is just one available item. At this point, another kernel control path, B , is activated and reads the same variable, which still contains the value 1 . Thus, B decreases V and starts using the resource item. Then A resumes the execution; because it has already read the value of V , it assumes that it can decrease V and take the resource item, which B already uses. As a final result, V contains -1, and two kernel control paths use the same resource item with potentially disastrous effects. When the outcome of a computation depends on how two or more processes are scheduled, the code is incorrect. We say that there is a race condition . In general, safe access to a global variable is ensured by using atomic operations . In the previous example, data corruption is not possible if the two control paths read and decrease V with a single, noninterruptible operation . However, kernels contain many data structures that cannot be accessed with a single operation. For example, it usually isn't possible to remove an element from a linked list with a single operation, because the kernel needs to access at least two pointers at once. Any section of code that should be finished by each process that begins it before another process can enter it is called a critical region . [*] [*] Synchronization problems have been fully described in other works; we refer the interested reader to books on the Unix operating systems (see the Bibliography). These problems occur not only among kernel control paths but also among processes sharing common data. Several synchronization techniques have been adopted. The following section concentrates on how to synchronize kernel control paths .","title":"1.6.5. Synchronization and Critical Regions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1651-kernel-preemption-disabling","text":"To provide a drastically simple solution to synchronization problems, some traditional Unix kernels are nonpreemptive: when a process executes in Kernel Mode, it cannot be arbitrarily suspended and substituted with another process. Therefore, on a uniprocessor system, all kernel data structures that are not updated by interrupts or exception handlers are safe for the kernel to access. Of course, a process in Kernel Mode can voluntarily relinquish the CPU, but in this case, it must ensure that all data structures are left in a consistent state. Moreover, when it resumes its execution, it must recheck the value of any previously accessed data structures that could be changed. A synchronization mechanism applicable to preemptive kernels consists of disabling kernel preemption before entering a critical region and reenabling it right after leaving the region. Nonpreemptability is not enough for multiprocessor systems, because two kernel control paths running on different CPUs can concurrently access the same data structure.","title":"1.6.5.1. Kernel preemption disabling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1652-interrupt-disabling","text":"Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts before entering a critical region and reenabling them right after leaving it. This mechanism, while simple, is far from optimal. If the critical region is large, interrupts can remain disabled for a relatively long time, potentially causing all hardware activities to freeze.","title":"1.6.5.2. Interrupt disabling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1653-semaphores","text":"A widely used mechanism, effective in both uniprocessor and multiprocessor systems, relies on the use of semaphores . A semaphore is simply a counter associated with a data structure; it is checked by all kernel threads before they try to access the data structure. Each semaphore may be viewed as an object composed of: An integer variable A list of waiting processes Two atomic methods: down( ) and up( ) The down( ) method decreases the value of the semaphore. If the new value is less than 0, the method adds the running process to the semaphore list and then blocks (i.e., invokes the scheduler ). The up( ) method increases the value of the semaphore and, if its new value is greater than or equal to 0, reactivates one or more processes in the semaphore list. Each data structure to be protected has its own semaphore, which is initialized to 1. When a kernel control path wishes to access the data structure, it executes the down( ) method on the proper semaphore. If the value of the new semaphore isn't negative, access to the data structure is granted. Otherwise, the process that is executing the kernel control path is added to the semaphore list and blocked. When another process executes the up( ) method on that semaphore, one of the processes in the semaphore list is allowed to proceed.","title":"1.6.5.3. Semaphores"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1654-spin-locks","text":"In multiprocessor systems, semaphores are not always the best solution to the synchronization problems. Some kernel data structures should be protected from being concurrently accessed by kernel control paths that run on different CPUs. In this case, if the time required to update the data structure is short, a semaphore could be very inefficient. To check a semaphore, the kernel must insert a process in the semaphore list and then suspend it. Because both operations are relatively expensive, in the time it takes to complete them, the other kernel control path could have already released the semaphore. In these cases, multiprocessor operating systems use spin locks . A spin lock is very similar to a semaphore, but it has no process list ; when a process finds the lock closed by another process, it \"spins\" around repeatedly, executing a tight instruction loop until the lock becomes open. Of course, spin locks are useless in a uniprocessor environment. When a kernel control path tries to access a locked data structure, it starts an endless loop. Therefore, the kernel control path that is updating the protected data structure would not have a chance to continue the execution and release the spin lock. The final result would be that the system hangs.","title":"1.6.5.4. Spin locks"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1655-avoiding-deadlocks","text":"Processes or kernel control paths that synchronize with other control paths may easily enter a deadlock state. The simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains access to b , but p1 then waits for b and p2 waits for a . Other more complex cyclic waits among groups of processes also may occur. Of course, a deadlock condition causes a complete freeze of the affected processes or kernel control paths. As far as kernel design is concerned, deadlocks become an issue when the number of kernel locks used is high. In this case, it may be quite difficult to ensure that no deadlock state will ever be reached for all possible ways to interleave kernel control paths. Several operating systems, including Linux, avoid this problem by requesting locks in a predefined order.","title":"1.6.5.5. Avoiding deadlocks"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.6-Signals-and-Interprocess-Communication/","text":"1.6.6. Signals and Interprocess Communication # Unix signals provide a mechanism for notifying processes of system events. Each event has its own signal number, which is usually referred to by a symbolic constant such as SIGTERM . There are two kinds of system events: Asynchronous notifications For instance, a user can send the interrupt signal SIGINT to a foreground process by pressing the interrupt keycode (usually Ctrl-C) at the terminal. Synchronous notifications For instance, the kernel sends the signal SIGSEGV to a process when it accesses a memory location at an invalid address.","title":"1.6.6-Signals-and-Interprocess-Communication"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.6-Signals-and-Interprocess-Communication/#166-signals-and-interprocess-communication","text":"Unix signals provide a mechanism for notifying processes of system events. Each event has its own signal number, which is usually referred to by a symbolic constant such as SIGTERM . There are two kinds of system events: Asynchronous notifications For instance, a user can send the interrupt signal SIGINT to a foreground process by pressing the interrupt keycode (usually Ctrl-C) at the terminal. Synchronous notifications For instance, the kernel sends the signal SIGSEGV to a process when it accesses a memory location at an invalid address.","title":"1.6.6. Signals and Interprocess Communication"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/","text":"1.6.7. Process Management # Unix makes a neat distinction between the process and the program it is executing. To that end, the fork( ) and _exit( ) system calls are used respectively to create a new process and to terminate it, while an exec( ) -like system call is invoked to load a new program. After such a system call is executed, the process resumes execution with a brand new address space containing the loaded program. The process that invokes a fork( ) is the parent , while the new process is its child . Parents and children can find one another because the data structure describing each process includes a pointer to its immediate parent and pointers to all its immediate children. A naive implementation of the fork( ) would require both the parent's data and the parent's code to be duplicated and the copies assigned to the child. This would be quite time consuming. Current kernels that can rely on hardware paging units follow the Copy-On-Write approach, which defers page duplication until the last moment (i.e., until the parent or the child is required to write into a page). We shall describe how Linux implements this technique in the section \"Copy On Write\" in Chapter 9. The _exit( ) system call terminates a process. The kernel handles this system call by releasing the resources owned by the process and sending the parent process a SIGCHLD signal, which is ignored by default. 1.6.7.1. Zombie processes # How can a parent process inquire about termination of its children? The wait4( ) system call allows a process to wait until one of its children terminates; it returns the process ID (PID) of the terminated child. When executing this system call, the kernel checks whether a child has already terminated. A special zombie process state is introduced to represent terminated processes: a process remains in that state until its parent process executes a wait4( ) system call on it. The system call handler extracts data about resource usage from the process descriptor fields; the process descriptor may be released once the data is collected. If no child process has already terminated when the wait4( ) system call is executed, the kernel usually puts the process in a wait state until a child terminates. Many kernels also implement a waitpid( ) system call, which allows a process to wait for a specific child process. Other variants of wait4( ) system calls are also quite common. It's good practice for the kernel to keep around information on a child process until the parent issues its wait4( ) call, but suppose the parent process terminates without issuing that call? The information takes up valuable memory slots that could be used to serve living processes. For example, many shells allow the user to start a command in the background and then log out. The process that is running the command shell terminates, but its children continue their execution. The solution lies in a special system process called init , which is created during system initialization. When a process terminates, the kernel changes the appropriate process descriptor pointers of all the existing children of the terminated process to make them become children of init . This process monitors the execution of all its children and routinely issues wait4( ) system calls, whose side effect is to get rid of all orphaned zombies. 1.6.7.2. Process groups and login sessions # Modern Unix operating systems introduce the notion of process groups to represent a \"job\" abstraction. For example, in order to execute the command line: $ ls | sort | more a shell that supports process groups , such as bash , creates a new group for the three processes corresponding to ls , sort , and more . In this way, the shell acts on the three processes as if they were a single entity (the job , to be precise). Each process descriptor includes a field containing the process group ID . Each group of processes may have a group leader , which is the process whose PID coincides with the process group ID . A newly created process is initially inserted into the process group of its parent. Modern Unix kernels also introduce login sessions . Informally, a login session contains all processes that are descendants of the process that has started a working session on a specific terminal usually, the first command shell process created for the user. All processes in a process group must be in the same login session . A login session may have several process groups active simultaneously; one of these process groups is always in the foreground, which means that it has access to the terminal. The other active process groups are in the background. When a background process tries to access the terminal, it receives a SIGTTIN or SIGTTOUT signal. In many command shells, the internal commands bg and fg can be used to put a process group in either the background or the foreground.","title":"1.6.7-Process-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/#167-process-management","text":"Unix makes a neat distinction between the process and the program it is executing. To that end, the fork( ) and _exit( ) system calls are used respectively to create a new process and to terminate it, while an exec( ) -like system call is invoked to load a new program. After such a system call is executed, the process resumes execution with a brand new address space containing the loaded program. The process that invokes a fork( ) is the parent , while the new process is its child . Parents and children can find one another because the data structure describing each process includes a pointer to its immediate parent and pointers to all its immediate children. A naive implementation of the fork( ) would require both the parent's data and the parent's code to be duplicated and the copies assigned to the child. This would be quite time consuming. Current kernels that can rely on hardware paging units follow the Copy-On-Write approach, which defers page duplication until the last moment (i.e., until the parent or the child is required to write into a page). We shall describe how Linux implements this technique in the section \"Copy On Write\" in Chapter 9. The _exit( ) system call terminates a process. The kernel handles this system call by releasing the resources owned by the process and sending the parent process a SIGCHLD signal, which is ignored by default.","title":"1.6.7. Process Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/#1671-zombie-processes","text":"How can a parent process inquire about termination of its children? The wait4( ) system call allows a process to wait until one of its children terminates; it returns the process ID (PID) of the terminated child. When executing this system call, the kernel checks whether a child has already terminated. A special zombie process state is introduced to represent terminated processes: a process remains in that state until its parent process executes a wait4( ) system call on it. The system call handler extracts data about resource usage from the process descriptor fields; the process descriptor may be released once the data is collected. If no child process has already terminated when the wait4( ) system call is executed, the kernel usually puts the process in a wait state until a child terminates. Many kernels also implement a waitpid( ) system call, which allows a process to wait for a specific child process. Other variants of wait4( ) system calls are also quite common. It's good practice for the kernel to keep around information on a child process until the parent issues its wait4( ) call, but suppose the parent process terminates without issuing that call? The information takes up valuable memory slots that could be used to serve living processes. For example, many shells allow the user to start a command in the background and then log out. The process that is running the command shell terminates, but its children continue their execution. The solution lies in a special system process called init , which is created during system initialization. When a process terminates, the kernel changes the appropriate process descriptor pointers of all the existing children of the terminated process to make them become children of init . This process monitors the execution of all its children and routinely issues wait4( ) system calls, whose side effect is to get rid of all orphaned zombies.","title":"1.6.7.1. Zombie processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/#1672-process-groups-and-login-sessions","text":"Modern Unix operating systems introduce the notion of process groups to represent a \"job\" abstraction. For example, in order to execute the command line: $ ls | sort | more a shell that supports process groups , such as bash , creates a new group for the three processes corresponding to ls , sort , and more . In this way, the shell acts on the three processes as if they were a single entity (the job , to be precise). Each process descriptor includes a field containing the process group ID . Each group of processes may have a group leader , which is the process whose PID coincides with the process group ID . A newly created process is initially inserted into the process group of its parent. Modern Unix kernels also introduce login sessions . Informally, a login session contains all processes that are descendants of the process that has started a working session on a specific terminal usually, the first command shell process created for the user. All processes in a process group must be in the same login session . A login session may have several process groups active simultaneously; one of these process groups is always in the foreground, which means that it has access to the terminal. The other active process groups are in the background. When a background process tries to access the terminal, it receives a SIGTTIN or SIGTTOUT signal. In many command shells, the internal commands bg and fg can be used to put a process group in either the background or the foreground.","title":"1.6.7.2. Process groups and login sessions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/","text":"1.6.8. Memory Management # Memory management is by far the most complex activity in a Unix kernel. More than a third of this book is dedicated just to describing how Linux handles memory management. This section illustrates some of the main issues related to memory management. 1.6.8.1. Virtual memory # All recent Unix systems provide a useful abstraction called virtual memory . Virtual memory acts as a logical layer between the application memory requests and the hardware Memory Management Unit (MMU). Virtual memory has many purposes and advantages: Several processes can be executed concurrently. It is possible to run applications whose memory needs are larger than the available physical memory. Processes can execute a program whose code is only partially loaded in memory. Each process is allowed to access a subset of the available physical memory. Processes can share a single memory image of a library or program. Programs can be relocatable that is, they can be placed anywhere in physical memory. The main ingredient of a virtual memory subsystem is the notion of virtual address space . The set of memory references that a process can use is different from physical memory addresses. When a process uses a virtual address , [*] the kernel and the MMU cooperate to find the actual physical location of the requested memory item. [*] These addresses have different nomenclatures, depending on the computer architecture. As we'll see in Chapter 2, Intel manuals refer to them as \"logical addresses.\" Today's CPUs include hardware circuits that automatically translate the virtual addresses into physical ones. To that end, the available RAM is partitioned into page frames typically 4 or 8 KB in length and a set of Page Tables is introduced to specify how virtual addresses correspond to physical addresses . These circuits make memory allocation simpler, because a request for a block of contiguous virtual addresses can be satisfied by allocating a group of page frames having noncontiguous physical addresses. NOTE: \u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u5bf9\u5982\u4e0b\u7ae0\u8282\u7684\u5185\u5bb9\u7684\u6982\u62ec\uff1a chapter 2.1. Memory Addresses chapter 2.4. Paging in Hardware chapter 2.5. Paging in Linux 1.6.8.2. Random access memory usage # All Unix operating systems clearly distinguish between two portions of the random access memory (RAM). A few megabytes are dedicated to storing the kernel image (i.e., the kernel code and the kernel static data structures). The remaining portion of RAM is usually handled by the virtual memory system and is used in three possible ways: To satisfy kernel requests for buffers, descriptors, and other dynamic kernel data structures To satisfy process requests for generic memory areas and for memory mapping of files To get better performance from disks and other buffered devices by means of caches NOTE: \u5173\u4e8eRAM\u7684usage\uff0c\u5728Chapter 8. Memory Management\u6709\u7c7b\u4f3c\u4e0a\u9762\u8fd9\u6bb5\u7684\u63cf\u8ff0\u3002 Each request type is valuable. On the other hand, because the available RAM is limited, some balancing among request types must be done, particularly when little available memory is left. Moreover, when some critical threshold of available memory is reached and a page-frame-reclaiming algorithm is invoked to free additional memory, which are the page frames most suitable for reclaiming? As we will see in Chapter 17, there is no simple answer to this question and very little support from theory. The only available solution lies in developing carefully tuned empirical algorithms. One major problem that must be solved by the virtual memory system is memory fragmentation . Ideally, a memory request should fail only when the number of free page frames is too small. However, the kernel is often forced to use physically contiguous memory areas. Hence the memory request could fail even if there is enough memory available, but it is not available as one contiguous chunk. 1.6.8.3. Kernel Memory Allocator # The Kernel Memory Allocator (KMA) is a subsystem that tries to satisfy the requests for memory areas from all parts of the system. Some of these requests come from other kernel subsystems needing memory for kernel use, and some requests come via system calls from user programs to increase their processes' address spaces. A good KMA should have the following features: It must be fast. Actually, this is the most crucial attribute, because it is invoked by all kernel subsystems (including the interrupt handlers). It should minimize the amount of wasted memory. It should try to reduce the memory fragmentation problem. It should be able to cooperate with the other memory management subsystems to borrow and release page frames from them. Several proposed KMAs, which are based on a variety of different algorithmic techniques, include: Resource map allocator Power-of-two free lists McKusick-Karels allocator Buddy system Mach's Zone allocator Dynix allocator Solaris 's Slab allocator As we will see in Chapter 8, Linux's KMA uses a Slab allocator on top of a buddy system. NOTE: Memory Allocation Guide 1.6.8.4. Process virtual address space handling # The address space of a process contains all the virtual memory addresses that the process is allowed to reference. The kernel usually stores a process virtual address space as a list of memory area descriptors . For example, when a process starts the execution of some program via an exec( ) -like system call, the kernel assigns to the process a virtual address space that comprises memory areas for: The executable code of the program The initialized data of the program The uninitialized data of the program The initial program stack (i.e., the User Mode stack) The executable code and data of needed shared libraries The heap (the memory dynamically requested by the program) All recent Unix operating systems adopt a memory allocation strategy called demand paging . With demand paging, a process can start program execution with none of its pages in physical memory. As it accesses a nonpresent page, the MMU generates an exception; the exception handler finds the affected memory region, allocates a free page, and initializes it with the appropriate data. In a similar fashion, when the process dynamically requires memory by using malloc( ) , or the brk( ) system call (which is invoked internally by malloc( ) ), the kernel just updates the size of the heap memory region of the process. A page frame is assigned to the process only when it generates an exception by trying to refer its virtual memory addresses. Virtual address spaces also allow other efficient strategies, such as the Copy On Write strategy mentioned earlier. For example, when a new process is created, the kernel just assigns the parent's page frames to the child address space, but marks them read-only. An exception is raised as soon the parent or the child tries to modify the contents of a page. The exception handler assigns a new page frame to the affected process and initializes it with the contents of the original page. 1.6.8.5. Caching # A good part of the available physical memory is used as cache for hard disks and other block devices. This is because hard drives are very slow: a disk access requires several milliseconds, which is a very long time compared with the RAM access time. Therefore, disks are often the bottleneck in system performance. As a general rule, one of the policies already implemented in the earliest Unix system is to defer writing to disk as long as possible. As a result, data read previously from disk and no longer used by any process continue to stay in RAM. This strategy is based on the fact that there is a good chance that new processes will require data read from or written to disk by processes that no longer exist. When a process asks to access a disk, the kernel checks first whether the required data are in the cache. Each time this happens (a cache hit), the kernel is able to service the process request without accessing the disk. The sync( ) system call forces disk synchronization by writing all of the \"dirty\" buffers (i.e., all the buffers whose contents differ from that of the corresponding disk blocks) into disk. To avoid data loss, all operating systems take care to periodically write dirty buffers back to disk.","title":"1.6.8-Memory-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#168-memory-management","text":"Memory management is by far the most complex activity in a Unix kernel. More than a third of this book is dedicated just to describing how Linux handles memory management. This section illustrates some of the main issues related to memory management.","title":"1.6.8. Memory Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1681-virtual-memory","text":"All recent Unix systems provide a useful abstraction called virtual memory . Virtual memory acts as a logical layer between the application memory requests and the hardware Memory Management Unit (MMU). Virtual memory has many purposes and advantages: Several processes can be executed concurrently. It is possible to run applications whose memory needs are larger than the available physical memory. Processes can execute a program whose code is only partially loaded in memory. Each process is allowed to access a subset of the available physical memory. Processes can share a single memory image of a library or program. Programs can be relocatable that is, they can be placed anywhere in physical memory. The main ingredient of a virtual memory subsystem is the notion of virtual address space . The set of memory references that a process can use is different from physical memory addresses. When a process uses a virtual address , [*] the kernel and the MMU cooperate to find the actual physical location of the requested memory item. [*] These addresses have different nomenclatures, depending on the computer architecture. As we'll see in Chapter 2, Intel manuals refer to them as \"logical addresses.\" Today's CPUs include hardware circuits that automatically translate the virtual addresses into physical ones. To that end, the available RAM is partitioned into page frames typically 4 or 8 KB in length and a set of Page Tables is introduced to specify how virtual addresses correspond to physical addresses . These circuits make memory allocation simpler, because a request for a block of contiguous virtual addresses can be satisfied by allocating a group of page frames having noncontiguous physical addresses. NOTE: \u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u5bf9\u5982\u4e0b\u7ae0\u8282\u7684\u5185\u5bb9\u7684\u6982\u62ec\uff1a chapter 2.1. Memory Addresses chapter 2.4. Paging in Hardware chapter 2.5. Paging in Linux","title":"1.6.8.1. Virtual memory"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1682-random-access-memory-usage","text":"All Unix operating systems clearly distinguish between two portions of the random access memory (RAM). A few megabytes are dedicated to storing the kernel image (i.e., the kernel code and the kernel static data structures). The remaining portion of RAM is usually handled by the virtual memory system and is used in three possible ways: To satisfy kernel requests for buffers, descriptors, and other dynamic kernel data structures To satisfy process requests for generic memory areas and for memory mapping of files To get better performance from disks and other buffered devices by means of caches NOTE: \u5173\u4e8eRAM\u7684usage\uff0c\u5728Chapter 8. Memory Management\u6709\u7c7b\u4f3c\u4e0a\u9762\u8fd9\u6bb5\u7684\u63cf\u8ff0\u3002 Each request type is valuable. On the other hand, because the available RAM is limited, some balancing among request types must be done, particularly when little available memory is left. Moreover, when some critical threshold of available memory is reached and a page-frame-reclaiming algorithm is invoked to free additional memory, which are the page frames most suitable for reclaiming? As we will see in Chapter 17, there is no simple answer to this question and very little support from theory. The only available solution lies in developing carefully tuned empirical algorithms. One major problem that must be solved by the virtual memory system is memory fragmentation . Ideally, a memory request should fail only when the number of free page frames is too small. However, the kernel is often forced to use physically contiguous memory areas. Hence the memory request could fail even if there is enough memory available, but it is not available as one contiguous chunk.","title":"1.6.8.2. Random access memory usage"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1683-kernel-memory-allocator","text":"The Kernel Memory Allocator (KMA) is a subsystem that tries to satisfy the requests for memory areas from all parts of the system. Some of these requests come from other kernel subsystems needing memory for kernel use, and some requests come via system calls from user programs to increase their processes' address spaces. A good KMA should have the following features: It must be fast. Actually, this is the most crucial attribute, because it is invoked by all kernel subsystems (including the interrupt handlers). It should minimize the amount of wasted memory. It should try to reduce the memory fragmentation problem. It should be able to cooperate with the other memory management subsystems to borrow and release page frames from them. Several proposed KMAs, which are based on a variety of different algorithmic techniques, include: Resource map allocator Power-of-two free lists McKusick-Karels allocator Buddy system Mach's Zone allocator Dynix allocator Solaris 's Slab allocator As we will see in Chapter 8, Linux's KMA uses a Slab allocator on top of a buddy system. NOTE: Memory Allocation Guide","title":"1.6.8.3. Kernel Memory Allocator"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1684-process-virtual-address-space-handling","text":"The address space of a process contains all the virtual memory addresses that the process is allowed to reference. The kernel usually stores a process virtual address space as a list of memory area descriptors . For example, when a process starts the execution of some program via an exec( ) -like system call, the kernel assigns to the process a virtual address space that comprises memory areas for: The executable code of the program The initialized data of the program The uninitialized data of the program The initial program stack (i.e., the User Mode stack) The executable code and data of needed shared libraries The heap (the memory dynamically requested by the program) All recent Unix operating systems adopt a memory allocation strategy called demand paging . With demand paging, a process can start program execution with none of its pages in physical memory. As it accesses a nonpresent page, the MMU generates an exception; the exception handler finds the affected memory region, allocates a free page, and initializes it with the appropriate data. In a similar fashion, when the process dynamically requires memory by using malloc( ) , or the brk( ) system call (which is invoked internally by malloc( ) ), the kernel just updates the size of the heap memory region of the process. A page frame is assigned to the process only when it generates an exception by trying to refer its virtual memory addresses. Virtual address spaces also allow other efficient strategies, such as the Copy On Write strategy mentioned earlier. For example, when a new process is created, the kernel just assigns the parent's page frames to the child address space, but marks them read-only. An exception is raised as soon the parent or the child tries to modify the contents of a page. The exception handler assigns a new page frame to the affected process and initializes it with the contents of the original page.","title":"1.6.8.4. Process virtual address space handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1685-caching","text":"A good part of the available physical memory is used as cache for hard disks and other block devices. This is because hard drives are very slow: a disk access requires several milliseconds, which is a very long time compared with the RAM access time. Therefore, disks are often the bottleneck in system performance. As a general rule, one of the policies already implemented in the earliest Unix system is to defer writing to disk as long as possible. As a result, data read previously from disk and no longer used by any process continue to stay in RAM. This strategy is based on the fact that there is a good chance that new processes will require data read from or written to disk by processes that no longer exist. When a process asks to access a disk, the kernel checks first whether the required data are in the cache. Each time this happens (a cache hit), the kernel is able to service the process request without accessing the disk. The sync( ) system call forces disk synchronization by writing all of the \"dirty\" buffers (i.e., all the buffers whose contents differ from that of the corresponding disk blocks) into disk. To avoid data loss, all operating systems take care to periodically write dirty buffers back to disk.","title":"1.6.8.5. Caching"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.9-Device-Drivers/","text":"1.6.9. Device Drivers # The kernel interacts with I/O devices by means of device drivers . Device drivers are included in the kernel and consist of data structures and functions that control one or more devices, such as hard disks, keyboards, mouses, monitors, network interfaces, and devices connected to an SCSI bus. Each driver interacts with the remaining part of the kernel (even with other drivers) through a specific interface. This approach has the following advantages: Device-specific code can be encapsulated in a specific module. Vendors can add new devices without knowing the kernel source code; only the interface specifications must be known. The kernel deals with all devices in a uniform way and accesses them through the same interface. It is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring the system to be rebooted. It is also possible to dynamically unload a module that is no longer needed, therefore minimizing the size of the kernel image stored in RAM. Figure 1-4 illustrates how device drivers interface with the rest of the kernel and with the processes. Figure 1-4. Device driver interface Some user programs (P) wish to operate on hardware devices. They make requests to the kernel using the usual file-related system calls and the device files normally found in the /dev directory. Actually, the device files are the user-visible portion of the device driver interface. Each device file refers to a specific device driver, which is invoked by the kernel to perform the requested operation on the hardware component. At the time Unix was introduced, graphical terminals were uncommon and expensive, so only alphanumeric terminals were handled directly by Unix kernels. When graphical terminals became widespread, ad hoc applications such as the X Window System were introduced that ran as standard processes and accessed the I/O ports of the graphics interface and the RAM video area directly. Some recent Unix kernels, such as Linux 2.6, provide an abstraction for the frame buffer of the graphic card and allow application software to access them without needing to know anything about the I/O ports of the graphics interface (see the section \"Levels of Kernel Support\" in Chapter 13.)","title":"1.6.9-Device-Drivers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.9-Device-Drivers/#169-device-drivers","text":"The kernel interacts with I/O devices by means of device drivers . Device drivers are included in the kernel and consist of data structures and functions that control one or more devices, such as hard disks, keyboards, mouses, monitors, network interfaces, and devices connected to an SCSI bus. Each driver interacts with the remaining part of the kernel (even with other drivers) through a specific interface. This approach has the following advantages: Device-specific code can be encapsulated in a specific module. Vendors can add new devices without knowing the kernel source code; only the interface specifications must be known. The kernel deals with all devices in a uniform way and accesses them through the same interface. It is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring the system to be rebooted. It is also possible to dynamically unload a module that is no longer needed, therefore minimizing the size of the kernel image stored in RAM. Figure 1-4 illustrates how device drivers interface with the rest of the kernel and with the processes. Figure 1-4. Device driver interface Some user programs (P) wish to operate on hardware devices. They make requests to the kernel using the usual file-related system calls and the device files normally found in the /dev directory. Actually, the device files are the user-visible portion of the device driver interface. Each device file refers to a specific device driver, which is invoked by the kernel to perform the requested operation on the hardware component. At the time Unix was introduced, graphical terminals were uncommon and expensive, so only alphanumeric terminals were handled directly by Unix kernels. When graphical terminals became widespread, ad hoc applications such as the X Window System were introduced that ran as standard processes and accessed the I/O ports of the graphics interface and the RAM video area directly. Some recent Unix kernels, such as Linux 2.6, provide an abstraction for the frame buffer of the graphic card and allow application software to access them without needing to know anything about the I/O ports of the graphics interface (see the section \"Levels of Kernel Support\" in Chapter 13.)","title":"1.6.9. Device Drivers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Chapter-1-Introduction/","text":"Chapter 1. Introduction # NOTE: \u5173\u4e8e\u66f4\u52a0\u8be6\u7ec6\u7684linux\u7684\u4ecb\u7ecd\uff0c\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a Linux Linux kernel","title":"Chapter-1-Introduction"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Chapter-1-Introduction/#chapter-1-introduction","text":"NOTE: \u5173\u4e8e\u66f4\u52a0\u8be6\u7ec6\u7684linux\u7684\u4ecb\u7ecd\uff0c\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a Linux Linux kernel","title":"Chapter 1. Introduction"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Giant-lock/","text":"Giant lock Linux Giant lock # In operating systems , a giant lock , also known as a big-lock or kernel-lock , is a lock that may be used in the kernel to provide concurrency control required by symmetric multiprocessing (SMP) systems. A giant lock is a solitary global lock that is held whenever a thread enters kernel space and released when the thread returns to user space ; a system call is the archetypal example. In this model, threads in user space can run concurrently on any available processors or processor cores , but no more than one thread can run in kernel space; any other threads that try to enter kernel space are forced to wait. In other words, the giant lock eliminates all concurrency in kernel space. By isolating the kernel from concurrency, many parts of the kernel no longer need to be modified to support SMP. However, as in giant-lock SMP systems only one processor can run the kernel code at a time, performance for applications spending significant amounts of time in the kernel is not much improved. Accordingly, the giant-lock approach is commonly seen as a preliminary means of bringing SMP support to an operating system, yielding benefits only in user space. Most modern operating systems use a fine-grained locking approach. Linux # The Linux kernel had a big kernel lock (BKL) since the introduction of SMP, until Arnd Bergmann removed it in 2011 in kernel version 2.6.39, with the remaining uses of the big lock removed or replaced by finer-grained locking. Linux distributions at or above CentOS 7 , Debian 7 (Wheezy) and Ubuntu 11.10 are therefore not using BKL.","title":"Giant-lock"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Giant-lock/#giant-lock","text":"In operating systems , a giant lock , also known as a big-lock or kernel-lock , is a lock that may be used in the kernel to provide concurrency control required by symmetric multiprocessing (SMP) systems. A giant lock is a solitary global lock that is held whenever a thread enters kernel space and released when the thread returns to user space ; a system call is the archetypal example. In this model, threads in user space can run concurrently on any available processors or processor cores , but no more than one thread can run in kernel space; any other threads that try to enter kernel space are forced to wait. In other words, the giant lock eliminates all concurrency in kernel space. By isolating the kernel from concurrency, many parts of the kernel no longer need to be modified to support SMP. However, as in giant-lock SMP systems only one processor can run the kernel code at a time, performance for applications spending significant amounts of time in the kernel is not much improved. Accordingly, the giant-lock approach is commonly seen as a preliminary means of bringing SMP support to an operating system, yielding benefits only in user space. Most modern operating systems use a fine-grained locking approach.","title":"Giant lock"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Giant-lock/#linux","text":"The Linux kernel had a big kernel lock (BKL) since the introduction of SMP, until Arnd Bergmann removed it in 2011 in kernel version 2.6.39, with the remaining uses of the big lock removed or replaced by finer-grained locking. Linux distributions at or above CentOS 7 , Debian 7 (Wheezy) and Ubuntu 11.10 are therefore not using BKL.","title":"Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/","text":"2.1. Memory Addresses Logical address Linear address (also known as virtual address) Physical address 2.1. Memory Addresses # Programmers casually refer to a memory address as the way to access the contents of a memory cell. But when dealing with 80 x 86 microprocessors, we have to distinguish three kinds of addresses: Logical address # Included in the machine language instructions to specify the address of an operand or of an instruction. This type of address embodies the well-known 80 x 86 segmented architecture that forces MS-DOS and Windows programmers to divide their programs into segments . Each logical address consists of a segment and an offset (or displacement) that denotes the distance from the start of the segment to the actual address. Linear address (also known as virtual address ) # A single 32-bit unsigned integer that can be used to address up to 4 GB that is, up to 4,294,967,296 memory cells. Linear addresses are usually represented in hexadecimal notation; their values range from 0x00000000 to 0xffffffff . NOTE: \u6bcf\u4e2aprocess\u90fd\u4e00\u4e2a\u72ec\u7acb\u7684 Virtual address space \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u5426\u53ef\u4ee5\u63a8\u65ad\u51fa\u6bcf\u4e2aprocess\u7684virtual address space\u7684address value range\u662f\u5426\u4e00\u81f4\uff1b\u53c2\u89c1\u8fd9\u7bc7\u6587\u7ae0 In virtual memory, can two different processes have the same address? Physical address # Used to address memory cells in memory chips. They correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus . Physical addresses are represented as 32-bit or 36-bit unsigned integers. The Memory Management Unit ( MMU ) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit ; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). In multiprocessor systems, all CPUs usually share the same memory; this means that RAM chips may be accessed concurrently by independent CPUs. Because read or write operations on a RAM chip must be performed serially, a hardware circuit called a memory arbiter is inserted between the bus and every RAM chip. Its role is to grant access to a CPU if the chip is free and to delay it if the chip is busy servicing a request by another processor. Even uniprocessor systems use memory arbiters , because they include specialized processors called DMA controllers that operate concurrently with the CPU (see the section \"Direct Memory Access (DMA)\" in Chapter 13). In the case of multiprocessor systems, the structure of the arbiter is more complex because it has more input ports. The dual Pentium, for instance, maintains a two-port arbiter at each chip entrance and requires that the two CPUs exchange synchronization messages before attempting to use the common bus. From the programming point of view, the arbiter is hidden because it is managed by hardware circuits.","title":"2.1-Memory-Addresses"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#21-memory-addresses","text":"Programmers casually refer to a memory address as the way to access the contents of a memory cell. But when dealing with 80 x 86 microprocessors, we have to distinguish three kinds of addresses:","title":"2.1. Memory Addresses"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#logical-address","text":"Included in the machine language instructions to specify the address of an operand or of an instruction. This type of address embodies the well-known 80 x 86 segmented architecture that forces MS-DOS and Windows programmers to divide their programs into segments . Each logical address consists of a segment and an offset (or displacement) that denotes the distance from the start of the segment to the actual address.","title":"Logical address"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#linear-address-also-known-as-virtual-address","text":"A single 32-bit unsigned integer that can be used to address up to 4 GB that is, up to 4,294,967,296 memory cells. Linear addresses are usually represented in hexadecimal notation; their values range from 0x00000000 to 0xffffffff . NOTE: \u6bcf\u4e2aprocess\u90fd\u4e00\u4e2a\u72ec\u7acb\u7684 Virtual address space \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u5426\u53ef\u4ee5\u63a8\u65ad\u51fa\u6bcf\u4e2aprocess\u7684virtual address space\u7684address value range\u662f\u5426\u4e00\u81f4\uff1b\u53c2\u89c1\u8fd9\u7bc7\u6587\u7ae0 In virtual memory, can two different processes have the same address?","title":"Linear address (also known as virtual address)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#physical-address","text":"Used to address memory cells in memory chips. They correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus . Physical addresses are represented as 32-bit or 36-bit unsigned integers. The Memory Management Unit ( MMU ) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit ; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). In multiprocessor systems, all CPUs usually share the same memory; this means that RAM chips may be accessed concurrently by independent CPUs. Because read or write operations on a RAM chip must be performed serially, a hardware circuit called a memory arbiter is inserted between the bus and every RAM chip. Its role is to grant access to a CPU if the chip is free and to delay it if the chip is busy servicing a request by another processor. Even uniprocessor systems use memory arbiters , because they include specialized processors called DMA controllers that operate concurrently with the CPU (see the section \"Direct Memory Access (DMA)\" in Chapter 13). In the case of multiprocessor systems, the structure of the arbiter is more complex because it has more input ports. The dual Pentium, for instance, maintains a two-port arbiter at each chip entrance and requires that the two CPUs exchange synchronization messages before attempting to use the common bus. From the programming point of view, the arbiter is hidden because it is managed by hardware circuits.","title":"Physical address"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/","text":"2.2. Segmentation in Hardware 2.2.1. Segment Selectors and Segmentation Registers 2.2.2. Segment Descriptors 2.2.3. Fast Access to Segment Descriptors 2.2.4. Segmentation Unit 2.2. Segmentation in Hardware # Starting with the 80286 model, Intel microprocessors perform address translation in two different ways called real mode and protected mode . We'll focus in the next sections on address translation when protected mode is enabled. Real mode exists mostly to maintain processor compatibility with older models and to allow the operating system to bootstrap (see Appendix A for a short description of real mode). 2.2.1. Segment Selectors and Segmentation Registers # A logical address consists of two parts: a segment identifier and an offset that specifies the relative address within the segment. The segment identifier is a 16-bit field called the Segment Selector (see Figure 2-2), while the offset is a 32-bit field. We'll describe the fields of Segment Selectors in the section \"Fast Access to Segment Descriptors\" later in this chapter. SUMMARY : \u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5730\u5740\u7684\u957f\u5ea6\u662f\uff1a16 + 32 = 48 SUMMARY : \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cSegment Selector\u9664\u4e86\u5305\u542b\u6709table index\u4e4b\u5916\uff0c\u8fd8\u5305\u542b\u6709\u5176\u4ed6\u7684\u4fe1\u606f\uff1b To make it easy to retrieve segment selectors quickly, the processor provides segmentation registers whose only purpose is to hold Segment Selectors; these registers are called cs , ss , ds , es , fs , and gs . Although there are only six of them, a program can reuse the same segmentation register for different purposes by saving its content in memory and then restoring it later. Three of the six segmentation registers have specific purposes: cs The code segment register, which points to a segment containing program instructions ss The stack segment register, which points to a segment containing the current program stack ds The data segment register, which points to a segment containing global and static data The remaining three segmentation registers are general purpose and may refer to arbitrary data segments. The cs register has another important function: it includes a 2-bit field that specifies the Current Privilege Level (CPL) of the CPU. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called Kernel Mode and User Mode . 2.2.2. Segment Descriptors # Each segment is represented by an 8-byte Segment Descriptor that describes the segment characteristics. Segment Descriptors are stored either in the Global Descriptor Table ( GDT ) or in the Local Descriptor Table( LDT ) . Usually only one GDT is defined, while each process is permitted to have its own LDT if it needs to create additional segments besides those stored in the GDT . The address and size of the GDT in main memory are contained in the gdtr control register, while the address and size of the currently used LDT are contained in the ldtr control register. Figure 2-3 illustrates the format of a Segment Descriptor ; the meaning of the various fields is explained in Table 2-1. Table 2-1. Segment Descriptor fields Field name Description Base Contains the linear address of the first byte of the segment. G Granularity flag : if it is cleared (equal to 0), the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes. Limit Holds the offset of the last memory cell in the segment, thus binding the segment length. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB. S System flag : if it is cleared, the segment is a system segment that stores critical data structures such as the Local Descriptor Table ; otherwise, it is a normal code or data segment. Type Characterizes the segment type and its access rights (see the text that follows this table). DPL Descriptor Privilege Level: used to restrict accesses to the segment. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 that is, in Kernel Mode while a segment with its DPL set to 3 is accessible with every CPL value. P Segment-Present flag : is equal to 0 if the segment is not stored currently in main memory. Linux always sets this flag (bit 47) to 1, because it never swaps out whole segments to disk. There are several types of segments, and thus several types of Segment Descriptors . The following list shows the types that are widely used in Linux. Code Segment Descriptor Indicates that the Segment Descriptor refers to a code segment; it may be included either in the GDT or in the LDT . The descriptor has the S flag set (non-system segment). Data Segment Descriptor Indicates that the Segment Descriptor refers to a data segment ; it may be included either in the GDT or in the LDT . The descriptor has the S flag set. Stack segments are implemented by means of generic data segments. Task State Segment Descriptor (TSSD) Indicates that the Segment Descriptor refers to a Task State Segment (TSS) that is, a segment used to save the contents of the processor registers (see the section \"Task State Segment\" in Chapter 3); it can appear only in the GDT. The corresponding Type field has the value 11 or 9, depending on whether the corresponding process is currently executing on a CPU. The S flag of such descriptors is set to 0. SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6ca1\u6709stack segment descriptor\uff1b\u6839\u636e\u7b2c2.3\u7ae0\u7684\u5185\u5bb9\u6765\u770b\uff0c\u8fd9\u662f\u56e0\u4e3astack segment\u662f inside data segment\u7684\uff1b 2.2.3. Fast Access to Segment Descriptors # We recall that logical addresses consist of a 16-bit Segment Selector and a 32-bit Offset, and that segmentation registers store only the Segment Selector. To speed up the translation of logical addresses into linear addresses , the 80 x 86 processor provides an additional nonprogrammable register that is, a register that cannot be set by a programmer for each of the six programmable segmentation registers. Each nonprogrammable register contains the 8-byte Segment Descriptor (described in the previous section) specified by the Segment Selector contained in the corresponding segmentation register . Every time a Segment Selector is loaded in a segmentation register , the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT or LDT stored in main memory; the processor can refer only directly to the CPU register containing the Segment Descriptor. Accesses to the GDT or LDT are necessary only when the contents of the segmentation registers change (see Figure 2-4). Any Segment Selector includes three fields that are described in Table 2-2. Table 2-2. Segment Selector fields Field name Description index Identifies the Segment Descriptor entry contained in the GDT or in the LDT (described further in the text following this table). TI Table Indicator : specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). RPL Requestor Privilege Level : specifies the Current Privilege Level of the CPU when the corresponding Segment Selector is loaded into the cs register; it also may be used to selectively weaken the processor privilege level when accessing data segments (see Intel documentation for details). Because a Segment Descriptor is 8 bytes long, its relative address inside the GDT or the LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8. For instance, if the GDT is at 0x00020000 (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is 0x00020000 + (2 x 8) , or 0x00020010 . 2.2.4. Segmentation Unit # Figure 2-5 shows in detail how a logical address is translated into a corresponding linear address . The segmentation unit performs the following operations: Examines the TI field of the Segment Selector to determine which Descriptor Table stores the Segment Descriptor . This field indicates that the Descriptor is either in the GDT (in which case the segmentation unit gets the base linear address of the GDT from the gdtr register) or in the active LDT (in which case the segmentation unit gets the base linear address of that LDT from the ldtr register). Computes the address of the Segment Descriptor from the index field of the Segment Selector. The index field is multiplied by 8 (the size of a Segment Descriptor), and the result is added to the content of the gdtr or ldtr register. Adds the offset of the logical address to the Base field of the Segment Descriptor, thus obtaining the linear address. Notice that, thanks to the nonprogrammable registers associated with the segmentation registers, the first two operations need to be performed only when a segmentation register has been changed.","title":"2.2 Segmentation in Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#22-segmentation-in-hardware","text":"Starting with the 80286 model, Intel microprocessors perform address translation in two different ways called real mode and protected mode . We'll focus in the next sections on address translation when protected mode is enabled. Real mode exists mostly to maintain processor compatibility with older models and to allow the operating system to bootstrap (see Appendix A for a short description of real mode).","title":"2.2. Segmentation in Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#221-segment-selectors-and-segmentation-registers","text":"A logical address consists of two parts: a segment identifier and an offset that specifies the relative address within the segment. The segment identifier is a 16-bit field called the Segment Selector (see Figure 2-2), while the offset is a 32-bit field. We'll describe the fields of Segment Selectors in the section \"Fast Access to Segment Descriptors\" later in this chapter. SUMMARY : \u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5730\u5740\u7684\u957f\u5ea6\u662f\uff1a16 + 32 = 48 SUMMARY : \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cSegment Selector\u9664\u4e86\u5305\u542b\u6709table index\u4e4b\u5916\uff0c\u8fd8\u5305\u542b\u6709\u5176\u4ed6\u7684\u4fe1\u606f\uff1b To make it easy to retrieve segment selectors quickly, the processor provides segmentation registers whose only purpose is to hold Segment Selectors; these registers are called cs , ss , ds , es , fs , and gs . Although there are only six of them, a program can reuse the same segmentation register for different purposes by saving its content in memory and then restoring it later. Three of the six segmentation registers have specific purposes: cs The code segment register, which points to a segment containing program instructions ss The stack segment register, which points to a segment containing the current program stack ds The data segment register, which points to a segment containing global and static data The remaining three segmentation registers are general purpose and may refer to arbitrary data segments. The cs register has another important function: it includes a 2-bit field that specifies the Current Privilege Level (CPL) of the CPU. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called Kernel Mode and User Mode .","title":"2.2.1. Segment Selectors and Segmentation Registers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#222-segment-descriptors","text":"Each segment is represented by an 8-byte Segment Descriptor that describes the segment characteristics. Segment Descriptors are stored either in the Global Descriptor Table ( GDT ) or in the Local Descriptor Table( LDT ) . Usually only one GDT is defined, while each process is permitted to have its own LDT if it needs to create additional segments besides those stored in the GDT . The address and size of the GDT in main memory are contained in the gdtr control register, while the address and size of the currently used LDT are contained in the ldtr control register. Figure 2-3 illustrates the format of a Segment Descriptor ; the meaning of the various fields is explained in Table 2-1. Table 2-1. Segment Descriptor fields Field name Description Base Contains the linear address of the first byte of the segment. G Granularity flag : if it is cleared (equal to 0), the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes. Limit Holds the offset of the last memory cell in the segment, thus binding the segment length. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB. S System flag : if it is cleared, the segment is a system segment that stores critical data structures such as the Local Descriptor Table ; otherwise, it is a normal code or data segment. Type Characterizes the segment type and its access rights (see the text that follows this table). DPL Descriptor Privilege Level: used to restrict accesses to the segment. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 that is, in Kernel Mode while a segment with its DPL set to 3 is accessible with every CPL value. P Segment-Present flag : is equal to 0 if the segment is not stored currently in main memory. Linux always sets this flag (bit 47) to 1, because it never swaps out whole segments to disk. There are several types of segments, and thus several types of Segment Descriptors . The following list shows the types that are widely used in Linux. Code Segment Descriptor Indicates that the Segment Descriptor refers to a code segment; it may be included either in the GDT or in the LDT . The descriptor has the S flag set (non-system segment). Data Segment Descriptor Indicates that the Segment Descriptor refers to a data segment ; it may be included either in the GDT or in the LDT . The descriptor has the S flag set. Stack segments are implemented by means of generic data segments. Task State Segment Descriptor (TSSD) Indicates that the Segment Descriptor refers to a Task State Segment (TSS) that is, a segment used to save the contents of the processor registers (see the section \"Task State Segment\" in Chapter 3); it can appear only in the GDT. The corresponding Type field has the value 11 or 9, depending on whether the corresponding process is currently executing on a CPU. The S flag of such descriptors is set to 0. SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6ca1\u6709stack segment descriptor\uff1b\u6839\u636e\u7b2c2.3\u7ae0\u7684\u5185\u5bb9\u6765\u770b\uff0c\u8fd9\u662f\u56e0\u4e3astack segment\u662f inside data segment\u7684\uff1b","title":"2.2.2. Segment Descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#223-fast-access-to-segment-descriptors","text":"We recall that logical addresses consist of a 16-bit Segment Selector and a 32-bit Offset, and that segmentation registers store only the Segment Selector. To speed up the translation of logical addresses into linear addresses , the 80 x 86 processor provides an additional nonprogrammable register that is, a register that cannot be set by a programmer for each of the six programmable segmentation registers. Each nonprogrammable register contains the 8-byte Segment Descriptor (described in the previous section) specified by the Segment Selector contained in the corresponding segmentation register . Every time a Segment Selector is loaded in a segmentation register , the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT or LDT stored in main memory; the processor can refer only directly to the CPU register containing the Segment Descriptor. Accesses to the GDT or LDT are necessary only when the contents of the segmentation registers change (see Figure 2-4). Any Segment Selector includes three fields that are described in Table 2-2. Table 2-2. Segment Selector fields Field name Description index Identifies the Segment Descriptor entry contained in the GDT or in the LDT (described further in the text following this table). TI Table Indicator : specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). RPL Requestor Privilege Level : specifies the Current Privilege Level of the CPU when the corresponding Segment Selector is loaded into the cs register; it also may be used to selectively weaken the processor privilege level when accessing data segments (see Intel documentation for details). Because a Segment Descriptor is 8 bytes long, its relative address inside the GDT or the LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8. For instance, if the GDT is at 0x00020000 (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is 0x00020000 + (2 x 8) , or 0x00020010 .","title":"2.2.3. Fast Access to Segment Descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#224-segmentation-unit","text":"Figure 2-5 shows in detail how a logical address is translated into a corresponding linear address . The segmentation unit performs the following operations: Examines the TI field of the Segment Selector to determine which Descriptor Table stores the Segment Descriptor . This field indicates that the Descriptor is either in the GDT (in which case the segmentation unit gets the base linear address of the GDT from the gdtr register) or in the active LDT (in which case the segmentation unit gets the base linear address of that LDT from the ldtr register). Computes the address of the Segment Descriptor from the index field of the Segment Selector. The index field is multiplied by 8 (the size of a Segment Descriptor), and the result is added to the content of the gdtr or ldtr register. Adds the offset of the logical address to the Base field of the Segment Descriptor, thus obtaining the linear address. Notice that, thanks to the nonprogrammable registers associated with the segmentation registers, the first two operations need to be performed only when a segmentation register has been changed.","title":"2.2.4. Segmentation Unit"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/","text":"2.3. Segmentation in Linux 2.3.1. The Linux GDT \u8865\u5145\u5185\u5bb9 Segmentation in Linux : Segmentation & Paging are redundant? A Does Linux not use segmentation but only paging? A A x86 memory segmentation Later developments 2.3. Segmentation in Linux # Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses . One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80 x 86 architecture. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. Notice that the linear addresses associated with such segments all start at 0 and reach the addressing limit of $2^{32} -1$. This means that all processes, either in User Mode or in Kernel Mode, may use the same logical addresses. SUMMARY : \u4e0a\u8ff0\u65ad\u8a00\u975e\u5e38\u5177\u6709\u4ef7\u503c Another important consequence of having all segments start at 0x00000000 is that in Linux, logical addresses coincide with linear addresses; that is, the value of the Offset field of a logical address always coincides with the value of the corresponding linear address . As stated earlier, the Current Privilege Level of the CPU indicates whether the processor is in User or Kernel Mode and is specified by the RPL field of the Segment Selector stored in the cs register. Whenever the CPL is changed, some segmentation registers must be correspondingly updated. For instance, when the CPL is equal to 3 (User Mode), the ds register must contain the Segment Selector of the user data segment, but when the CPL is equal to 0, the ds register must contain the Segment Selector of the kernel data segment. A similar situation occurs for the ss register. It must refer to a User Mode stack inside the user data segment when the CPL is 3, and it must refer to a Kernel Mode stack inside the kernel data segment when the CPL is 0. When switching from User Mode to Kernel Mode, Linux always makes sure that the ss register contains the Segment Selector of the kernel data segment . SUMMARY : stack\u662finside data segment\u7684\uff0c\u5e76\u4e14\u5b83\u4eec\u5171\u7528\u540c\u4e00\u4e2adescriptor\uff1b When saving a pointer to an instruction or to a data structure, the kernel does not need to store the Segment Selector component of the logical address , because the ss register contains the current Segment Selector . As an example, when the kernel invokes a function, it executes a call assembly language instruction specifying just the Offset component of its logical address; the Segment Selector is implicitly selected as the one referred to by the cs register. Because there is just one segment of type \"executable in Kernel Mode,\" namely the code segment identified by __KERNEL_CS , it is sufficient to load __ KERNEL_CS into cs whenever the CPU switches to Kernel Mode. The same argument goes for pointers to kernel data structures (implicitly using the ds register), as well as for pointers to user data structures (the kernel explicitly uses the es register). SUMMARY : \u6ca1\u6709\u8bfb\u61c2 Besides the four segments just described, Linux makes use of a few other specialized segments. We'll introduce them in the next section while describing the Linux GDT . 2.3.1. The Linux GDT # In uniprocessor systems there is only one GDT , while in multiprocessor systems there is one GDT for every CPU in the system. All GDT s are stored in the cpu_gdt_table array, while the addresses and sizes of the GDT s (used when initializing the gdtr registers) are stored in the cpu_gdt_descr array. If you look in the Source Code Index, you can see that these symbols are defined in the file arch/i386/kernel/head.S . Every macro, function, and other symbol in this book is listed in the Source Code Index, so you can quickly find it in the source code. \u8865\u5145\u5185\u5bb9 # Segmentation in Linux : Segmentation & Paging are redundant? # I'm reading \"Understanding Linux Kernel\". This is the snippet that explains how Linux uses Segmentation which I didn't understand. Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant , because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses. One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. I'm unable to understand 1st and last paragraph. A # The 80x86 family of CPUs generate a real address by adding the contents of a CPU register called a segment register to that of the program counter. Thus by changing the segment register contents you can change the physical addresses that the program accesses. Paging does something similar by mapping the same virtual address to different real addresses . Linux using uses the latter - the segment registers for Linux processes will always have the same unchanging contents. +1. Linux, and everyone else too nowadays. \u2013 Billy ONeal Jun 12 '10 at 15:14 7 In protected mode it's not actually the contents of the segment register itself that is added to addresses; the segment register contains a reference to a segment descriptor (stored in memory, in a descriptor table), and one of the fields of the segment descriptor is the base address of the segment, which is added to the offset to generate a linear address. \u2013 caf Jun 16 '10 at 7:42 Segmentation was dropped in x86-64 architecture (or amd64 is Linux calls it). This newer architecture uses the flat memory model. \u2013 hebbo May 19 '14 at 3:43 @caf Thanks for the elaboration about protected mode. Here's more info about the different CPU modes for those who are curious. \u2013 GDP2 Nov 12 '17 at 20:18 Does Linux not use segmentation but only paging? # The Linux Programming Interface shows the layout of a virtual address space of a process. Is each region in the diagram a segment? From Understanding The Linux Kernel , is it correct that the following means that the segmentation unit in MMU maps the segments and offsets within segments into the virtual memory address, and the paging unit then maps the virtual memory address to the physical memory address? The Memory Management Unit (MMU) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). Then why does it say that Linux doesn't use segmentation but only paging? Segmentation has been included in 80x86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: \u2022 Memory management is simpler when all processes use the same segment register values\u2014that is, when they share the same set of linear addresses. \u2022 One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures, in particular, have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80x86 architecture. A # The x86-64 architecture does not use segmentation in long mode (64-bit mode). Four of the segment registers: CS, SS, DS, and ES are forced to 0, and the limit to 2^64. https://en.wikipedia.org/wiki/X86_memory_segmentation#Later_developments It is no longer possible for the OS to limit which ranges of the \"linear addresses\" are available. Therefore it cannot use segmentation for memory protection; it must rely entirely on paging. Do not worry about the details of x86 CPUs which would only apply when running in the legacy 32-bit modes. Linux for the 32-bit modes is not used as much. It may even be considered \"in a state of benign neglect for several years\". See 32-Bit x86 support in Fedora [LWN.net, 2017]. (It happens that 32-bit Linux does not use segmentation either. But you don't need to trust me on that, you can just ignore it :-). A # As the x86 has segments, it is not possible to not use them. But both cs (code segment) and ds (data segment) base addresses are set to zero, so the segmentation is not really used. An exception is thread local data, one of the normally unused segment registers points to thread local data . But that is mainly to avoid reserving one of the general purpose registers for this task. It doesn't say that Linux doesn't use segmentation on the x86, as that would not be possible. You already highlighted one part, Linux uses segmentation in a very limited way . The second part is Linux uses segmentation only when required by the 80x86 architecture You already quoted the reasons, paging is easier and more portable. x86 memory segmentation Later developments #","title":"2.3 Segmentation in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#23-segmentation-in-linux","text":"Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses . One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80 x 86 architecture. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. Notice that the linear addresses associated with such segments all start at 0 and reach the addressing limit of $2^{32} -1$. This means that all processes, either in User Mode or in Kernel Mode, may use the same logical addresses. SUMMARY : \u4e0a\u8ff0\u65ad\u8a00\u975e\u5e38\u5177\u6709\u4ef7\u503c Another important consequence of having all segments start at 0x00000000 is that in Linux, logical addresses coincide with linear addresses; that is, the value of the Offset field of a logical address always coincides with the value of the corresponding linear address . As stated earlier, the Current Privilege Level of the CPU indicates whether the processor is in User or Kernel Mode and is specified by the RPL field of the Segment Selector stored in the cs register. Whenever the CPL is changed, some segmentation registers must be correspondingly updated. For instance, when the CPL is equal to 3 (User Mode), the ds register must contain the Segment Selector of the user data segment, but when the CPL is equal to 0, the ds register must contain the Segment Selector of the kernel data segment. A similar situation occurs for the ss register. It must refer to a User Mode stack inside the user data segment when the CPL is 3, and it must refer to a Kernel Mode stack inside the kernel data segment when the CPL is 0. When switching from User Mode to Kernel Mode, Linux always makes sure that the ss register contains the Segment Selector of the kernel data segment . SUMMARY : stack\u662finside data segment\u7684\uff0c\u5e76\u4e14\u5b83\u4eec\u5171\u7528\u540c\u4e00\u4e2adescriptor\uff1b When saving a pointer to an instruction or to a data structure, the kernel does not need to store the Segment Selector component of the logical address , because the ss register contains the current Segment Selector . As an example, when the kernel invokes a function, it executes a call assembly language instruction specifying just the Offset component of its logical address; the Segment Selector is implicitly selected as the one referred to by the cs register. Because there is just one segment of type \"executable in Kernel Mode,\" namely the code segment identified by __KERNEL_CS , it is sufficient to load __ KERNEL_CS into cs whenever the CPU switches to Kernel Mode. The same argument goes for pointers to kernel data structures (implicitly using the ds register), as well as for pointers to user data structures (the kernel explicitly uses the es register). SUMMARY : \u6ca1\u6709\u8bfb\u61c2 Besides the four segments just described, Linux makes use of a few other specialized segments. We'll introduce them in the next section while describing the Linux GDT .","title":"2.3. Segmentation in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#231-the-linux-gdt","text":"In uniprocessor systems there is only one GDT , while in multiprocessor systems there is one GDT for every CPU in the system. All GDT s are stored in the cpu_gdt_table array, while the addresses and sizes of the GDT s (used when initializing the gdtr registers) are stored in the cpu_gdt_descr array. If you look in the Source Code Index, you can see that these symbols are defined in the file arch/i386/kernel/head.S . Every macro, function, and other symbol in this book is listed in the Source Code Index, so you can quickly find it in the source code.","title":"2.3.1. The Linux GDT"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#_1","text":"","title":"\u8865\u5145\u5185\u5bb9"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#segmentation-in-linux-segmentation-paging-are-redundant","text":"I'm reading \"Understanding Linux Kernel\". This is the snippet that explains how Linux uses Segmentation which I didn't understand. Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant , because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses. One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. I'm unable to understand 1st and last paragraph.","title":"Segmentation in Linux : Segmentation &amp; Paging are redundant?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#a","text":"The 80x86 family of CPUs generate a real address by adding the contents of a CPU register called a segment register to that of the program counter. Thus by changing the segment register contents you can change the physical addresses that the program accesses. Paging does something similar by mapping the same virtual address to different real addresses . Linux using uses the latter - the segment registers for Linux processes will always have the same unchanging contents. +1. Linux, and everyone else too nowadays. \u2013 Billy ONeal Jun 12 '10 at 15:14 7 In protected mode it's not actually the contents of the segment register itself that is added to addresses; the segment register contains a reference to a segment descriptor (stored in memory, in a descriptor table), and one of the fields of the segment descriptor is the base address of the segment, which is added to the offset to generate a linear address. \u2013 caf Jun 16 '10 at 7:42 Segmentation was dropped in x86-64 architecture (or amd64 is Linux calls it). This newer architecture uses the flat memory model. \u2013 hebbo May 19 '14 at 3:43 @caf Thanks for the elaboration about protected mode. Here's more info about the different CPU modes for those who are curious. \u2013 GDP2 Nov 12 '17 at 20:18","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#does-linux-not-use-segmentation-but-only-paging","text":"The Linux Programming Interface shows the layout of a virtual address space of a process. Is each region in the diagram a segment? From Understanding The Linux Kernel , is it correct that the following means that the segmentation unit in MMU maps the segments and offsets within segments into the virtual memory address, and the paging unit then maps the virtual memory address to the physical memory address? The Memory Management Unit (MMU) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). Then why does it say that Linux doesn't use segmentation but only paging? Segmentation has been included in 80x86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: \u2022 Memory management is simpler when all processes use the same segment register values\u2014that is, when they share the same set of linear addresses. \u2022 One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures, in particular, have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80x86 architecture.","title":"Does Linux not use segmentation but only paging?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#a_1","text":"The x86-64 architecture does not use segmentation in long mode (64-bit mode). Four of the segment registers: CS, SS, DS, and ES are forced to 0, and the limit to 2^64. https://en.wikipedia.org/wiki/X86_memory_segmentation#Later_developments It is no longer possible for the OS to limit which ranges of the \"linear addresses\" are available. Therefore it cannot use segmentation for memory protection; it must rely entirely on paging. Do not worry about the details of x86 CPUs which would only apply when running in the legacy 32-bit modes. Linux for the 32-bit modes is not used as much. It may even be considered \"in a state of benign neglect for several years\". See 32-Bit x86 support in Fedora [LWN.net, 2017]. (It happens that 32-bit Linux does not use segmentation either. But you don't need to trust me on that, you can just ignore it :-).","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#a_2","text":"As the x86 has segments, it is not possible to not use them. But both cs (code segment) and ds (data segment) base addresses are set to zero, so the segmentation is not really used. An exception is thread local data, one of the normally unused segment registers points to thread local data . But that is mainly to avoid reserving one of the general purpose registers for this task. It doesn't say that Linux doesn't use segmentation on the x86, as that would not be possible. You already highlighted one part, Linux uses segmentation in a very limited way . The second part is Linux uses segmentation only when required by the 80x86 architecture You already quoted the reasons, paging is easier and more portable.","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#x86-memory-segmentation-later-developments","text":"","title":"x86 memory segmentation Later developments"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/","text":"2.4. Paging in Hardware # The paging unit translates linear addresses into physical ones . One key task in the unit is to check the requested access type against the access rights of the linear address . If the memory access is not valid, it generates a Page Fault exception (see Chapter 4 and Chapter 8). For the sake of efficiency, linear addresses are grouped in fixed-length intervals called pages ; contiguous linear addresses within a page are mapped into contiguous physical addresses. In this way, the kernel can specify the physical address and the access rights of a page instead of those of all the linear addresses included in it. Following the usual convention, we shall use the term \"page\" to refer both to a set of linear addresses and to the data contained in this group of addresses. The paging unit thinks of all RAM as partitioned into fixed-length page frames (sometimes referred to as physical pages ). Each page frame contains a page that is, the length of a page frame coincides with that of a page . A page frame is a constituent of main memory, and hence it is a storage area. It is important to distinguish a page from a page frame ; the former is just a block of data, which may be stored in any page frame or on disk . The data structures that map linear to physical addresses are called page tables ; they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit . NOTE: \u6709\u5fc5\u8981\u68b3\u7406\u4e00\u4e0bpaging unit\u548cpage tables\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a\u4e24\u8005\u7ec4\u5408\u8d77\u6765\u5b9e\u73b0\u5c06linear address\u8f6c\u6362\u4e3aphysical address\uff0cpaging unit\u9700\u8981\u4f9d\u8d56\u4e8epage tables\u4e2d\u7684\u6570\u636e\u3002 Starting with the 80386, all 80 x 86 processors support paging ; it is enabled by setting the PG flag of a control register named cr0 . When PG = 0 , linear addresses are interpreted as physical addresses . 2.4.1. Regular Paging # Starting with the 80386, the paging unit of Intel processors handles 4 KB pages. The 32 bits of a linear address are divided into three fields: Directory The most significant 10 bits Table The intermediate 10 bits Offset The least significant 12 bits The translation of linear addresses is accomplished in two steps, each based on a type of translation table . The first translation table is called the Page Directory , and the second is called the Page Table . [ * ] [ * ] In the discussion that follows, the lowercase \"page table\" term denotes any page storing the mapping between linear and physical addresses, while the capitalized \"Page Table\" term denotes a page in the last level of page tables. The aim of this two-level scheme is to reduce the amount of RAM required for per-process Page Tables . If a simple one-level Page Table was used, then it would require up to 220 entries (i.e., at 4 bytes per entry, 4 MB of RAM) to represent the Page Table for each process (if the process used a full 4 GB linear address space), even though a process does not use all addresses in that range. The two-level scheme reduces the memory by requiring Page Tables only for those virtual memory regions actually used by a process. Each active process must have a Page Directory assigned to it. However, there is no need to allocate RAM for all Page Tables of a process at once; it is more efficient to allocate RAM for a Page Table only when the process effectively needs it. NOTE: \u7efc\u5408\u4e0a\u9762\u7684\u5185\u5bb9\u53ef\u4ee5\u77e5\u9053\uff0cpage table\u7684\u5b9e\u73b0\u53ef\u4ee5\u4e0e\u4e24\u79cd\u65b9\u6848\uff1a \u4f7f\u7528\u7ebf\u6027\u7ed3\u6784\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u6240\u8bf4\u7684one-level page table \u4f7f\u7528multi-level\u7ed3\u6784\uff0cmulti-level\u7ed3\u6784\u5176\u5b9e\u672c\u8d28\u4e0a\u662fTree\u7ed3\u6784\uff0c\u4e0a\u8ff0two-level scheme\u6240\u5bf9\u5e94\u7684page table\u7684\u7ed3\u6784\u5c31\u662f\u4e00\u4e2atwo-level\u7ed3\u6784\uff0c\u53c2\u89c1Figure 2-7\u3002\u5b83\u7684Tree\u7ed3\u6784\u662f\u8fd9\u6837\u7684\uff1a cr3 \u76f8\u5f53\u4e8eroot\u8282\u70b9\uff0c\u8fd9\u4e2aroot\u8282\u70b9\u53ea\u6709\u4e00\u4e2a\u5b50\u8282\u70b9Page Directory\uff0cPage Directory\u5171\u6709$2^{10}$\u4e2aentry\uff0c\u6bcf\u4e2aentry\u6307\u5411\u4e00\u4e2aPage Table\uff0c\u5373Page Directory\u5171\u6709$2^{10}$\u4e2a\u5b50\u8282\u70b9\uff0c\u5b50\u8282\u70b9\u7684\u7c7b\u578b\u662fPage Table\u3002\u6bcf\u4e2aPage Table\u6709$2^{10}$\u4e2apage\uff0c\u5373Page Table\u6709$2^{10}$\u4e2a\u5b50\u8282\u70b9\uff0c\u5b50\u8282\u70b9\u7684\u7c7b\u578b\u662fpage\uff0cpage\u6ca1\u6709\u5b50\u8282\u70b9\uff0c\u6240\u4ee5\u5b83\u662fleaf\u3002\u5728multi-level\u7ed3\u6784\u7684page table\u4e2d\u8fdb\u884c\u6620\u5c04\u7684\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u4eceroot\u8282\u70b9\u6cbf\u7740\u5185\u8282\u70b9\u5230leaf\u3002 \u6bcf\u4e2aprocess\u6709\u4e00\u4e2a Page Directory \uff0c\u800c\u4e0d\u662f Page Directory \u4e2d\u7684\u4e00\u6761\u8bb0\u5f55\uff1b\u4e3a\u4ec0\u4e48two-level scheme\u80fd\u591f\u51cf\u5c11\u9700\u8981\u7684RAM\uff0c\u53c2\u89c1\u4e0b\u9762\u5185\u5bb9\uff1a Multilevel Paging in Operating System https://www.clear.rice.edu/comp425/slides/L31.pdf How does multi-level page table save memory space? The physical address of the Page Directory in use is stored in a control register named cr3 . The Directory field within the linear address determines the entry in the Page Directory that points to the proper Page Table . The address's Table field, in turn, determines the entry in the Page Table that contains the physical address of the page frame containing the page. The Offset field determines the relative position within the page frame (see Figure 2-7). Because it is 12 bits long, each page consists of 4096 bytes of data. Both the Directory and the Table fields are 10 bits long, so Page Directories and Page Tables can include up to 1,024\uff08$2^{10}=1024$\uff09 entries. It follows that a Page Directory can address up to $1024 * 1024 * 4096=2^{32}$ memory cells, as you'd expect in 32-bit addresses. The entries of Page Directories and Page Tables have the same structure. Each entry includes the following fields: Present flag If it is set, the referred-to page (or Page Table) is contained in main memory; if the flag is 0, the page is not contained in main memory and the remaining entry bits may be used by the operating system for its own purposes. If the entry of a Page Table or Page Directory needed to perform an address translation has the Present flag cleared, the paging unit stores the linear address in a control register named cr2 and generates exception 14: the Page Fault exception. (We will see in Chapter 17 how Linux uses this field.) NOTE: \u4ea7\u751f\u4e86Page Fault exception\u540e\uff0c\u5c31\u9700\u8981\u5c06demand page swap\u5230memory\u4e2d Field containing the 20 most significant bits of a page frame physical address Accessed flag Dirty flag 2.4.2. Extended Paging # Starting with the Pentium model, 80 x 86 microprocessors introduce extended paging , which allows page frames to be 4 MB instead of 4 KB in size (see Figure 2-8). Extended paging is used to translate large contiguous linear address ranges into corresponding physical ones; in these cases, the kernel can do without intermediate Page Tables and thus save memory and preserve TLB entries (see the section \"Translation Lookaside Buffers (TLB)\"). 2.4.3. Hardware Protection Scheme # 2.4.4. An Example of Regular Paging # 2.4.5. The Physical Address Extension (PAE) Paging Mechanism # 2.4.6. Paging for 64-bit Architectures # 2.4.7. Hardware Cache # 2.4.8. Translation Lookaside Buffers (TLB) #","title":"2.4-Paging-in-Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#24-paging-in-hardware","text":"The paging unit translates linear addresses into physical ones . One key task in the unit is to check the requested access type against the access rights of the linear address . If the memory access is not valid, it generates a Page Fault exception (see Chapter 4 and Chapter 8). For the sake of efficiency, linear addresses are grouped in fixed-length intervals called pages ; contiguous linear addresses within a page are mapped into contiguous physical addresses. In this way, the kernel can specify the physical address and the access rights of a page instead of those of all the linear addresses included in it. Following the usual convention, we shall use the term \"page\" to refer both to a set of linear addresses and to the data contained in this group of addresses. The paging unit thinks of all RAM as partitioned into fixed-length page frames (sometimes referred to as physical pages ). Each page frame contains a page that is, the length of a page frame coincides with that of a page . A page frame is a constituent of main memory, and hence it is a storage area. It is important to distinguish a page from a page frame ; the former is just a block of data, which may be stored in any page frame or on disk . The data structures that map linear to physical addresses are called page tables ; they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit . NOTE: \u6709\u5fc5\u8981\u68b3\u7406\u4e00\u4e0bpaging unit\u548cpage tables\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a\u4e24\u8005\u7ec4\u5408\u8d77\u6765\u5b9e\u73b0\u5c06linear address\u8f6c\u6362\u4e3aphysical address\uff0cpaging unit\u9700\u8981\u4f9d\u8d56\u4e8epage tables\u4e2d\u7684\u6570\u636e\u3002 Starting with the 80386, all 80 x 86 processors support paging ; it is enabled by setting the PG flag of a control register named cr0 . When PG = 0 , linear addresses are interpreted as physical addresses .","title":"2.4. Paging in Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#241-regular-paging","text":"Starting with the 80386, the paging unit of Intel processors handles 4 KB pages. The 32 bits of a linear address are divided into three fields: Directory The most significant 10 bits Table The intermediate 10 bits Offset The least significant 12 bits The translation of linear addresses is accomplished in two steps, each based on a type of translation table . The first translation table is called the Page Directory , and the second is called the Page Table . [ * ] [ * ] In the discussion that follows, the lowercase \"page table\" term denotes any page storing the mapping between linear and physical addresses, while the capitalized \"Page Table\" term denotes a page in the last level of page tables. The aim of this two-level scheme is to reduce the amount of RAM required for per-process Page Tables . If a simple one-level Page Table was used, then it would require up to 220 entries (i.e., at 4 bytes per entry, 4 MB of RAM) to represent the Page Table for each process (if the process used a full 4 GB linear address space), even though a process does not use all addresses in that range. The two-level scheme reduces the memory by requiring Page Tables only for those virtual memory regions actually used by a process. Each active process must have a Page Directory assigned to it. However, there is no need to allocate RAM for all Page Tables of a process at once; it is more efficient to allocate RAM for a Page Table only when the process effectively needs it. NOTE: \u7efc\u5408\u4e0a\u9762\u7684\u5185\u5bb9\u53ef\u4ee5\u77e5\u9053\uff0cpage table\u7684\u5b9e\u73b0\u53ef\u4ee5\u4e0e\u4e24\u79cd\u65b9\u6848\uff1a \u4f7f\u7528\u7ebf\u6027\u7ed3\u6784\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u6240\u8bf4\u7684one-level page table \u4f7f\u7528multi-level\u7ed3\u6784\uff0cmulti-level\u7ed3\u6784\u5176\u5b9e\u672c\u8d28\u4e0a\u662fTree\u7ed3\u6784\uff0c\u4e0a\u8ff0two-level scheme\u6240\u5bf9\u5e94\u7684page table\u7684\u7ed3\u6784\u5c31\u662f\u4e00\u4e2atwo-level\u7ed3\u6784\uff0c\u53c2\u89c1Figure 2-7\u3002\u5b83\u7684Tree\u7ed3\u6784\u662f\u8fd9\u6837\u7684\uff1a cr3 \u76f8\u5f53\u4e8eroot\u8282\u70b9\uff0c\u8fd9\u4e2aroot\u8282\u70b9\u53ea\u6709\u4e00\u4e2a\u5b50\u8282\u70b9Page Directory\uff0cPage Directory\u5171\u6709$2^{10}$\u4e2aentry\uff0c\u6bcf\u4e2aentry\u6307\u5411\u4e00\u4e2aPage Table\uff0c\u5373Page Directory\u5171\u6709$2^{10}$\u4e2a\u5b50\u8282\u70b9\uff0c\u5b50\u8282\u70b9\u7684\u7c7b\u578b\u662fPage Table\u3002\u6bcf\u4e2aPage Table\u6709$2^{10}$\u4e2apage\uff0c\u5373Page Table\u6709$2^{10}$\u4e2a\u5b50\u8282\u70b9\uff0c\u5b50\u8282\u70b9\u7684\u7c7b\u578b\u662fpage\uff0cpage\u6ca1\u6709\u5b50\u8282\u70b9\uff0c\u6240\u4ee5\u5b83\u662fleaf\u3002\u5728multi-level\u7ed3\u6784\u7684page table\u4e2d\u8fdb\u884c\u6620\u5c04\u7684\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u4eceroot\u8282\u70b9\u6cbf\u7740\u5185\u8282\u70b9\u5230leaf\u3002 \u6bcf\u4e2aprocess\u6709\u4e00\u4e2a Page Directory \uff0c\u800c\u4e0d\u662f Page Directory \u4e2d\u7684\u4e00\u6761\u8bb0\u5f55\uff1b\u4e3a\u4ec0\u4e48two-level scheme\u80fd\u591f\u51cf\u5c11\u9700\u8981\u7684RAM\uff0c\u53c2\u89c1\u4e0b\u9762\u5185\u5bb9\uff1a Multilevel Paging in Operating System https://www.clear.rice.edu/comp425/slides/L31.pdf How does multi-level page table save memory space? The physical address of the Page Directory in use is stored in a control register named cr3 . The Directory field within the linear address determines the entry in the Page Directory that points to the proper Page Table . The address's Table field, in turn, determines the entry in the Page Table that contains the physical address of the page frame containing the page. The Offset field determines the relative position within the page frame (see Figure 2-7). Because it is 12 bits long, each page consists of 4096 bytes of data. Both the Directory and the Table fields are 10 bits long, so Page Directories and Page Tables can include up to 1,024\uff08$2^{10}=1024$\uff09 entries. It follows that a Page Directory can address up to $1024 * 1024 * 4096=2^{32}$ memory cells, as you'd expect in 32-bit addresses. The entries of Page Directories and Page Tables have the same structure. Each entry includes the following fields: Present flag If it is set, the referred-to page (or Page Table) is contained in main memory; if the flag is 0, the page is not contained in main memory and the remaining entry bits may be used by the operating system for its own purposes. If the entry of a Page Table or Page Directory needed to perform an address translation has the Present flag cleared, the paging unit stores the linear address in a control register named cr2 and generates exception 14: the Page Fault exception. (We will see in Chapter 17 how Linux uses this field.) NOTE: \u4ea7\u751f\u4e86Page Fault exception\u540e\uff0c\u5c31\u9700\u8981\u5c06demand page swap\u5230memory\u4e2d Field containing the 20 most significant bits of a page frame physical address Accessed flag Dirty flag","title":"2.4.1. Regular Paging"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#242-extended-paging","text":"Starting with the Pentium model, 80 x 86 microprocessors introduce extended paging , which allows page frames to be 4 MB instead of 4 KB in size (see Figure 2-8). Extended paging is used to translate large contiguous linear address ranges into corresponding physical ones; in these cases, the kernel can do without intermediate Page Tables and thus save memory and preserve TLB entries (see the section \"Translation Lookaside Buffers (TLB)\").","title":"2.4.2. Extended Paging"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#243-hardware-protection-scheme","text":"","title":"2.4.3. Hardware Protection Scheme"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#244-an-example-of-regular-paging","text":"","title":"2.4.4. An Example of Regular Paging"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#245-the-physical-address-extension-pae-paging-mechanism","text":"","title":"2.4.5. The Physical Address Extension (PAE) Paging Mechanism"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#246-paging-for-64-bit-architectures","text":"","title":"2.4.6. Paging for 64-bit Architectures"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#247-hardware-cache","text":"","title":"2.4.7. Hardware Cache"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#248-translation-lookaside-buffers-tlb","text":"","title":"2.4.8. Translation Lookaside Buffers (TLB)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.5-Paging-in-Linux/","text":"2.5. Paging in Linux # Linux adopts a common paging model that fits both 32-bit and 64-bit architectures. As explained in the earlier section \"Paging for 64-bit Architectures,\" two paging levels are sufficient for 32-bit architectures, while 64-bit architectures require a higher number of paging levels. Up to version 2.6.10, the Linux paging model consisted of three paging levels . Starting with version 2.6.11, a four-level paging model has been adopted. [ * ] The four types of page tables illustrated in Figure 2-12 are called: [ * ] This change has been made to fully support the linear address bit splitting used by the x86_64 platform (see Table 2-4). Page Global Directory Page Upper Directory Page Middle Directory Page Table Figure 2-12. The Linux paging model The Page Global Directory includes the addresses of several Page Upper Directories , which in turn include the addresses of several Page Middle Directories , which in turn include the addresses of several Page Tables . Each Page Table entry points to a page frame . Thus the linear address can be split into up to five parts. Figure 2-12 does not show the bit numbers, because the size of each part depends on the computer architecture. For 32-bit architectures with no Physical Address Extension , two paging levels are sufficient. Linux essentially eliminates the Page Upper Directory and the Page Middle Directory fields by saying that they contain zero bits. However, the positions of the Page Upper Directory and the Page Middle Directory in the sequence of pointers are kept so that the same code can work on 32-bit and 64-bit architectures. The kernel keeps a position for the Page Upper Directory and the Page Middle Directory by setting the number of entries in them to 1 and mapping these two entries into the proper entry of the Page Global Directory . Finally, for 64-bit architectures three or four levels of paging are used depending on the linear address bit splitting performed by the hardware (see Table 2-2). Linux's handling of processes relies heavily on paging . In fact, the automatic translation of linear addresses into physical ones makes the following design objectives feasible: Assign a different physical address space to each process, ensuring an efficient protection against addressing errors. Distinguish pages (groups of data) from page frames (physical addresses in main memory). This allows the same page to be stored in a page frame, then saved to disk and later reloaded in a different page frame. This is the basic ingredient of the virtual memory mechanism (see Chapter 17). In the remaining part of this chapter, we will refer for the sake of concreteness to the paging circuitry used by the 80 x 86 processors. As we will see in Chapter 9, each process has its own Page Global Directory and its own set of Page Tables . When a process switch occurs (see the section \"Process Switch\" in Chapter 3), Linux saves the cr3 control register in the descriptor of the process previously in execution and then loads cr3 with the value stored in the descriptor of the process to be executed next. Thus, when the new process resumes its execution on the CPU, the paging unit refers to the correct set of Page Tables . Mapping linear to physical addresses now becomes a mechanical task, although it is still somewhat complex. The next few sections of this chapter are a rather tedious list of functions and macros that retrieve information the kernel needs to find addresses and manage the tables; most of the functions are one or two lines long. You may want to only skim these sections now, but it is useful to know the role of these functions and macros, because you'll see them often in discussions throughout this book. 2.5.1. The Linear Address Fields # The following macros simplify Page Table handling:","title":"2.5-Paging-in-Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.5-Paging-in-Linux/#25-paging-in-linux","text":"Linux adopts a common paging model that fits both 32-bit and 64-bit architectures. As explained in the earlier section \"Paging for 64-bit Architectures,\" two paging levels are sufficient for 32-bit architectures, while 64-bit architectures require a higher number of paging levels. Up to version 2.6.10, the Linux paging model consisted of three paging levels . Starting with version 2.6.11, a four-level paging model has been adopted. [ * ] The four types of page tables illustrated in Figure 2-12 are called: [ * ] This change has been made to fully support the linear address bit splitting used by the x86_64 platform (see Table 2-4). Page Global Directory Page Upper Directory Page Middle Directory Page Table Figure 2-12. The Linux paging model The Page Global Directory includes the addresses of several Page Upper Directories , which in turn include the addresses of several Page Middle Directories , which in turn include the addresses of several Page Tables . Each Page Table entry points to a page frame . Thus the linear address can be split into up to five parts. Figure 2-12 does not show the bit numbers, because the size of each part depends on the computer architecture. For 32-bit architectures with no Physical Address Extension , two paging levels are sufficient. Linux essentially eliminates the Page Upper Directory and the Page Middle Directory fields by saying that they contain zero bits. However, the positions of the Page Upper Directory and the Page Middle Directory in the sequence of pointers are kept so that the same code can work on 32-bit and 64-bit architectures. The kernel keeps a position for the Page Upper Directory and the Page Middle Directory by setting the number of entries in them to 1 and mapping these two entries into the proper entry of the Page Global Directory . Finally, for 64-bit architectures three or four levels of paging are used depending on the linear address bit splitting performed by the hardware (see Table 2-2). Linux's handling of processes relies heavily on paging . In fact, the automatic translation of linear addresses into physical ones makes the following design objectives feasible: Assign a different physical address space to each process, ensuring an efficient protection against addressing errors. Distinguish pages (groups of data) from page frames (physical addresses in main memory). This allows the same page to be stored in a page frame, then saved to disk and later reloaded in a different page frame. This is the basic ingredient of the virtual memory mechanism (see Chapter 17). In the remaining part of this chapter, we will refer for the sake of concreteness to the paging circuitry used by the 80 x 86 processors. As we will see in Chapter 9, each process has its own Page Global Directory and its own set of Page Tables . When a process switch occurs (see the section \"Process Switch\" in Chapter 3), Linux saves the cr3 control register in the descriptor of the process previously in execution and then loads cr3 with the value stored in the descriptor of the process to be executed next. Thus, when the new process resumes its execution on the CPU, the paging unit refers to the correct set of Page Tables . Mapping linear to physical addresses now becomes a mechanical task, although it is still somewhat complex. The next few sections of this chapter are a rather tedious list of functions and macros that retrieve information the kernel needs to find addresses and manage the tables; most of the functions are one or two lines long. You may want to only skim these sections now, but it is useful to know the role of these functions and macros, because you'll see them often in discussions throughout this book.","title":"2.5. Paging in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.5-Paging-in-Linux/#251-the-linear-address-fields","text":"The following macros simplify Page Table handling:","title":"2.5.1. The Linear Address Fields"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Chapter-2-Memory-Addressing/","text":"Chapter 2. Memory Addressing # This chapter deals with addressing techniques. Luckily, an operating system is not forced to keep track of physical memory all by itself; today's microprocessors include several hardware circuits to make memory management both more efficient and more robust so that programming errors cannot cause improper accesses to memory outside the program. As in the rest of this book, we offer details in this chapter on how 80 x 86 microprocessors address memory chips and how Linux uses the available addressing circuits. You will find, we hope, that when you learn the implementation details on Linux's most popular platform you will better understand both the general theory of paging and how to research the implementation on other platforms. This is the first of three chapters related to memory management; Chapter 8 discusses how the kernel allocates main memory to itself, while Chapter 9 considers how linear addresses are assigned to processes . NOTE: \u76ee\u524dlinux\u91c7\u7528\u7684\u662f\u57fa\u4e8epage\u7684memory management\uff0c\u6240\u4ee5\u672c\u7ae0\u76842.2. Segmentation in Hardware\u548c2.3. Segmentation in Linux\u7701\u7565\u4e86\u3002","title":"Chapter-2-Memory-Addressing"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Chapter-2-Memory-Addressing/#chapter-2-memory-addressing","text":"This chapter deals with addressing techniques. Luckily, an operating system is not forced to keep track of physical memory all by itself; today's microprocessors include several hardware circuits to make memory management both more efficient and more robust so that programming errors cannot cause improper accesses to memory outside the program. As in the rest of this book, we offer details in this chapter on how 80 x 86 microprocessors address memory chips and how Linux uses the available addressing circuits. You will find, we hope, that when you learn the implementation details on Linux's most popular platform you will better understand both the general theory of paging and how to research the implementation on other platforms. This is the first of three chapters related to memory management; Chapter 8 discusses how the kernel allocates main memory to itself, while Chapter 9 considers how linear addresses are assigned to processes . NOTE: \u76ee\u524dlinux\u91c7\u7528\u7684\u662f\u57fa\u4e8epage\u7684memory management\uff0c\u6240\u4ee5\u672c\u7ae0\u76842.2. Segmentation in Hardware\u548c2.3. Segmentation in Linux\u7701\u7565\u4e86\u3002","title":"Chapter 2. Memory Addressing"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/How-are-the-segment-registers-used-in-Linux/","text":"How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? #","title":"How are the segment registers used in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/How-are-the-segment-registers-used-in-Linux/#how-are-the-segment-registers-fs-gs-cs-ss-ds-es-used-in-linux","text":"","title":"How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Memory-Addresses-note/","text":"A logical address consists of two parts: a segment identifier and an offset that specifies the relative address within the segment. The segment identifier is a 16-bit field called the Segment Selector (see Figure 2-2), while the offset is a 32-bit field. the processor provides segmentation registers whose only purpose is to hold Segment Selectors SUMMARY : \u5728Segment Selector\u4e2d\u6709Request Privilege Level\u5b57\u6bb5\uff0cThe cs register has another important function: it includes a 2-bit field that specifies the Current Privilege Level (CPL) of the CPU. Segment Selector\u7684Request Privilege Level\u5b57\u6bb5\u4e0e cs register \u7684Current Privilege Level (CPL) \u5b57\u6bb5\u76f8\u5bf9\u5e94\uff1b\u53c2\u89c1Table 2-2. Segment Selector fields\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u63cf\u8ff0\uff1a Requestor Privilege Level : specifies the Current Privilege Level of the CPU when the corresponding Segment Selector is loaded into the cs register; it also may be used to selectively weaken the processor privilege level when accessing data segments (see Intel documentation for details).","title":"Memory Addresses note"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Multilevel-Paging-in-Operating-System/","text":"Multilevel Paging in Operating System Multilevel Paging in Operating System # Prerequisite \u2013 Paging Multilevel Paging is a paging scheme which consist of two or more levels of page tables in a hierarchical manner. It is also known as hierarchical paging. The entries of the level 1 page table are pointers to a level 2 page table and entries of the level 2 page tables are pointers to a level 3 page table and so on. The entries of the last level page table are stores actual frame information. Level 1 contain single page table and address of that table is stored in PTBR (Page Table Base Register). Virtual address: In multilevel paging whatever may be levels of paging all the page tables will be stored in main memory.So it requires more than one memory access to get the physical address of page frame. One access for each level needed. Each page table entry except the last level page table entry contains base address of the next level page table. Reference to actual page frame: Reference to PTE in level 1 page table = PTBR value + Level 1 offset present in virtual address. Reference to PTE in level 2 page table = Base address (present in Level 1 PTE) + Level 2 offset (present in VA). Reference to PTE in level 3 page table= Base address (present in Level 2 PTE) + Level 3 offset (present in VA). Actual page frame address = PTE (present in level 3). Generally the page table size will be equal to the size of page. Assumptions: Byte addressable memory, and n is the number of bits used to represent virtual address. Important formula: Number of entries in page table: = (virtual address space size) / (page size) = Number of pages Virtual address space size: = 2^{n} B Size of page table: <>= (number of entries in page table)*(size of PTE) If page table size > desired size then create 1 more level. Disadvantage: Extra memory references to access address translation tables can slow programs down by a factor of two or more. Use translation look aside buffer (TLB) to speed up address translation by storing page table entries. Example: Q. Consider a virtual memory system with physical memory of 8GB, a page size of 8KB and 46 bit virtual address. Assume every page table exactly fits into a single page . If page table entry size is 4B then how many levels of page tables would be required. Explanation: Page size = 8KB = 2^{13} B Virtual address space size = 2^{46} B PTE = 4B = 2^2 B Number of pages or number of entries in page table, = (virtual address space size) / (page size) = 2^{46} B/2^{13} B = 2^{33} Size of page table, = (number of entries in page table)*(size of PTE) = 2^{33} * 2^2 B = 2^{35} B To create one more level, Size of page table > page size Number of page tables in last level, = 2^{35} B / 2^{13} B = 2^{22}","title":"Multilevel Paging in Operating System"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Multilevel-Paging-in-Operating-System/#multilevel-paging-in-operating-system","text":"Prerequisite \u2013 Paging Multilevel Paging is a paging scheme which consist of two or more levels of page tables in a hierarchical manner. It is also known as hierarchical paging. The entries of the level 1 page table are pointers to a level 2 page table and entries of the level 2 page tables are pointers to a level 3 page table and so on. The entries of the last level page table are stores actual frame information. Level 1 contain single page table and address of that table is stored in PTBR (Page Table Base Register). Virtual address: In multilevel paging whatever may be levels of paging all the page tables will be stored in main memory.So it requires more than one memory access to get the physical address of page frame. One access for each level needed. Each page table entry except the last level page table entry contains base address of the next level page table. Reference to actual page frame: Reference to PTE in level 1 page table = PTBR value + Level 1 offset present in virtual address. Reference to PTE in level 2 page table = Base address (present in Level 1 PTE) + Level 2 offset (present in VA). Reference to PTE in level 3 page table= Base address (present in Level 2 PTE) + Level 3 offset (present in VA). Actual page frame address = PTE (present in level 3). Generally the page table size will be equal to the size of page. Assumptions: Byte addressable memory, and n is the number of bits used to represent virtual address. Important formula: Number of entries in page table: = (virtual address space size) / (page size) = Number of pages Virtual address space size: = 2^{n} B Size of page table: <>= (number of entries in page table)*(size of PTE) If page table size > desired size then create 1 more level. Disadvantage: Extra memory references to access address translation tables can slow programs down by a factor of two or more. Use translation look aside buffer (TLB) to speed up address translation by storing page table entries. Example: Q. Consider a virtual memory system with physical memory of 8GB, a page size of 8KB and 46 bit virtual address. Assume every page table exactly fits into a single page . If page table entry size is 4B then how many levels of page tables would be required. Explanation: Page size = 8KB = 2^{13} B Virtual address space size = 2^{46} B PTE = 4B = 2^2 B Number of pages or number of entries in page table, = (virtual address space size) / (page size) = 2^{46} B/2^{13} B = 2^{33} Size of page table, = (number of entries in page table)*(size of PTE) = 2^{33} * 2^2 B = 2^{35} B To create one more level, Size of page table > page size Number of page tables in last level, = 2^{35} B / 2^{13} B = 2^{22}","title":"Multilevel Paging in Operating System"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.1-Processes-and-Lightweight-Processes-and-Threads/","text":"3.1. Processes, Lightweight Processes, and Threads # The term \"process\" is often used with several different meanings. In this book, we stick to the usual OS textbook definition: a process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process Processes are like human beings: they are generated, they have a more or less significant life, they optionally generate one or more child processes, and eventually they die. A small difference is that sex is not really common among processes each process has just one parent. NOTE: process\u7684\u751f\u547d\u5468\u671f From the kernel's point of view, the purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated. When a process is created, it is almost identical to its parent. It receives a (logical) copy of the parent's address space and executes the same code as the parent, beginning at the next instruction following the process creation system call. Although the parent and child may share the pages containing the program code (text), they have separate copies of the data (stack and heap), so that changes by the child to a memory location are invisible to the parent (and vice versa). While earlier Unix kernels employed this simple model, modern Unix systems do not. They support multithreaded applications user programs having many relatively independent execution flows sharing a large portion of the application data structures. In such systems, a process is composed of several user threads (or simply threads ), each of which represents an execution flow of the process. Nowadays, most multithreaded applications are written using standard sets of library functions called pthread (POSIX thread) libraries . NOTE : \u8fd9\u6bb5\u8bdd\u4e2d\u7684 user threads \u5177\u6709\u7279\u6b8a\u7684\u542b\u4e49\uff0c\u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7406\u89e3\u5b83\u5bf9\u4e8e\u7406\u89e3\u672c\u6bb5\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 Older versions of the Linux kernel offered no support for multithreaded applications. From the kernel point of view, a multithreaded application was just a normal process . The multiple execution flows of a multithreaded application were created, handled, and scheduled entirely in User Mode , usually by means of a POSIX-compliant pthread library. NOTE: \u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u523b\u5206\u6790 However, such an implementation of multithreaded applications is not very satisfactory. For instance, suppose a chess program uses two threads: one of them controls the graphical chessboard, waiting for the moves of the human player and showing the moves of the computer, while the other thread ponders the next move of the game. While the first thread waits for the human move, the second thread should run continuously, thus exploiting the thinking time of the human player. However, if the chess program is just a single process, the first thread cannot simply issue a blocking system call waiting for a user action; otherwise, the second thread is blocked as well. Instead, the first thread must employ sophisticated nonblocking techniques to ensure that the process remains runnable. NOTE : \u663e\u7136Older versions of the Linux kernel offered no support for multithreaded applications. Linux uses lightweight processes to offer better support for multithreaded applications. Basically, two lightweight processes may share some resources, like the address space , the open files , and so on. Whenever one of them modifies a shared resource, the other immediately sees the change. Of course, the two processes must synchronize themselves when accessing the shared resource. A straightforward way to implement multithreaded applications is to associate a lightweight process with each thread . In this way, the threads can access the same set of application data structures by simply sharing the same memory address space, the same set of open files, and so on; at the same time, each thread can be scheduled independently by the kernel so that one may sleep while another remains runnable. Examples of POSIX-compliant pthread libraries that use Linux's lightweight processes are LinuxThreads, Native POSIX Thread Library (NPTL), and IBM's Next Generation Posix Threading Package (NGPT). NOTE: \u5173\u4e8eLinuxThreads\u3001Native POSIX Thread Library (NPTL)\uff0c\u53c2\u89c1 PTHREADS(7) \uff0c\u5176\u4e2d\u7684Linux implementations of POSIX threads\u7ae0\u8282\u63cf\u8ff0\u4e86linux\u7684GNU C library\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\u7684\u7ec6\u8282\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u5982\u4f55\u67e5\u770b\u7cfb\u7edf\u7684 pthread libraries\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u547d\u4ee4\u3002 POSIX-compliant multithreaded applications are best handled by kernels that support \" thread groups .\" In Linux a thread group is basically a set of lightweight processes that implement a multithreaded application and act as a whole with regards to some system calls such as getpid( ) , kill( ) , and _exit( ) . We are going to describe them at length later in this chapter. NOTE : \u65b0\u7248\u672c\u7684linux\u4f7f\u7528 lightweight process \u6765\u5b9e\u73b0thread\uff1b\u4f7f\u7528 thread group \u6765\u4f5c\u4e3aprocess\uff1b \u5176\u5b9elinux kernel\u63d0\u4f9b\u4e86\u975e\u5e38\u7075\u6d3b\u7684 CLONE(2) system call\u6765\u8ba9\u7528\u6237\u7075\u6d3b\u5730\u521b\u5efaprocess\u6216thread\u3002","title":"3.1-Processes-and-Lightweight-Processes-and-Threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.1-Processes-and-Lightweight-Processes-and-Threads/#31-processes-lightweight-processes-and-threads","text":"The term \"process\" is often used with several different meanings. In this book, we stick to the usual OS textbook definition: a process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process Processes are like human beings: they are generated, they have a more or less significant life, they optionally generate one or more child processes, and eventually they die. A small difference is that sex is not really common among processes each process has just one parent. NOTE: process\u7684\u751f\u547d\u5468\u671f From the kernel's point of view, the purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated. When a process is created, it is almost identical to its parent. It receives a (logical) copy of the parent's address space and executes the same code as the parent, beginning at the next instruction following the process creation system call. Although the parent and child may share the pages containing the program code (text), they have separate copies of the data (stack and heap), so that changes by the child to a memory location are invisible to the parent (and vice versa). While earlier Unix kernels employed this simple model, modern Unix systems do not. They support multithreaded applications user programs having many relatively independent execution flows sharing a large portion of the application data structures. In such systems, a process is composed of several user threads (or simply threads ), each of which represents an execution flow of the process. Nowadays, most multithreaded applications are written using standard sets of library functions called pthread (POSIX thread) libraries . NOTE : \u8fd9\u6bb5\u8bdd\u4e2d\u7684 user threads \u5177\u6709\u7279\u6b8a\u7684\u542b\u4e49\uff0c\u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7406\u89e3\u5b83\u5bf9\u4e8e\u7406\u89e3\u672c\u6bb5\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 Older versions of the Linux kernel offered no support for multithreaded applications. From the kernel point of view, a multithreaded application was just a normal process . The multiple execution flows of a multithreaded application were created, handled, and scheduled entirely in User Mode , usually by means of a POSIX-compliant pthread library. NOTE: \u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u523b\u5206\u6790 However, such an implementation of multithreaded applications is not very satisfactory. For instance, suppose a chess program uses two threads: one of them controls the graphical chessboard, waiting for the moves of the human player and showing the moves of the computer, while the other thread ponders the next move of the game. While the first thread waits for the human move, the second thread should run continuously, thus exploiting the thinking time of the human player. However, if the chess program is just a single process, the first thread cannot simply issue a blocking system call waiting for a user action; otherwise, the second thread is blocked as well. Instead, the first thread must employ sophisticated nonblocking techniques to ensure that the process remains runnable. NOTE : \u663e\u7136Older versions of the Linux kernel offered no support for multithreaded applications. Linux uses lightweight processes to offer better support for multithreaded applications. Basically, two lightweight processes may share some resources, like the address space , the open files , and so on. Whenever one of them modifies a shared resource, the other immediately sees the change. Of course, the two processes must synchronize themselves when accessing the shared resource. A straightforward way to implement multithreaded applications is to associate a lightweight process with each thread . In this way, the threads can access the same set of application data structures by simply sharing the same memory address space, the same set of open files, and so on; at the same time, each thread can be scheduled independently by the kernel so that one may sleep while another remains runnable. Examples of POSIX-compliant pthread libraries that use Linux's lightweight processes are LinuxThreads, Native POSIX Thread Library (NPTL), and IBM's Next Generation Posix Threading Package (NGPT). NOTE: \u5173\u4e8eLinuxThreads\u3001Native POSIX Thread Library (NPTL)\uff0c\u53c2\u89c1 PTHREADS(7) \uff0c\u5176\u4e2d\u7684Linux implementations of POSIX threads\u7ae0\u8282\u63cf\u8ff0\u4e86linux\u7684GNU C library\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\u7684\u7ec6\u8282\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u5982\u4f55\u67e5\u770b\u7cfb\u7edf\u7684 pthread libraries\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u547d\u4ee4\u3002 POSIX-compliant multithreaded applications are best handled by kernels that support \" thread groups .\" In Linux a thread group is basically a set of lightweight processes that implement a multithreaded application and act as a whole with regards to some system calls such as getpid( ) , kill( ) , and _exit( ) . We are going to describe them at length later in this chapter. NOTE : \u65b0\u7248\u672c\u7684linux\u4f7f\u7528 lightweight process \u6765\u5b9e\u73b0thread\uff1b\u4f7f\u7528 thread group \u6765\u4f5c\u4e3aprocess\uff1b \u5176\u5b9elinux kernel\u63d0\u4f9b\u4e86\u975e\u5e38\u7075\u6d3b\u7684 CLONE(2) system call\u6765\u8ba9\u7528\u6237\u7075\u6d3b\u5730\u521b\u5efaprocess\u6216thread\u3002","title":"3.1. Processes, Lightweight Processes, and Threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/","text":"3.2. Process Descriptor # To manage processes, the kernel must have a clear picture of what each process is doing. It must know, for instance, the process's priority\uff08\u8c03\u5ea6\u7684\u4f18\u5148\u7ea7\uff09, whether it is running on a CPU or blocked on an event, what address space has been assigned to it, which files it is allowed to address, and so on. This is the role of the process descriptor a task_struct type structure whose fields contain all the information related to a single process . [ * ] As the repository of so much information, the process descriptor is rather complex. In addition to a large number of fields containing process attributes, the process descriptor contains several pointers to other data structures that, in turn, contain pointers to other structures. Figure 3-1 describes the Linux process descriptor schematically\uff08\u6309\u7167\u56fe\u5f0f\uff09. [ * ] The kernel also defines the task_t data type to be equivalent to struct task_struct NOTE : task_struct \u7684\u6e90\u4ee3\u7801\u53c2\u89c1\uff1a https://elixir.bootlin.com/linux/latest/ident/task_struct https://github.com/torvalds/linux/blob/master/include/linux/sched.h \u5173\u4e8e task_struct \u548cprocess\uff0cthread\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728Chapter 3. Processes\u4e2d\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u5206\u6790\uff1b\u6b64\u5904\u4e0d\u518d\u8d58\u8ff0\uff1b The six data structures on the right side of the figure refer to specific resources owned by the process. Most of these resources will be covered in future chapters. This chapter focuses on two types of fields that refer to the process state and to process parent/child relationships . Figure 3-1. The Linux process descriptor task_structure\u7684\u6210\u5458\u53d8\u91cf # name type note pid lightweight process ID\uff0c\u79c1\u6709 chapter 3.2.2. Identifying a Process tgid \u6807\u51c6process id\uff0c\u5171\u4eab chapter 3.2.2. Identifying a Process thread_info \u79c1\u6709 mm_struct process address space\uff0c\u5171\u4eab 9.2. The Memory Descriptor","title":"3.2-Process-Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#32-process-descriptor","text":"To manage processes, the kernel must have a clear picture of what each process is doing. It must know, for instance, the process's priority\uff08\u8c03\u5ea6\u7684\u4f18\u5148\u7ea7\uff09, whether it is running on a CPU or blocked on an event, what address space has been assigned to it, which files it is allowed to address, and so on. This is the role of the process descriptor a task_struct type structure whose fields contain all the information related to a single process . [ * ] As the repository of so much information, the process descriptor is rather complex. In addition to a large number of fields containing process attributes, the process descriptor contains several pointers to other data structures that, in turn, contain pointers to other structures. Figure 3-1 describes the Linux process descriptor schematically\uff08\u6309\u7167\u56fe\u5f0f\uff09. [ * ] The kernel also defines the task_t data type to be equivalent to struct task_struct NOTE : task_struct \u7684\u6e90\u4ee3\u7801\u53c2\u89c1\uff1a https://elixir.bootlin.com/linux/latest/ident/task_struct https://github.com/torvalds/linux/blob/master/include/linux/sched.h \u5173\u4e8e task_struct \u548cprocess\uff0cthread\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728Chapter 3. Processes\u4e2d\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u5206\u6790\uff1b\u6b64\u5904\u4e0d\u518d\u8d58\u8ff0\uff1b The six data structures on the right side of the figure refer to specific resources owned by the process. Most of these resources will be covered in future chapters. This chapter focuses on two types of fields that refer to the process state and to process parent/child relationships . Figure 3-1. The Linux process descriptor","title":"3.2. Process Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#task_structure","text":"name type note pid lightweight process ID\uff0c\u79c1\u6709 chapter 3.2.2. Identifying a Process tgid \u6807\u51c6process id\uff0c\u5171\u4eab chapter 3.2.2. Identifying a Process thread_info \u79c1\u6709 mm_struct process address space\uff0c\u5171\u4eab 9.2. The Memory Descriptor","title":"task_structure\u7684\u6210\u5458\u53d8\u91cf"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/","text":"3.2.1. Process State # As its name implies, the state field of the process descriptor describes what is currently happening to the process. It consists of an array of flags, each of which describes a possible process state. In the current Linux version, these states are mutually exclusive , and hence exactly one flag of state always is set; the remaining flags are cleared. The following are the possible process states: TASK_RUNNING # The process is either executing on a CPU or waiting to be executed. TASK_INTERRUPTIBLE # The process is suspended ( sleeping ) until some condition becomes true. Raising a hardware interrupt, releasing a system resource the process is waiting for, or delivering a signal are examples of conditions that might wake up the process (put its state back to TASK_RUNNING ). NOTE \uff1a sleeping \u76f8\u5f53\u4e8esuspended TASK_UNINTERRUPTIBLE # Like TASK_INTERRUPTIBLE , except that delivering a signal to the sleeping process leaves its state unchanged. This process state is seldom used. It is valuable, however, under certain specific conditions in which a process must wait until a given event occurs without being interrupted. For instance, this state may be used when a process opens a device file and the corresponding device driver starts probing for a corresponding hardware device . The device driver must not be interrupted until the probing is complete, or the hardware device could be left in an unpredictable state. TASK_STOPPED # Process execution has been stopped; the process enters this state after receiving a SIGSTOP , SIGTSTP , SIGTTIN , or SIGTTOU signal. TASK_TRACED # Process execution has been stopped by a debugger. When a process is being monitored by another (such as when a debugger executes a ptrace( ) system call to monitor a test program), each signal may put the process in the TASK_TRACED state. Two additional states of the process can be stored both in the state field and in the exit_state field of the process descriptor; as the field name suggests, a process reaches one of these two states only when its execution is terminated: EXIT_ZOMBIE # Process execution is terminated, but the parent process has not yet issued a wait4( ) or waitpid( ) system call to return information about the dead process. [*] Before the wait( )-like call is issued, the kernel cannot discard the data contained in the dead process descriptor because the parent might need it. (See the section \"Process Removal\" near the end of this chapter.) [*] There are other wait( )-like library functions, such as wait3( ) and wait( ) , but in Linux they are implemented by means of the wait4( ) and waitpid( ) system calls. EXIT_DEAD # The final state: the process is being removed by the system because the parent process has just issued a wait4( ) or waitpid( ) system call for it. Changing its state from EXIT_ZOMBIE to EXIT_DEAD avoids race conditions due to other threads of execution that execute wait( ) -like calls on the same process (see Chapter 5). The value of the state field is usually set with a simple assignment. For instance: p->state = TASK_RUNNING; The kernel also uses the set_task_state and set_current_state macros: they set the state of a specified process and of the process currently executed, respectively. Moreover, these macros ensure that the assignment operation is not mixed with other instructions by the compiler or the CPU control unit . Mixing the instruction order may sometimes lead to catastrophic results (see Chapter 5). NOTE: process state \u8865\u5145\u5185\u5bb9\uff1a https://lwn.net/Articles/288056/","title":"3.2.1-Process-State"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#321-process-state","text":"As its name implies, the state field of the process descriptor describes what is currently happening to the process. It consists of an array of flags, each of which describes a possible process state. In the current Linux version, these states are mutually exclusive , and hence exactly one flag of state always is set; the remaining flags are cleared. The following are the possible process states:","title":"3.2.1. Process State"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#task_running","text":"The process is either executing on a CPU or waiting to be executed.","title":"TASK_RUNNING"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#task_interruptible","text":"The process is suspended ( sleeping ) until some condition becomes true. Raising a hardware interrupt, releasing a system resource the process is waiting for, or delivering a signal are examples of conditions that might wake up the process (put its state back to TASK_RUNNING ). NOTE \uff1a sleeping \u76f8\u5f53\u4e8esuspended","title":"TASK_INTERRUPTIBLE"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#task_uninterruptible","text":"Like TASK_INTERRUPTIBLE , except that delivering a signal to the sleeping process leaves its state unchanged. This process state is seldom used. It is valuable, however, under certain specific conditions in which a process must wait until a given event occurs without being interrupted. For instance, this state may be used when a process opens a device file and the corresponding device driver starts probing for a corresponding hardware device . The device driver must not be interrupted until the probing is complete, or the hardware device could be left in an unpredictable state.","title":"TASK_UNINTERRUPTIBLE"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#task_stopped","text":"Process execution has been stopped; the process enters this state after receiving a SIGSTOP , SIGTSTP , SIGTTIN , or SIGTTOU signal.","title":"TASK_STOPPED"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#task_traced","text":"Process execution has been stopped by a debugger. When a process is being monitored by another (such as when a debugger executes a ptrace( ) system call to monitor a test program), each signal may put the process in the TASK_TRACED state. Two additional states of the process can be stored both in the state field and in the exit_state field of the process descriptor; as the field name suggests, a process reaches one of these two states only when its execution is terminated:","title":"TASK_TRACED"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#exit_zombie","text":"Process execution is terminated, but the parent process has not yet issued a wait4( ) or waitpid( ) system call to return information about the dead process. [*] Before the wait( )-like call is issued, the kernel cannot discard the data contained in the dead process descriptor because the parent might need it. (See the section \"Process Removal\" near the end of this chapter.) [*] There are other wait( )-like library functions, such as wait3( ) and wait( ) , but in Linux they are implemented by means of the wait4( ) and waitpid( ) system calls.","title":"EXIT_ZOMBIE"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.1-Process-State/#exit_dead","text":"The final state: the process is being removed by the system because the parent process has just issued a wait4( ) or waitpid( ) system call for it. Changing its state from EXIT_ZOMBIE to EXIT_DEAD avoids race conditions due to other threads of execution that execute wait( ) -like calls on the same process (see Chapter 5). The value of the state field is usually set with a simple assignment. For instance: p->state = TASK_RUNNING; The kernel also uses the set_task_state and set_current_state macros: they set the state of a specified process and of the process currently executed, respectively. Moreover, these macros ensure that the assignment operation is not mixed with other instructions by the compiler or the CPU control unit . Mixing the instruction order may sometimes lead to catastrophic results (see Chapter 5). NOTE: process state \u8865\u5145\u5185\u5bb9\uff1a https://lwn.net/Articles/288056/","title":"EXIT_DEAD"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/","text":"3.2.2. Identifying a Process 3.2.2. Identifying a Process # \u6ce8\u610f\uff1a \u672c\u6587\u4e2d\u7684process\u6240\u6307\u4e3alightweight process\uff0c pid \u6240\u6307\u6307\u4e3alightweight process\u7684 pid \uff0c\u5e76\u975e\u6807\u51c6\u7684process\u548c\u6807\u51c6\u7684 pid \u3002\u5728\u672c\u6587\u4e2d\uff0cthread group\u8868\u793a\u7684\u662f\u6807\u51c6\u7684process\uff0c tgid \u4e3a\u6807\u51c6\u7684 pid \u3002 As a general rule, each execution context that can be independently scheduled must have its own process descriptor ; therefore, even lightweight processes , which share a large portion of their kernel data structures, have their own task_struct structures. The strict one-to-one correspondence between the process and process descriptor makes the 32-bit address [ ] of the task_struct structure a useful means for the kernel to identify processes. These addresses are referred to as process descriptor pointers . Most of the references to processes that the kernel makes are through process descriptor pointers . [ ] As already noted in the section \"Segmentation in Linux\" in Chapter 2, although technically these 32 bits are only the offset component of a logical address, they coincide with the linear address On the other hand, Unix-like operating systems allow users to identify processes by means of a number called the Process ID (or PID ), which is stored in the pid field of the process descriptor . PIDs are numbered sequentially: the PID of a newly created process is normally the PID of the previously created process increased by one. Of course, there is an upper limit on the PID values; when the kernel reaches such limit, it must start recycling the lower, unused PIDs. By default, the maximum PID number is 32,767 ( PID_MAX_DEFAULT - 1 ); the system administrator may reduce this limit by writing a smaller value into the /proc/sys/kernel/pid_max file (/proc is the mount point of a special filesystem, see the section \"Special Filesystems\" in Chapter 12). In 64-bit architectures, the system administrator can enlarge the maximum PID number up to 4,194,303. When recycling PID numbers, the kernel must manage a pidmap_array bitmap that denotes which are the PIDs currently assigned and which are the free ones. Because a page frame contains 32,768\uff08 4K \uff09 bits, in 32-bit architectures the pidmap_array bitmap is stored in a single page. In 64-bit architectures, however, additional pages can be added to the bitmap when the kernel assigns a PID number too large for the current bitmap size. These pages are never released. NOTE: \u5173\u4e8e pidmap_array \uff0c\u53c2\u89c1 - https://elixir.bootlin.com/linux/v2.6.17.7/source/kernel/pid.c#L46 - https://blog.csdn.net/Jay14/article/details/54863073 Linux associates a different PID with each process or lightweight process in the system. (As we shall see later in this chapter, there is a tiny exception on multiprocessor systems.) This approach allows the maximum flexibility, because every execution context in the system can be uniquely identified. On the other hand, Unix programmers expect threads in the same group to have a common PID . For instance, it should be possible to a send a signal specifying a PID that affects all threads in the group. In fact, the POSIX 1003.1c standard states that all threads of a multithreaded application must have the same PID . To comply with this standard, Linux makes use of thread groups . The identifier shared by the threads is the PID of the thread group leader , that is, the PID of the first lightweight process in the group; it is stored in the tgid field of the process descriptors . The getpid( ) system call returns the value of tgid relative to the current process instead of the value of pid , so all the threads of a multithreaded application share the same identifier. Most processes belong to a thread group consisting of a single member; as thread group leaders, they have the tgid field equal to the pid field, thus the getpid( ) system call works as usual for this kind of process. Later, we'll show you how it is possible to derive a true process descriptor pointer efficiently from its respective PID . Efficiency is important because many system calls such as kill( ) use the PID to denote the affected process.","title":"3.2.2-Identifying-a-Process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#322-identifying-a-process","text":"\u6ce8\u610f\uff1a \u672c\u6587\u4e2d\u7684process\u6240\u6307\u4e3alightweight process\uff0c pid \u6240\u6307\u6307\u4e3alightweight process\u7684 pid \uff0c\u5e76\u975e\u6807\u51c6\u7684process\u548c\u6807\u51c6\u7684 pid \u3002\u5728\u672c\u6587\u4e2d\uff0cthread group\u8868\u793a\u7684\u662f\u6807\u51c6\u7684process\uff0c tgid \u4e3a\u6807\u51c6\u7684 pid \u3002 As a general rule, each execution context that can be independently scheduled must have its own process descriptor ; therefore, even lightweight processes , which share a large portion of their kernel data structures, have their own task_struct structures. The strict one-to-one correspondence between the process and process descriptor makes the 32-bit address [ ] of the task_struct structure a useful means for the kernel to identify processes. These addresses are referred to as process descriptor pointers . Most of the references to processes that the kernel makes are through process descriptor pointers . [ ] As already noted in the section \"Segmentation in Linux\" in Chapter 2, although technically these 32 bits are only the offset component of a logical address, they coincide with the linear address On the other hand, Unix-like operating systems allow users to identify processes by means of a number called the Process ID (or PID ), which is stored in the pid field of the process descriptor . PIDs are numbered sequentially: the PID of a newly created process is normally the PID of the previously created process increased by one. Of course, there is an upper limit on the PID values; when the kernel reaches such limit, it must start recycling the lower, unused PIDs. By default, the maximum PID number is 32,767 ( PID_MAX_DEFAULT - 1 ); the system administrator may reduce this limit by writing a smaller value into the /proc/sys/kernel/pid_max file (/proc is the mount point of a special filesystem, see the section \"Special Filesystems\" in Chapter 12). In 64-bit architectures, the system administrator can enlarge the maximum PID number up to 4,194,303. When recycling PID numbers, the kernel must manage a pidmap_array bitmap that denotes which are the PIDs currently assigned and which are the free ones. Because a page frame contains 32,768\uff08 4K \uff09 bits, in 32-bit architectures the pidmap_array bitmap is stored in a single page. In 64-bit architectures, however, additional pages can be added to the bitmap when the kernel assigns a PID number too large for the current bitmap size. These pages are never released. NOTE: \u5173\u4e8e pidmap_array \uff0c\u53c2\u89c1 - https://elixir.bootlin.com/linux/v2.6.17.7/source/kernel/pid.c#L46 - https://blog.csdn.net/Jay14/article/details/54863073 Linux associates a different PID with each process or lightweight process in the system. (As we shall see later in this chapter, there is a tiny exception on multiprocessor systems.) This approach allows the maximum flexibility, because every execution context in the system can be uniquely identified. On the other hand, Unix programmers expect threads in the same group to have a common PID . For instance, it should be possible to a send a signal specifying a PID that affects all threads in the group. In fact, the POSIX 1003.1c standard states that all threads of a multithreaded application must have the same PID . To comply with this standard, Linux makes use of thread groups . The identifier shared by the threads is the PID of the thread group leader , that is, the PID of the first lightweight process in the group; it is stored in the tgid field of the process descriptors . The getpid( ) system call returns the value of tgid relative to the current process instead of the value of pid , so all the threads of a multithreaded application share the same identifier. Most processes belong to a thread group consisting of a single member; as thread group leaders, they have the tgid field equal to the pid field, thus the getpid( ) system call works as usual for this kind of process. Later, we'll show you how it is possible to derive a true process descriptor pointer efficiently from its respective PID . Efficiency is important because many system calls such as kill( ) use the PID to denote the affected process.","title":"3.2.2. Identifying a Process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.1-Process-descriptors-handling/","text":"3.2.2.1. Process descriptors handling # Processes are dynamic entities whose lifetimes range from a few milliseconds to months. Thus, the kernel must be able to handle many processes at the same time, and process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel. For each process, Linux packs two different data structures in a single per-process memory area : a small data structure linked to the process descriptor , namely the thread_info structure the Kernel Mode process stack . The length of this memory area is usually 8,192 bytes (two page frames). For reasons of efficiency the kernel stores the 8-KB memory area in two consecutive page frames with the first page frame aligned to a multiple of $2^{13}$ ; this may turn out to be a problem when little dynamic memory is available, because the free memory may become highly fragmented (see the section \"The Buddy System Algorithm\" in Chapter 8). Therefore, in the 80x86 architecture the kernel can be configured at compilation time so that the memory area including stack and thread_info structure spans a single page frame (4,096 bytes). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u4e2d\u7684 dynamic memory \u5728Chapter 8. Memory Management\u4e2d\u5b9a\u4e49\u3002 NOTE: 1.6.3. Reentrant Kernels\u4e2d Kernel Mode process stack \u662f\u4e3a kernel control path \u800c\u51c6\u5907\u7684\uff0ckernel control path\u7684\u6267\u884c\u662fReentrant\u7684\u3002 In the section \"Segmentation in Linux\" in Chapter 2, we learned that a process in Kernel Mode accesses a stack contained in the kernel data segment , which is different from the stack used by the process in User Mode . Because kernel control paths make little use of the stack, only a few thousand bytes of kernel stack are required. Therefore, 8 KB is ample space for the stack and the thread_info structure. However, when stack and thread_info structure are contained in a single page frame, the kernel uses a few additional stacks to avoid the overflows caused by deeply nested interrupts and exceptions (see Chapter 4). NOTE: \u4e00\u4e2aprocess\u6709\u4e24\u4e2astack\uff1a Kernel Mode process stack\uff0c\u7531 kernel control path \u4f7f\u7528 User Mode process stack Figure 3-2 shows how the two data structures are stored in the 2-page (8 KB) memory area. The thread_info structure resides at the beginning of the memory area, and the stack grows downward from the end. The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and thread_info , respectively. Figure 3-2. Storing the thread_info structure and the process kernel stack in two page frames The esp register is the CPU stack pointer , which is used to address the stack's top location. On 80x86 systems, the stack starts at the end and grows toward the beginning of the memory area \uff08\u5373\u4ece\u4f4e\u5730\u5740\u5411\u9ad8\u5730\u5740\uff09. Right after switching from User Mode to Kernel Mode , the kernel stack of a process is always empty, and therefore the esp register points to the byte immediately following the stack. The value of the esp is decreased as soon as data is written into the stack. Because the thread_info structure is 52 bytes long, the kernel stack can expand up to 8,140 bytes. The C language allows the thread_info structure and the kernel stack of a process to be conveniently represented by means of the following union construct: union thread_union { struct thread_info thread_info; unsigned long stack[2048]; /* 1024 for 4KB stacks */ }; NOTE: thread_union \u5c31\u662f\u4e0a\u8ff0\u7684memory area\u7684\u5b9e\u73b0\u3002 thread_union \u6e90\u7801\uff1ahttps://elixir.bootlin.com/linux/latest/ident/thread_union The thread_info structure shown in Figure 3-2 is stored starting at address 0x015fa000 , and the stack is stored starting at address 0x015fc000 . The value of the esp register points to the current top of the stack at 0x015fa878 . The kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a thread_info structure and a kernel stack . \u6ce8\u89e3 # The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and tHRead_info , respectively. \u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u9700\u8981\u641e\u6e05\u695a struct thread_info \u7684\u5b9a\u4e49\uff0c\u4ee5\u4e0b\u662f i386\u7684 struct thread_info struct thread_info { struct task_struct *task; /* main task structure */ struct exec_domain *exec_domain; /* execution domain */ unsigned long flags; /* low level flags */ unsigned long status; /* thread-synchronous flags */ __u32 cpu; /* current CPU */ __s32 preempt_count; /* 0 => preemptable, <0 => BUG */ mm_segment_t addr_limit; /* thread address space: 0-0xBFFFFFFF for user-thead 0-0xFFFFFFFF for kernel-thread */ struct restart_block restart_block; unsigned long previous_esp; /* ESP of the previous stack in case of nested (IRQ) stacks */ __u8 supervisor_stack[0]; }; \u53ef\u4ee5\u770b\u5230 struct thread_info \u6709\u6210\u5458\u53d8\u91cf struct task_struct *task \uff0c\u800c\u5728 struct task_struct \u4e2d\uff0c\u6709\u6210\u5458\u53d8\u91cf struct thread_info *thread_info; \uff0c\u8fd9\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u6240\u63cf\u8ff0\u7684\uff1a thread_union \u662f\u4fdd\u5b58\u5728per-process memory area\uff0c\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\uff1a Kernel Mode process stack \u4e5f\u4fdd\u5b58\u5728per-process memory area\u4e2d\uff1b\u800c\u8fd9\u4e00\u6bb5\u4e2d\u53c8\u63d0\u53ca\uff1aa process in Kernel Mode accesses a stack contained in the kernel data segment \uff1b\u90a3kernel data segment\u662f\u5b58\u653e\u5728\u4f55\u5904\u5462\uff1f \u4e00\u4e2a\u5173\u952e\u70b9\u662f\u8981\u77e5\u9053\u672c\u4e66\u7684\u57fa\u4e8e i386 \u67b6\u6784\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u5728 i386 \u4e2d\uff0c\u4f7f\u7528\u4e86segmentation\uff0c\u4f46\u662f\u5728\u540e\u6765\u8fd9\u79cd\u65b9\u5f0f\u88ab\u53d6\u4ee3\u4e86\uff1b\u6240\u4ee5\u5f88\u591a\u67b6\u6784\u4e2d\u538b\u6839\u53ef\u80fd\u5c31\u6ca1\u6709 Kernel Mode stack contained in the kernel data segment \u7684\u8fd9\u79cd\u7ed3\u6784\uff0c\u5728 How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 \u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u8fdb\u884c\u4e86Google\uff1a is kernel data segment in process address space \uff1b\u76ee\u524d\u6240\u6709\u7684\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u5185\u5bb9\u90fd\u5728\u300a virtual-memory-address-space-thinking.md \u300b\u4e2d\uff1b\u5728\u9605\u8bfb\u8fd9\u4e00\u6bb5\u7684\u65f6\u5019\uff0c","title":"3.2.2.1-Process-descriptors-handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.1-Process-descriptors-handling/#3221-process-descriptors-handling","text":"Processes are dynamic entities whose lifetimes range from a few milliseconds to months. Thus, the kernel must be able to handle many processes at the same time, and process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel. For each process, Linux packs two different data structures in a single per-process memory area : a small data structure linked to the process descriptor , namely the thread_info structure the Kernel Mode process stack . The length of this memory area is usually 8,192 bytes (two page frames). For reasons of efficiency the kernel stores the 8-KB memory area in two consecutive page frames with the first page frame aligned to a multiple of $2^{13}$ ; this may turn out to be a problem when little dynamic memory is available, because the free memory may become highly fragmented (see the section \"The Buddy System Algorithm\" in Chapter 8). Therefore, in the 80x86 architecture the kernel can be configured at compilation time so that the memory area including stack and thread_info structure spans a single page frame (4,096 bytes). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u4e2d\u7684 dynamic memory \u5728Chapter 8. Memory Management\u4e2d\u5b9a\u4e49\u3002 NOTE: 1.6.3. Reentrant Kernels\u4e2d Kernel Mode process stack \u662f\u4e3a kernel control path \u800c\u51c6\u5907\u7684\uff0ckernel control path\u7684\u6267\u884c\u662fReentrant\u7684\u3002 In the section \"Segmentation in Linux\" in Chapter 2, we learned that a process in Kernel Mode accesses a stack contained in the kernel data segment , which is different from the stack used by the process in User Mode . Because kernel control paths make little use of the stack, only a few thousand bytes of kernel stack are required. Therefore, 8 KB is ample space for the stack and the thread_info structure. However, when stack and thread_info structure are contained in a single page frame, the kernel uses a few additional stacks to avoid the overflows caused by deeply nested interrupts and exceptions (see Chapter 4). NOTE: \u4e00\u4e2aprocess\u6709\u4e24\u4e2astack\uff1a Kernel Mode process stack\uff0c\u7531 kernel control path \u4f7f\u7528 User Mode process stack Figure 3-2 shows how the two data structures are stored in the 2-page (8 KB) memory area. The thread_info structure resides at the beginning of the memory area, and the stack grows downward from the end. The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and thread_info , respectively. Figure 3-2. Storing the thread_info structure and the process kernel stack in two page frames The esp register is the CPU stack pointer , which is used to address the stack's top location. On 80x86 systems, the stack starts at the end and grows toward the beginning of the memory area \uff08\u5373\u4ece\u4f4e\u5730\u5740\u5411\u9ad8\u5730\u5740\uff09. Right after switching from User Mode to Kernel Mode , the kernel stack of a process is always empty, and therefore the esp register points to the byte immediately following the stack. The value of the esp is decreased as soon as data is written into the stack. Because the thread_info structure is 52 bytes long, the kernel stack can expand up to 8,140 bytes. The C language allows the thread_info structure and the kernel stack of a process to be conveniently represented by means of the following union construct: union thread_union { struct thread_info thread_info; unsigned long stack[2048]; /* 1024 for 4KB stacks */ }; NOTE: thread_union \u5c31\u662f\u4e0a\u8ff0\u7684memory area\u7684\u5b9e\u73b0\u3002 thread_union \u6e90\u7801\uff1ahttps://elixir.bootlin.com/linux/latest/ident/thread_union The thread_info structure shown in Figure 3-2 is stored starting at address 0x015fa000 , and the stack is stored starting at address 0x015fc000 . The value of the esp register points to the current top of the stack at 0x015fa878 . The kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a thread_info structure and a kernel stack .","title":"3.2.2.1. Process descriptors handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.1-Process-descriptors-handling/#_1","text":"The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and tHRead_info , respectively. \u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u9700\u8981\u641e\u6e05\u695a struct thread_info \u7684\u5b9a\u4e49\uff0c\u4ee5\u4e0b\u662f i386\u7684 struct thread_info struct thread_info { struct task_struct *task; /* main task structure */ struct exec_domain *exec_domain; /* execution domain */ unsigned long flags; /* low level flags */ unsigned long status; /* thread-synchronous flags */ __u32 cpu; /* current CPU */ __s32 preempt_count; /* 0 => preemptable, <0 => BUG */ mm_segment_t addr_limit; /* thread address space: 0-0xBFFFFFFF for user-thead 0-0xFFFFFFFF for kernel-thread */ struct restart_block restart_block; unsigned long previous_esp; /* ESP of the previous stack in case of nested (IRQ) stacks */ __u8 supervisor_stack[0]; }; \u53ef\u4ee5\u770b\u5230 struct thread_info \u6709\u6210\u5458\u53d8\u91cf struct task_struct *task \uff0c\u800c\u5728 struct task_struct \u4e2d\uff0c\u6709\u6210\u5458\u53d8\u91cf struct thread_info *thread_info; \uff0c\u8fd9\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u6240\u63cf\u8ff0\u7684\uff1a thread_union \u662f\u4fdd\u5b58\u5728per-process memory area\uff0c\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\uff1a Kernel Mode process stack \u4e5f\u4fdd\u5b58\u5728per-process memory area\u4e2d\uff1b\u800c\u8fd9\u4e00\u6bb5\u4e2d\u53c8\u63d0\u53ca\uff1aa process in Kernel Mode accesses a stack contained in the kernel data segment \uff1b\u90a3kernel data segment\u662f\u5b58\u653e\u5728\u4f55\u5904\u5462\uff1f \u4e00\u4e2a\u5173\u952e\u70b9\u662f\u8981\u77e5\u9053\u672c\u4e66\u7684\u57fa\u4e8e i386 \u67b6\u6784\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u5728 i386 \u4e2d\uff0c\u4f7f\u7528\u4e86segmentation\uff0c\u4f46\u662f\u5728\u540e\u6765\u8fd9\u79cd\u65b9\u5f0f\u88ab\u53d6\u4ee3\u4e86\uff1b\u6240\u4ee5\u5f88\u591a\u67b6\u6784\u4e2d\u538b\u6839\u53ef\u80fd\u5c31\u6ca1\u6709 Kernel Mode stack contained in the kernel data segment \u7684\u8fd9\u79cd\u7ed3\u6784\uff0c\u5728 How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 \u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u8fdb\u884c\u4e86Google\uff1a is kernel data segment in process address space \uff1b\u76ee\u524d\u6240\u6709\u7684\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u5185\u5bb9\u90fd\u5728\u300a virtual-memory-address-space-thinking.md \u300b\u4e2d\uff1b\u5728\u9605\u8bfb\u8fd9\u4e00\u6bb5\u7684\u65f6\u5019\uff0c","title":"\u6ce8\u89e3"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.2-Identifying-the-current-process/","text":"3.2.2.2. Identifying the current process # The close association between the thread_info structure and the Kernel Mode stack just described offers a key benefit in terms of efficiency: the kernel can easily obtain the address of the thread_info structure of the process currently running on a CPU from the value of the esp register. In fact, if the thread_union structure is 8 KB ($2^{13}$ bytes) long, the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure; on the other hand, if the thread_union structure is 4 KB long, the kernel masks out the 12 least significant bits of esp . This is done by the current_thread_info( ) function, which produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl %ecx,p After executing these three instructions, p contains the thread_info structure pointer of the process running on the CPU that executes the instruction. Most often the kernel needs the address of the process descriptor rather than the address of the thread_info structure. To get the process descriptor pointer of the process currently running on a CPU, the kernel makes use of the current macro, which is essentially equivalent to current_thread_info( )->task and produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl (%ecx),p Because the task field is at offset 0 in the thread_info structure, after executing these three instructions p contains the process descriptor pointer of the process running on the CPU. The current macro often appears in kernel code as a prefix to fields of the process descriptor . For example, current->pid returns the process ID of the process currently running on the CPU. Another advantage of storing the process descriptor with the stack emerges on multiprocessor systems: the correct current process for each hardware processor can be derived just by checking the stack, as shown previously. Earlier versions of Linux did not store the kernel stack and the process descriptor together. Instead, they were forced to introduce a global static variable called current to identify the process descriptor of the running process. On multiprocessor systems, it was necessary to define current as an array one element for each available CPU. SUMMARY : \u6240\u6709\u7684 process descriptor \u548c kernel stack \u90fd\u662f\u4f4d\u4e8ekernel\u4e2d\uff1b\u7531kernel\u6765\u6267\u884c\u8c03\u5ea6\uff1b\u5f53CPU\u9700\u8981\u6267\u884c\u67d0\u4e2a process descriptor \u7684\u65f6\u5019\uff0c\u5b83\u9700\u8981\u8bfb\u53d6\u8fd9\u4e2a process descriptor \u7684\u4e00\u4e9b\u6570\u636e\uff0c\u6bd4\u5982\u4e4b\u524d\u4fdd\u5b58\u7684register\u6570\u636e\u7b49\u4ee5\u4fbfresume\uff1b\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u5e76\u4e14\uff0c\u4ece\u524d\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u6bcf\u4e2a thread_union \u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 kernel stack \uff0c\u800c\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u6240\u4ee5CPU\u662f\u5728\u67d0\u4e2a`thread_union\u7684 kernel stack \u4e2d\u6267\u884c\uff0c\u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684 process descriptor \uff1b SUMMARY : \u56e0\u4e3ascheduler\u5728\u8c03\u5ea6\u4e00\u4e2atask\u5f00\u59cb\u8fd0\u884c\u4e4b\u524d\u4f1a\u5c06\u8fd9\u4e2atask\u7684\u6240\u6709\u7684register\u90fd\u6062\u590d\u5230CPU\u4e2d\uff0c\u6240\u4ee5\u5fc5\u7136\u4f1a\u5305\u542b esp \uff0c\u6240\u4ee5\u5b83\u5c31\u53ef\u4ee5\u6839\u636e esp \u5feb\u901f\u5730\u5b9a\u4f4d\u5230process descriptor\uff1b","title":"3.2.2.2-Identifying-the-current-process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.2-Identifying-the-current-process/#3222-identifying-the-current-process","text":"The close association between the thread_info structure and the Kernel Mode stack just described offers a key benefit in terms of efficiency: the kernel can easily obtain the address of the thread_info structure of the process currently running on a CPU from the value of the esp register. In fact, if the thread_union structure is 8 KB ($2^{13}$ bytes) long, the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure; on the other hand, if the thread_union structure is 4 KB long, the kernel masks out the 12 least significant bits of esp . This is done by the current_thread_info( ) function, which produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl %ecx,p After executing these three instructions, p contains the thread_info structure pointer of the process running on the CPU that executes the instruction. Most often the kernel needs the address of the process descriptor rather than the address of the thread_info structure. To get the process descriptor pointer of the process currently running on a CPU, the kernel makes use of the current macro, which is essentially equivalent to current_thread_info( )->task and produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl (%ecx),p Because the task field is at offset 0 in the thread_info structure, after executing these three instructions p contains the process descriptor pointer of the process running on the CPU. The current macro often appears in kernel code as a prefix to fields of the process descriptor . For example, current->pid returns the process ID of the process currently running on the CPU. Another advantage of storing the process descriptor with the stack emerges on multiprocessor systems: the correct current process for each hardware processor can be derived just by checking the stack, as shown previously. Earlier versions of Linux did not store the kernel stack and the process descriptor together. Instead, they were forced to introduce a global static variable called current to identify the process descriptor of the running process. On multiprocessor systems, it was necessary to define current as an array one element for each available CPU. SUMMARY : \u6240\u6709\u7684 process descriptor \u548c kernel stack \u90fd\u662f\u4f4d\u4e8ekernel\u4e2d\uff1b\u7531kernel\u6765\u6267\u884c\u8c03\u5ea6\uff1b\u5f53CPU\u9700\u8981\u6267\u884c\u67d0\u4e2a process descriptor \u7684\u65f6\u5019\uff0c\u5b83\u9700\u8981\u8bfb\u53d6\u8fd9\u4e2a process descriptor \u7684\u4e00\u4e9b\u6570\u636e\uff0c\u6bd4\u5982\u4e4b\u524d\u4fdd\u5b58\u7684register\u6570\u636e\u7b49\u4ee5\u4fbfresume\uff1b\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u5e76\u4e14\uff0c\u4ece\u524d\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u6bcf\u4e2a thread_union \u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 kernel stack \uff0c\u800c\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u6240\u4ee5CPU\u662f\u5728\u67d0\u4e2a`thread_union\u7684 kernel stack \u4e2d\u6267\u884c\uff0c\u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684 process descriptor \uff1b SUMMARY : \u56e0\u4e3ascheduler\u5728\u8c03\u5ea6\u4e00\u4e2atask\u5f00\u59cb\u8fd0\u884c\u4e4b\u524d\u4f1a\u5c06\u8fd9\u4e2atask\u7684\u6240\u6709\u7684register\u90fd\u6062\u590d\u5230CPU\u4e2d\uff0c\u6240\u4ee5\u5fc5\u7136\u4f1a\u5305\u542b esp \uff0c\u6240\u4ee5\u5b83\u5c31\u53ef\u4ee5\u6839\u636e esp \u5feb\u901f\u5730\u5b9a\u4f4d\u5230process descriptor\uff1b","title":"3.2.2.2. Identifying the current process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.3-Doubly-linked-lists/","text":"3.2.2.3. Doubly linked lists # Before moving on and describing how the kernel keeps track of the various processes in the system, we would like to emphasize the role of special data structures that implement doubly linked lists. For each list, a set of primitive operations must be implemented: initializing the list, inserting and deleting an element, scanning the list, and so on. It would be both a waste of programmers' efforts and a waste of memory to replicate the primitive operations for each different list. Therefore, the Linux kernel defines the list_head data structure, whose only fields next and prev represent the forward and back pointers of a generic doubly linked list element, respectively. It is important to note, however, that the pointers in a list_head field store the addresses of other list_head fields rather than the addresses of the whole data structures in which the list_head structure is included; see Figure 3-3 (a). SUMMARY : list_head A new list is created by using the LIST_HEAD(list_name) macro. It declares a new variable named list_name of type list_head , which is a dummy first element that acts as a placeholder for the head of the new list, and initializes the prev and next fields of the list_head data structure so as to point to the list_name variable itself; see Figure 3-3 (b). Several functions and macros implement the primitives, including those shown in Table Table 3-1. The Linux kernel 2.6 sports another kind of doubly linked list, which mainly differs from a list_head list because it is not circular; it is mainly used for hash tables , where space is important, and finding the the last element in constant time is not. The list head is stored in an hlist_head data structure, which is simply a pointer to the first element in the list ( NULL if the list is empty). Each element is represented by an hlist_node data structure, which includes a pointer next to the next element, and a pointer pprev to the next field of the previous element. Because the list is not circular, the pprev field of the first element and the next field of the last element are set to NULL . The list can be handled by means of several helper functions and macros similar to those listed in Table 3-1: hlist_add_head( ) , hlist_del( ) , hlist_empty( ) , hlist_entry , hlist_for_each_entry , and so on.","title":"3.2.2.3-Doubly-linked-lists"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.3-Doubly-linked-lists/#3223-doubly-linked-lists","text":"Before moving on and describing how the kernel keeps track of the various processes in the system, we would like to emphasize the role of special data structures that implement doubly linked lists. For each list, a set of primitive operations must be implemented: initializing the list, inserting and deleting an element, scanning the list, and so on. It would be both a waste of programmers' efforts and a waste of memory to replicate the primitive operations for each different list. Therefore, the Linux kernel defines the list_head data structure, whose only fields next and prev represent the forward and back pointers of a generic doubly linked list element, respectively. It is important to note, however, that the pointers in a list_head field store the addresses of other list_head fields rather than the addresses of the whole data structures in which the list_head structure is included; see Figure 3-3 (a). SUMMARY : list_head A new list is created by using the LIST_HEAD(list_name) macro. It declares a new variable named list_name of type list_head , which is a dummy first element that acts as a placeholder for the head of the new list, and initializes the prev and next fields of the list_head data structure so as to point to the list_name variable itself; see Figure 3-3 (b). Several functions and macros implement the primitives, including those shown in Table Table 3-1. The Linux kernel 2.6 sports another kind of doubly linked list, which mainly differs from a list_head list because it is not circular; it is mainly used for hash tables , where space is important, and finding the the last element in constant time is not. The list head is stored in an hlist_head data structure, which is simply a pointer to the first element in the list ( NULL if the list is empty). Each element is represented by an hlist_node data structure, which includes a pointer next to the next element, and a pointer pprev to the next field of the previous element. Because the list is not circular, the pprev field of the first element and the next field of the last element are set to NULL . The list can be handled by means of several helper functions and macros similar to those listed in Table 3-1: hlist_add_head( ) , hlist_del( ) , hlist_empty( ) , hlist_entry , hlist_for_each_entry , and so on.","title":"3.2.2.3. Doubly linked lists"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.4-The-process-list/","text":"3.2.2.4. The process list # The first example of a doubly linked list we will examine is the process list , a list that links together all existing process descriptors. Each task_struct structure includes a tasks field of type list_head whose prev and next fields point, respectively, to the previous and to the next task_struct element. The head of the process list is the init_task task_struct descriptor; it is the process descriptor of the so-called process 0 or swapper (see the section \"Kernel Threads\" later in this chapter). The tasks->prev field of init_task points to the tasks field of the process descriptor inserted last in the list. The SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process list , respectively. These macros also take care of the parenthood relationship of the process (see the section \"How Processes Are Organized\" later in this chapter). Another useful macro, called for_each_process , scans the whole process list. It is defined as: #define for_each_process(p) \\ for (p=&init_task; (p=list_entry((p)->tasks.next, \\ struct task_struct, tasks) \\ ) != &init_task; ) The macro is the loop control statement after which the kernel programmer supplies the loop. Notice how the init_task process descriptor just plays the role of list header . The macro starts by moving past init_task to the next task and continues until it reaches init_task again (thanks to the circularity of the list). At each iteration, the variable passed as the argument of the macro contains the address of the currently scanned process descriptor, as returned by the list_entry macro. SUMMARY : \u5728multiprocessor\u4e2d\uff0c\u662f\u5426\u662f\u6bcf\u4e2aprocessor\u90fd\u6709\u4e00\u4e2aprocess list\uff0c\u8fd8\u662f\u8bf4\u6240\u6709\u7684process descriptor\u90fd\u653e\u5728\u4e00\u4e2aprocess list\u4e2d\uff1f","title":"3.2.2.4-The-process-list"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.4-The-process-list/#3224-the-process-list","text":"The first example of a doubly linked list we will examine is the process list , a list that links together all existing process descriptors. Each task_struct structure includes a tasks field of type list_head whose prev and next fields point, respectively, to the previous and to the next task_struct element. The head of the process list is the init_task task_struct descriptor; it is the process descriptor of the so-called process 0 or swapper (see the section \"Kernel Threads\" later in this chapter). The tasks->prev field of init_task points to the tasks field of the process descriptor inserted last in the list. The SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process list , respectively. These macros also take care of the parenthood relationship of the process (see the section \"How Processes Are Organized\" later in this chapter). Another useful macro, called for_each_process , scans the whole process list. It is defined as: #define for_each_process(p) \\ for (p=&init_task; (p=list_entry((p)->tasks.next, \\ struct task_struct, tasks) \\ ) != &init_task; ) The macro is the loop control statement after which the kernel programmer supplies the loop. Notice how the init_task process descriptor just plays the role of list header . The macro starts by moving past init_task to the next task and continues until it reaches init_task again (thanks to the circularity of the list). At each iteration, the variable passed as the argument of the macro contains the address of the currently scanned process descriptor, as returned by the list_entry macro. SUMMARY : \u5728multiprocessor\u4e2d\uff0c\u662f\u5426\u662f\u6bcf\u4e2aprocessor\u90fd\u6709\u4e00\u4e2aprocess list\uff0c\u8fd8\u662f\u8bf4\u6240\u6709\u7684process descriptor\u90fd\u653e\u5728\u4e00\u4e2aprocess list\u4e2d\uff1f","title":"3.2.2.4. The process list"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.5-The-lists-of-TASK_RUNNING-processes/","text":"3.2.2.5. The lists of TASK_RUNNING processes # When looking for a new process to run on a CPU, the kernel has to consider only the runnable processes (that is, the processes in the TASK_RUNNING state). Earlier Linux versions put all runnable processes in the same list called runqueue . Because it would be too costly to maintain the list ordered according to process priorities, the earlier schedulers were compelled to scan the whole list in order to select the \"best\" runnable process. Linux 2.6 implements the runqueue differently. The aim is to allow the scheduler to select the best runnable process in constant time, independently of the number of runnable processes. We'll defer to Chapter 7 a detailed description of this new kind of runqueue , and we'll provide here only some basic information. The trick used to achieve the scheduler speedup consists of splitting the runqueue in many lists of runnable processes, one list per process priority . Each task_struct descriptor includes a run_list field of type list_head . If the process priority is equal to k (a value ranging between 0 and 139), the run_list field links the process descriptor into the list of runnable processes having priority k . Furthermore, on a multiprocessor system, each CPU has its own runqueue , that is, its own set of lists of processes. This is a classic example of making a data structures more complex to improve performance: to make scheduler operations more efficient, the runqueue list has been split into 140 different lists! As we'll see, the kernel must preserve a lot of data for every runqueue in the system; however, the main data structures of a runqueue are the lists of process descriptors belonging to the runqueue ; all these lists are implemented by a single prio_array_t data structure, whose fields are shown in Table 3-2. SUMMARY : prio_array The enqueue_task(p,array) function inserts a process descriptor into a runqueue list; its code is essentially equivalent to: list_add_tail(&p->run_list, &array->queue[p->prio]); __set_bit(p->prio, array->bitmap); array->nr_active++; p->array = array; The prio field of the process descriptor stores the dynamic priority of the process, while the array field is a pointer to the prio_array_t data structure of its current runqueue . Similarly, the dequeue_task(p,array) function removes a process descriptor from a runqueue list.","title":"3.2.2.5-The-lists-of-TASK_RUNNING-processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2.5-The-lists-of-TASK_RUNNING-processes/#3225-the-lists-of-task_running-processes","text":"When looking for a new process to run on a CPU, the kernel has to consider only the runnable processes (that is, the processes in the TASK_RUNNING state). Earlier Linux versions put all runnable processes in the same list called runqueue . Because it would be too costly to maintain the list ordered according to process priorities, the earlier schedulers were compelled to scan the whole list in order to select the \"best\" runnable process. Linux 2.6 implements the runqueue differently. The aim is to allow the scheduler to select the best runnable process in constant time, independently of the number of runnable processes. We'll defer to Chapter 7 a detailed description of this new kind of runqueue , and we'll provide here only some basic information. The trick used to achieve the scheduler speedup consists of splitting the runqueue in many lists of runnable processes, one list per process priority . Each task_struct descriptor includes a run_list field of type list_head . If the process priority is equal to k (a value ranging between 0 and 139), the run_list field links the process descriptor into the list of runnable processes having priority k . Furthermore, on a multiprocessor system, each CPU has its own runqueue , that is, its own set of lists of processes. This is a classic example of making a data structures more complex to improve performance: to make scheduler operations more efficient, the runqueue list has been split into 140 different lists! As we'll see, the kernel must preserve a lot of data for every runqueue in the system; however, the main data structures of a runqueue are the lists of process descriptors belonging to the runqueue ; all these lists are implemented by a single prio_array_t data structure, whose fields are shown in Table 3-2. SUMMARY : prio_array The enqueue_task(p,array) function inserts a process descriptor into a runqueue list; its code is essentially equivalent to: list_add_tail(&p->run_list, &array->queue[p->prio]); __set_bit(p->prio, array->bitmap); array->nr_active++; p->array = array; The prio field of the process descriptor stores the dynamic priority of the process, while the array field is a pointer to the prio_array_t data structure of its current runqueue . Similarly, the dequeue_task(p,array) function removes a process descriptor from a runqueue list.","title":"3.2.2.5. The lists of TASK_RUNNING processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.3-Relationships-Among-Processes/","text":"3.2.3. Relationships Among Processes 3.2.3.1. The pidhash table and chained lists 3.2.3. Relationships Among Processes # Processes created by a program have a parent/child relationship . When a process creates multiple children , these children have sibling relationships . Several fields must be introduced in a process descriptor to represent these relationships; they are listed in Table 3-3 with respect to a given process P . Processes 0 and 1 are created by the kernel ; as we'll see later in the chapter, process 1 ( init ) is the ancestor of all other processes. Table 3-3. Fields of a process descriptor used to express parenthood relationships Field name Description real_parent Points to the process descriptor of the process that created P or to the descriptor of process 1 ( init ) if the parent process no longer exists. (Therefore, when a user starts a background process and exits the shell, the background process becomes the child of init .) parent Points to the current parent of P (this is the process that must be signaled when the child process terminates); its value usually coincides with that of real_parent . It may occasionally differ, such as when another process issues a ptrace( ) system call requesting that it be allowed to monitor P (see the section \"Execution Tracing\" in Chapter 20). children The head of the list containing all children created by P . sibling The pointers to the next and previous elements in the list of the sibling processes, those that have the same parent as P . Figure 3-4 illustrates the parent and sibling relationships of a group of processes. Process P0 successively created P1 , P2 , and P3 . Process P3 , in turn, created process P4 . Furthermore, there exist other relationships among processes: a process can be a leader of a process group or of a login session (see \"Process Management\" in Chapter 1), it can be a leader of a thread group (see \"Identifying a Process\" earlier in this chapter), and it can also trace the execution of other processes (see the section \"Execution Tracing\" in Chapter 20). Table 3-4 lists the fields of the process descriptor that establish these relationships between a process P and the other processes. Table 3-4. The fields of the process descriptor that establish non-parenthood relationships Field name Description group_leader Process descriptor pointer of the group leader of P SUMMARY : \u662fthread group leader signal->pgrp PID of the group leader of P SUMMARY : \u662fprocess group tgid PID of the thread group leader of P signal->session PID of the login session leader of P ptrace_children The head of a list containing all children of P being traced by a debugger ptrace_list The pointers to the next and previous elements in the real parent's list of traced processes (used when P is being traced) SUMMARY : struct signal_struct SUMMARY : pgrp \u8868\u793a\u7684\u662fprocess group\uff0c tgid \u8868\u793a\u7684\u662fthread group ID\u3002 SUMMARY : How does Linux tell threads apart from child processes? 3.2.3.1. The pidhash table and chained lists # In several circumstances, the kernel must be able to derive the process descriptor pointer corresponding to a PID . This occurs, for instance, in servicing the kill( ) system call. When process P1 wishes to send a signal to another process, P2 , it invokes the kill( ) system call specifying the PID of P2 as the parameter. The kernel derives the process descriptor pointer from the PID and then extracts the pointer to the data structure that records the pending signals from P2 's process descriptor. Scanning the process list sequentially and checking the pid fields of the process descriptors is feasible but rather inefficient. To speed up the search, four hash tables have been introduced. Why multiple hash tables? Simply because the process descriptor includes fields that represent different types of PID (see Table 3-5), and each type of PID requires its own hash table. Table 3-5. The four hash tables and corresponding fields in the process descriptor Hash table type Field name Description PIDTYPE_PID pid PID of the process PIDTYPE_TGID tgid PID of thread group leader process PIDTYPE_PGID pgrp PID of the group leader process PIDTYPE_SID session PID of the session leader process The four hash tables are dynamically allocated during the kernel initialization phase , and their addresses are stored in the pid_hash array. The size of a single hash table depends on the amount of available RAM; for example, for systems having 512 MB of RAM, each hash table is stored in four page frames and includes 2,048 entries. SUMMARY : pid_hash \u5728 / kernel / pid.c \u4e2d\u5b9a\u4e49\uff0c\u5b9a\u4e49\u5982\u4e0b\uff1a static struct hlist_head *pid_hash[PIDTYPE_MAX]; hlist_head \u5728 / include / linux / list.h \u4e2d\u5b9a\u4e49\uff1b SUMMARY :2018\u8868\u793a\u7684\u662fhash table\u7684\u957f\u5ea6\uff0c\u6240\u4ee5hash\u51fd\u6570\u9700\u8981\u5c06 pid \u6620\u5c04\u5230 [0-2017] \u8303\u56f4\u5185\uff1b The PID is transformed into a table index using the pid_hashfn macro, which expands to: #define pid_hashfn(x) hash_long((unsigned long) x, pidhash_shift) The pidhash_shift variable stores the length in bits of a table index (11, in our example). The hash_long( ) function is used by many hash functions; on a 32-bit architecture it is essentially equivalent to: unsigned long hash_long(unsigned long val, unsigned int bits) { unsigned long hash = val * 0x9e370001UL; return hash >> (32 - bits); } Because in our example pidhash_shift is equal to 11, pid_hashfn yields values ranging between 0 and $2^{11} - 1 = 2047$. The Magic Constant You might wonder where the 0x9e370001 constant (= 2,654,404,609) comes from. This hash function is based on a multiplication of the index by a suitable large number, so that the result overflows and the value remaining in the 32-bit variable can be considered as the result of a modulus operation. Knuth suggested that good results are obtained when the large multiplier is a prime approximately in golden ratio to 2 32 (32 bit being the size of the 80x86's registers). Now, 2,654,404,609 is a prime near to that can also be easily multiplied by additions and bit shifts, because it is equal to SUMMARY : \u53c2\u89c1 \u9b54\u6570\u5e38\u91cf The Magic Constant As every basic computer science course explains, a hash function does not always ensure a one-to- one correspondence between PID s and table indexes. Two different PID s that hash into the same table index are said to be colliding . Linux uses chaining to handle colliding PID s; each table entry is the head of a doubly linked list of colliding process descriptors . Figure 3-5 illustrates a PID hash table with two lists. The processes having PID s 2,890 and 29,384 hash into the 200th element of the table, while the process having PID 29,385 hashes into the 1,466 th element of the table. Hashing with chaining is preferable to a linear transformation from PID s to table indexes because at any given instance, the number of processes in the system is usually far below 32,768 (the maximum number of allowed PID s). It would be a waste of storage to define a table consisting of 32,768 entries, if, at any given instance, most such entries are unused. The data structures used in the PID hash tables are quite sophisticated, because they must keep track of the relationships between the processes. As an example, suppose that the kernel must retrieve all processes belonging to a given thread group , that is, all processes whose tgid field is equal to a given number. Looking in the hash table for the given thread group number returns just one process descriptor , that is, the descriptor of the thread group leader . To quickly retrieve the other processes in the group, the kernel must maintain a list of processes for each thread group . The same situation arises when looking for the processes belonging to a given login session or belonging to a given process group . The PID hash table s' data structures solve all these problems, because they allow the definition of a list of processes for any PID number included in a hash table . The core data structure is an array of four pid structures embedded in the pids field of the process descriptor ; the fields of the pid structure are shown in Table 3-6. Table 3-6. The fields of the pid data structures Type Name Description int nr The PID number struct hlist_node pid_chain The links to the next and previous elements in the hash chain list struct list_head pid_list The head of the per- PID list SUMMARY : per- PID list \u5176\u5b9e\u5c31\u662fthread group SUMMARY : hlist_node SUMMARY : / include / linux / pid.h Figure 3-6 shows an example based on the PIDTYPE_TGID hash table. The second entry of the pid_hash array stores the address of the hash table , that is, the array of hlist_head structures representing the heads of the chain lists. In the chain list rooted at the 71 st entry of the hash table, there are two process descriptors corresponding to the PID numbers 246 and 4,351 (double-arrow lines represent a couple of forward and backward pointers). The PID numbers are stored in the nr field of the pid structure embedded in the process descriptor (by the way, because the thread group number coincides with the PID of its leader, these numbers also are stored in the pid field of the process descriptors ). Let us consider the per- PID list of the thread group 4,351: the head of the list is stored in the pid_list field of the process descriptor included in the hash table, while the links to the next and previous elements of the per- PID list also are stored in the pid_list field of each list element. The following functions and macros are used to handle the PID hash tables: do_each_task_pid(nr, type, task); SUMMARY : type \u5728 / include / linux / pid.h \u4e2d\u5b9a\u4e49 while_each_task_pid(nr, type, task); Mark begin and end of a do-while loop that iterates over the per- PID list associated with the PID number nr of type type ; in any iteration, task points to the process descriptor of the currently scanned element. find_task_by_pid_type(type, nr) Looks for the process having PID nr in the hash table of type type . The function returns a process descriptor pointer if a match is found, otherwise it returns NULL . find_task_by_pid(nr) Same as find_task_by_pid_type(PIDTYPE_PID, nr) . attach_pid(task, type, nr) Inserts the process descriptor pointed to by task in the PID hash table of type type according to the PID number nr ; if a process descriptor having PID nr is already in the hash table, the function simply inserts task in the per- PID list of the already present process. detach_pid(task, type) Removes the process descriptor pointed to by task from the per- PID list of type type to which the descriptor belongs. If the per- PID list does not become empty, the function terminates. Otherwise, the function removes the process descriptor from the hash table of type type ; finally, if the PID number does not occur in any other hash table, the function clears the corresponding bit in the PID bitmap, so that the number can be recycled. next_thread(task) Returns the process descriptor address of the lightweight process that follows task in the hash table list of type PIDTYPE_TGID . Because the hash table list is circular, when applied to a conventional process the macro returns the descriptor address of the process itself.","title":"3.2.3-Relationships-Among-Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.3-Relationships-Among-Processes/#323-relationships-among-processes","text":"Processes created by a program have a parent/child relationship . When a process creates multiple children , these children have sibling relationships . Several fields must be introduced in a process descriptor to represent these relationships; they are listed in Table 3-3 with respect to a given process P . Processes 0 and 1 are created by the kernel ; as we'll see later in the chapter, process 1 ( init ) is the ancestor of all other processes. Table 3-3. Fields of a process descriptor used to express parenthood relationships Field name Description real_parent Points to the process descriptor of the process that created P or to the descriptor of process 1 ( init ) if the parent process no longer exists. (Therefore, when a user starts a background process and exits the shell, the background process becomes the child of init .) parent Points to the current parent of P (this is the process that must be signaled when the child process terminates); its value usually coincides with that of real_parent . It may occasionally differ, such as when another process issues a ptrace( ) system call requesting that it be allowed to monitor P (see the section \"Execution Tracing\" in Chapter 20). children The head of the list containing all children created by P . sibling The pointers to the next and previous elements in the list of the sibling processes, those that have the same parent as P . Figure 3-4 illustrates the parent and sibling relationships of a group of processes. Process P0 successively created P1 , P2 , and P3 . Process P3 , in turn, created process P4 . Furthermore, there exist other relationships among processes: a process can be a leader of a process group or of a login session (see \"Process Management\" in Chapter 1), it can be a leader of a thread group (see \"Identifying a Process\" earlier in this chapter), and it can also trace the execution of other processes (see the section \"Execution Tracing\" in Chapter 20). Table 3-4 lists the fields of the process descriptor that establish these relationships between a process P and the other processes. Table 3-4. The fields of the process descriptor that establish non-parenthood relationships Field name Description group_leader Process descriptor pointer of the group leader of P SUMMARY : \u662fthread group leader signal->pgrp PID of the group leader of P SUMMARY : \u662fprocess group tgid PID of the thread group leader of P signal->session PID of the login session leader of P ptrace_children The head of a list containing all children of P being traced by a debugger ptrace_list The pointers to the next and previous elements in the real parent's list of traced processes (used when P is being traced) SUMMARY : struct signal_struct SUMMARY : pgrp \u8868\u793a\u7684\u662fprocess group\uff0c tgid \u8868\u793a\u7684\u662fthread group ID\u3002 SUMMARY : How does Linux tell threads apart from child processes?","title":"3.2.3. Relationships Among Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.3-Relationships-Among-Processes/#3231-the-pidhash-table-and-chained-lists","text":"In several circumstances, the kernel must be able to derive the process descriptor pointer corresponding to a PID . This occurs, for instance, in servicing the kill( ) system call. When process P1 wishes to send a signal to another process, P2 , it invokes the kill( ) system call specifying the PID of P2 as the parameter. The kernel derives the process descriptor pointer from the PID and then extracts the pointer to the data structure that records the pending signals from P2 's process descriptor. Scanning the process list sequentially and checking the pid fields of the process descriptors is feasible but rather inefficient. To speed up the search, four hash tables have been introduced. Why multiple hash tables? Simply because the process descriptor includes fields that represent different types of PID (see Table 3-5), and each type of PID requires its own hash table. Table 3-5. The four hash tables and corresponding fields in the process descriptor Hash table type Field name Description PIDTYPE_PID pid PID of the process PIDTYPE_TGID tgid PID of thread group leader process PIDTYPE_PGID pgrp PID of the group leader process PIDTYPE_SID session PID of the session leader process The four hash tables are dynamically allocated during the kernel initialization phase , and their addresses are stored in the pid_hash array. The size of a single hash table depends on the amount of available RAM; for example, for systems having 512 MB of RAM, each hash table is stored in four page frames and includes 2,048 entries. SUMMARY : pid_hash \u5728 / kernel / pid.c \u4e2d\u5b9a\u4e49\uff0c\u5b9a\u4e49\u5982\u4e0b\uff1a static struct hlist_head *pid_hash[PIDTYPE_MAX]; hlist_head \u5728 / include / linux / list.h \u4e2d\u5b9a\u4e49\uff1b SUMMARY :2018\u8868\u793a\u7684\u662fhash table\u7684\u957f\u5ea6\uff0c\u6240\u4ee5hash\u51fd\u6570\u9700\u8981\u5c06 pid \u6620\u5c04\u5230 [0-2017] \u8303\u56f4\u5185\uff1b The PID is transformed into a table index using the pid_hashfn macro, which expands to: #define pid_hashfn(x) hash_long((unsigned long) x, pidhash_shift) The pidhash_shift variable stores the length in bits of a table index (11, in our example). The hash_long( ) function is used by many hash functions; on a 32-bit architecture it is essentially equivalent to: unsigned long hash_long(unsigned long val, unsigned int bits) { unsigned long hash = val * 0x9e370001UL; return hash >> (32 - bits); } Because in our example pidhash_shift is equal to 11, pid_hashfn yields values ranging between 0 and $2^{11} - 1 = 2047$. The Magic Constant You might wonder where the 0x9e370001 constant (= 2,654,404,609) comes from. This hash function is based on a multiplication of the index by a suitable large number, so that the result overflows and the value remaining in the 32-bit variable can be considered as the result of a modulus operation. Knuth suggested that good results are obtained when the large multiplier is a prime approximately in golden ratio to 2 32 (32 bit being the size of the 80x86's registers). Now, 2,654,404,609 is a prime near to that can also be easily multiplied by additions and bit shifts, because it is equal to SUMMARY : \u53c2\u89c1 \u9b54\u6570\u5e38\u91cf The Magic Constant As every basic computer science course explains, a hash function does not always ensure a one-to- one correspondence between PID s and table indexes. Two different PID s that hash into the same table index are said to be colliding . Linux uses chaining to handle colliding PID s; each table entry is the head of a doubly linked list of colliding process descriptors . Figure 3-5 illustrates a PID hash table with two lists. The processes having PID s 2,890 and 29,384 hash into the 200th element of the table, while the process having PID 29,385 hashes into the 1,466 th element of the table. Hashing with chaining is preferable to a linear transformation from PID s to table indexes because at any given instance, the number of processes in the system is usually far below 32,768 (the maximum number of allowed PID s). It would be a waste of storage to define a table consisting of 32,768 entries, if, at any given instance, most such entries are unused. The data structures used in the PID hash tables are quite sophisticated, because they must keep track of the relationships between the processes. As an example, suppose that the kernel must retrieve all processes belonging to a given thread group , that is, all processes whose tgid field is equal to a given number. Looking in the hash table for the given thread group number returns just one process descriptor , that is, the descriptor of the thread group leader . To quickly retrieve the other processes in the group, the kernel must maintain a list of processes for each thread group . The same situation arises when looking for the processes belonging to a given login session or belonging to a given process group . The PID hash table s' data structures solve all these problems, because they allow the definition of a list of processes for any PID number included in a hash table . The core data structure is an array of four pid structures embedded in the pids field of the process descriptor ; the fields of the pid structure are shown in Table 3-6. Table 3-6. The fields of the pid data structures Type Name Description int nr The PID number struct hlist_node pid_chain The links to the next and previous elements in the hash chain list struct list_head pid_list The head of the per- PID list SUMMARY : per- PID list \u5176\u5b9e\u5c31\u662fthread group SUMMARY : hlist_node SUMMARY : / include / linux / pid.h Figure 3-6 shows an example based on the PIDTYPE_TGID hash table. The second entry of the pid_hash array stores the address of the hash table , that is, the array of hlist_head structures representing the heads of the chain lists. In the chain list rooted at the 71 st entry of the hash table, there are two process descriptors corresponding to the PID numbers 246 and 4,351 (double-arrow lines represent a couple of forward and backward pointers). The PID numbers are stored in the nr field of the pid structure embedded in the process descriptor (by the way, because the thread group number coincides with the PID of its leader, these numbers also are stored in the pid field of the process descriptors ). Let us consider the per- PID list of the thread group 4,351: the head of the list is stored in the pid_list field of the process descriptor included in the hash table, while the links to the next and previous elements of the per- PID list also are stored in the pid_list field of each list element. The following functions and macros are used to handle the PID hash tables: do_each_task_pid(nr, type, task); SUMMARY : type \u5728 / include / linux / pid.h \u4e2d\u5b9a\u4e49 while_each_task_pid(nr, type, task); Mark begin and end of a do-while loop that iterates over the per- PID list associated with the PID number nr of type type ; in any iteration, task points to the process descriptor of the currently scanned element. find_task_by_pid_type(type, nr) Looks for the process having PID nr in the hash table of type type . The function returns a process descriptor pointer if a match is found, otherwise it returns NULL . find_task_by_pid(nr) Same as find_task_by_pid_type(PIDTYPE_PID, nr) . attach_pid(task, type, nr) Inserts the process descriptor pointed to by task in the PID hash table of type type according to the PID number nr ; if a process descriptor having PID nr is already in the hash table, the function simply inserts task in the per- PID list of the already present process. detach_pid(task, type) Removes the process descriptor pointed to by task from the per- PID list of type type to which the descriptor belongs. If the per- PID list does not become empty, the function terminates. Otherwise, the function removes the process descriptor from the hash table of type type ; finally, if the PID number does not occur in any other hash table, the function clears the corresponding bit in the PID bitmap, so that the number can be recycled. next_thread(task) Returns the process descriptor address of the lightweight process that follows task in the hash table list of type PIDTYPE_TGID . Because the hash table list is circular, when applied to a conventional process the macro returns the descriptor address of the process itself.","title":"3.2.3.1. The pidhash table and chained lists"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.4-How-Processes-Are-Organized/","text":"3.2.4. How Processes Are Organized 3.2.4.1. Wait queues 3.2.4.2. Handling wait queues 3.2.4. How Processes Are Organized # The runqueue lists group all processes in a TASK_RUNNING state. When it comes to grouping processes in other states, the various states call for different types of treatment, with Linux opting for one of the choices shown in the following list. Processes in a TASK_STOPPED , EXIT_ZOMBIE , or EXIT_DEAD state are not linked in specific lists. There is no need to group processes in any of these three states, because stopped, zombie, and dead processes are accessed only via PID or via linked lists of the child processes for a particular parent. Processes in a TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE state are subdivided into many classes, each of which corresponds to a specific event . In this case, the process state does not provide enough information to retrieve the process quickly, so it is necessary to introduce additional lists of processes. These are called wait queues and are discussed next. 3.2.4.1. Wait queues # Wait queues have several uses in the kernel, particularly for interrupt handling , process synchronization , and timing . Because these topics are discussed in later chapters, we'll just say here that a process must often wait for some event to occur, such as for a disk operation to terminate, a system resource to be released, or a fixed interval of time to elapse. Wait queues implement conditional waits on events: a process wishing to wait for a specific event places itself in the proper wait queue and relinquishes\uff08\u8ba9\u6e21\uff09 control. Therefore, a wait queue represents a set of sleeping processes , which are woken up by the kernel when some condition becomes true. Wait queues are implemented as doubly linked lists whose elements include pointers to process descriptors . Each wait queue is identified by a wait queue head , a data structure of type wait_queue_head_t : struct __wait_queue_head { spinlock_t lock; struct list_head task_list; }; typedef struct __wait_queue_head wait_queue_head_t; Because wait queues are modified by interrupt handlers as well as by major kernel functions, the doubly linked lists must be protected from concurrent accesses, which could induce unpredictable results (see Chapter 5). Synchronization is achieved by the lock spin lock in the wait queue head. The task_list field is the head of the list of waiting processes. SUMMARY : \u57281.6.3. Reentrant Kernels\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u63cf\u8ff0\uff0c\u975e\u5e38\u6709\u4ef7\u503c\uff1a All Unix kernels are reentrant. This means that several processes may be executing in Kernel Mode at the same time. Of course, on uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes. An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution. Elements of a wait queue list are of type wait_queue_t : struct __wait_queue { unsigned int flags; struct task_struct * task; wait_queue_func_t func; struct list_head task_list; }; typedef struct __wait_queue wait_queue_t; Each element in the wait queue list represents a sleeping process , which is waiting for some event to occur; its descriptor address is stored in the task field. The task_list field contains the pointers that link this element to the list of processes waiting for the same event. However, it is not always convenient to wake up all sleeping processes in a wait queue. For instance, if two or more processes are waiting for exclusive access to some resource to be released, it makes sense to wake up just one process in the wait queue . This process takes the resource, while the other processes continue to sleep. (This avoids a problem known as the \" thundering herd ,\" with which multiple processes are wakened only to race for a resource that can be accessed by one of them, with the result that remaining processes must once more be put back to sleep.) Thus, there are two kinds of sleeping processes: exclusive processes (denoted by the value 1 in the flags field of the corresponding wait queue element) are selectively woken up by the kernel , while nonexclusive processes (denoted by the value 0 in the flags field) are always woken up by the kernel when the event occurs. A process waiting for a resource that can be granted to just one process at a time is a typical exclusive process . Processes waiting for an event that may concern any of them are nonexclusive . Consider, for instance, a group of processes that are waiting for the termination of a group of disk block transfers: as soon as the transfers complete, all waiting processes must be woken up. As we'll see next, the func field of a wait queue element is used to specify how the processes sleeping in the wait queue should be woken up. 3.2.4.2. Handling wait queues # A new wait queue head may be defined by using the DECLARE_WAIT_QUEUE_HEAD(name) macro, which statically declares a new wait queue head variable called name and initializes its lock and task_list fields. The init_waitqueue_head( ) function may be used to initialize a wait queue head variable that was allocated dynamically. The init_waitqueue_entry(q,p ) function initializes a wait_queue_t structure q as follows: q->flags = 0; q->task = p; q->func = default_wake_function; The nonexclusive process p will be awakened by default_wake_function( ) , which is a simple wrapper for the try_to_wake_up( ) function discussed in Chapter 7. Alternatively, the DEFINE_WAIT macro declares a new wait_queue_t variable and initializes it with the descriptor of the process currently executing on the CPU and the address of the autoremove_wake_function( ) wake-up function. This function invokes default_wake_function( ) to awaken the sleeping process, and then removes the wait queue element from the wait queue list . Finally, a kernel developer can define a custom awakening function by initializing the wait queue element with the init_waitqueue_func_entry( ) function. Once an element is defined, it must be inserted into a wait queue . The add_wait_queue( ) function inserts a nonexclusive process in the first position of a wait queue list. The add_wait_queue_exclusive( ) function inserts an exclusive process in the last position of a wait queue list. The remove_wait_queue( ) function removes a process from a wait queue list. The waitqueue_active( ) function checks whether a given wait queue list is empty. A process wishing to wait for a specific condition can invoke any of the functions shown in the following list. The sleep_on( ) function operates on the current process: void sleep_on(wait_queue_head_t *wq) { wait_queue_t wait; init_waitqueue_entry(&wait, current); current->state = TASK_UNINTERRUPTIBLE; add_wait_queue(wq,&wait); /* wq points to the wait queue head */ schedule( ); remove_wait_queue(wq, &wait); } The function sets the state of the current process to TASK_UNINTERRUPTIBLE and inserts it into the specified wait queue. Then it invokes the scheduler, which resumes the execution of another process. When the sleeping process is awakened, the scheduler resumes execution of the sleep_on( ) function, which removes the process from the wait queue. SUMMARY :Wait queues implement conditional waits on events: a process wishing to wait for a specific event places itself in the proper wait queue and relinquishes control.\u663e\u7136\uff0c\u4e0a\u8ff0\u4ee3\u7801\u4e2d\u7684 schedule( ) \u5c31\u8868\u793arelinquishes control\uff0c\u663e\u7136\uff0c\u8fd9\u5c31\u662f\u53d1\u751f\u5728chapter 3.3\u4e2d\u4ecb\u7ecd\u7684Process Switch\uff1b\u663e\u7136\uff0c\u5f53\u8fd9\u4e2aprocess\u518d\u6b21\u88ab\u5524\u9192\u7684\u65f6\u5019\uff0c\u5b83\u5c31\u9700\u8981\u63a5\u7740\u5b83\u4e0a\u6b21\u88ab\u7ec8\u6b62\u7684\u5730\u65b9\u7ee7\u7eed\u8fd0\u884c\uff0c\u5373\u4ece remove_wait_queue(wq, &wait); \u5f00\u59cb\u8fd0\u884c\u3002 The interruptible_sleep_on( ) function is identical to sleep_on( ) , except that it sets the state of the current process to TASK_INTERRUPTIBLE instead of setting it to TASK_UNINTERRUPTIBLE , so that the process also can be woken up by receiving a signal. The sleep_on_timeout( ) and interruptible_sleep_on_timeout( ) functions are similar to the previous ones, but they also allow the caller to define a time interval after which the process will be woken up by the kernel. To do this, they invoke the schedule_timeout( ) function instead of schedule( ) (see the section \"An Application of Dynamic Timers: the nanosleep( ) System Call\" in Chapter 6). The prepare_to_wait( ) , prepare_to_wait_exclusive( ) , and finish_wait( ) functions, introduced in Linux 2.6, offer yet another way to put the current process to sleep in a wait queue. Typically, they are used as follows: DEFINE_WAIT(wait); prepare_to_wait_exclusive(&wq, &wait, TASK_INTERRUPTIBLE); /* wq is the head of the wait queue */ ... if (!condition) schedule(); finish_wait(&wq, &wait); The prepare_to_wait( ) and prepare_to_wait_exclusive( ) functions set the process state to the value passed as the third parameter, then set the exclusive flag in the wait queue element respectively to 0 (nonexclusive) or 1 (exclusive), and finally insert the wait queue element wait into the list of the wait queue head wq . As soon as the process is awakened, it executes the finish_wait( ) function, which sets again the process state to TASK_RUNNING (just in case the awaking condition becomes true before invoking schedule( ) ), and removes the wait queue element from the wait queue list (unless this has already been done by the wake-up function). The wait_event and wait_event_interruptible macros put the calling process to sleep on a wait queue until a given condition is verified. For instance, the wait_event(wq,condition) macro essentially yields the following fragment: DEFINE_WAIT(_ _wait); for (;;) { prepare_to_wait(&wq, &_ _wait, TASK_UNINTERRUPTIBLE); if (condition) break; schedule( ); } finish_wait(&wq, &_ _wait); SUMMARY : \u963b\u585e\u7684\u7cfb\u7edf\u8c03\u7528\u4e5f\u4f1a\u5bfc\u81f4kernel\u8fdb\u884cschedule\u3002 SUMMARY : \u4e3a\u4ec0\u4e48\u8981\u52a0\u4e0a for \uff1f A few comments on the functions mentioned in the above list: the sleep_on( ) -like functions cannot be used in the common situation where one has to test a condition and atomically put the process to sleep when the condition is not verified; therefore, because they are a well-known source of race conditions, their use is discouraged\uff08 Time-of-check to time-of-use \uff09. Moreover, in order to insert an exclusive process into a wait queue , the kernel must make use of the prepare_to_wait_exclusive( ) function (or just invoke add_wait_queue_exclusive( ) directly); any other helper function inserts the process as nonexclusive. Finally, unless DEFINE_WAIT or finish_wait( ) are used, the kernel must remove the wait queue element from the list after the waiting process has been awakened. The kernel awakens processes in the wait queues , putting them in the TASK_RUNNING state, by means of one of the following macros: wake_up , wake_up_nr , wake_up_all , wake_up_interruptible , wake_up_interruptible_nr , wake_up_interruptible_all , wake_up_interruptible_sync , and wake_up_locked . One can understand what each of these nine macros does from its name: All macros take into consideration sleeping processes in the TASK_INTERRUPTIBLE state; if the macro name does not include the string \"interruptible,\" sleeping processes in the TASK_UNINTERRUPTIBLE state also are considered. SUMMARY : \u6ca1\u6709\u641e\u6e05\u695a\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u542b\u4e49 All macros wake all nonexclusive processes having the required state (see the previous bullet item). The macros whose name include the string \" nr \" wake a given number of exclusive processes having the required state; this number is a parameter of the macro. The macros whose names include the string \" all \" wake all exclusive processes having the required state. Finally, the macros whose names don't include \" nr \" or \" all \" wake exactly one exclusive process that has the required state. The macros whose names don't include the string \" sync \" check whether the priority of any of the woken processes is higher than that of the processes currently running in the systems and invoke schedule( ) if necessary. These checks are not made by the macro whose name includes the string \" sync \"; as a result, execution of a high priority process might be slightly delayed. The wake_up_locked macro is similar to wake_up , except that it is called when the spin lock in wait_queue_head_t is already held. For instance, the wake_up macro is essentially equivalent to the following code fragment: void wake_up(wait_queue_head_t *q) { struct list_head *tmp; wait_queue_t *curr; list_for_each(tmp, &q->task_list) { curr = list_entry(tmp, wait_queue_t, task_list); if (curr->func(curr, TASK_INTERRUPTIBLE|TASK_UNINTERRUPTIBLE, 0, NULL) && curr->flags) break; } } The list_for_each macro scans all items in the q->task_list doubly linked list, that is, all processes in the wait queue . For each item, the list_entry macro computes the address of the corresponding wait_queue_t variable. The func field of this variable stores the address of the wake-up function, which tries to wake up the process identified by the task field of the wait queue element. If a process has been effectively awakened (the function returned 1) and if the process is exclusive ( curr->flags equal to 1), the loop terminates. Because all nonexclusive processes are always at the beginning of the doubly linked list and all exclusive processes are at the end, the function always wakes the nonexclusive processes and then wakes one exclusive process, if any exists. [ * ] [ * ] By the way, it is rather uncommon that a wait queue includes both exclusive and nonexclusive processes.","title":"3.2.4-How-Processes-Are-Organized"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.4-How-Processes-Are-Organized/#324-how-processes-are-organized","text":"The runqueue lists group all processes in a TASK_RUNNING state. When it comes to grouping processes in other states, the various states call for different types of treatment, with Linux opting for one of the choices shown in the following list. Processes in a TASK_STOPPED , EXIT_ZOMBIE , or EXIT_DEAD state are not linked in specific lists. There is no need to group processes in any of these three states, because stopped, zombie, and dead processes are accessed only via PID or via linked lists of the child processes for a particular parent. Processes in a TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE state are subdivided into many classes, each of which corresponds to a specific event . In this case, the process state does not provide enough information to retrieve the process quickly, so it is necessary to introduce additional lists of processes. These are called wait queues and are discussed next.","title":"3.2.4. How Processes Are Organized"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.4-How-Processes-Are-Organized/#3241-wait-queues","text":"Wait queues have several uses in the kernel, particularly for interrupt handling , process synchronization , and timing . Because these topics are discussed in later chapters, we'll just say here that a process must often wait for some event to occur, such as for a disk operation to terminate, a system resource to be released, or a fixed interval of time to elapse. Wait queues implement conditional waits on events: a process wishing to wait for a specific event places itself in the proper wait queue and relinquishes\uff08\u8ba9\u6e21\uff09 control. Therefore, a wait queue represents a set of sleeping processes , which are woken up by the kernel when some condition becomes true. Wait queues are implemented as doubly linked lists whose elements include pointers to process descriptors . Each wait queue is identified by a wait queue head , a data structure of type wait_queue_head_t : struct __wait_queue_head { spinlock_t lock; struct list_head task_list; }; typedef struct __wait_queue_head wait_queue_head_t; Because wait queues are modified by interrupt handlers as well as by major kernel functions, the doubly linked lists must be protected from concurrent accesses, which could induce unpredictable results (see Chapter 5). Synchronization is achieved by the lock spin lock in the wait queue head. The task_list field is the head of the list of waiting processes. SUMMARY : \u57281.6.3. Reentrant Kernels\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u63cf\u8ff0\uff0c\u975e\u5e38\u6709\u4ef7\u503c\uff1a All Unix kernels are reentrant. This means that several processes may be executing in Kernel Mode at the same time. Of course, on uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes. An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution. Elements of a wait queue list are of type wait_queue_t : struct __wait_queue { unsigned int flags; struct task_struct * task; wait_queue_func_t func; struct list_head task_list; }; typedef struct __wait_queue wait_queue_t; Each element in the wait queue list represents a sleeping process , which is waiting for some event to occur; its descriptor address is stored in the task field. The task_list field contains the pointers that link this element to the list of processes waiting for the same event. However, it is not always convenient to wake up all sleeping processes in a wait queue. For instance, if two or more processes are waiting for exclusive access to some resource to be released, it makes sense to wake up just one process in the wait queue . This process takes the resource, while the other processes continue to sleep. (This avoids a problem known as the \" thundering herd ,\" with which multiple processes are wakened only to race for a resource that can be accessed by one of them, with the result that remaining processes must once more be put back to sleep.) Thus, there are two kinds of sleeping processes: exclusive processes (denoted by the value 1 in the flags field of the corresponding wait queue element) are selectively woken up by the kernel , while nonexclusive processes (denoted by the value 0 in the flags field) are always woken up by the kernel when the event occurs. A process waiting for a resource that can be granted to just one process at a time is a typical exclusive process . Processes waiting for an event that may concern any of them are nonexclusive . Consider, for instance, a group of processes that are waiting for the termination of a group of disk block transfers: as soon as the transfers complete, all waiting processes must be woken up. As we'll see next, the func field of a wait queue element is used to specify how the processes sleeping in the wait queue should be woken up.","title":"3.2.4.1. Wait queues"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.4-How-Processes-Are-Organized/#3242-handling-wait-queues","text":"A new wait queue head may be defined by using the DECLARE_WAIT_QUEUE_HEAD(name) macro, which statically declares a new wait queue head variable called name and initializes its lock and task_list fields. The init_waitqueue_head( ) function may be used to initialize a wait queue head variable that was allocated dynamically. The init_waitqueue_entry(q,p ) function initializes a wait_queue_t structure q as follows: q->flags = 0; q->task = p; q->func = default_wake_function; The nonexclusive process p will be awakened by default_wake_function( ) , which is a simple wrapper for the try_to_wake_up( ) function discussed in Chapter 7. Alternatively, the DEFINE_WAIT macro declares a new wait_queue_t variable and initializes it with the descriptor of the process currently executing on the CPU and the address of the autoremove_wake_function( ) wake-up function. This function invokes default_wake_function( ) to awaken the sleeping process, and then removes the wait queue element from the wait queue list . Finally, a kernel developer can define a custom awakening function by initializing the wait queue element with the init_waitqueue_func_entry( ) function. Once an element is defined, it must be inserted into a wait queue . The add_wait_queue( ) function inserts a nonexclusive process in the first position of a wait queue list. The add_wait_queue_exclusive( ) function inserts an exclusive process in the last position of a wait queue list. The remove_wait_queue( ) function removes a process from a wait queue list. The waitqueue_active( ) function checks whether a given wait queue list is empty. A process wishing to wait for a specific condition can invoke any of the functions shown in the following list. The sleep_on( ) function operates on the current process: void sleep_on(wait_queue_head_t *wq) { wait_queue_t wait; init_waitqueue_entry(&wait, current); current->state = TASK_UNINTERRUPTIBLE; add_wait_queue(wq,&wait); /* wq points to the wait queue head */ schedule( ); remove_wait_queue(wq, &wait); } The function sets the state of the current process to TASK_UNINTERRUPTIBLE and inserts it into the specified wait queue. Then it invokes the scheduler, which resumes the execution of another process. When the sleeping process is awakened, the scheduler resumes execution of the sleep_on( ) function, which removes the process from the wait queue. SUMMARY :Wait queues implement conditional waits on events: a process wishing to wait for a specific event places itself in the proper wait queue and relinquishes control.\u663e\u7136\uff0c\u4e0a\u8ff0\u4ee3\u7801\u4e2d\u7684 schedule( ) \u5c31\u8868\u793arelinquishes control\uff0c\u663e\u7136\uff0c\u8fd9\u5c31\u662f\u53d1\u751f\u5728chapter 3.3\u4e2d\u4ecb\u7ecd\u7684Process Switch\uff1b\u663e\u7136\uff0c\u5f53\u8fd9\u4e2aprocess\u518d\u6b21\u88ab\u5524\u9192\u7684\u65f6\u5019\uff0c\u5b83\u5c31\u9700\u8981\u63a5\u7740\u5b83\u4e0a\u6b21\u88ab\u7ec8\u6b62\u7684\u5730\u65b9\u7ee7\u7eed\u8fd0\u884c\uff0c\u5373\u4ece remove_wait_queue(wq, &wait); \u5f00\u59cb\u8fd0\u884c\u3002 The interruptible_sleep_on( ) function is identical to sleep_on( ) , except that it sets the state of the current process to TASK_INTERRUPTIBLE instead of setting it to TASK_UNINTERRUPTIBLE , so that the process also can be woken up by receiving a signal. The sleep_on_timeout( ) and interruptible_sleep_on_timeout( ) functions are similar to the previous ones, but they also allow the caller to define a time interval after which the process will be woken up by the kernel. To do this, they invoke the schedule_timeout( ) function instead of schedule( ) (see the section \"An Application of Dynamic Timers: the nanosleep( ) System Call\" in Chapter 6). The prepare_to_wait( ) , prepare_to_wait_exclusive( ) , and finish_wait( ) functions, introduced in Linux 2.6, offer yet another way to put the current process to sleep in a wait queue. Typically, they are used as follows: DEFINE_WAIT(wait); prepare_to_wait_exclusive(&wq, &wait, TASK_INTERRUPTIBLE); /* wq is the head of the wait queue */ ... if (!condition) schedule(); finish_wait(&wq, &wait); The prepare_to_wait( ) and prepare_to_wait_exclusive( ) functions set the process state to the value passed as the third parameter, then set the exclusive flag in the wait queue element respectively to 0 (nonexclusive) or 1 (exclusive), and finally insert the wait queue element wait into the list of the wait queue head wq . As soon as the process is awakened, it executes the finish_wait( ) function, which sets again the process state to TASK_RUNNING (just in case the awaking condition becomes true before invoking schedule( ) ), and removes the wait queue element from the wait queue list (unless this has already been done by the wake-up function). The wait_event and wait_event_interruptible macros put the calling process to sleep on a wait queue until a given condition is verified. For instance, the wait_event(wq,condition) macro essentially yields the following fragment: DEFINE_WAIT(_ _wait); for (;;) { prepare_to_wait(&wq, &_ _wait, TASK_UNINTERRUPTIBLE); if (condition) break; schedule( ); } finish_wait(&wq, &_ _wait); SUMMARY : \u963b\u585e\u7684\u7cfb\u7edf\u8c03\u7528\u4e5f\u4f1a\u5bfc\u81f4kernel\u8fdb\u884cschedule\u3002 SUMMARY : \u4e3a\u4ec0\u4e48\u8981\u52a0\u4e0a for \uff1f A few comments on the functions mentioned in the above list: the sleep_on( ) -like functions cannot be used in the common situation where one has to test a condition and atomically put the process to sleep when the condition is not verified; therefore, because they are a well-known source of race conditions, their use is discouraged\uff08 Time-of-check to time-of-use \uff09. Moreover, in order to insert an exclusive process into a wait queue , the kernel must make use of the prepare_to_wait_exclusive( ) function (or just invoke add_wait_queue_exclusive( ) directly); any other helper function inserts the process as nonexclusive. Finally, unless DEFINE_WAIT or finish_wait( ) are used, the kernel must remove the wait queue element from the list after the waiting process has been awakened. The kernel awakens processes in the wait queues , putting them in the TASK_RUNNING state, by means of one of the following macros: wake_up , wake_up_nr , wake_up_all , wake_up_interruptible , wake_up_interruptible_nr , wake_up_interruptible_all , wake_up_interruptible_sync , and wake_up_locked . One can understand what each of these nine macros does from its name: All macros take into consideration sleeping processes in the TASK_INTERRUPTIBLE state; if the macro name does not include the string \"interruptible,\" sleeping processes in the TASK_UNINTERRUPTIBLE state also are considered. SUMMARY : \u6ca1\u6709\u641e\u6e05\u695a\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u542b\u4e49 All macros wake all nonexclusive processes having the required state (see the previous bullet item). The macros whose name include the string \" nr \" wake a given number of exclusive processes having the required state; this number is a parameter of the macro. The macros whose names include the string \" all \" wake all exclusive processes having the required state. Finally, the macros whose names don't include \" nr \" or \" all \" wake exactly one exclusive process that has the required state. The macros whose names don't include the string \" sync \" check whether the priority of any of the woken processes is higher than that of the processes currently running in the systems and invoke schedule( ) if necessary. These checks are not made by the macro whose name includes the string \" sync \"; as a result, execution of a high priority process might be slightly delayed. The wake_up_locked macro is similar to wake_up , except that it is called when the spin lock in wait_queue_head_t is already held. For instance, the wake_up macro is essentially equivalent to the following code fragment: void wake_up(wait_queue_head_t *q) { struct list_head *tmp; wait_queue_t *curr; list_for_each(tmp, &q->task_list) { curr = list_entry(tmp, wait_queue_t, task_list); if (curr->func(curr, TASK_INTERRUPTIBLE|TASK_UNINTERRUPTIBLE, 0, NULL) && curr->flags) break; } } The list_for_each macro scans all items in the q->task_list doubly linked list, that is, all processes in the wait queue . For each item, the list_entry macro computes the address of the corresponding wait_queue_t variable. The func field of this variable stores the address of the wake-up function, which tries to wake up the process identified by the task field of the wait queue element. If a process has been effectively awakened (the function returned 1) and if the process is exclusive ( curr->flags equal to 1), the loop terminates. Because all nonexclusive processes are always at the beginning of the doubly linked list and all exclusive processes are at the end, the function always wakes the nonexclusive processes and then wakes one exclusive process, if any exists. [ * ] [ * ] By the way, it is rather uncommon that a wait queue includes both exclusive and nonexclusive processes.","title":"3.2.4.2. Handling wait queues"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.5-Process-Resource-Limits/","text":"","title":"3.2.5-Process-Resource-Limits"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.3-Process-Switch/","text":"3.3. Process Switch # To control the execution of processes, the kernel must be able to suspend the execution of the process running on the CPU and resume the execution of some other process previously suspended. This activity goes variously by the names process switch , task switch , or context switch . The next sections describe the elements of process switching in Linux. 3.3.1. Hardware Context # While each process can have its own address space , all processes have to share the CPU registers . So before resuming the execution of a process, the kernel must ensure that each such register is loaded with the value it had when the process was suspended. The set of data that must be loaded into the registers before the process resumes its execution on the CPU is called the hardware context . The hardware context is a subset of the process execution context , which includes all information needed for the process execution. In Linux, a part of the hardware context of a process is stored in the process descriptor , while the remaining part is saved in the Kernel Mode stack . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981 In the description that follows, we will assume the prev local variable refers to the process descriptor of the process being switched out and next refers to the one being switched in to replace it. We can thus define a process switch as the activity consisting of saving the hardware context of prev and replacing it with the hardware context of next . Because process switches occur quite often, it is important to minimize the time spent in saving and loading hardware contexts. Old versions of Linux took advantage of the hardware support offered by the 80x86 architecture and performed a process switch through a far jmp instruction [ * ] to the selector of the Task State Segment Descriptor of the next process. While executing the instruction, the CPU performs a hardware context switch by automatically saving the old hardware context and loading a new one. But Linux 2.6 uses software to perform a process switch for the following reasons: [ * ] far jmp instructions modify both the cs and eip registers, while simple jmp instructions modify only eip Step-by-step switching performed through a sequence of mov instructions allows better control over the validity of the data being loaded. In particular, it is possible to check the values of the ds and es segmentation registers , which might have been forged by a malicious user. This type of checking is not possible when using a single far jmp instruction. The amount of time required by the old approach and the new approach is about the same. However, it is not possible to optimize a hardware context switch , while there might be room for improving the current switching code. Process switching occurs only in Kernel Mode . The contents of all registers used by a process in User Mode have already been saved on the Kernel Mode stack before performing process switching (see Chapter 4). This includes the contents of the ss and esp pair that specifies the User Mode stack pointer address.","title":"3.3-Process-Switch"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.3-Process-Switch/#33-process-switch","text":"To control the execution of processes, the kernel must be able to suspend the execution of the process running on the CPU and resume the execution of some other process previously suspended. This activity goes variously by the names process switch , task switch , or context switch . The next sections describe the elements of process switching in Linux.","title":"3.3. Process Switch"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.3-Process-Switch/#331-hardware-context","text":"While each process can have its own address space , all processes have to share the CPU registers . So before resuming the execution of a process, the kernel must ensure that each such register is loaded with the value it had when the process was suspended. The set of data that must be loaded into the registers before the process resumes its execution on the CPU is called the hardware context . The hardware context is a subset of the process execution context , which includes all information needed for the process execution. In Linux, a part of the hardware context of a process is stored in the process descriptor , while the remaining part is saved in the Kernel Mode stack . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981 In the description that follows, we will assume the prev local variable refers to the process descriptor of the process being switched out and next refers to the one being switched in to replace it. We can thus define a process switch as the activity consisting of saving the hardware context of prev and replacing it with the hardware context of next . Because process switches occur quite often, it is important to minimize the time spent in saving and loading hardware contexts. Old versions of Linux took advantage of the hardware support offered by the 80x86 architecture and performed a process switch through a far jmp instruction [ * ] to the selector of the Task State Segment Descriptor of the next process. While executing the instruction, the CPU performs a hardware context switch by automatically saving the old hardware context and loading a new one. But Linux 2.6 uses software to perform a process switch for the following reasons: [ * ] far jmp instructions modify both the cs and eip registers, while simple jmp instructions modify only eip Step-by-step switching performed through a sequence of mov instructions allows better control over the validity of the data being loaded. In particular, it is possible to check the values of the ds and es segmentation registers , which might have been forged by a malicious user. This type of checking is not possible when using a single far jmp instruction. The amount of time required by the old approach and the new approach is about the same. However, it is not possible to optimize a hardware context switch , while there might be room for improving the current switching code. Process switching occurs only in Kernel Mode . The contents of all registers used by a process in User Mode have already been saved on the Kernel Mode stack before performing process switching (see Chapter 4). This includes the contents of the ss and esp pair that specifies the User Mode stack pointer address.","title":"3.3.1. Hardware Context"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4-Creating-Processes/","text":"3.4. Creating Processes # Unix operating systems rely heavily on process creation to satisfy user requests. For example, the shell creates a new process that executes another copy of the shell whenever the user enters a command. Traditional Unix systems treat all processes in the same way: resources owned by the parent process are duplicated in the child process. This approach makes process creation very slow and inefficient, because it requires copying the entire address space of the parent process. The child process rarely needs to read or modify all the resources inherited from the parent; in many cases, it issues an immediate execve( ) and wipes out the address space that was so carefully copied. Modern Unix kernels solve this problem by introducing three different mechanisms: The Copy On Write technique allows both the parent and the child to read the same physical pages. Whenever either one tries to write on a physical page, the kernel copies its contents into a new physical page that is assigned to the writing process. The implementation of this technique in Linux is fully explained in Chapter 9. Lightweight processes allow both the parent and the child to share many per-process kernel data structures , such as the paging tables (and therefore the entire User Mode address space ), the open file tables, and the signal dispositions . The vfork( ) system call creates a process that shares the memory address space of its parent. To prevent the parent from overwriting data needed by the child, the parent's execution is blocked until the child exits or executes a new program. We'll learn more about the vfork( ) system call in the following section. 3.4.1. The clone( ) , fork( ) , and vfork( ) System Calls # Lightweight processes are created in Linux by using a function named clone( ) , which uses the following parameters: NOTE: \u53c2\u89c1\u8be5\u51fd\u6570\u7684man page\u83b7\u53d6\u5173\u4e8e\u8be5\u51fd\u6570\u7684\u5404\u79cd\u4fe1\u606f\u3002 fn Specifies a function to be executed by the new process; when the function returns, the child terminates. The function returns an integer, which represents the exit code for the child process. arg Points to data passed to the fn( ) function. flags Miscellaneous information. The low byte specifies the signal number to be sent to the parent process when the child terminates; the SIGCHLD signal is generally selected. The remaining three bytes encode a group of clone flags , which are shown in Table 3-8. child_stack Specifies the User Mode stack pointer to be assigned to the esp register of the child process. The invoking process (the parent) should always allocate a new stack for the child. tls Specifies the address of a data structure that defines a Thread Local Storage segment for the new lightweight process (see the section \"The Linux GDT\" in Chapter 2). Meaningful only if the CLONE_SETTLS flag is set. ptid Specifies the address of a User Mode variable of the parent process that will hold the PID of the new lightweight process. Meaningful only if the CLONE_PARENT_SETTID flag is set. ctid Specifies the address of a User Mode variable of the new lightweight process that will hold the PID of such process. Meaningful only if the CLONE_CHILD_SETTID flag is set. Table 3-8. Clone flags Flag name Description CLONE_VM Shares the memory descriptor and all Page Tables (see Chapter 9). CLONE_FS Shares the table that identifies the root directory and the current working directory, as well as the value of the bitmask used to mask the initial file permissions of a new file (the so-called file umask ). CLONE_FILES Shares the table that identifies the open files (see Chapter 12). CLONE_SIGHAND Shares the tables that identify the signal handlers and the blocked and pending signals (see Chapter 11). If this flag is true, the CLONE_VM flag must also be set. CLONE_PTRACE If traced, the parent wants the child to be traced too. Furthermore, the debugger may want to trace the child on its own; in this case, the kernel forces the flag to 1. CLONE_VFORK Set when the system call issued is a vfork( ) (see later in this section). CLONE_PARENT Sets the parent of the child ( parent and real_parent fields in the process descriptor) to the parent of the calling process. CLONE_THREAD Inserts the child into the same thread group of the parent, and forces the child to share the signal descriptor of the parent. The child's tgid and group_leader fields are set accordingly. If this flag is true, the CLONE_SIGHAND flag must also be set. CLONE_NEWNS Set if the clone needs its own namespace, that is, its own view of the mounted filesystems (see Chapter 12); it is not possible to specify both CLONE_NEWNS and CLONE_FS . CLONE_SYSVSEM Shares the System V IPC undoable semaphore operations (see the section \"IPC Semaphores\" in Chapter 19). CLONE_SETTLS Creates a new Thread Local Storage (TLS) segment for the lightweight process; the segment is described in the structure pointed to by the tls parameter. CLONE_PARENT_SETTID Writes the PID of the child into the User Mode variable of the parent pointed to by the ptid parameter. CLONE_CHILD_CLEARTID When set, the kernel sets up a mechanism to be triggered when the child process will exit or when it will start executing a new program. In these cases, the kernel will clear the User Mode variable pointed to by the ctid parameter and will awaken any process waiting for this event. CLONE_DETACHED A legacy flag ignored by the kernel. CLONE_UNTRACED Set by the kernel to override the value of the CLONE_PTRACE flag (used for disabling tracing of kernel threads ; see the section \"Kernel Threads\" later in this chapter). CLONE_CHILD_SETTID Writes the PID of the child into the User Mode variable of the child pointed to by the ctid parameter. CLONE_STOPPED Forces the child to start in the TASK_STOPPED state. clone( ) is actually a wrapper function defined in the C library (see the section \"POSIX APIs and System Calls\" in Chapter 10), which sets up the stack of the new lightweight process and invokes a clone( ) system call hidden to the programmer. The sys_clone( ) service routine that implements the clone( ) system call does not have the fn and arg parameters. In fact, the wrapper function saves the pointer fn into the child's stack position corresponding to the return address of the wrapper function itself; the pointer arg is saved on the child's stack right below fn . When the wrapper function terminates, the CPU fetches the return address from the stack and executes the fn(arg) function. The traditional fork( ) system call is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and all the clone flags cleared, and whose child_stack parameter is the current parent stack pointer . Therefore, the parent and child temporarily share the same User Mode stack . But thanks to the Copy On Write mechanism, they usually get separate copies of the User Mode stack as soon as one tries to change the stack. The vfork( ) system call, introduced in the previous section, is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and the flags CLONE_VM and CLONE_VFORK , and whose child_stack parameter is equal to the current parent stack pointer .","title":"3.4-Creating-Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4-Creating-Processes/#34-creating-processes","text":"Unix operating systems rely heavily on process creation to satisfy user requests. For example, the shell creates a new process that executes another copy of the shell whenever the user enters a command. Traditional Unix systems treat all processes in the same way: resources owned by the parent process are duplicated in the child process. This approach makes process creation very slow and inefficient, because it requires copying the entire address space of the parent process. The child process rarely needs to read or modify all the resources inherited from the parent; in many cases, it issues an immediate execve( ) and wipes out the address space that was so carefully copied. Modern Unix kernels solve this problem by introducing three different mechanisms: The Copy On Write technique allows both the parent and the child to read the same physical pages. Whenever either one tries to write on a physical page, the kernel copies its contents into a new physical page that is assigned to the writing process. The implementation of this technique in Linux is fully explained in Chapter 9. Lightweight processes allow both the parent and the child to share many per-process kernel data structures , such as the paging tables (and therefore the entire User Mode address space ), the open file tables, and the signal dispositions . The vfork( ) system call creates a process that shares the memory address space of its parent. To prevent the parent from overwriting data needed by the child, the parent's execution is blocked until the child exits or executes a new program. We'll learn more about the vfork( ) system call in the following section.","title":"3.4. Creating Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4-Creating-Processes/#341-the-clone-fork-and-vfork-system-calls","text":"Lightweight processes are created in Linux by using a function named clone( ) , which uses the following parameters: NOTE: \u53c2\u89c1\u8be5\u51fd\u6570\u7684man page\u83b7\u53d6\u5173\u4e8e\u8be5\u51fd\u6570\u7684\u5404\u79cd\u4fe1\u606f\u3002 fn Specifies a function to be executed by the new process; when the function returns, the child terminates. The function returns an integer, which represents the exit code for the child process. arg Points to data passed to the fn( ) function. flags Miscellaneous information. The low byte specifies the signal number to be sent to the parent process when the child terminates; the SIGCHLD signal is generally selected. The remaining three bytes encode a group of clone flags , which are shown in Table 3-8. child_stack Specifies the User Mode stack pointer to be assigned to the esp register of the child process. The invoking process (the parent) should always allocate a new stack for the child. tls Specifies the address of a data structure that defines a Thread Local Storage segment for the new lightweight process (see the section \"The Linux GDT\" in Chapter 2). Meaningful only if the CLONE_SETTLS flag is set. ptid Specifies the address of a User Mode variable of the parent process that will hold the PID of the new lightweight process. Meaningful only if the CLONE_PARENT_SETTID flag is set. ctid Specifies the address of a User Mode variable of the new lightweight process that will hold the PID of such process. Meaningful only if the CLONE_CHILD_SETTID flag is set. Table 3-8. Clone flags Flag name Description CLONE_VM Shares the memory descriptor and all Page Tables (see Chapter 9). CLONE_FS Shares the table that identifies the root directory and the current working directory, as well as the value of the bitmask used to mask the initial file permissions of a new file (the so-called file umask ). CLONE_FILES Shares the table that identifies the open files (see Chapter 12). CLONE_SIGHAND Shares the tables that identify the signal handlers and the blocked and pending signals (see Chapter 11). If this flag is true, the CLONE_VM flag must also be set. CLONE_PTRACE If traced, the parent wants the child to be traced too. Furthermore, the debugger may want to trace the child on its own; in this case, the kernel forces the flag to 1. CLONE_VFORK Set when the system call issued is a vfork( ) (see later in this section). CLONE_PARENT Sets the parent of the child ( parent and real_parent fields in the process descriptor) to the parent of the calling process. CLONE_THREAD Inserts the child into the same thread group of the parent, and forces the child to share the signal descriptor of the parent. The child's tgid and group_leader fields are set accordingly. If this flag is true, the CLONE_SIGHAND flag must also be set. CLONE_NEWNS Set if the clone needs its own namespace, that is, its own view of the mounted filesystems (see Chapter 12); it is not possible to specify both CLONE_NEWNS and CLONE_FS . CLONE_SYSVSEM Shares the System V IPC undoable semaphore operations (see the section \"IPC Semaphores\" in Chapter 19). CLONE_SETTLS Creates a new Thread Local Storage (TLS) segment for the lightweight process; the segment is described in the structure pointed to by the tls parameter. CLONE_PARENT_SETTID Writes the PID of the child into the User Mode variable of the parent pointed to by the ptid parameter. CLONE_CHILD_CLEARTID When set, the kernel sets up a mechanism to be triggered when the child process will exit or when it will start executing a new program. In these cases, the kernel will clear the User Mode variable pointed to by the ctid parameter and will awaken any process waiting for this event. CLONE_DETACHED A legacy flag ignored by the kernel. CLONE_UNTRACED Set by the kernel to override the value of the CLONE_PTRACE flag (used for disabling tracing of kernel threads ; see the section \"Kernel Threads\" later in this chapter). CLONE_CHILD_SETTID Writes the PID of the child into the User Mode variable of the child pointed to by the ctid parameter. CLONE_STOPPED Forces the child to start in the TASK_STOPPED state. clone( ) is actually a wrapper function defined in the C library (see the section \"POSIX APIs and System Calls\" in Chapter 10), which sets up the stack of the new lightweight process and invokes a clone( ) system call hidden to the programmer. The sys_clone( ) service routine that implements the clone( ) system call does not have the fn and arg parameters. In fact, the wrapper function saves the pointer fn into the child's stack position corresponding to the return address of the wrapper function itself; the pointer arg is saved on the child's stack right below fn . When the wrapper function terminates, the CPU fetches the return address from the stack and executes the fn(arg) function. The traditional fork( ) system call is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and all the clone flags cleared, and whose child_stack parameter is the current parent stack pointer . Therefore, the parent and child temporarily share the same User Mode stack . But thanks to the Copy On Write mechanism, they usually get separate copies of the User Mode stack as soon as one tries to change the stack. The vfork( ) system call, introduced in the previous section, is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and the flags CLONE_VM and CLONE_VFORK , and whose child_stack parameter is equal to the current parent stack pointer .","title":"3.4.1. The clone( ), fork( ), and vfork( ) System Calls"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4.2-Kernel-Threads/","text":"3.4.2. Kernel Threads 3.4.2.1. Creating a kernel thread 3.4.2.2. Process 0 3.4.2.3. Process 1 3.4.2.4. Other kernel threads 3.4.2. Kernel Threads # Traditional Unix systems delegate some critical tasks to intermittently(\u95f4\u6b47\u5730) running processes, including flushing disk caches, swapping out unused pages, servicing network connections, and so on. Indeed, it is not efficient to perform these tasks in strict linear fashion; both their functions and the end user processes get better response if they are scheduled in the background . Because some of the system processes run only in Kernel Mode , modern operating systems delegate their functions to kernel threads , which are not encumbered(\u963b\u788d) with the unnecessary User Mode context . In Linux, kernel threads differ from regular processes in the following ways: Kernel threads run only in Kernel Mode , while regular processes run alternatively in Kernel Mode and in User Mode . Because kernel threads run only in Kernel Mode, they use only linear addresses greater than PAGE_OFFSET . Regular processes, on the other hand, use all four gigabytes of linear addresses, in either User Mode or Kernel Mode. SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u662f\u6240\u6709\u7684system process\u90fd\u8fd0\u884c\u5728kernel mode\uff1b SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684user mode context\u6240\u6307\u7684\u662f\u4ec0\u4e48\uff1f 3.4.2.1. Creating a kernel thread # The kernel_thread( ) function creates a new kernel thread. It receives as parameters the address of the kernel function to be executed ( fn ), the argument to be passed to that function ( arg ), and a set of clone flags ( flags ). The function essentially invokes do_fork( ) as follows: do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, pregs, 0, NULL, NULL); The CLONE_VM flag avoids the duplication of the page tables of the calling process: this duplication would be a waste of time and memory, because the new kernel thread will not access the User Mode address space anyway. The CLONE_UNTRACED flag ensures that no process will be able to trace the new kernel thread, even if the calling process is being traced. SUMMARY \uff1a kernel_thread The pregs parameter passed to do_fork( ) corresponds to the address in the Kernel Mode stack where the copy_thread( ) function will find the initial values of the CPU registers for the new thread. The kernel_thread( ) function builds up this stack area so that: The ebx and edx registers will be set by copy_thread() to the values of the parameters fn and arg , respectively. The eip register will be set to the address of the following assembly language fragment: assembly movl %edx,%eax pushl %edx call *%ebx pushl %eax call do_exit Therefore, the new kernel thread starts by executing the fn(arg) function. If this function terminates, the kernel thread executes the _exit( ) system call passing to it the return value of fn() (see the section \"Destroying Processes\" later in this chapter). 3.4.2.2. Process 0 # The ancestor of all processes, called process 0 , the idle process , or, for historical reasons, the swapper process , is a kernel thread created from scratch during the initialization phase of Linux (see Appendix A). This ancestor process uses the following statically allocated data structures (data structures for all other processes are dynamically allocated): SUMMARY : statically allocated data structures (data structures for all other processes are dynamically allocated)\u7684\u5177\u4f53\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u5b83\u6709\u4ec0\u4e48\u7279\u522b\u4e4b\u5904\u5462\uff1f A process descriptor stored in the init_task variable, which is initialized by the INIT_TASK macro. A thread_info descriptor and a Kernel Mode stack stored in the init_thread_union variable and initialized by the INIT_THREAD_INFO macro. The following tables, which the process descriptor points to: init_mm init_fs init_files init_signals init_sighand The tables are initialized, respectively, by the following macros: INIT_MM INIT_FS INIT_FILES INIT_SIGNALS INIT_SIGHAND The master kernel Page Global Directory stored in swapper_pg_dir (see the section \"Kernel Page Tables\" in Chapter 2). The start_kernel( ) function initializes all the data structures needed by the kernel, enables interrupts, and creates another kernel thread , named process 1 (more commonly referred to as the init process ): kernel_thread(init, NULL, CLONE_FS|CLONE_SIGHAND); The newly created kernel thread has PID 1 and shares all per-process kernel data structures with process 0 . When selected by the scheduler, the init process starts executing the init( ) function. After having created the init process , process 0 executes the cpu_idle( ) function, which essentially consists of repeatedly executing the hlt assembly language instruction with the interrupts enabled (see Chapter 4). Process 0 is selected by the scheduler only when there are no other processes in the TASK_RUNNING state. In multiprocessor systems there is a process 0 for each CPU. Right after the power-on, the BIOS of the computer starts a single CPU while disabling the others. The swapper process running on CPU 0 initializes the kernel data structures, then enables the other CPUs and creates the additional swapper processes by means of the copy_process( ) function passing to it the value 0 as the new PID . Moreover, the kernel sets the cpu field of the thread_info descriptor of each forked process to the proper CPU index. 3.4.2.3. Process 1 # The kernel thread created by process 0 executes the init( ) function, which in turn completes the initialization of the kernel. Then init( ) invokes the execve( ) system call to load the executable program init . As a result, the init kernel thread becomes a regular process having its own per-process kernel data structure (see Chapter 20). The init process stays alive until the system is shut down, because it creates and monitors the activity of all processes that implement the outer layers of the operating system. 3.4.2.4. Other kernel threads # Linux uses many other kernel threads . Some of them are created in the initialization phase and run until shutdown; others are created \"on demand,\" when the kernel must execute a task that is better performed in its own execution context . A few examples of kernel threads (besides process 0 and process 1) are: keventd (also called events) Executes the functions in the keventd_wq workqueue (see Chapter 4). kapmd Handles the events related to the Advanced Power Management (APM). kswapd Reclaims memory, as described in the section \"Periodic Reclaiming\" in Chapter 17. pdflush Flushes \"dirty\" buffers to disk to reclaim\uff08\u56de\u6536\u518d\u5229\u7528\uff09 memory, as described in the section \"The pdflush Kernel Threads\" in Chapter 15. kblockd Executes the functions in the kblockd_workqueue workqueue. Essentially, it periodically activates the block device drivers, as described in the section \"Activating the Block Device Driver\" in Chapter 14. ksoftirqd Runs the tasklets (see section \"Softirqs and Tasklets\" in Chapter 4); there is one of these kernel threads for each CPU in the system.","title":"3.4.2-Kernel-Threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4.2-Kernel-Threads/#342-kernel-threads","text":"Traditional Unix systems delegate some critical tasks to intermittently(\u95f4\u6b47\u5730) running processes, including flushing disk caches, swapping out unused pages, servicing network connections, and so on. Indeed, it is not efficient to perform these tasks in strict linear fashion; both their functions and the end user processes get better response if they are scheduled in the background . Because some of the system processes run only in Kernel Mode , modern operating systems delegate their functions to kernel threads , which are not encumbered(\u963b\u788d) with the unnecessary User Mode context . In Linux, kernel threads differ from regular processes in the following ways: Kernel threads run only in Kernel Mode , while regular processes run alternatively in Kernel Mode and in User Mode . Because kernel threads run only in Kernel Mode, they use only linear addresses greater than PAGE_OFFSET . Regular processes, on the other hand, use all four gigabytes of linear addresses, in either User Mode or Kernel Mode. SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u662f\u6240\u6709\u7684system process\u90fd\u8fd0\u884c\u5728kernel mode\uff1b SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684user mode context\u6240\u6307\u7684\u662f\u4ec0\u4e48\uff1f","title":"3.4.2. Kernel Threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4.2-Kernel-Threads/#3421-creating-a-kernel-thread","text":"The kernel_thread( ) function creates a new kernel thread. It receives as parameters the address of the kernel function to be executed ( fn ), the argument to be passed to that function ( arg ), and a set of clone flags ( flags ). The function essentially invokes do_fork( ) as follows: do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, pregs, 0, NULL, NULL); The CLONE_VM flag avoids the duplication of the page tables of the calling process: this duplication would be a waste of time and memory, because the new kernel thread will not access the User Mode address space anyway. The CLONE_UNTRACED flag ensures that no process will be able to trace the new kernel thread, even if the calling process is being traced. SUMMARY \uff1a kernel_thread The pregs parameter passed to do_fork( ) corresponds to the address in the Kernel Mode stack where the copy_thread( ) function will find the initial values of the CPU registers for the new thread. The kernel_thread( ) function builds up this stack area so that: The ebx and edx registers will be set by copy_thread() to the values of the parameters fn and arg , respectively. The eip register will be set to the address of the following assembly language fragment: assembly movl %edx,%eax pushl %edx call *%ebx pushl %eax call do_exit Therefore, the new kernel thread starts by executing the fn(arg) function. If this function terminates, the kernel thread executes the _exit( ) system call passing to it the return value of fn() (see the section \"Destroying Processes\" later in this chapter).","title":"3.4.2.1. Creating a kernel thread"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4.2-Kernel-Threads/#3422-process-0","text":"The ancestor of all processes, called process 0 , the idle process , or, for historical reasons, the swapper process , is a kernel thread created from scratch during the initialization phase of Linux (see Appendix A). This ancestor process uses the following statically allocated data structures (data structures for all other processes are dynamically allocated): SUMMARY : statically allocated data structures (data structures for all other processes are dynamically allocated)\u7684\u5177\u4f53\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u5b83\u6709\u4ec0\u4e48\u7279\u522b\u4e4b\u5904\u5462\uff1f A process descriptor stored in the init_task variable, which is initialized by the INIT_TASK macro. A thread_info descriptor and a Kernel Mode stack stored in the init_thread_union variable and initialized by the INIT_THREAD_INFO macro. The following tables, which the process descriptor points to: init_mm init_fs init_files init_signals init_sighand The tables are initialized, respectively, by the following macros: INIT_MM INIT_FS INIT_FILES INIT_SIGNALS INIT_SIGHAND The master kernel Page Global Directory stored in swapper_pg_dir (see the section \"Kernel Page Tables\" in Chapter 2). The start_kernel( ) function initializes all the data structures needed by the kernel, enables interrupts, and creates another kernel thread , named process 1 (more commonly referred to as the init process ): kernel_thread(init, NULL, CLONE_FS|CLONE_SIGHAND); The newly created kernel thread has PID 1 and shares all per-process kernel data structures with process 0 . When selected by the scheduler, the init process starts executing the init( ) function. After having created the init process , process 0 executes the cpu_idle( ) function, which essentially consists of repeatedly executing the hlt assembly language instruction with the interrupts enabled (see Chapter 4). Process 0 is selected by the scheduler only when there are no other processes in the TASK_RUNNING state. In multiprocessor systems there is a process 0 for each CPU. Right after the power-on, the BIOS of the computer starts a single CPU while disabling the others. The swapper process running on CPU 0 initializes the kernel data structures, then enables the other CPUs and creates the additional swapper processes by means of the copy_process( ) function passing to it the value 0 as the new PID . Moreover, the kernel sets the cpu field of the thread_info descriptor of each forked process to the proper CPU index.","title":"3.4.2.2. Process 0"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4.2-Kernel-Threads/#3423-process-1","text":"The kernel thread created by process 0 executes the init( ) function, which in turn completes the initialization of the kernel. Then init( ) invokes the execve( ) system call to load the executable program init . As a result, the init kernel thread becomes a regular process having its own per-process kernel data structure (see Chapter 20). The init process stays alive until the system is shut down, because it creates and monitors the activity of all processes that implement the outer layers of the operating system.","title":"3.4.2.3. Process 1"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4.2-Kernel-Threads/#3424-other-kernel-threads","text":"Linux uses many other kernel threads . Some of them are created in the initialization phase and run until shutdown; others are created \"on demand,\" when the kernel must execute a task that is better performed in its own execution context . A few examples of kernel threads (besides process 0 and process 1) are: keventd (also called events) Executes the functions in the keventd_wq workqueue (see Chapter 4). kapmd Handles the events related to the Advanced Power Management (APM). kswapd Reclaims memory, as described in the section \"Periodic Reclaiming\" in Chapter 17. pdflush Flushes \"dirty\" buffers to disk to reclaim\uff08\u56de\u6536\u518d\u5229\u7528\uff09 memory, as described in the section \"The pdflush Kernel Threads\" in Chapter 15. kblockd Executes the functions in the kblockd_workqueue workqueue. Essentially, it periodically activates the block device drivers, as described in the section \"Activating the Block Device Driver\" in Chapter 14. ksoftirqd Runs the tasklets (see section \"Softirqs and Tasklets\" in Chapter 4); there is one of these kernel threads for each CPU in the system.","title":"3.4.2.4. Other kernel threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/Chapter-3-Processes/","text":"Chapter 3. Processes # The concept of a process is fundamental to any multiprogramming operating system. A process is usually defined as an instance of a program in execution; thus, if 16 users are running vi at once, there are 16 separate processes (although they can share the same executable code). Processes are often called tasks or threads in the Linux source code. \u6ce8\u610f\uff0c tasks \u548c threads \u90fd\u662f\u590d\u6570\u5f62\u5f0f\uff0c\u5b83\u8868\u793a\u4e00\u4e2aprocess\u7531\u591a\u4e2atask\u6216\u8005thread\u6765\u7ec4\u6210\uff1b\u57283.2. Process Descriptor\u4e2d\u4ecb\u7ecd\u4e86 task_struct \uff0c\u4f5c\u8005\u5c06\u5b83\u53eb\u505a process descriptor \uff0c\u5176\u5b9e\u8fd9\u662f\u6bd4\u8f83\u5bb9\u6613\u5bf9\u8bfb\u8005\u9020\u6210\u8bef\u89e3\u7684\uff0c\u6211\u89c9\u5f97\u53eb\u4ed6task\u6700\u6700\u51c6\u786e\uff0c\u5f53\u7136\u5b83\u66f4\u52a0\u63a5\u8fd1\u4e8ethread\uff1b\u4ee5\u4e0b\u4e24\u7bc7\u6587\u7ae0\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u5206\u6790\uff1a Process Control Block , Process Descriptor in Linux and task_struct? Neither of those terms (\"Process Control Block\" or \"Process Descriptor\") are considered \"terms of art\" in Linux kernel development. Of course, there is no official Linux kernel glossary so people are free to call things whatever makes sense to them. In contrast, however, task_struct is a specific C structure that is used by the linux kernel to maintain state about a task . A task in Linux corresponds roughly to a thread. Each user process has at least one thread so each process maps to one or more task_structs . More particularly, a process is one or more tasks that happen to share certain resources -- file descriptors, address space / memory map, signal handling, process and process group IDs, etc. Each thread in a process has its own individual version of certain other resources: registers/execution context, scheduling parameters, and so forth. It's quite common for a process to have only a single thread. In that case, you could consider a process to be represented by a single task_struct . How does Linux tell threads apart from child processes? From a task_struct perspective, a process\u2019s threads have the same thread group leader ( group_leader in task_struct ), whereas child processes have a different thread group leader (each individual child process). This information is exposed to user space via the /proc file system. You can trace parents and children by looking at the ppid field in /proc/${pid}/stat or .../status (this gives the parent pid); you can trace threads by looking at the tgid field in .../status (this gives the thread group id, which is also the group leader\u2019s pid). A process\u2019s threads are made visible in the /proc/${pid}/task directory: each thread gets its own subdirectory. (Every process has at least one thread.) In practice, programs wishing to keep track of their own threads would rely on APIs provided by the threading library they\u2019re using, instead of using OS-specific information. Typically on Unix-like systems that means using pthreads. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63d0\u53ca\u7684 thread group \u57283.1. Processes, Lightweight Processes, and Threads\u4e2d\u7ed9\u51fa\u5b9a\u4e49\uff0c thread group leader \u57283.2.2. Identifying a Process\u4e2d\u7ed9\u51fa\u5b9a\u4e49\u3002 In this chapter, we discuss static properties of processes and then describe how process switching is performed by the kernel. The last two sections describe how processes can be created and destroyed. We also describe how Linux supports multithreaded applications as mentioned in Chapter 1, it relies on so-called lightweight processes (LWP) .","title":"Chapter-3-Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/Chapter-3-Processes/#chapter-3-processes","text":"The concept of a process is fundamental to any multiprogramming operating system. A process is usually defined as an instance of a program in execution; thus, if 16 users are running vi at once, there are 16 separate processes (although they can share the same executable code). Processes are often called tasks or threads in the Linux source code. \u6ce8\u610f\uff0c tasks \u548c threads \u90fd\u662f\u590d\u6570\u5f62\u5f0f\uff0c\u5b83\u8868\u793a\u4e00\u4e2aprocess\u7531\u591a\u4e2atask\u6216\u8005thread\u6765\u7ec4\u6210\uff1b\u57283.2. Process Descriptor\u4e2d\u4ecb\u7ecd\u4e86 task_struct \uff0c\u4f5c\u8005\u5c06\u5b83\u53eb\u505a process descriptor \uff0c\u5176\u5b9e\u8fd9\u662f\u6bd4\u8f83\u5bb9\u6613\u5bf9\u8bfb\u8005\u9020\u6210\u8bef\u89e3\u7684\uff0c\u6211\u89c9\u5f97\u53eb\u4ed6task\u6700\u6700\u51c6\u786e\uff0c\u5f53\u7136\u5b83\u66f4\u52a0\u63a5\u8fd1\u4e8ethread\uff1b\u4ee5\u4e0b\u4e24\u7bc7\u6587\u7ae0\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u5206\u6790\uff1a Process Control Block , Process Descriptor in Linux and task_struct? Neither of those terms (\"Process Control Block\" or \"Process Descriptor\") are considered \"terms of art\" in Linux kernel development. Of course, there is no official Linux kernel glossary so people are free to call things whatever makes sense to them. In contrast, however, task_struct is a specific C structure that is used by the linux kernel to maintain state about a task . A task in Linux corresponds roughly to a thread. Each user process has at least one thread so each process maps to one or more task_structs . More particularly, a process is one or more tasks that happen to share certain resources -- file descriptors, address space / memory map, signal handling, process and process group IDs, etc. Each thread in a process has its own individual version of certain other resources: registers/execution context, scheduling parameters, and so forth. It's quite common for a process to have only a single thread. In that case, you could consider a process to be represented by a single task_struct . How does Linux tell threads apart from child processes? From a task_struct perspective, a process\u2019s threads have the same thread group leader ( group_leader in task_struct ), whereas child processes have a different thread group leader (each individual child process). This information is exposed to user space via the /proc file system. You can trace parents and children by looking at the ppid field in /proc/${pid}/stat or .../status (this gives the parent pid); you can trace threads by looking at the tgid field in .../status (this gives the thread group id, which is also the group leader\u2019s pid). A process\u2019s threads are made visible in the /proc/${pid}/task directory: each thread gets its own subdirectory. (Every process has at least one thread.) In practice, programs wishing to keep track of their own threads would rely on APIs provided by the threading library they\u2019re using, instead of using OS-specific information. Typically on Unix-like systems that means using pthreads. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63d0\u53ca\u7684 thread group \u57283.1. Processes, Lightweight Processes, and Threads\u4e2d\u7ed9\u51fa\u5b9a\u4e49\uff0c thread group leader \u57283.2.2. Identifying a Process\u4e2d\u7ed9\u51fa\u5b9a\u4e49\u3002 In this chapter, we discuss static properties of processes and then describe how process switching is performed by the kernel. The last two sections describe how processes can be created and destroyed. We also describe how Linux supports multithreaded applications as mentioned in Chapter 1, it relies on so-called lightweight processes (LWP) .","title":"Chapter 3. Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/wikipedia-Copy-on-write/","text":"Copy-on-write In virtual memory management Copy-on-write # Copy-on-write ( CoW or COW ), sometimes referred to as implicit sharing or shadowing , is a resource-management technique used in computer programming to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write \uff08\u8fd9\u53e5\u8bdd\u662f\u5bf9copy on write\u7684\u6700\u597d\u7684\u89e3\u91ca\uff09. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations. NOTE : \u4e0a\u9762\u6240\u63d0\u53ca\u7684resource management\u8ba9\u6211\u60f3\u5230\u4e86 C++ \u4e2d\u7684RAII\uff0c\u548cpython\u4e2d\u7684with\uff0c\u663e\u7136\u8fd9\u4e24\u8005\u90fd\u4fa7\u91cd\u4e8e\u907f\u514dresource leak\u3002 In virtual memory management # Copy-on-write finds its main use in sharing the virtual memory of operating system processes , in the implementation of the fork system call . Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used. Copy-on-write can be implemented efficiently using the page table by marking\uff08\u6807\u5fd7\uff09 certain pages of memory as read-only and keeping a count of the number of references\uff08\u5f15\u7528\u8ba1\u6570\uff09 to the page. When data is written to these pages, the kernel intercepts\uff08\u62e6\u622a\uff09 the write attempt and allocates a new physical page, initialized with the copy-on-write data, although the allocation can be skipped if there is only one reference. The kernel then updates the page table with the new (writable) page, decrements the number of references, and performs the write. The new allocation ensures that a change in the memory of one process is not visible in another's. The copy-on-write technique can be extended to support efficient memory allocation by having a page of physical memory filled with zeros. When the memory is allocated, all the pages returned refer to the page of zeros and are all marked copy-on-write. This way, physical memory is not allocated for the process until data is written, allowing processes to reserve more virtual memory than physical memory and use memory sparsely, at the risk of running out of virtual address space. The combined algorithm is similar to demand paging . Copy-on-write pages are also used in the Linux kernel 's kernel same-page merging feature. Loading the libraries for an application is also a use of copy-on-write technique. The dynamic linker maps libraries as private like follows. Any writing action on the libraries will trigger a COW in virtual memory management. openat(AT_FDCWD, \"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 mmap(NULL, 3906144, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) mmap(0x7f8a3ced4000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1b0000)","title":"Copy-on-write"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/wikipedia-Copy-on-write/#copy-on-write","text":"Copy-on-write ( CoW or COW ), sometimes referred to as implicit sharing or shadowing , is a resource-management technique used in computer programming to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write \uff08\u8fd9\u53e5\u8bdd\u662f\u5bf9copy on write\u7684\u6700\u597d\u7684\u89e3\u91ca\uff09. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations. NOTE : \u4e0a\u9762\u6240\u63d0\u53ca\u7684resource management\u8ba9\u6211\u60f3\u5230\u4e86 C++ \u4e2d\u7684RAII\uff0c\u548cpython\u4e2d\u7684with\uff0c\u663e\u7136\u8fd9\u4e24\u8005\u90fd\u4fa7\u91cd\u4e8e\u907f\u514dresource leak\u3002","title":"Copy-on-write"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/wikipedia-Copy-on-write/#in-virtual-memory-management","text":"Copy-on-write finds its main use in sharing the virtual memory of operating system processes , in the implementation of the fork system call . Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used. Copy-on-write can be implemented efficiently using the page table by marking\uff08\u6807\u5fd7\uff09 certain pages of memory as read-only and keeping a count of the number of references\uff08\u5f15\u7528\u8ba1\u6570\uff09 to the page. When data is written to these pages, the kernel intercepts\uff08\u62e6\u622a\uff09 the write attempt and allocates a new physical page, initialized with the copy-on-write data, although the allocation can be skipped if there is only one reference. The kernel then updates the page table with the new (writable) page, decrements the number of references, and performs the write. The new allocation ensures that a change in the memory of one process is not visible in another's. The copy-on-write technique can be extended to support efficient memory allocation by having a page of physical memory filled with zeros. When the memory is allocated, all the pages returned refer to the page of zeros and are all marked copy-on-write. This way, physical memory is not allocated for the process until data is written, allowing processes to reserve more virtual memory than physical memory and use memory sparsely, at the risk of running out of virtual address space. The combined algorithm is similar to demand paging . Copy-on-write pages are also used in the Linux kernel 's kernel same-page merging feature. Loading the libraries for an application is also a use of copy-on-write technique. The dynamic linker maps libraries as private like follows. Any writing action on the libraries will trigger a COW in virtual memory management. openat(AT_FDCWD, \"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 mmap(NULL, 3906144, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) mmap(0x7f8a3ced4000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1b0000)","title":"In virtual memory management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.1-The-Role-of-Interrupt-Signals/","text":"4.1. The Role of Interrupt Signals 4.1. The Role of Interrupt Signals # As the name suggests, interrupt signals provide a way to divert\uff08\u8f6c\u6362\uff09 the processor to code outside the normal flow of control. When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter . NOTE: context switch There are some things in this chapter that will remind you of the context switch described in the previous chapter, carried out when a kernel substitutes one process for another. But there is a key difference between interrupt handling and process switching : the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a kernel control path , the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). SUMMARY : \u6267\u884csystem call\u4e5f\u662fkernel control path\uff0c\u90a3\u4e48\u662f\u5426system call\u7684\u6267\u884c\u6b65\u9aa4\u548c\u4e0a\u9762\u63cf\u8ff0\u7684\u7c7b\u4f3c\uff1f\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6709\u5982\u4e0b\u7684\u63d0\u95ee\uff1a Is the Unix process scheduler itself a process? Is the Unix process scheduler itself a process, or does it piggyback on other processes in the same way a system call does (running kernel code in the user process with the kernel bit set)? \u6309\u7167\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u63cf\u8ff0\u6765\u770b\uff0cinterrupt\u7684\u6267\u884c\u662fpiggyback on processes \uff1b\u6309\u71671.6.3. Reentrant Kernels\u4e2d\u6240\u5b9a\u4e49\u7684kernel control path\uff0c\u5b83\u652f\u6301A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call, an exception, or an interrupt.\u663e\u7136\uff0csystem call\u548cexception handler\u90fd\u662fkernel control path\uff1b\u4e0a\u9762\u6240\u63cf\u8ff0\u7684exception handler\u7684\u6267\u884c\u65b9\u5f0f\u662f\u5426\u4e5f\u9002\u7528\u4e8esystem call\uff1b Unix\u8fdb\u7a0b\u8c03\u5ea6\u7a0b\u5e8f\u672c\u8eab\u662f\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8fd8\u662f\u4ee5\u4e0e\u7cfb\u7edf\u8c03\u7528\u76f8\u540c\u7684\u65b9\u5f0f\u642d\u8f7d\u5728\u5176\u4ed6\u8fdb\u7a0b\u4e0a\uff08\u5728\u5185\u6838\u4f4d\u8bbe\u7f6e\u7684\u7528\u6237\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u5185\u6838\u4ee3\u7801\uff09\uff1f Interrupt handling is one of the most sensitive tasks performed by the kernel, because it must satisfy the following constraints: Interrupts can come anytime, when the kernel may want to finish something else it was trying to do. The kernel's goal is therefore to get the interrupt out of the way as soon as possible and defer as much processing as it can. For instance, suppose a block of data has arrived on a network line . When the hardware interrupts the kernel , it could simply mark the presence of data, give the processor back to whatever was running before, and do the rest of the processing later (such as moving the data into a buffer where its recipient process can find it, and then restarting the process). The activities that the kernel needs to perform in response to an interrupt are thus divided into a critical urgent part that the kernel executes right away and a deferrable part that is left for later. Because interrupts can come anytime, the kernel might be handling one of them while another one (of a different type) occurs. This should be allowed as much as possible, because it keeps the I/O devices busy (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a result, the interrupt handlers must be coded so that the corresponding kernel control paths can be executed in a nested manner. When the last kernel control path terminates, the kernel must be able to resume execution of the interrupted process or switch to another process if the interrupt signal has caused a rescheduling activity. Although the kernel may accept a new interrupt signal while handling a previous one, some critical regions exist inside the kernel code where interrupts must be disabled. Such critical regions must be limited as much as possible because, according to the previous requirement, the kernel, and particularly the interrupt handlers, should run most of the time with the interrupts enabled.","title":"4.1-The-Role-of-Interrupt-Signals"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.1-The-Role-of-Interrupt-Signals/#41-the-role-of-interrupt-signals","text":"As the name suggests, interrupt signals provide a way to divert\uff08\u8f6c\u6362\uff09 the processor to code outside the normal flow of control. When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter . NOTE: context switch There are some things in this chapter that will remind you of the context switch described in the previous chapter, carried out when a kernel substitutes one process for another. But there is a key difference between interrupt handling and process switching : the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a kernel control path , the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). SUMMARY : \u6267\u884csystem call\u4e5f\u662fkernel control path\uff0c\u90a3\u4e48\u662f\u5426system call\u7684\u6267\u884c\u6b65\u9aa4\u548c\u4e0a\u9762\u63cf\u8ff0\u7684\u7c7b\u4f3c\uff1f\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6709\u5982\u4e0b\u7684\u63d0\u95ee\uff1a Is the Unix process scheduler itself a process? Is the Unix process scheduler itself a process, or does it piggyback on other processes in the same way a system call does (running kernel code in the user process with the kernel bit set)? \u6309\u7167\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u63cf\u8ff0\u6765\u770b\uff0cinterrupt\u7684\u6267\u884c\u662fpiggyback on processes \uff1b\u6309\u71671.6.3. Reentrant Kernels\u4e2d\u6240\u5b9a\u4e49\u7684kernel control path\uff0c\u5b83\u652f\u6301A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call, an exception, or an interrupt.\u663e\u7136\uff0csystem call\u548cexception handler\u90fd\u662fkernel control path\uff1b\u4e0a\u9762\u6240\u63cf\u8ff0\u7684exception handler\u7684\u6267\u884c\u65b9\u5f0f\u662f\u5426\u4e5f\u9002\u7528\u4e8esystem call\uff1b Unix\u8fdb\u7a0b\u8c03\u5ea6\u7a0b\u5e8f\u672c\u8eab\u662f\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8fd8\u662f\u4ee5\u4e0e\u7cfb\u7edf\u8c03\u7528\u76f8\u540c\u7684\u65b9\u5f0f\u642d\u8f7d\u5728\u5176\u4ed6\u8fdb\u7a0b\u4e0a\uff08\u5728\u5185\u6838\u4f4d\u8bbe\u7f6e\u7684\u7528\u6237\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u5185\u6838\u4ee3\u7801\uff09\uff1f Interrupt handling is one of the most sensitive tasks performed by the kernel, because it must satisfy the following constraints: Interrupts can come anytime, when the kernel may want to finish something else it was trying to do. The kernel's goal is therefore to get the interrupt out of the way as soon as possible and defer as much processing as it can. For instance, suppose a block of data has arrived on a network line . When the hardware interrupts the kernel , it could simply mark the presence of data, give the processor back to whatever was running before, and do the rest of the processing later (such as moving the data into a buffer where its recipient process can find it, and then restarting the process). The activities that the kernel needs to perform in response to an interrupt are thus divided into a critical urgent part that the kernel executes right away and a deferrable part that is left for later. Because interrupts can come anytime, the kernel might be handling one of them while another one (of a different type) occurs. This should be allowed as much as possible, because it keeps the I/O devices busy (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a result, the interrupt handlers must be coded so that the corresponding kernel control paths can be executed in a nested manner. When the last kernel control path terminates, the kernel must be able to resume execution of the interrupted process or switch to another process if the interrupt signal has caused a rescheduling activity. Although the kernel may accept a new interrupt signal while handling a previous one, some critical regions exist inside the kernel code where interrupts must be disabled. Such critical regions must be limited as much as possible because, according to the previous requirement, the kernel, and particularly the interrupt handlers, should run most of the time with the interrupts enabled.","title":"4.1. The Role of Interrupt Signals"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/","text":"4.2. Interrupts and Exceptions # The Intel documentation classifies interrupts and exceptions as follows: Interrupts: # Maskable interrupts # All Interrupt Requests (IRQs) issued by I/O devices give rise to maskable interrupts . A maskable interrupt can be in two states: masked or unmasked; a masked interrupt is ignored by the control unit as long as it remains masked. Nonmaskable interrupts # Only a few critical events (such as hardware failures) give rise to nonmaskable interrupts . Nonmaskable interrupts are always recognized by the CPU. Exceptions: # NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cIntel\u5c06exception\u5206\u4e3a\u4e24\u7c7b\uff1a Processor-detected exceptions Programmed exceptions Processor-detected exceptions # Generated when the CPU detects an anomalous\uff08\u5f02\u5e38\u7684\uff09 condition while executing an instruction. These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception. Faults # Can generally be corrected; once corrected, the program is allowed to restart with no loss of continuity. The saved value of eip is the address of the instruction that caused the fault , and hence that instruction can be resumed when the exception handler terminates. As we'll see in the section \"Page Fault Exception Handler\" in Chapter 9, resuming the same instruction is necessary whenever the handler is able to correct the anomalous condition that caused the exception. NOTE: \u7ed3\u5408\u7f3a\u9875\u4e2d\u65ad\uff0c\u8fd9\u4e2a\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 Traps # Reported immediately following the execution of the trapping instruction ; after the kernel returns control to the program, it is allowed to continue its execution with no loss of continuity. The saved value of eip is the address of the instruction that should be executed after the one that caused the trap. A trap is triggered only when there is no need to reexecute the instruction that terminated. The main use of traps is for debugging purposes. The role of the interrupt signal in this case is to notify the debugger that a specific instruction has been executed (for instance, a breakpoint has been reached within a program). Once the user has examined the data provided by the debugger, she may ask that execution of the debugged program resume, starting from the next instruction. NOTE: see also\uff1a Traps Aborts # A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors , such as hardware failures and invalid or inconsistent values in system tables. The interrupt signal sent by the control unit is an emergency signal used to switch control to the corresponding abort exception handler . This handler has no choice but to force the affected process to terminate. Programmed exceptions # Occur at the request of the programmer. They are triggered by int or int3 instructions; the into (check for overflow) and bound (check on address bound) instructions also give rise to a programmed exception when the condition they are checking is not true. Programmed exceptions are handled by the control unit as traps ; they are often called software interrupts . Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event (see Chapter 10). Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a vector. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller (see the next section). 4.2.1. IRQs and Interrupts # NOTE: Interrupt request (PC architecture) Programmable interrupt controller Interrupt vector table 4.2.2. Exceptions # The 80x86 microprocessors issue roughly 20 different exceptions . [*] The kernel must provide a dedicated exception handler for each exception type. For some exceptions, the CPU control unit also generates a hardware error code and pushes it on the Kernel Mode stack before starting the exception handler . [*] The exact number depends on the processor model. The following list gives the vector, the name, the type, and a brief description of the exceptions found in 80x86 processors. Additional information may be found in the Intel technical documentation. 0 - \"Divide error\" (fault) # Raised when a program issues an integer division by 0. 1- \"Debug\" (trap or fault) # Raised when the TF flag of eflags is set (quite useful to implement single-step execution of a debugged program) or when the address of an instruction or operand falls within the range of an active debug register (see the section \"Hardware Context\" in Chapter 3). 3 - \"Breakpoint\" (trap) # Caused by an int3 (breakpoint) instruction (usually inserted by a debugger). 4 - \"Overflow\" (trap) # An into (check for overflow) instruction has been executed while the OF (overflow) flag of eflags is set. 4.2.3. Interrupt Descriptor Table # NOTE: Interrupt descriptor table 4.2.4. Hardware Handling of Interrupts and Exceptions #","title":"4.2-Interrupts-and-Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#42-interrupts-and-exceptions","text":"The Intel documentation classifies interrupts and exceptions as follows:","title":"4.2. Interrupts and Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#interrupts","text":"","title":"Interrupts:"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#maskable-interrupts","text":"All Interrupt Requests (IRQs) issued by I/O devices give rise to maskable interrupts . A maskable interrupt can be in two states: masked or unmasked; a masked interrupt is ignored by the control unit as long as it remains masked.","title":"Maskable interrupts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#nonmaskable-interrupts","text":"Only a few critical events (such as hardware failures) give rise to nonmaskable interrupts . Nonmaskable interrupts are always recognized by the CPU.","title":"Nonmaskable interrupts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#exceptions","text":"NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cIntel\u5c06exception\u5206\u4e3a\u4e24\u7c7b\uff1a Processor-detected exceptions Programmed exceptions","title":"Exceptions:"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#processor-detected-exceptions","text":"Generated when the CPU detects an anomalous\uff08\u5f02\u5e38\u7684\uff09 condition while executing an instruction. These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception.","title":"Processor-detected exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#faults","text":"Can generally be corrected; once corrected, the program is allowed to restart with no loss of continuity. The saved value of eip is the address of the instruction that caused the fault , and hence that instruction can be resumed when the exception handler terminates. As we'll see in the section \"Page Fault Exception Handler\" in Chapter 9, resuming the same instruction is necessary whenever the handler is able to correct the anomalous condition that caused the exception. NOTE: \u7ed3\u5408\u7f3a\u9875\u4e2d\u65ad\uff0c\u8fd9\u4e2a\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002","title":"Faults"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#traps","text":"Reported immediately following the execution of the trapping instruction ; after the kernel returns control to the program, it is allowed to continue its execution with no loss of continuity. The saved value of eip is the address of the instruction that should be executed after the one that caused the trap. A trap is triggered only when there is no need to reexecute the instruction that terminated. The main use of traps is for debugging purposes. The role of the interrupt signal in this case is to notify the debugger that a specific instruction has been executed (for instance, a breakpoint has been reached within a program). Once the user has examined the data provided by the debugger, she may ask that execution of the debugged program resume, starting from the next instruction. NOTE: see also\uff1a Traps","title":"Traps"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#aborts","text":"A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors , such as hardware failures and invalid or inconsistent values in system tables. The interrupt signal sent by the control unit is an emergency signal used to switch control to the corresponding abort exception handler . This handler has no choice but to force the affected process to terminate.","title":"Aborts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#programmed-exceptions","text":"Occur at the request of the programmer. They are triggered by int or int3 instructions; the into (check for overflow) and bound (check on address bound) instructions also give rise to a programmed exception when the condition they are checking is not true. Programmed exceptions are handled by the control unit as traps ; they are often called software interrupts . Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event (see Chapter 10). Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a vector. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller (see the next section).","title":"Programmed exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#421-irqs-and-interrupts","text":"NOTE: Interrupt request (PC architecture) Programmable interrupt controller Interrupt vector table","title":"4.2.1. IRQs and Interrupts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#422-exceptions","text":"The 80x86 microprocessors issue roughly 20 different exceptions . [*] The kernel must provide a dedicated exception handler for each exception type. For some exceptions, the CPU control unit also generates a hardware error code and pushes it on the Kernel Mode stack before starting the exception handler . [*] The exact number depends on the processor model. The following list gives the vector, the name, the type, and a brief description of the exceptions found in 80x86 processors. Additional information may be found in the Intel technical documentation.","title":"4.2.2. Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#0-divide-error-fault","text":"Raised when a program issues an integer division by 0.","title":"0 - \"Divide error\" (fault)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#1-debug-trap-or-fault","text":"Raised when the TF flag of eflags is set (quite useful to implement single-step execution of a debugged program) or when the address of an instruction or operand falls within the range of an active debug register (see the section \"Hardware Context\" in Chapter 3).","title":"1- \"Debug\" (trap or fault)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#3-breakpoint-trap","text":"Caused by an int3 (breakpoint) instruction (usually inserted by a debugger).","title":"3 - \"Breakpoint\" (trap)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#4-overflow-trap","text":"An into (check for overflow) instruction has been executed while the OF (overflow) flag of eflags is set.","title":"4 - \"Overflow\" (trap)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#423-interrupt-descriptor-table","text":"NOTE: Interrupt descriptor table","title":"4.2.3. Interrupt Descriptor Table"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#424-hardware-handling-of-interrupts-and-exceptions","text":"","title":"4.2.4. Hardware Handling of Interrupts and Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.3-Nested-Execution-of-Exception-and-Interrupt-Handlers/","text":"4.3. Nested Execution of Exception and Interrupt Handlers # Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process . For instance, when an I/O device raises an interrupt, the first instructions of the corresponding kernel control path are those that save the contents of the CPU registers in the Kernel Mode stack , while the last are those that restore the contents of the registers. Kernel control paths may be arbitrarily nested; an interrupt handler may be interrupted by another interrupt handler , thus giving rise to a nested execution of kernel control paths , as shown in Figure 4-3. As a result, the last instructions of a kernel control path that is taking care of an interrupt do not always put the current process back into User Mode: if the level of nesting is greater than 1, these instructions will put into execution the kernel control path that was interrupted last, and the CPU will continue to run in Kernel Mode. The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, no process switch can take place until an interrupt handler is running. In fact, all the data needed to resume a nested kernel control path is stored in the Kernel Mode stack, which is tightly bound to the current process. Assuming that the kernel is bug free, most exceptions can occur only while the CPU is in User Mode . Indeed, they are either caused by programming errors or triggered by debuggers. However, the \"Page Fault \" exception may occur in Kernel Mode. This happens when the process attempts to address a page that belongs to its address space but is not currently in RAM. While handling such an exception, the kernel may suspend the current process and replace it with another one until the requested page is available. The kernel control path that handles the \"Page Fault\" exception resumes execution as soon as the process gets the processor again. Because the \"Page Fault\" exception handler never gives rise to further exceptions, at most two kernel control paths associated with exceptions (the first one caused by a system call invocation, the second one caused by a Page Fault) may be stacked, one on top of the other. In contrast to exceptions, interrupts issued by I/O devices do not refer to data structures specific to the current process , although the kernel control paths that handle them run on behalf of that process. As a matter of fact, it is impossible to predict which process will be running when a given interrupt occurs. An interrupt handler may preempt both other interrupt handlers and exception handlers . Conversely, an exception handler never preempts an interrupt handler . The only exception that can be triggered in Kernel Mode is \"Page Fault,\" which we just described. But interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch. Linux interleaves kernel control paths for two major reasons: To improve the throughput of programmable interrupt controllers and device controllers . Assume that a device controller issues a signal on an IRQ line: the PIC transforms it into an external interrupt, and then both the PIC and the device controller remain blocked until the PIC receives an acknowledgment from the CPU. Thanks to kernel control path interleaving, the kernel is able to send the acknowledgment even when it is handling a previous interrupt. To implement an interrupt model without priority levels. Because each interrupt handler may be deferred by another one, there is no need to establish predefined priorities among hardware devices. This simplifies the kernel code and improves its portability. On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.","title":"4.3-Nested-Execution-of-Exception-and-Interrupt-Handlers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.3-Nested-Execution-of-Exception-and-Interrupt-Handlers/#43-nested-execution-of-exception-and-interrupt-handlers","text":"Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process . For instance, when an I/O device raises an interrupt, the first instructions of the corresponding kernel control path are those that save the contents of the CPU registers in the Kernel Mode stack , while the last are those that restore the contents of the registers. Kernel control paths may be arbitrarily nested; an interrupt handler may be interrupted by another interrupt handler , thus giving rise to a nested execution of kernel control paths , as shown in Figure 4-3. As a result, the last instructions of a kernel control path that is taking care of an interrupt do not always put the current process back into User Mode: if the level of nesting is greater than 1, these instructions will put into execution the kernel control path that was interrupted last, and the CPU will continue to run in Kernel Mode. The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, no process switch can take place until an interrupt handler is running. In fact, all the data needed to resume a nested kernel control path is stored in the Kernel Mode stack, which is tightly bound to the current process. Assuming that the kernel is bug free, most exceptions can occur only while the CPU is in User Mode . Indeed, they are either caused by programming errors or triggered by debuggers. However, the \"Page Fault \" exception may occur in Kernel Mode. This happens when the process attempts to address a page that belongs to its address space but is not currently in RAM. While handling such an exception, the kernel may suspend the current process and replace it with another one until the requested page is available. The kernel control path that handles the \"Page Fault\" exception resumes execution as soon as the process gets the processor again. Because the \"Page Fault\" exception handler never gives rise to further exceptions, at most two kernel control paths associated with exceptions (the first one caused by a system call invocation, the second one caused by a Page Fault) may be stacked, one on top of the other. In contrast to exceptions, interrupts issued by I/O devices do not refer to data structures specific to the current process , although the kernel control paths that handle them run on behalf of that process. As a matter of fact, it is impossible to predict which process will be running when a given interrupt occurs. An interrupt handler may preempt both other interrupt handlers and exception handlers . Conversely, an exception handler never preempts an interrupt handler . The only exception that can be triggered in Kernel Mode is \"Page Fault,\" which we just described. But interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch. Linux interleaves kernel control paths for two major reasons: To improve the throughput of programmable interrupt controllers and device controllers . Assume that a device controller issues a signal on an IRQ line: the PIC transforms it into an external interrupt, and then both the PIC and the device controller remain blocked until the PIC receives an acknowledgment from the CPU. Thanks to kernel control path interleaving, the kernel is able to send the acknowledgment even when it is handling a previous interrupt. To implement an interrupt model without priority levels. Because each interrupt handler may be deferred by another one, there is no need to establish predefined priorities among hardware devices. This simplifies the kernel code and improves its portability. On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.","title":"4.3. Nested Execution of Exception and Interrupt Handlers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.4-Initializing-the-Interrupt-Descriptor-Table/","text":"4.4. Initializing the Interrupt Descriptor Table # \u672c\u8282\u7684\u5185\u5bb9\u504f\u786c\u4ef6\uff0cpass\u3002","title":"4.4-Initializing-the-Interrupt-Descriptor-Table"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.4-Initializing-the-Interrupt-Descriptor-Table/#44-initializing-the-interrupt-descriptor-table","text":"\u672c\u8282\u7684\u5185\u5bb9\u504f\u786c\u4ef6\uff0cpass\u3002","title":"4.4. Initializing the Interrupt Descriptor Table"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.5-Exception-Handling/","text":"4.5. Exception Handling #","title":"4.5-Exception-Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.5-Exception-Handling/#45-exception-handling","text":"","title":"4.5. Exception Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.6-Interrupt-Handling/","text":"4.6. Interrupt Handling 4.6.1. I/O Interrupt Handling 4.6. Interrupt Handling # As we explained earlier, most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly. This approach does not hold for interrupts , because they frequently arrive long after the process to which they are related (for instance, a process that requested a data transfer) has been suspended and a completely unrelated process is running. So it would make no sense to send a Unix signal to the current process . Interrupt handling depends on the type of interrupt. For our purposes, we'll distinguish three main classes of interrupts: I/O interrupts An I/O device requires attention; the corresponding interrupt handler must query the device to determine the proper course of action. We cover this type of interrupt in the later section \"I/O Interrupt Handling.\" Timer interrupts Some timer, either a local APIC timer or an external timer , has issued an interrupt; this kind of interrupt tells the kernel that a fixed-time interval has elapsed. These interrupts are handled mostly as I/O interrupts; we discuss the peculiar characteristics of timer interrupts in Chapter 6. Interprocessor interrupts A CPU issued an interrupt to another CPU of a multiprocessor system. We cover such interrupts in the later section \"Interprocessor Interrupt Handling.\" 4.6.1. I/O Interrupt Handling # In general, an I/O interrupt handler must be flexible enough to service several devices at the same time. In the PCI bus architecture, for instance, several devices may share the same IRQ line . This means that the interrupt vector alone does not tell the whole story. In the example shown in Table 4-3, the same vector 43 is assigned to the USB port and to the sound card. However, some hardware devices found in older PC architectures (such as ISA) do not reliably operate if their IRQ line is shared with other devices. Interrupt handler flexibility is achieved in two distinct ways, as discussed in the following list.","title":"4.6-Interrupt-Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.6-Interrupt-Handling/#46-interrupt-handling","text":"As we explained earlier, most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly. This approach does not hold for interrupts , because they frequently arrive long after the process to which they are related (for instance, a process that requested a data transfer) has been suspended and a completely unrelated process is running. So it would make no sense to send a Unix signal to the current process . Interrupt handling depends on the type of interrupt. For our purposes, we'll distinguish three main classes of interrupts: I/O interrupts An I/O device requires attention; the corresponding interrupt handler must query the device to determine the proper course of action. We cover this type of interrupt in the later section \"I/O Interrupt Handling.\" Timer interrupts Some timer, either a local APIC timer or an external timer , has issued an interrupt; this kind of interrupt tells the kernel that a fixed-time interval has elapsed. These interrupts are handled mostly as I/O interrupts; we discuss the peculiar characteristics of timer interrupts in Chapter 6. Interprocessor interrupts A CPU issued an interrupt to another CPU of a multiprocessor system. We cover such interrupts in the later section \"Interprocessor Interrupt Handling.\"","title":"4.6. Interrupt Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.6-Interrupt-Handling/#461-io-interrupt-handling","text":"In general, an I/O interrupt handler must be flexible enough to service several devices at the same time. In the PCI bus architecture, for instance, several devices may share the same IRQ line . This means that the interrupt vector alone does not tell the whole story. In the example shown in Table 4-3, the same vector 43 is assigned to the USB port and to the sound card. However, some hardware devices found in older PC architectures (such as ISA) do not reliably operate if their IRQ line is shared with other devices. Interrupt handler flexibility is achieved in two distinct ways, as discussed in the following list.","title":"4.6.1. I/O Interrupt Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/Chapter-4-Interrupts-and-Exceptions/","text":"Chapter 4. Interrupts and Exceptions # An interrupt is usually defined as an event that alters the sequence of instructions executed by a processor. Such events correspond to electrical signals generated by hardware circuits both inside and outside the CPU chip. Interrupts are often divided into synchronous and asynchronous interrupts : Synchronous interrupts are produced by the CPU control unit while executing instructions and are called synchronous because the control unit issues them only after terminating the execution of an instruction. Asynchronous interrupts are generated by other hardware devices at arbitrary times with respect to the CPU clock signals. NOTE : \u4e24\u8005\u7684\u6765\u6e90\u4e0d\u540c\uff0c\u4e00\u4e2a\u662f\u6e90\u81eaCPU\uff0c\u4e00\u4e2a\u662f\u6e90\u81ea\u5176\u4ed6\u7684 hardware devices Intel microprocessor manuals designate synchronous and asynchronous interrupts as exceptions and interrupts , respectively. We'll adopt this classification, although we'll occasionally use the term \" interrupt signal \" to designate both types together (synchronous as well as asynchronous). \u82f1\u7279\u5c14\u5fae\u5904\u7406\u5668\u624b\u518c\u5206\u522b\u5c06\u540c\u6b65\u548c\u5f02\u6b65\u4e2d\u65ad\u6307\u5b9a\u4e3a\u5f02\u5e38\u548c\u4e2d\u65ad\u3002 \u6211\u4eec\u5c06\u91c7\u7528\u8fd9\u79cd\u5206\u7c7b\uff0c\u5c3d\u7ba1\u6211\u4eec\u5076\u5c14\u4f1a\u4f7f\u7528\u672f\u8bed\u201c\u4e2d\u65ad\u4fe1\u53f7\u201d\u6765\u6307\u5b9a\u4e24\u79cd\u7c7b\u578b\uff08\u540c\u6b65\u548c\u5f02\u6b65\uff09\u3002 Interrupts are issued by interval timers and I/O devices ; for instance, the arrival of a keystroke from a user sets off an interrupt. Exceptions , on the other hand, are caused either by programming errors or by anomalous\uff08\u5f02\u5e38\u7684\uff09 conditions that must be handled by the kernel. In the first case, the kernel handles the exception by delivering to the current process one of the signals familiar to every Unix programmer . In the second case, the kernel performs all the steps needed to recover from the anomalous condition , such as a Page Fault or a request via an assembly language instruction such as int or sysenter for a kernel service. We start by describing in the next section the motivation for introducing such signals. We then show how the well-known IRQs (Interrupt Requests) issued by I/O devices give rise to interrupts, and we detail how 80x86 processors handle interrupts and exceptions at the hardware level . Then we illustrate, in the section \"Initializing the Interrupt Descriptor Table,\" how Linux initializes all the data structures required by the 80x86 interrupt architecture. The remaining three sections describe how Linux handles interrupt signals at the software level . NOTE: \u601d\u8003Unix signal\u548cexceptions\u548cinterrupts\u4e4b\u95f4\u7684\u5173\u7cfb\u3002Unix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53c2\u52a0\u300adocs/Programming/Signal/Signal.md\u300b","title":"Chapter-4-Interrupts-and-Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/Chapter-4-Interrupts-and-Exceptions/#chapter-4-interrupts-and-exceptions","text":"An interrupt is usually defined as an event that alters the sequence of instructions executed by a processor. Such events correspond to electrical signals generated by hardware circuits both inside and outside the CPU chip. Interrupts are often divided into synchronous and asynchronous interrupts : Synchronous interrupts are produced by the CPU control unit while executing instructions and are called synchronous because the control unit issues them only after terminating the execution of an instruction. Asynchronous interrupts are generated by other hardware devices at arbitrary times with respect to the CPU clock signals. NOTE : \u4e24\u8005\u7684\u6765\u6e90\u4e0d\u540c\uff0c\u4e00\u4e2a\u662f\u6e90\u81eaCPU\uff0c\u4e00\u4e2a\u662f\u6e90\u81ea\u5176\u4ed6\u7684 hardware devices Intel microprocessor manuals designate synchronous and asynchronous interrupts as exceptions and interrupts , respectively. We'll adopt this classification, although we'll occasionally use the term \" interrupt signal \" to designate both types together (synchronous as well as asynchronous). \u82f1\u7279\u5c14\u5fae\u5904\u7406\u5668\u624b\u518c\u5206\u522b\u5c06\u540c\u6b65\u548c\u5f02\u6b65\u4e2d\u65ad\u6307\u5b9a\u4e3a\u5f02\u5e38\u548c\u4e2d\u65ad\u3002 \u6211\u4eec\u5c06\u91c7\u7528\u8fd9\u79cd\u5206\u7c7b\uff0c\u5c3d\u7ba1\u6211\u4eec\u5076\u5c14\u4f1a\u4f7f\u7528\u672f\u8bed\u201c\u4e2d\u65ad\u4fe1\u53f7\u201d\u6765\u6307\u5b9a\u4e24\u79cd\u7c7b\u578b\uff08\u540c\u6b65\u548c\u5f02\u6b65\uff09\u3002 Interrupts are issued by interval timers and I/O devices ; for instance, the arrival of a keystroke from a user sets off an interrupt. Exceptions , on the other hand, are caused either by programming errors or by anomalous\uff08\u5f02\u5e38\u7684\uff09 conditions that must be handled by the kernel. In the first case, the kernel handles the exception by delivering to the current process one of the signals familiar to every Unix programmer . In the second case, the kernel performs all the steps needed to recover from the anomalous condition , such as a Page Fault or a request via an assembly language instruction such as int or sysenter for a kernel service. We start by describing in the next section the motivation for introducing such signals. We then show how the well-known IRQs (Interrupt Requests) issued by I/O devices give rise to interrupts, and we detail how 80x86 processors handle interrupts and exceptions at the hardware level . Then we illustrate, in the section \"Initializing the Interrupt Descriptor Table,\" how Linux initializes all the data structures required by the 80x86 interrupt architecture. The remaining three sections describe how Linux handles interrupt signals at the software level . NOTE: \u601d\u8003Unix signal\u548cexceptions\u548cinterrupts\u4e4b\u95f4\u7684\u5173\u7cfb\u3002Unix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53c2\u52a0\u300adocs/Programming/Signal/Signal.md\u300b","title":"Chapter 4. Interrupts and Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/Trap/","text":"","title":"Trap"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-6-Timing-Measurements/Chapter-6-Timing-Measurements/","text":"Chapter 6. Timing Measurements Chapter 6. Timing Measurements # Countless computerized activities are driven by timing measurements , often behind the user's back. For instance, if the screen is automatically switched off after you have stopped using the computer's console, it is due to a timer that allows the kernel to keep track of how much time has elapsed since you pushed a key or moved the mouse. If you receive a warning from the system asking you to remove a set of unused files, it is the outcome of a program that identifies all user files that have not been accessed for a long time. To do these things, programs must be able to retrieve a timestamp identifying its last access time from each file. Such a timestamp must be automatically written by the kernel . More significantly, timing drives process switches along with even more visible kernel activities such as checking for time-outs. SUMMARY : CPU\u7684\u63a7\u5236\u5668\u4e5f\u662f\u53d7\u65f6\u949f\u63a7\u5236\u7684\uff1aclock generator\uff1b We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel: Keeping the current time and date so they can be returned to user programs through the time() , ftime( ) , and gettimeofday( ) APIs (see the section \"The time( ) and gettimeofday( ) System Calls\" later in this chapter) and used by the kernel itself as timestamps for files and network packets Maintaining timers mechanisms that are able to notify the kernel (see the later section \"Software Timers and Delay Functions\") or a user program (see the later sections \"The setitimer( ) and alarm( ) System Calls\" and \"System Calls for POSIX Timers\") that a certain interval of time has elapsed Timing measurements are performed by several hardware circuits based on fixed-frequency oscillators and counters. This chapter consists of four different parts. The first two sections describe the hardware devices that underly timing and give an overall picture of Linux timekeeping architecture . The following sections describe the main time-related duties of the kernel: implementing CPU time sharing , updating system time and resource usage statistics, and maintaining software timers . The last section discusses the system calls related to timing measurements and the corresponding service routines.","title":"Chapter-6-Timing-Measurements"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-6-Timing-Measurements/Chapter-6-Timing-Measurements/#chapter-6-timing-measurements","text":"Countless computerized activities are driven by timing measurements , often behind the user's back. For instance, if the screen is automatically switched off after you have stopped using the computer's console, it is due to a timer that allows the kernel to keep track of how much time has elapsed since you pushed a key or moved the mouse. If you receive a warning from the system asking you to remove a set of unused files, it is the outcome of a program that identifies all user files that have not been accessed for a long time. To do these things, programs must be able to retrieve a timestamp identifying its last access time from each file. Such a timestamp must be automatically written by the kernel . More significantly, timing drives process switches along with even more visible kernel activities such as checking for time-outs. SUMMARY : CPU\u7684\u63a7\u5236\u5668\u4e5f\u662f\u53d7\u65f6\u949f\u63a7\u5236\u7684\uff1aclock generator\uff1b We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel: Keeping the current time and date so they can be returned to user programs through the time() , ftime( ) , and gettimeofday( ) APIs (see the section \"The time( ) and gettimeofday( ) System Calls\" later in this chapter) and used by the kernel itself as timestamps for files and network packets Maintaining timers mechanisms that are able to notify the kernel (see the later section \"Software Timers and Delay Functions\") or a user program (see the later sections \"The setitimer( ) and alarm( ) System Calls\" and \"System Calls for POSIX Timers\") that a certain interval of time has elapsed Timing measurements are performed by several hardware circuits based on fixed-frequency oscillators and counters. This chapter consists of four different parts. The first two sections describe the hardware devices that underly timing and give an overall picture of Linux timekeeping architecture . The following sections describe the main time-related duties of the kernel: implementing CPU time sharing , updating system time and resource usage statistics, and maintaining software timers . The last section discusses the system calls related to timing measurements and the corresponding service routines.","title":"Chapter 6. Timing Measurements"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-7-Process-Scheduling/Chapter-7-Process-Scheduling/","text":"Chapter 7. Process Scheduling # Like every time sharing system, Linux achieves the magical effect of an apparent simultaneous execution of multiple processes by switching from one process to another in a very short time frame. Process switching itself was discussed in Chapter 3; this chapter deals with scheduling , which is concerned with when to switch and which process to choose. The chapter consists of three parts. The section \"Scheduling Policy\" introduces the choices made by Linux in the abstract to schedule processes. The section \"The Scheduling Algorithm\" discusses the data structures used to implement scheduling and the corresponding algorithm. Finally, the section \"System Calls Related to Scheduling\" describes the system calls that affect process scheduling. To simplify the description, we refer as usual to the 80 x 86 architecture; in particular, we assume that the system uses the Uniform Memory Access model, and that the system tick is set to 1 ms.","title":"Chapter-7-Process-Scheduling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-7-Process-Scheduling/Chapter-7-Process-Scheduling/#chapter-7-process-scheduling","text":"Like every time sharing system, Linux achieves the magical effect of an apparent simultaneous execution of multiple processes by switching from one process to another in a very short time frame. Process switching itself was discussed in Chapter 3; this chapter deals with scheduling , which is concerned with when to switch and which process to choose. The chapter consists of three parts. The section \"Scheduling Policy\" introduces the choices made by Linux in the abstract to schedule processes. The section \"The Scheduling Algorithm\" discusses the data structures used to implement scheduling and the corresponding algorithm. Finally, the section \"System Calls Related to Scheduling\" describes the system calls that affect process scheduling. To simplify the description, we refer as usual to the 80 x 86 architecture; in particular, we assume that the system uses the Uniform Memory Access model, and that the system tick is set to 1 ms.","title":"Chapter 7. Process Scheduling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/8.1-Page-Frame-Management/","text":"8.1. Page Frame Management # We saw in the section \"Paging in Hardware\" in Chapter 2 how the Intel Pentium processor can use two different page frame sizes: 4 KB and 4 MB (or 2 MB if PAE is enabled see the section \"The Physical Address Extension (PAE) Paging Mechanism\" in Chapter 2). Linux adopts the smaller 4 KB page frame size as the standard memory allocation unit . This makes things simpler for two reasons: The Page Fault exceptions issued by the paging circuitry are easily interpreted. Either the page requested exists but the process is not allowed to address it, or the page does not exist. In the second case, the memory allocator must find a free 4 KB page frame and assign it to the process. Although both 4 KB and 4 MB are multiples of all disk block sizes, transfers of data between main memory and disks are in most cases more efficient when the smaller size is used. 8.1.1. Page Descriptors # The kernel must keep track of the current status of each page frame . For instance, it must be able to distinguish the page frames that are used to contain pages that belong to processes from those that contain kernel code or kernel data structures. Similarly, it must be able to determine whether a page frame in dynamic memory is free. A page frame in dynamic memory is free if it does not contain any useful data. It is not free when the page frame contains data of a User Mode process, data of a software cache, dynamically allocated kernel data structures, buffered data of a device driver, code of a kernel module, and so on. State information of a page frame is kept in a page descriptor of type page , whose fields are shown in Table 8-1. All page descriptors are stored in the mem_map array. Because each descriptor is 32 bytes long, the space required by mem_map is slightly less than 1% of the whole RAM. The virt_to_page(addr) macro yields the address of the page descriptor associated with the linear address addr . The pfn_to_page(pfn) macro yields the address of the page descriptor associated with the page frame having number pfn . NOTE: \u6839\u636epage frame\u6765\u83b7\u5f97\u5176\u5bf9\u5e94\u7684page descriptor\u3002 Table 8-1. The fields of the page descriptor Type Name Description unsigned long flags Array of flags (see Table 8-2). Also encodes the zone number to which the page frame belongs. atomic_t _count Page frame's reference counter. atomic_t _mapcount Number of Page Table entries that refer to the page frame ( - 1 if none). unsigned long private Available to the kernel component that is using the page (for instance, it is a buffer head pointer in case of buffer page; see \"Block Buffers and Buffer Heads\" in Chapter 15). If the page is free, this field is used by the buddy system (see later in this chapter). struct address_space * mapping Used when the page is inserted into the page cache (see the section \"The Page Cache\" in Chapter 15), or when it belongs to an anonymous region (see the section \"Reverse Mapping for Anonymous Pages\" in Chapter 17). unsigned long index Used by several kernel components with different meanings. For instance, it identifies the position of the data stored in the page frame within the page's disk image or within an anonymous region (Chapter 15), or it stores a swapped-out page identifier (Chapter 17). struct list_head lru Contains pointers to the least recently used doubly linked list of pages. You don't have to fully understand the role of all fields in the page descriptor right now. In the following chapters, we often come back to the fields of the page descriptor. Moreover, several fields have different meaning, according to whether the page frame is free or what kernel component is using the page frame.","title":"8.1-Page-Frame-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/8.1-Page-Frame-Management/#81-page-frame-management","text":"We saw in the section \"Paging in Hardware\" in Chapter 2 how the Intel Pentium processor can use two different page frame sizes: 4 KB and 4 MB (or 2 MB if PAE is enabled see the section \"The Physical Address Extension (PAE) Paging Mechanism\" in Chapter 2). Linux adopts the smaller 4 KB page frame size as the standard memory allocation unit . This makes things simpler for two reasons: The Page Fault exceptions issued by the paging circuitry are easily interpreted. Either the page requested exists but the process is not allowed to address it, or the page does not exist. In the second case, the memory allocator must find a free 4 KB page frame and assign it to the process. Although both 4 KB and 4 MB are multiples of all disk block sizes, transfers of data between main memory and disks are in most cases more efficient when the smaller size is used.","title":"8.1. Page Frame Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/8.1-Page-Frame-Management/#811-page-descriptors","text":"The kernel must keep track of the current status of each page frame . For instance, it must be able to distinguish the page frames that are used to contain pages that belong to processes from those that contain kernel code or kernel data structures. Similarly, it must be able to determine whether a page frame in dynamic memory is free. A page frame in dynamic memory is free if it does not contain any useful data. It is not free when the page frame contains data of a User Mode process, data of a software cache, dynamically allocated kernel data structures, buffered data of a device driver, code of a kernel module, and so on. State information of a page frame is kept in a page descriptor of type page , whose fields are shown in Table 8-1. All page descriptors are stored in the mem_map array. Because each descriptor is 32 bytes long, the space required by mem_map is slightly less than 1% of the whole RAM. The virt_to_page(addr) macro yields the address of the page descriptor associated with the linear address addr . The pfn_to_page(pfn) macro yields the address of the page descriptor associated with the page frame having number pfn . NOTE: \u6839\u636epage frame\u6765\u83b7\u5f97\u5176\u5bf9\u5e94\u7684page descriptor\u3002 Table 8-1. The fields of the page descriptor Type Name Description unsigned long flags Array of flags (see Table 8-2). Also encodes the zone number to which the page frame belongs. atomic_t _count Page frame's reference counter. atomic_t _mapcount Number of Page Table entries that refer to the page frame ( - 1 if none). unsigned long private Available to the kernel component that is using the page (for instance, it is a buffer head pointer in case of buffer page; see \"Block Buffers and Buffer Heads\" in Chapter 15). If the page is free, this field is used by the buddy system (see later in this chapter). struct address_space * mapping Used when the page is inserted into the page cache (see the section \"The Page Cache\" in Chapter 15), or when it belongs to an anonymous region (see the section \"Reverse Mapping for Anonymous Pages\" in Chapter 17). unsigned long index Used by several kernel components with different meanings. For instance, it identifies the position of the data stored in the page frame within the page's disk image or within an anonymous region (Chapter 15), or it stores a swapped-out page identifier (Chapter 17). struct list_head lru Contains pointers to the least recently used doubly linked list of pages. You don't have to fully understand the role of all fields in the page descriptor right now. In the following chapters, we often come back to the fields of the page descriptor. Moreover, several fields have different meaning, according to whether the page frame is free or what kernel component is using the page frame.","title":"8.1.1. Page Descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/Chapter-8-Memory-Management/","text":"Chapter 8. Memory Management # We saw in Chapter 2 how Linux takes advantage of 80 x 86's segmentation and paging circuits to translate logical addresses into physical ones. We also mentioned that some portion of RAM is permanently assigned to the kernel and used to store both the kernel code and the static kernel data structures. NOTE: \u73b0\u4ee3\u5927\u591a\u6570\u90fd\u662f\u91c7\u7528\u7684\u57fa\u4e8epage\u7684memory management\u3002 The remaining part of the RAM is called dynamic memory . It is a valuable resource, needed not only by the processes but also by the kernel itself. In fact, the performance of the entire system depends on how efficiently dynamic memory is managed. Therefore, all current multitasking operating systems try to optimize the use of dynamic memory , assigning it only when it is needed and freeing it as soon as possible. Figure 8-1 shows schematically the page frames used as dynamic memory ; see the section \"Physical Memory Layout\" in Chapter 2 for details. This chapter, which consists of three main sections, describes how the kernel allocates dynamic memory for its own use. The sections \"Page Frame Management\" and \"Memory Area Management\" illustrate two different techniques for handling physically contiguous memory areas, while the section \"Noncontiguous Memory Area Management\" illustrates a third technique that handles noncontiguous memory areas. In these sections we'll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.","title":"Chapter-8-Memory-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/Chapter-8-Memory-Management/#chapter-8-memory-management","text":"We saw in Chapter 2 how Linux takes advantage of 80 x 86's segmentation and paging circuits to translate logical addresses into physical ones. We also mentioned that some portion of RAM is permanently assigned to the kernel and used to store both the kernel code and the static kernel data structures. NOTE: \u73b0\u4ee3\u5927\u591a\u6570\u90fd\u662f\u91c7\u7528\u7684\u57fa\u4e8epage\u7684memory management\u3002 The remaining part of the RAM is called dynamic memory . It is a valuable resource, needed not only by the processes but also by the kernel itself. In fact, the performance of the entire system depends on how efficiently dynamic memory is managed. Therefore, all current multitasking operating systems try to optimize the use of dynamic memory , assigning it only when it is needed and freeing it as soon as possible. Figure 8-1 shows schematically the page frames used as dynamic memory ; see the section \"Physical Memory Layout\" in Chapter 2 for details. This chapter, which consists of three main sections, describes how the kernel allocates dynamic memory for its own use. The sections \"Page Frame Management\" and \"Memory Area Management\" illustrate two different techniques for handling physically contiguous memory areas, while the section \"Noncontiguous Memory Area Management\" illustrates a third technique that handles noncontiguous memory areas. In these sections we'll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.","title":"Chapter 8. Memory Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.1-The-Process's-Address-Space/","text":"9.1. The Process's Address Space # The address space of a process consists of all linear addresses that the process is allowed to use. Each process sees a different set of linear addresses ; the address used by one process bears no relation to the address used by another. As we will see later, the kernel may dynamically modify a process address space by adding or removing intervals of linear addresses . The kernel represents intervals of linear addresses by means of resources called memory regions, which are characterized by an initial linear address , a length , and some access rights . For reasons of efficiency, both the initial address and the length of a memory region must be multiples of 4,096, so that the data identified by each memory region completely fills up the page frames allocated to it. Following are some typical situations in which a process gets new memory regions : When the user types a command at the console, the shell process creates a new process to execute the command. As a result, a fresh address space, and thus a set of memory regions , is assigned to the new process (see the section \"Creating and Deleting a Process Address Space\" later in this chapter; also, see Chapter 20). A running process may decide to load an entirely different program. In this case, the process ID remains unchanged, but the memory regions used before loading the program are released and a new set of memory regions is assigned to the process (see the section \"The exec Functions\" in Chapter 20). A running process may perform a \"memory mapping\" on a file (or on a portion of it). In such cases, the kernel assigns a new memory region to the process to map the file (see the section \"Memory Mapping\" in Chapter 16). A process may keep adding data on its User Mode stack until all addresses in the memory region that map the stack have been used. In this case, the kernel may decide to expand the size of that memory region (see the section \"Page Fault Exception Handler\" later in this chapter). A process may create an IPC-shared memory region to share data with other cooperating processes. In this case, the kernel assigns a new memory region to the process to implement this construct (see the section \"IPC Shared Memory\" in Chapter 19). A process may expand its dynamic area (the heap) through a function such as malloc( ) . As a result, the kernel may decide to expand the size of the memory region assigned to the heap (see the section \"Managing the Heap\" later in this chapter). Table 9-1 illustrates some of the system calls related to the previously mentioned tasks. brk( ) is discussed at the end of this chapter, while the remaining system calls are described in other chapters. Table 9-1. System calls related to memory region creation and deletion System call Description brk( ) Changes the heap size of the process execve( ) Loads a new executable file, thus changing the process address space _exit( ) Terminates the current process and destroys its address space fork( ) Creates a new process, and thus a new address space mmap( ) , mmap2( ) Creates a memory mapping for a file, thus enlarging the process address space mremap( ) Expands or shrinks a memory region remap_file_pages() Creates a non-linear mapping for a file (see Chapter 16) munmap( ) Destroys a memory mapping for a file, thus contracting the process address space shmat( ) Attaches a shared memory region shmdt( ) Detaches a shared memory region As we'll see in the later section \"Page Fault Exception Handler,\" it is essential for the kernel to identify the memory regions currently owned by a process (the address space of a process), because that allows the Page Fault exception handler to efficiently distinguish between two types of invalid linear addresses that cause it to be invoked: Those caused by programming errors. Those caused by a missing page; even though the linear address belongs to the process's address space, the page frame corresponding to that address has yet to be allocated. The latter addresses are not invalid from the process's point of view; the induced Page Faults are exploited by the kernel to implement demand paging : the kernel provides the missing page frame and lets the process continue.","title":"9.1-The-Process's-Address-Space."},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.1-The-Process's-Address-Space/#91-the-processs-address-space","text":"The address space of a process consists of all linear addresses that the process is allowed to use. Each process sees a different set of linear addresses ; the address used by one process bears no relation to the address used by another. As we will see later, the kernel may dynamically modify a process address space by adding or removing intervals of linear addresses . The kernel represents intervals of linear addresses by means of resources called memory regions, which are characterized by an initial linear address , a length , and some access rights . For reasons of efficiency, both the initial address and the length of a memory region must be multiples of 4,096, so that the data identified by each memory region completely fills up the page frames allocated to it. Following are some typical situations in which a process gets new memory regions : When the user types a command at the console, the shell process creates a new process to execute the command. As a result, a fresh address space, and thus a set of memory regions , is assigned to the new process (see the section \"Creating and Deleting a Process Address Space\" later in this chapter; also, see Chapter 20). A running process may decide to load an entirely different program. In this case, the process ID remains unchanged, but the memory regions used before loading the program are released and a new set of memory regions is assigned to the process (see the section \"The exec Functions\" in Chapter 20). A running process may perform a \"memory mapping\" on a file (or on a portion of it). In such cases, the kernel assigns a new memory region to the process to map the file (see the section \"Memory Mapping\" in Chapter 16). A process may keep adding data on its User Mode stack until all addresses in the memory region that map the stack have been used. In this case, the kernel may decide to expand the size of that memory region (see the section \"Page Fault Exception Handler\" later in this chapter). A process may create an IPC-shared memory region to share data with other cooperating processes. In this case, the kernel assigns a new memory region to the process to implement this construct (see the section \"IPC Shared Memory\" in Chapter 19). A process may expand its dynamic area (the heap) through a function such as malloc( ) . As a result, the kernel may decide to expand the size of the memory region assigned to the heap (see the section \"Managing the Heap\" later in this chapter). Table 9-1 illustrates some of the system calls related to the previously mentioned tasks. brk( ) is discussed at the end of this chapter, while the remaining system calls are described in other chapters. Table 9-1. System calls related to memory region creation and deletion System call Description brk( ) Changes the heap size of the process execve( ) Loads a new executable file, thus changing the process address space _exit( ) Terminates the current process and destroys its address space fork( ) Creates a new process, and thus a new address space mmap( ) , mmap2( ) Creates a memory mapping for a file, thus enlarging the process address space mremap( ) Expands or shrinks a memory region remap_file_pages() Creates a non-linear mapping for a file (see Chapter 16) munmap( ) Destroys a memory mapping for a file, thus contracting the process address space shmat( ) Attaches a shared memory region shmdt( ) Detaches a shared memory region As we'll see in the later section \"Page Fault Exception Handler,\" it is essential for the kernel to identify the memory regions currently owned by a process (the address space of a process), because that allows the Page Fault exception handler to efficiently distinguish between two types of invalid linear addresses that cause it to be invoked: Those caused by programming errors. Those caused by a missing page; even though the linear address belongs to the process's address space, the page frame corresponding to that address has yet to be allocated. The latter addresses are not invalid from the process's point of view; the induced Page Faults are exploited by the kernel to implement demand paging : the kernel provides the missing page frame and lets the process continue.","title":"9.1. The Process's Address Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.2-The-Memory-Descriptor/","text":"9.2. The Memory Descriptor # All information related to the process address space is included in an object called the memory descriptor of type mm_struct . This object is referenced by the mm field of the process descriptor . The fields of a memory descriptor are listed in Table 9-2. Table 9-2. The fields of the memory descriptor Type Field Description \u6ce8\u91ca struct vm_area_struct * mmap Pointer to the head of the list of memory region objects \u53c2\u89c1chapter 9.3. Memory Regions struct rb_root mm_rb Pointer to the root of the red-black tree of memory region objects struct vm_area_struct * mmap_cache Pointer to the last referenced memory region object unsigned long (*)( ) get_unmapped_area Method that searches an available linear address interval in the process address space void (*)( ) unmap_area Method invoked when releasing a linear address interval unsigned long mmap_base Identifies the linear address of the first allocated anonymous memory region or file memory mapping (see the section \"Program Segments and Process Memory Regions\" in Chapter 20) unsigned long free_area_cache Address from which the kernel will look for a free interval of linear addresses in the process address space pgd_t * pgd Pointer to the Page Global Directory \u5173\u4e8ePage Global Directory\uff0c\u53c2\u89c1Section 2.4. Paging in Hardware\u3001Section 2.5. Paging in Linux atomic_t mm_users Secondary usage counter atomic_t mm_count Main usage counter All memory descriptors are stored in a doubly linked list. Each descriptor stores the address of the adjacent list items in the mmlist field. The first element of the list is the mmlist field of init_mm , the memory descriptor used by process 0 in the initialization phase. The list is protected against concurrent accesses in multiprocessor systems by the mmlist_lock spin lock. The mm_users field stores the number of lightweight processes that share the mm_struct data structure (see the section \"The clone( ) , fork( ) , and vfork( ) System Calls\" in Chapter 3). The mm_count field is the main usage counter of the memory descriptor; all \"users\" in mm_users count as one unit in mm_count . Every time the mm_count field is decreased, the kernel checks whether it becomes zero; if so, the memory descriptor is deallocated because it is no longer in use.","title":"9.2-The-Memory-Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.2-The-Memory-Descriptor/#92-the-memory-descriptor","text":"All information related to the process address space is included in an object called the memory descriptor of type mm_struct . This object is referenced by the mm field of the process descriptor . The fields of a memory descriptor are listed in Table 9-2. Table 9-2. The fields of the memory descriptor Type Field Description \u6ce8\u91ca struct vm_area_struct * mmap Pointer to the head of the list of memory region objects \u53c2\u89c1chapter 9.3. Memory Regions struct rb_root mm_rb Pointer to the root of the red-black tree of memory region objects struct vm_area_struct * mmap_cache Pointer to the last referenced memory region object unsigned long (*)( ) get_unmapped_area Method that searches an available linear address interval in the process address space void (*)( ) unmap_area Method invoked when releasing a linear address interval unsigned long mmap_base Identifies the linear address of the first allocated anonymous memory region or file memory mapping (see the section \"Program Segments and Process Memory Regions\" in Chapter 20) unsigned long free_area_cache Address from which the kernel will look for a free interval of linear addresses in the process address space pgd_t * pgd Pointer to the Page Global Directory \u5173\u4e8ePage Global Directory\uff0c\u53c2\u89c1Section 2.4. Paging in Hardware\u3001Section 2.5. Paging in Linux atomic_t mm_users Secondary usage counter atomic_t mm_count Main usage counter All memory descriptors are stored in a doubly linked list. Each descriptor stores the address of the adjacent list items in the mmlist field. The first element of the list is the mmlist field of init_mm , the memory descriptor used by process 0 in the initialization phase. The list is protected against concurrent accesses in multiprocessor systems by the mmlist_lock spin lock. The mm_users field stores the number of lightweight processes that share the mm_struct data structure (see the section \"The clone( ) , fork( ) , and vfork( ) System Calls\" in Chapter 3). The mm_count field is the main usage counter of the memory descriptor; all \"users\" in mm_users count as one unit in mm_count . Every time the mm_count field is decreased, the kernel checks whether it becomes zero; if so, the memory descriptor is deallocated because it is no longer in use.","title":"9.2. The Memory Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/Chapter-9-Process-Address-Space/","text":"Chapter 9. Process Address Space # As seen in the previous chapter, a kernel function gets dynamic memory in a fairly straightforward manner by invoking one of a variety of functions: __get_free_pages( ) or alloc_pages( ) to get pages from the zoned page frame allocator, kmem_cache_alloc( ) or kmalloc( ) to use the slab allocator for specialized or general-purpose objects, and vmalloc( ) or vmalloc_32( ) to get a noncontiguous memory area. If the request can be satisfied, each of these functions returns a page descriptor address or a linear address identifying the beginning of the allocated dynamic memory area. NOTE: \u666e\u901a\u7684process\u662f\u4e0d\u4f1a\u63a5\u89e6\u5230page descriptor\u7684 These simple approaches work for two reasons: The kernel is the highest-priority component of the operating system. If a kernel function makes a request for dynamic memory , it must have a valid reason to issue that request, and there is no point in trying to defer it. The kernel trusts itself. All kernel functions are assumed to be error-free, so the kernel does not need to insert any protection against programming errors. When allocating memory to User Mode processes, the situation is entirely different: Process requests for dynamic memory are considered non-urgent. When a process's executable file is loaded, for instance, it is unlikely that the process will address all the pages of code in the near future. Similarly, when a process invokes malloc( ) to get additional dynamic memory, it doesn't mean the process will soon access all the additional memory obtained. Thus, as a general rule, the kernel tries to defer allocating dynamic memory to User Mode processes. Because user programs cannot be trusted, the kernel must be prepared to catch all addressing errors caused by processes in User Mode. As this chapter describes, the kernel succeeds in deferring the allocation of dynamic memory to processes by using a new kind of resource. When a User Mode process asks for dynamic memory , it doesn't get additional page frames ; instead, it gets the right to use a new range of linear addresses , which become part of its address space . This interval is called a \" memory region .\" In the next section, we discuss how the process views dynamic memory . We then describe the basic components of the process address space in the section \"Memory Regions.\" Next, we examine in detail the role played by the Page Fault exception handler in deferring the allocation of page frames to processes and illustrate how the kernel creates and deletes whole process address spaces . Last, we discuss the APIs and system calls related to address space management .","title":"Chapter-9-Process-Address-Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/Chapter-9-Process-Address-Space/#chapter-9-process-address-space","text":"As seen in the previous chapter, a kernel function gets dynamic memory in a fairly straightforward manner by invoking one of a variety of functions: __get_free_pages( ) or alloc_pages( ) to get pages from the zoned page frame allocator, kmem_cache_alloc( ) or kmalloc( ) to use the slab allocator for specialized or general-purpose objects, and vmalloc( ) or vmalloc_32( ) to get a noncontiguous memory area. If the request can be satisfied, each of these functions returns a page descriptor address or a linear address identifying the beginning of the allocated dynamic memory area. NOTE: \u666e\u901a\u7684process\u662f\u4e0d\u4f1a\u63a5\u89e6\u5230page descriptor\u7684 These simple approaches work for two reasons: The kernel is the highest-priority component of the operating system. If a kernel function makes a request for dynamic memory , it must have a valid reason to issue that request, and there is no point in trying to defer it. The kernel trusts itself. All kernel functions are assumed to be error-free, so the kernel does not need to insert any protection against programming errors. When allocating memory to User Mode processes, the situation is entirely different: Process requests for dynamic memory are considered non-urgent. When a process's executable file is loaded, for instance, it is unlikely that the process will address all the pages of code in the near future. Similarly, when a process invokes malloc( ) to get additional dynamic memory, it doesn't mean the process will soon access all the additional memory obtained. Thus, as a general rule, the kernel tries to defer allocating dynamic memory to User Mode processes. Because user programs cannot be trusted, the kernel must be prepared to catch all addressing errors caused by processes in User Mode. As this chapter describes, the kernel succeeds in deferring the allocation of dynamic memory to processes by using a new kind of resource. When a User Mode process asks for dynamic memory , it doesn't get additional page frames ; instead, it gets the right to use a new range of linear addresses , which become part of its address space . This interval is called a \" memory region .\" In the next section, we discuss how the process views dynamic memory . We then describe the basic components of the process address space in the section \"Memory Regions.\" Next, we examine in detail the role played by the Page Fault exception handler in deferring the allocation of page frames to processes and illustrate how the kernel creates and deletes whole process address spaces . Last, we discuss the APIs and system calls related to address space management .","title":"Chapter 9. Process Address Space"},{"location":"Kernel/Guide/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u7684\u5185\u5bb9\u662f\u6211\u57fa\u4e8e\u9605\u8bfb\u672c\u4e66\uff08\u90e8\u5206\u7ae0\u8282\uff09\u540e\u6240\u603b\u7ed3\u7684\uff0c\u5bf9\u4e00\u4e9b\u4e13\u9898\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u68b3\u7406\u4e86\u8109\u7edc\u3002 \u5728Architecture\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86OS\u7684\u4e24\u4e2aobjective\uff1a The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). \u8fd9\u4e24\u4e2aobjective\uff08\u5176\u5b9e\u5c31\u662fOS\u7684\u4f5c\u7528\u3001\u4f7f\u547d\uff09\u76f8\u5f53\u4e8e\u4e24\u6761\u7ebf\uff0c\u540e\u9762\u6211\u4eec\u5c06\u6cbf\u7740\u8fd9\u4e24\u6761\u7ebf\u6df1\u5165\u5bf9Linux OS\u7684\u5b66\u4e60\u3002 \u672c\u7ae0\u4f1a\u5bf9\u7b2c\u4e00\u6761\u7ebf\u201cInteract with the hardware components\u201d\u8fdb\u884c\u603b\u7ed3\u3002 \u672c\u7ae0\u4f1a\u5bf9Linux OS\u7684 multitasking \u8fdb\u884c\u603b\u7ed3\uff0c\u8fd9\u662f\u548c\u7b2c\u4e8c\u6761\u7ebf\u5bc6\u5207\u76f8\u5173\u7684\u3002 \u672c\u7ae0\u4f1a\u7ed9\u51fa\u9605\u8bfbLinux OS kernel\u6e90\u7801\u7684\u6307\u5bfc\uff0c\u5e76\u5bf9Linux OS kernel\u6e90\u7801\u4e2d\u7684\u4e00\u4e9b\u91cd\u8981\u7684data structure\u8fdb\u884c\u603b\u7ed3\u3002 \u9605\u8bfb\u672c\u4e66\uff0c\u9700\u8981\u4e00\u4e9bhardware\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u53c2\u89c1\u6211\u7684\u53e6\u5916\u4e00\u4e2a\u9879\u76ee Hardware \u3002 \u603b\u7684\u6765\u8bf4\uff0c\u672c\u7ae0\u65e8\u5728\u5e2e\u52a9\u81ea\u5df1\u548c\u8bfb\u8005\u66f4\u597d\u5730\u7406\u89e3\u672c\u4e66\u7684\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Guide/#_1","text":"\u672c\u7ae0\u7684\u5185\u5bb9\u662f\u6211\u57fa\u4e8e\u9605\u8bfb\u672c\u4e66\uff08\u90e8\u5206\u7ae0\u8282\uff09\u540e\u6240\u603b\u7ed3\u7684\uff0c\u5bf9\u4e00\u4e9b\u4e13\u9898\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u68b3\u7406\u4e86\u8109\u7edc\u3002 \u5728Architecture\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86OS\u7684\u4e24\u4e2aobjective\uff1a The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). \u8fd9\u4e24\u4e2aobjective\uff08\u5176\u5b9e\u5c31\u662fOS\u7684\u4f5c\u7528\u3001\u4f7f\u547d\uff09\u76f8\u5f53\u4e8e\u4e24\u6761\u7ebf\uff0c\u540e\u9762\u6211\u4eec\u5c06\u6cbf\u7740\u8fd9\u4e24\u6761\u7ebf\u6df1\u5165\u5bf9Linux OS\u7684\u5b66\u4e60\u3002 \u672c\u7ae0\u4f1a\u5bf9\u7b2c\u4e00\u6761\u7ebf\u201cInteract with the hardware components\u201d\u8fdb\u884c\u603b\u7ed3\u3002 \u672c\u7ae0\u4f1a\u5bf9Linux OS\u7684 multitasking \u8fdb\u884c\u603b\u7ed3\uff0c\u8fd9\u662f\u548c\u7b2c\u4e8c\u6761\u7ebf\u5bc6\u5207\u76f8\u5173\u7684\u3002 \u672c\u7ae0\u4f1a\u7ed9\u51fa\u9605\u8bfbLinux OS kernel\u6e90\u7801\u7684\u6307\u5bfc\uff0c\u5e76\u5bf9Linux OS kernel\u6e90\u7801\u4e2d\u7684\u4e00\u4e9b\u91cd\u8981\u7684data structure\u8fdb\u884c\u603b\u7ed3\u3002 \u9605\u8bfb\u672c\u4e66\uff0c\u9700\u8981\u4e00\u4e9bhardware\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u53c2\u89c1\u6211\u7684\u53e6\u5916\u4e00\u4e2a\u9879\u76ee Hardware \u3002 \u603b\u7684\u6765\u8bf4\uff0c\u672c\u7ae0\u65e8\u5728\u5e2e\u52a9\u81ea\u5df1\u548c\u8bfb\u8005\u66f4\u597d\u5730\u7406\u89e3\u672c\u4e66\u7684\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Guide/Entry-point-of-kernel/","text":"Dose kernel have main function # Linux Kernel And Its Functions Does the kernel have a main() function? closed Does kernel have main function?","title":"Dose kernel have main function"},{"location":"Kernel/Guide/Entry-point-of-kernel/#dose-kernel-have-main-function","text":"Linux Kernel And Its Functions Does the kernel have a main() function? closed Does kernel have main function?","title":"Dose kernel have main function"},{"location":"Kernel/Guide/How-to-understand-linux-kernel-source-code/","text":"\u5982\u4f55\u9605\u8bfblinux OS kernel\u6e90\u4ee3\u7801 # \u5982\u4f55\u9605\u8bfblinux kernel\u7684source code\uff1f\u5728\u62ff\u8d77\u672c\u4e66\u7684\u65f6\u5019\u6211\u601d\u8003\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e0b\u9762\u662f\u6211\u68c0\u7d22\u5230\u7684\u6211\u89c9\u5f97\u6709\u9053\u7406\u7684 \u89c2\u70b9 \uff1a Focus on data structures . Understanding data structures is usually more important than code . If you are only shown data structures but no code, you still get the big picture of the system. Vice versa, if shown only code but not data structures, it's very hard to understand the system. \"I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.\" -- Linus Torvalds \"Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won't usually need your flowcharts; they'll be obvious.\" -- Fred Brooks. How to understand Linux kernel source code for a beginner? \u4ecestructure\u5165\u624b\uff0c\u672c\u4e66\u4e5f\u662f\u5982\u6b64\u3002","title":"How-to-understand-linux-kernel-source-code"},{"location":"Kernel/Guide/How-to-understand-linux-kernel-source-code/#linux-os-kernel","text":"\u5982\u4f55\u9605\u8bfblinux kernel\u7684source code\uff1f\u5728\u62ff\u8d77\u672c\u4e66\u7684\u65f6\u5019\u6211\u601d\u8003\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e0b\u9762\u662f\u6211\u68c0\u7d22\u5230\u7684\u6211\u89c9\u5f97\u6709\u9053\u7406\u7684 \u89c2\u70b9 \uff1a Focus on data structures . Understanding data structures is usually more important than code . If you are only shown data structures but no code, you still get the big picture of the system. Vice versa, if shown only code but not data structures, it's very hard to understand the system. \"I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.\" -- Linus Torvalds \"Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won't usually need your flowcharts; they'll be obvious.\" -- Fred Brooks. How to understand Linux kernel source code for a beginner? \u4ecestructure\u5165\u624b\uff0c\u672c\u4e66\u4e5f\u662f\u5982\u6b64\u3002","title":"\u5982\u4f55\u9605\u8bfblinux OS kernel\u6e90\u4ee3\u7801"},{"location":"Kernel/Guide/Kernel-data-structure/","text":"\u5404\u79cd\u5404\u6837\u7684descriptor # \u5404\u79cd\u5404\u6837\u7684descriptor\uff0c\u4ee5\u53ca\u5176\u5bf9\u5e94\u7684\u6570\u636e\u7ed3\u6784 Descriptor Chapter Struct Source Code Process Descriptor 3.2. Process Descriptor task_struct - https://github.com/torvalds/linux/blob/master/include/linux/sched.h - https://elixir.bootlin.com/linux/latest/ident/task_struct Memory Descriptor 9.2. The Memory Descriptor mm_struct - https://elixir.bootlin.com/linux/latest/ident/mm_struct - https://github.com/torvalds/linux/blob/master/include/linux/mm_types.h Page Descriptor 8.1.1. Page Descriptors page - https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h#L68 Task State Segment Descriptor 3.3. Process Switch Global Descriptor Table memory descriptor signal descriptor file descriptors Interrupt Descriptor Table","title":"Kernel-data-structure"},{"location":"Kernel/Guide/Kernel-data-structure/#descriptor","text":"\u5404\u79cd\u5404\u6837\u7684descriptor\uff0c\u4ee5\u53ca\u5176\u5bf9\u5e94\u7684\u6570\u636e\u7ed3\u6784 Descriptor Chapter Struct Source Code Process Descriptor 3.2. Process Descriptor task_struct - https://github.com/torvalds/linux/blob/master/include/linux/sched.h - https://elixir.bootlin.com/linux/latest/ident/task_struct Memory Descriptor 9.2. The Memory Descriptor mm_struct - https://elixir.bootlin.com/linux/latest/ident/mm_struct - https://github.com/torvalds/linux/blob/master/include/linux/mm_types.h Page Descriptor 8.1.1. Page Descriptors page - https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h#L68 Task State Segment Descriptor 3.3. Process Switch Global Descriptor Table memory descriptor signal descriptor file descriptors Interrupt Descriptor Table","title":"\u5404\u79cd\u5404\u6837\u7684descriptor"},{"location":"Kernel/Guide/Debugger/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u662f\u6211\u5728\u9605\u8bfb\u672c\u4e66\u76844.2. Interrupts and Exceptions\u65f6\u6709\u611f\u800c\u5199\u7684\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u4e00\u4e9b\u5b9e\u73b0 Debugger \u7684\u91cd\u8981\u5e95\u5c42\u6280\u672f\uff0c\u6240\u4ee5\u6211\u51b3\u5b9a\u5bf9Debugger\u7684\u5b9e\u73b0\u8fdb\u884c\u603b\u7ed3\u3002 TODO: 3.2.1. Process State TASK_TRACED ptrace Say this five times fast: strace, ptrace, dtrace, dtruss","title":"Introduction"},{"location":"Kernel/Guide/Debugger/#_1","text":"\u672c\u7ae0\u662f\u6211\u5728\u9605\u8bfb\u672c\u4e66\u76844.2. Interrupts and Exceptions\u65f6\u6709\u611f\u800c\u5199\u7684\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u4e00\u4e9b\u5b9e\u73b0 Debugger \u7684\u91cd\u8981\u5e95\u5c42\u6280\u672f\uff0c\u6240\u4ee5\u6211\u51b3\u5b9a\u5bf9Debugger\u7684\u5b9e\u73b0\u8fdb\u884c\u603b\u7ed3\u3002 TODO: 3.2.1. Process State TASK_TRACED ptrace Say this five times fast: strace, ptrace, dtrace, dtruss","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Guide/Debugger/Debugger/","text":"Debugger #","title":"Debugger"},{"location":"Kernel/Guide/Debugger/Debugger/#debugger","text":"","title":"Debugger"},{"location":"Kernel/Guide/Debugger/Trap/","text":"Trap # \u672c\u4e66\u76844.2. Interrupts and Exceptions\u8282\u5bf9Trap\u7684\u5b9a\u4e49\u548c\u7ef4\u57fa\u767e\u79d1 Trap (computing) \u4e2d\u7684\u63cf\u8ff0\u6709\u5dee\u5f02\uff0c\u7ecf\u8fc7\u5bf9\u6bd4\uff0c\u672c\u4e66\u7684\u63cf\u8ff0\u662f\u66f4\u52a0\u51c6\u786e\u7684\u3002","title":"Trap"},{"location":"Kernel/Guide/Debugger/Trap/#trap","text":"\u672c\u4e66\u76844.2. Interrupts and Exceptions\u8282\u5bf9Trap\u7684\u5b9a\u4e49\u548c\u7ef4\u57fa\u767e\u79d1 Trap (computing) \u4e2d\u7684\u63cf\u8ff0\u6709\u5dee\u5f02\uff0c\u7ecf\u8fc7\u5bf9\u6bd4\uff0c\u672c\u4e66\u7684\u63cf\u8ff0\u662f\u66f4\u52a0\u51c6\u786e\u7684\u3002","title":"Trap"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/","text":"Control path # Control path\u8fd9\u4e2a\u6982\u5ff5\u662f\u6211\u7531kernel control path\u542f\u53d1\u800c\u521b\u5efa\u7684\uff0c\u5b83\u8868\u793aOS\u4e2d\u6240\u6709\u53ef\u80fd\u7684\u6d3b\u52a8/\u6267\u884c\u6d41\u7a0b\uff0c\u4e4b\u6240\u4ee5\u521b\u5efa\u8fd9\u4e2a\u6982\u5ff5\uff0c\u662f\u56e0\u4e3a\u5b83\u53ef\u4ee5\u65b9\u4fbf\u6211\u4eec\u6765\u7edf\u4e00\u5730\u3001\u6982\u62ec\u5730\u63cf\u8ff0\u4e00\u4e9b\u95ee\u9898\uff08\u4e00\u4e2a\u62bd\u8c61\u8fc7\u7a0b\uff09\u3002\u4e0e\u5b83\u6bd4\u8f83\u63a5\u8fd1\u7684\u4e00\u4e2a\u6982\u5ff5\u662f Control flow \u3002 OS\u4e2d\u6709\u5982\u4e0bcontrol path\uff1a kernel control path kernel thread task\uff08process/thread\uff0c\u73b0\u4ee3OS\u9700\u8981\u652f\u6301 multitasking \uff09 \u5728\u672c\u4e66\u7684\u6709\u4e9b\u7ae0\u8282\u4f1a\u4f7f\u7528\u201cexecution context\u201d\u3001\u201cexecution flow\u201d\u7b49\u8bcd\u8bed\uff0c\u5176\u5b9e\u5b83\u4eec\u548c\u672c\u6587\u6240\u5b9a\u4e49\u7684control path\u8868\u793a\u7684\u662f\u76f8\u540c\u7684\u610f\u601d\u3002 Control path\u7684\u5178\u578b\u7279\u5f81\u662freentrant\uff0c\u5373\u5b83\u7684\u6267\u884c\u53ef\u80fd\u4f1a\u88absuspend\u800c\u540e\u88abresume\uff1a \u4e00\u65e6\u53d1\u751f\u4e86hardware interrupt\uff0cOS kernel\u4f1a\u7acb\u5373\u53bb\u54cd\u5e94\uff0c\u4ece\u800cinterrupt\uff08suspend\uff09\u5f53\u524d\u6267\u884c\u7684kernel control path\uff0c\u8f6c\u53bb\u6267\u884c\u65b0\u7684kernel control path\uff0c\u5373\u539fkernel control path\u4f1a\u88abinterrupted\u3002 task\u662f\u73b0\u4ee3OS\u4e3a\u652f\u6301 multitasking \u800c\u521b\u5efa\u7684\uff0c\u5b83\u7531 scheduler \u8fdb\u884c\u8c03\u5ea6\u6267\u884c\u7684\uff0c\u76ee\u524dlinux\u91c7\u53d6\u7684\u8c03\u5ea6\u7b56\u7565\u662f Preemptive multitasking \uff0c\u8fd9\u79cd\u7b56\u7565\u7684\u672c\u8d28\u662f\uff1a It is normally carried out by a privileged task or part of the system known as a preemptive scheduler , which has the power to preempt , or interrupt, and later resume, other tasks in the system. \u5373\u5b83\u53ef\u80fd\u4f1apreempt\uff08suspend\uff09\u6b63\u5728\u6267\u884c\u7684task\uff0c\u7136\u540e\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2atask\u3002 \u5982\u4f55\u5b9e\u73b0Reentrant\uff1f # \u663e\u7136\u8fd9\u662fOS\u4e3a\u4e86\u9ad8\u6548\uff0c\u8ba9\u591a\u4e2acontrol path interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\uff0c\u4e3a\u4e86\u5b9e\u73b0 Reentrancy \uff0c\u6bcf\u4e2acontrol path\u90fd\u8981\u6709\u81ea\u5df1private\u7684context\u3001address space\uff08\u8fd9\u5176\u5b9e\u662f\u4e00\u4e2aseparation\u673a\u5236\uff09\uff0c\u5b83\u80fd\u591f\u4fdd\u8bc1\u4e00\u4e2acontrol path\u5728\u88absuspend\u540e\uff0c\u8fc7\u540e\u80fd\u591f\u88abresume\u3002 \u663e\u7136context\u5305\u62ec\u6bcf\u4e2acontrol path\u7684private\u6570\u636e\uff0c\u5982\u4e0b\uff1a hardware context\uff1a Program counter \u6bcf\u5f53\u4e00\u4e2a\u6b63\u5728\u6267\u884c\u7684control path\u8981\u88absuspend\u4e4b\u524d\uff0c\u9700\u8981\u5c06\u5b83\u7684context\u7f6e\u4e8e\u5b83\u7684\u5f53\u524d\u6267\u884c\u5b83\u7684process\uff08linux \u7684lightweight process\uff0c\u800c\u4e0d\u662f\u6807\u51c6\u7684process\uff09\u7684 call stack \uff08\u53ef\u80fd\u662fKernel Mode process stack\uff0c\u4e5f\u53ef\u80fd\u662fUser Mode process stack\uff09\uff0c\u5728\u5b83\u88abrestart\u7684\u65f6\u5019\uff0c\u518d\u5c06\u4fdd\u5b58\u5728 call stack \u4e0a\u7684context\u6062\u590d\uff0c\u8fd9\u5c31\u6240\u8c13\u7684context switch\uff0c\u540e\u9762\u4f1a\u8fdb\u884c\u4e13\u95e8\u4ecb\u7ecd\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u8bc1\u636e\u6765\u6e90\u4e8e\uff1a chapter 4.1. The Role of Interrupt Signals \u9f99\u4e667.2.2 Activation Records Context switch # \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u672c\u8282\u6240\u8ff0\u7684context switch\u662f\u5e7f\u4e49\u7684\uff0c\u800c\u4e0d\u662f Computer multitasking \u4e2d\u4e13\u6307task\uff08process/thread\uff09\u7684 context switch \u3002 \u53d1\u751fcontext switch\u7684\u573a\u666f\uff1a Scheduler\u89e6\u53d1Process Switch # 3.3. Process Switch kernel substitutes one process for another process Interrupt Signals\u89e6\u53d1Switch # 4.1. The Role of Interrupt Signals the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). 4.3. Nested Execution of Exception and Interrupt Handlers \u601d\u8003\uff1acontext switch\u7684\u6210\u672c # \u4e0d\u540c\u7684control path\u8fdb\u884ccontext switch\u7684\u6210\u672c\u662f\u4e0d\u540c\u7684\uff0c \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7ecf\u5e38\u542c\u8bf4\u7684\u5c31\u662fthread\u7684context switch\u6bd4process\u7684context switch\u8981\u5feb\uff0c\u5c31\u662f\u8bf4\u7684\u8fd9\u4e2a\u9053\u7406\u3002 Control path context switch VS function call # control path\u7684context switch\u548cfunction call\u4e2d\u5c06 return state \u4fdd\u5b58\u5230 call stack \u5f85\u88ab\u8c03\u51fd\u6570\u8fd4\u56de\u540e\u518d\u8fdb\u884c\u6062\u590d\u7684\u505a\u6cd5\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\u3002 How kernel control path execute? # kernel control path \uff08\u6ce8\u610f\u4e0d\u662fcontrol path\uff09\u7684\u6267\u884c\u7ec6\u8282\u6bd4\u8f83\u590d\u6742\uff0c\u540e\u7eed\u9700\u8981\u8fdb\u884c\u8865\u5145\u3002 Kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662f\"kernel control path runs on behalf of process\"\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u603b\u7ed3 # \u901a\u8fc7control path\u6a21\u578b\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cOS\u5728\u8fd0\u884c\u548c\u63a7\u5236\u5b83\u4eec\u7684\u65f6\u5019\u4f1a\u9762\u4e34\u4e2d\u7c7b\u4f3c\u7684\u95ee\u9898\u3002","title":"Control-path-&-Context-&-Context-switch"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#control-path","text":"Control path\u8fd9\u4e2a\u6982\u5ff5\u662f\u6211\u7531kernel control path\u542f\u53d1\u800c\u521b\u5efa\u7684\uff0c\u5b83\u8868\u793aOS\u4e2d\u6240\u6709\u53ef\u80fd\u7684\u6d3b\u52a8/\u6267\u884c\u6d41\u7a0b\uff0c\u4e4b\u6240\u4ee5\u521b\u5efa\u8fd9\u4e2a\u6982\u5ff5\uff0c\u662f\u56e0\u4e3a\u5b83\u53ef\u4ee5\u65b9\u4fbf\u6211\u4eec\u6765\u7edf\u4e00\u5730\u3001\u6982\u62ec\u5730\u63cf\u8ff0\u4e00\u4e9b\u95ee\u9898\uff08\u4e00\u4e2a\u62bd\u8c61\u8fc7\u7a0b\uff09\u3002\u4e0e\u5b83\u6bd4\u8f83\u63a5\u8fd1\u7684\u4e00\u4e2a\u6982\u5ff5\u662f Control flow \u3002 OS\u4e2d\u6709\u5982\u4e0bcontrol path\uff1a kernel control path kernel thread task\uff08process/thread\uff0c\u73b0\u4ee3OS\u9700\u8981\u652f\u6301 multitasking \uff09 \u5728\u672c\u4e66\u7684\u6709\u4e9b\u7ae0\u8282\u4f1a\u4f7f\u7528\u201cexecution context\u201d\u3001\u201cexecution flow\u201d\u7b49\u8bcd\u8bed\uff0c\u5176\u5b9e\u5b83\u4eec\u548c\u672c\u6587\u6240\u5b9a\u4e49\u7684control path\u8868\u793a\u7684\u662f\u76f8\u540c\u7684\u610f\u601d\u3002 Control path\u7684\u5178\u578b\u7279\u5f81\u662freentrant\uff0c\u5373\u5b83\u7684\u6267\u884c\u53ef\u80fd\u4f1a\u88absuspend\u800c\u540e\u88abresume\uff1a \u4e00\u65e6\u53d1\u751f\u4e86hardware interrupt\uff0cOS kernel\u4f1a\u7acb\u5373\u53bb\u54cd\u5e94\uff0c\u4ece\u800cinterrupt\uff08suspend\uff09\u5f53\u524d\u6267\u884c\u7684kernel control path\uff0c\u8f6c\u53bb\u6267\u884c\u65b0\u7684kernel control path\uff0c\u5373\u539fkernel control path\u4f1a\u88abinterrupted\u3002 task\u662f\u73b0\u4ee3OS\u4e3a\u652f\u6301 multitasking \u800c\u521b\u5efa\u7684\uff0c\u5b83\u7531 scheduler \u8fdb\u884c\u8c03\u5ea6\u6267\u884c\u7684\uff0c\u76ee\u524dlinux\u91c7\u53d6\u7684\u8c03\u5ea6\u7b56\u7565\u662f Preemptive multitasking \uff0c\u8fd9\u79cd\u7b56\u7565\u7684\u672c\u8d28\u662f\uff1a It is normally carried out by a privileged task or part of the system known as a preemptive scheduler , which has the power to preempt , or interrupt, and later resume, other tasks in the system. \u5373\u5b83\u53ef\u80fd\u4f1apreempt\uff08suspend\uff09\u6b63\u5728\u6267\u884c\u7684task\uff0c\u7136\u540e\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2atask\u3002","title":"Control path"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#reentrant","text":"\u663e\u7136\u8fd9\u662fOS\u4e3a\u4e86\u9ad8\u6548\uff0c\u8ba9\u591a\u4e2acontrol path interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\uff0c\u4e3a\u4e86\u5b9e\u73b0 Reentrancy \uff0c\u6bcf\u4e2acontrol path\u90fd\u8981\u6709\u81ea\u5df1private\u7684context\u3001address space\uff08\u8fd9\u5176\u5b9e\u662f\u4e00\u4e2aseparation\u673a\u5236\uff09\uff0c\u5b83\u80fd\u591f\u4fdd\u8bc1\u4e00\u4e2acontrol path\u5728\u88absuspend\u540e\uff0c\u8fc7\u540e\u80fd\u591f\u88abresume\u3002 \u663e\u7136context\u5305\u62ec\u6bcf\u4e2acontrol path\u7684private\u6570\u636e\uff0c\u5982\u4e0b\uff1a hardware context\uff1a Program counter \u6bcf\u5f53\u4e00\u4e2a\u6b63\u5728\u6267\u884c\u7684control path\u8981\u88absuspend\u4e4b\u524d\uff0c\u9700\u8981\u5c06\u5b83\u7684context\u7f6e\u4e8e\u5b83\u7684\u5f53\u524d\u6267\u884c\u5b83\u7684process\uff08linux \u7684lightweight process\uff0c\u800c\u4e0d\u662f\u6807\u51c6\u7684process\uff09\u7684 call stack \uff08\u53ef\u80fd\u662fKernel Mode process stack\uff0c\u4e5f\u53ef\u80fd\u662fUser Mode process stack\uff09\uff0c\u5728\u5b83\u88abrestart\u7684\u65f6\u5019\uff0c\u518d\u5c06\u4fdd\u5b58\u5728 call stack \u4e0a\u7684context\u6062\u590d\uff0c\u8fd9\u5c31\u6240\u8c13\u7684context switch\uff0c\u540e\u9762\u4f1a\u8fdb\u884c\u4e13\u95e8\u4ecb\u7ecd\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u8bc1\u636e\u6765\u6e90\u4e8e\uff1a chapter 4.1. The Role of Interrupt Signals \u9f99\u4e667.2.2 Activation Records","title":"\u5982\u4f55\u5b9e\u73b0Reentrant\uff1f"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#context-switch","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u672c\u8282\u6240\u8ff0\u7684context switch\u662f\u5e7f\u4e49\u7684\uff0c\u800c\u4e0d\u662f Computer multitasking \u4e2d\u4e13\u6307task\uff08process/thread\uff09\u7684 context switch \u3002 \u53d1\u751fcontext switch\u7684\u573a\u666f\uff1a","title":"Context switch"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#schedulerprocess-switch","text":"3.3. Process Switch kernel substitutes one process for another process","title":"Scheduler\u89e6\u53d1Process Switch"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#interrupt-signalsswitch","text":"4.1. The Role of Interrupt Signals the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). 4.3. Nested Execution of Exception and Interrupt Handlers","title":"Interrupt Signals\u89e6\u53d1Switch"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#context-switch_1","text":"\u4e0d\u540c\u7684control path\u8fdb\u884ccontext switch\u7684\u6210\u672c\u662f\u4e0d\u540c\u7684\uff0c \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7ecf\u5e38\u542c\u8bf4\u7684\u5c31\u662fthread\u7684context switch\u6bd4process\u7684context switch\u8981\u5feb\uff0c\u5c31\u662f\u8bf4\u7684\u8fd9\u4e2a\u9053\u7406\u3002","title":"\u601d\u8003\uff1acontext switch\u7684\u6210\u672c"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#control-path-context-switch-vs-function-call","text":"control path\u7684context switch\u548cfunction call\u4e2d\u5c06 return state \u4fdd\u5b58\u5230 call stack \u5f85\u88ab\u8c03\u51fd\u6570\u8fd4\u56de\u540e\u518d\u8fdb\u884c\u6062\u590d\u7684\u505a\u6cd5\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\u3002","title":"Control path context switch VS function call"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#how-kernel-control-path-execute","text":"kernel control path \uff08\u6ce8\u610f\u4e0d\u662fcontrol path\uff09\u7684\u6267\u884c\u7ec6\u8282\u6bd4\u8f83\u590d\u6742\uff0c\u540e\u7eed\u9700\u8981\u8fdb\u884c\u8865\u5145\u3002 Kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662f\"kernel control path runs on behalf of process\"\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002","title":"How kernel control path execute?"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Control-path-&-Context-&-Context-switch/#_1","text":"\u901a\u8fc7control path\u6a21\u578b\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cOS\u5728\u8fd0\u884c\u548c\u63a7\u5236\u5b83\u4eec\u7684\u65f6\u5019\u4f1a\u9762\u4e34\u4e2d\u7c7b\u4f3c\u7684\u95ee\u9898\u3002","title":"\u603b\u7ed3"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Kernel-control-path-and-reentrant-kernel/","text":"Kernel control path and reentrant kernel # \u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81eachapter 1.6.3. Reentrant Kernels \u5728\u524d\u4e00\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5efa\u7acb\u8d77\u6765\u4e86Linux OS kernel\u7684\u8fd0\u884c\u6a21\u578b\u4e86\uff0c\u5373OS kernel\u662fevent-driven\u7684\uff0c\u90a3\u73b0\u5728\u8ba9\u6211\u4eec\u7ad9\u5728\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u5982\u4f55\u6765\u5b9e\u73b0\uff1f \u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2asystem call\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2asystem call\uff1b\u5f53\u8be5system call\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u4e00\u65e6\u51fa\u73b0\uff0c\u5219\u9700\u8981\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684interrupt handler\uff0c\u5f53\u8fd9\u4e2ainterrupt handler\u6267\u884c\u5b8c\u6210\u540e\uff0c\u518d\u91cd\u542f\u4e4b\u524d\u88ab\u4e2d\u65ad\u7684\u6d41\u7a0b\uff08\u662f\u5426\u4f1a\u91cd\u542f\u5176\u5b9e\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\uff0c\u540e\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u4e13\u95e8\u5206\u6790\uff09\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662fchapter 1.6.3. Reentrant Kernels\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002 Kernel control path # \u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\u601d\u8def\uff0c\u5728chapter 1.6.3. Reentrant Kernels\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684activity\uff0c\u5728 Linux-OS-kernel-is-event-driven \u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\uff0ckernel\u7684activity\u53ef\u80fd\u6709\u5982\u4e0b\u51e0\u79cd\u60c5\u51b5\u89e6\u53d1\uff1a system call interrupt and exception(\u5728Chapter 4. Interrupts and Exceptions\u533a\u5206\u8fd9\u4e24\u8005) \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff08\u6302\u8d77\uff09\uff0cresume\uff08\u91cd\u542f\uff09\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u4f7f\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728chapter 1.6.3. Reentrant Kernels\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83\u5bf9kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization See also # Kernel Control Path Definition","title":"Kernel-control-path-and-reentrant-kernel"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Kernel-control-path-and-reentrant-kernel/#kernel-control-path-and-reentrant-kernel","text":"\u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81eachapter 1.6.3. Reentrant Kernels \u5728\u524d\u4e00\u8282\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5efa\u7acb\u8d77\u6765\u4e86Linux OS kernel\u7684\u8fd0\u884c\u6a21\u578b\u4e86\uff0c\u5373OS kernel\u662fevent-driven\u7684\uff0c\u90a3\u73b0\u5728\u8ba9\u6211\u4eec\u7ad9\u5728\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u5982\u4f55\u6765\u5b9e\u73b0\uff1f \u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2asystem call\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2asystem call\uff1b\u5f53\u8be5system call\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u4e00\u65e6\u51fa\u73b0\uff0c\u5219\u9700\u8981\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684interrupt handler\uff0c\u5f53\u8fd9\u4e2ainterrupt handler\u6267\u884c\u5b8c\u6210\u540e\uff0c\u518d\u91cd\u542f\u4e4b\u524d\u88ab\u4e2d\u65ad\u7684\u6d41\u7a0b\uff08\u662f\u5426\u4f1a\u91cd\u542f\u5176\u5b9e\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\uff0c\u540e\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u4e13\u95e8\u5206\u6790\uff09\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662fchapter 1.6.3. Reentrant Kernels\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002","title":"Kernel control path and reentrant kernel"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Kernel-control-path-and-reentrant-kernel/#kernel-control-path","text":"\u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\u601d\u8def\uff0c\u5728chapter 1.6.3. Reentrant Kernels\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684activity\uff0c\u5728 Linux-OS-kernel-is-event-driven \u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\uff0ckernel\u7684activity\u53ef\u80fd\u6709\u5982\u4e0b\u51e0\u79cd\u60c5\u51b5\u89e6\u53d1\uff1a system call interrupt and exception(\u5728Chapter 4. Interrupts and Exceptions\u533a\u5206\u8fd9\u4e24\u8005) \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff08\u6302\u8d77\uff09\uff0cresume\uff08\u91cd\u542f\uff09\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u4f7f\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728chapter 1.6.3. Reentrant Kernels\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83\u5bf9kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization","title":"Kernel control path"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Kernel-control-path-and-reentrant-kernel/#see-also","text":"Kernel Control Path Definition","title":"See also"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Linux-OS-interrupt-and-interrupte-handler/","text":"Linux OS interrupt and interrupt handler # \u672c\u4e66\u4e2d\u96c6\u4e2d\u8bb2\u8ff0\u548cinterrupt\u7684\u7ae0\u8282\u5982\u4e0b\uff1a \u7ae0\u8282 \u4e3b\u8981\u5185\u5bb9 Chapter 4. Interrupts and Exceptions \u8bb2\u8ff0interrupt\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u662f\u540e\u7eed\u76f8\u5173\u7ae0\u8282\u7684\u57fa\u7840\u3002\u5185\u5bb9\u504f\u786c\u4ef6\u3002 Chapter 6. Timing Measurements \u8bb2\u8ff0hardware devices that underly timing\u3001time-related duties of the kernel Chapter 13. I/O Architecture and Device Drivers \u8bb2\u8ff0I/O devices \u672c\u6587\u7684\u5185\u5bb9\u662f\u5bf9\u8fd9\u4e9b\u7ae0\u8282\u7684\u5185\u5bb9\u7684\u68b3\u7406\u3002 Hardware\u901a\u8fc7interrupt\u6765\u901a\u77e5linux kernel\u3002\u5728Chapter 4. Interrupts and Exceptions\u4e2d\u5bf9interrupt\u8fdb\u884c\u4e86\u5206\u7c7b\uff1a \u6765\u6e90 Intel microprocessor manuals CPU control unit Synchronous interrupt exceptions Other hardware devices at arbitrary times with respect to the CPU clock signals, such as interval timers and I/O devices Asynchronous interrupt interrupts \u5bf9\u4e8ehardware\u7684\u77e5\u8bc6\uff0c\u6211\u4eec\u4e0d\u505a\u6df1\u5165\u5206\u6790\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8software\u90e8\u5206\uff0c\u5373\u7531interrupt\u6240\u89e6\u53d1\u7684kernel control path\uff08\u662fOS\u5c42\u7684interrupt handler\uff09\uff0c\u6211\u4eec\u5bf9\u4e00\u4e9b\u4e3b\u8981\u7684interrupt\u548c\u5176kernel control path\u8fdb\u884c\u603b\u7ed3\u3002","title":"Linux-OS-interrupt-and-interrupte-handler"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Linux-OS-interrupt-and-interrupte-handler/#linux-os-interrupt-and-interrupt-handler","text":"\u672c\u4e66\u4e2d\u96c6\u4e2d\u8bb2\u8ff0\u548cinterrupt\u7684\u7ae0\u8282\u5982\u4e0b\uff1a \u7ae0\u8282 \u4e3b\u8981\u5185\u5bb9 Chapter 4. Interrupts and Exceptions \u8bb2\u8ff0interrupt\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u662f\u540e\u7eed\u76f8\u5173\u7ae0\u8282\u7684\u57fa\u7840\u3002\u5185\u5bb9\u504f\u786c\u4ef6\u3002 Chapter 6. Timing Measurements \u8bb2\u8ff0hardware devices that underly timing\u3001time-related duties of the kernel Chapter 13. I/O Architecture and Device Drivers \u8bb2\u8ff0I/O devices \u672c\u6587\u7684\u5185\u5bb9\u662f\u5bf9\u8fd9\u4e9b\u7ae0\u8282\u7684\u5185\u5bb9\u7684\u68b3\u7406\u3002 Hardware\u901a\u8fc7interrupt\u6765\u901a\u77e5linux kernel\u3002\u5728Chapter 4. Interrupts and Exceptions\u4e2d\u5bf9interrupt\u8fdb\u884c\u4e86\u5206\u7c7b\uff1a \u6765\u6e90 Intel microprocessor manuals CPU control unit Synchronous interrupt exceptions Other hardware devices at arbitrary times with respect to the CPU clock signals, such as interval timers and I/O devices Asynchronous interrupt interrupts \u5bf9\u4e8ehardware\u7684\u77e5\u8bc6\uff0c\u6211\u4eec\u4e0d\u505a\u6df1\u5165\u5206\u6790\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8software\u90e8\u5206\uff0c\u5373\u7531interrupt\u6240\u89e6\u53d1\u7684kernel control path\uff08\u662fOS\u5c42\u7684interrupt handler\uff09\uff0c\u6211\u4eec\u5bf9\u4e00\u4e9b\u4e3b\u8981\u7684interrupt\u548c\u5176kernel control path\u8fdb\u884c\u603b\u7ed3\u3002","title":"Linux OS interrupt and interrupt handler"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Linux-OS-kernel-is-event-driven/","text":"Linux OS kernel is event-driven # \u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u672c\u4e66\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u5efa\u7acb\u8d77\u5bf9OS\u8fd0\u884c\u6982\u51b5\u7684\u9ad8\u5c4b\u5efa\u74f4\u7684\u3001\u6574\u4f53\u7684\u8ba4\u77e5\uff08big picture\uff09\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u68b3\u7406\u6e05\u695a\u4e66\u4e2d\u5404\u4e2a\u7ae0\u8282\u4e4b\u95f4\u7684\u5173\u8054\u3002 \u4ece\u4e00\u4e2asoftware engineer\u7684\u89c6\u89d2\u6765\u770b\uff0c\u6211\u89c9\u5f97OS kernel\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a event-driven system \uff0c\u5373\u6574\u4e2aOS kernel\u7684\u8fd0\u884c\u662f event \u9a71\u52a8\u7684\uff0clinux OS kernel\u7684\u5b9e\u73b0\u91c7\u7528\uff08\u90e8\u5206\uff09\u7684\u662f Event-driven architecture ( Event-driven programming )\u3002\u4e0b\u9762\u5bf9\u8fd9\u4e2a\u8bba\u65ad\u7684\u5206\u6790\uff1a \u5728\u672c\u4e66chapter 1.4. Basic Operating System Concepts\u6240\u4ecb\u7ecd\u7684\uff1a The operating system interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. A Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. \u663e\u7136\uff0cOS kernel\u76f4\u63a5\u548chardware\u6253\u4ea4\u9053\uff0c\u90a3\u6709\u54ea\u4e9bhardware\u5462\uff1f\u5982\u4e0b\uff1a I/O devices Chapter 13. I/O Architecture and Device Drivers \u8865\u5145\uff1a Operating System - I/O Hardware Clock and Timer Circuits Chapter 6. Timing Measurements \u76ee\u524d\uff0c\u57fa\u672c\u4e0a\u6240\u6709\u7684hardware\u90fd\u662f\u901a\u8fc7 interrupt \u6765\u901a\u77e5OS kernel\u7684\uff0c\u7136\u540e\u5176\u5bf9\u5e94\u7684 Interrupt handler \u4f1a\u88ab\u89e6\u53d1\u6267\u884c\uff0c\u4e5f\u5c31\u662fOS kernel\u662f interrupt-driven \u7684\u3002\u62e5\u6709\u8fd9\u6837\u7684\u8ba4\u77e5\u5bf9\u4e8e\u5b8c\u6574\u5730\u638c\u63e1\u672c\u4e66\u7684\u5185\u5bb9\u5341\u5206\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u63cf\u8ff0\u4e86OS kernel\u8fd0\u884c\u7684\u6982\u51b5\u3002 \u672c\u4e66\u7684Chapter 4. Interrupts and Exceptions\u4e13\u95e8\u63cf\u8ff0\u4e2d\u65ad\u76f8\u5173\u5185\u5bb9\uff0c\u5b83\u662f\u540e\u9762\u5f88\u591a\u7ae0\u8282\u7684\u57fa\u7840\uff0c\u56e0\u4e3aOS\u4e2d\u6709\u592a\u591a\u592a\u591a\u7684\u6d3b\u52a8\u90fd\u662finterrupt\u89e6\u53d1\u7684\uff0c\u6bd4\u5982\uff1a TODO: \u6b64\u5904\u6dfb\u52a0\u4e00\u4e9b\u4f8b\u5b50 Chapter 6. Timing Measurements\u4e3b\u8981\u63cf\u8ff0\u7684\u662ftiming measurements\u76f8\u5173\u7684hardware\uff08\u4e3b\u8981\u5305\u62ecClock and Timer Circuits\uff09\u4ee5\u53caOS kernel\u4e2d\u7531timing measurement\u9a71\u52a8\u7684\u91cd\u8981\u7684\u6d3b\u52a8\uff08\u4e0b\u9762\u4f1a\u6709\u4ecb\u7ecd\uff09\uff0c\u6b63\u5982\u672c\u7ae0\u5f00\u5934\u6240\u8ff0\uff1a Countless computerized activities are driven by timing measurements OS kernel\u7684\u4f17\u591a\u6838\u5fc3activity\u662fdriven by timing measurements\uff0c\u6b63\u59826.2. The Linux Timekeeping Architecture\u4e2d\u6240\u603b\u7ed3\u7684\uff1a Updates the time elapsed since system startup Updates the time and date Determines, for every CPU, how long the current process has been running, and preempts it if it has exceeded the time allocated to it. The allocation of time slots (also called \"quanta\") is discussed in Chapter 7. NOTE: \u8fd9\u4e2a\u6d3b\u52a8\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662fOS\u5b9e\u73b0 Time-sharing \uff0c\u8fdb\u800c\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5728linux kernel\u7684\u5b9e\u73b0\u4e2d\uff0c\u5b83\u7684\u5165\u53e3\u51fd\u6570\u662f scheduler_tick \uff0c\u641c\u7d22\u8fd9\u4e2a\u51fd\u6570\uff0c\u53ef\u4ee5\u67e5\u8be2\u5230\u975e\u5e38\u591a\u5173\u4e8e\u5b83\u7684\u5206\u6790\u3002 \u672c\u4e66\u4e2d\u5173\u4e8e\u8fd9\u4e2a\u51fd\u6570\u7684\u7ae0\u8282\uff1a 6.4. Updating System Statistics 7.4. Functions Used by the Scheduler Updates resource usage statistics. Checks whether the interval of time associated with each software timer (see the later section \"Software Timers and Delay Functions\") has elapsed. \u901a\u8fc7\u4e0a\u9762\u7684\u5185\u5bb9\u53ef\u4ee5\u770b\u5230\uff1atimer interrupt\u5bf9\u7cfb\u7edf\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u7cfb\u7edf\u7684heartbeat\uff0c\u5b83\u9a71\u52a8\u7740\u7cfb\u7edf\u7684\u8fd0\u8f6c\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u76f8\u540c\u7684\u7cfb\u7edf\u7684 Electric motor \uff08\u5185\u71c3\u673a\u8fd0\u8f6c\u5e26\u52a8\u6574\u4e2a\u7cfb\u7edf\u8fd0\u8f6c\u8d77\u6765\uff09\u3002 System call\u4e5f\u76f8\u5f53\u4e8einterrupt # \u4e0a\u9762\u4f7f\u7528\u7684\u662f\u201c\u76f8\u5f53\u4e8e\u201d\uff0c\u800c\u4e0d\u662f\u201c\u662f\u201d\uff0c\u8fd9\u662f\u56e0\u4e3a\u968f\u7740\u6280\u672f\u7684\u66f4\u65b0\u8fed\u4ee3\uff0c\u5b9e\u73b0system call\u7684assembly instruction\u4e5f\u5728\u8fdb\u884c\u66f4\u65b0\u8fed\u4ee3\uff0c\u53ef\u80fd\u539f\u6765\u4f7f\u7528\u7684\u4e2d\u65ad\u6307\u4ee4\uff08 int assembly instruction\uff09\u4f1a\u66ff\u6362\u4e3a\u66f4\u52a0\u9ad8\u6548\u7684assembly instruction\u3002\u572810.3. Entering and Exiting a System Call\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u8bf4\u660e\uff0c\u5982\u4e0b\uff1a Applications can invoke a system call in two different ways: By executing the int $0x80 assembly language instruction; in older versions of the Linux kernel, this was the only way to switch from User Mode to Kernel Mode. By executing the sysenter assembly language instruction, introduced in the Intel Pentium II microprocessors; this instruction is now supported by the Linux 2.6 kernel. \u4f7f\u7528 int $0x80 \u7684\u65b9\u5f0f\u662finterrupt\uff0c\u4f7f\u7528 sysenter \u7684\u65b9\u5f0f\u5219\u4e0d\u662finterrupt\uff0c\u4f46\u662f\u5b83\u7684\u4f5c\u7528\u5176\u5b9e\u548cinterrupt\u975e\u5e38\u7c7b\u4f3c\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u770b\u505a\u662finterrupt\u3002 \u5173\u4e8e sysenter \uff0c\u53c2\u52a0\uff1a https://wiki.osdev.org/Sysenter \u4e0a\u9762\u63cf\u8ff0\u7684interrupt\u4e3b\u8981\u6765\u81ea\u4e8ehardware\uff0c\u5176\u5b9esystem call\u7684\u5b9e\u73b0\u4e5f\u662f\u4f9d\u8d56\u4e8einterrupt\u3002 \u5728chapter 4.2. Interrupts and Exceptions\u7684\u201cProgrammed exceptions\u201d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event (see Chapter 10). \u603b\u7ed3 # \u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230OS kernel\u7684\u6240\u6709activity\u5176\u5b9e\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662fevent-driven\u7684\uff1aOS kernel\u7ba1\u7406\u7740hardware\u3001process\uff0c\u5b83\u4f5c\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u4e2d\u95f4\u5c42\uff0c\u53ef\u4ee5\u8ba4\u4e3aOS\u7684\u6240\u6709\u7684activity\u90fd\u662f\u7531\u5b83\u4eec\u89e6\u53d1\u7684\u3002 \u5efa\u7acb\u8fd9\u6837\u7684\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u5bf9\u4e8e\u540e\u9762\u8ba8\u8bbaOS kernel\u7684\u5b9e\u73b0\u601d\u8def\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002 \u6211\u4eec\u60ca\u559c\u7684\u53d1\u73b0\u7ad9\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u63cf\u8ff0\u672c\u8d28\u4e0a\u975e\u5e38\u7c7b\u4f3c\u7684\u4e8b\u52a1\u6709\u7740\u4e0d\u540c\u7684\u8bf4\u6cd5\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a Hardware Software Interrupt-driven Event-driven architecture / Event-driven programming Interrupt Event (computing) Interrupt handler / Interrupt service routine Event handler / Callback function \u5404\u79cdinterrupt\u5c31\u662f\u6240\u8c13\u7684event\u3002","title":"Linux-OS-kernel-is-event-driven"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Linux-OS-kernel-is-event-driven/#linux-os-kernel-is-event-driven","text":"\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u672c\u4e66\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u5efa\u7acb\u8d77\u5bf9OS\u8fd0\u884c\u6982\u51b5\u7684\u9ad8\u5c4b\u5efa\u74f4\u7684\u3001\u6574\u4f53\u7684\u8ba4\u77e5\uff08big picture\uff09\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u68b3\u7406\u6e05\u695a\u4e66\u4e2d\u5404\u4e2a\u7ae0\u8282\u4e4b\u95f4\u7684\u5173\u8054\u3002 \u4ece\u4e00\u4e2asoftware engineer\u7684\u89c6\u89d2\u6765\u770b\uff0c\u6211\u89c9\u5f97OS kernel\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a event-driven system \uff0c\u5373\u6574\u4e2aOS kernel\u7684\u8fd0\u884c\u662f event \u9a71\u52a8\u7684\uff0clinux OS kernel\u7684\u5b9e\u73b0\u91c7\u7528\uff08\u90e8\u5206\uff09\u7684\u662f Event-driven architecture ( Event-driven programming )\u3002\u4e0b\u9762\u5bf9\u8fd9\u4e2a\u8bba\u65ad\u7684\u5206\u6790\uff1a \u5728\u672c\u4e66chapter 1.4. Basic Operating System Concepts\u6240\u4ecb\u7ecd\u7684\uff1a The operating system interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. A Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. \u663e\u7136\uff0cOS kernel\u76f4\u63a5\u548chardware\u6253\u4ea4\u9053\uff0c\u90a3\u6709\u54ea\u4e9bhardware\u5462\uff1f\u5982\u4e0b\uff1a I/O devices Chapter 13. I/O Architecture and Device Drivers \u8865\u5145\uff1a Operating System - I/O Hardware Clock and Timer Circuits Chapter 6. Timing Measurements \u76ee\u524d\uff0c\u57fa\u672c\u4e0a\u6240\u6709\u7684hardware\u90fd\u662f\u901a\u8fc7 interrupt \u6765\u901a\u77e5OS kernel\u7684\uff0c\u7136\u540e\u5176\u5bf9\u5e94\u7684 Interrupt handler \u4f1a\u88ab\u89e6\u53d1\u6267\u884c\uff0c\u4e5f\u5c31\u662fOS kernel\u662f interrupt-driven \u7684\u3002\u62e5\u6709\u8fd9\u6837\u7684\u8ba4\u77e5\u5bf9\u4e8e\u5b8c\u6574\u5730\u638c\u63e1\u672c\u4e66\u7684\u5185\u5bb9\u5341\u5206\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u63cf\u8ff0\u4e86OS kernel\u8fd0\u884c\u7684\u6982\u51b5\u3002 \u672c\u4e66\u7684Chapter 4. Interrupts and Exceptions\u4e13\u95e8\u63cf\u8ff0\u4e2d\u65ad\u76f8\u5173\u5185\u5bb9\uff0c\u5b83\u662f\u540e\u9762\u5f88\u591a\u7ae0\u8282\u7684\u57fa\u7840\uff0c\u56e0\u4e3aOS\u4e2d\u6709\u592a\u591a\u592a\u591a\u7684\u6d3b\u52a8\u90fd\u662finterrupt\u89e6\u53d1\u7684\uff0c\u6bd4\u5982\uff1a TODO: \u6b64\u5904\u6dfb\u52a0\u4e00\u4e9b\u4f8b\u5b50 Chapter 6. Timing Measurements\u4e3b\u8981\u63cf\u8ff0\u7684\u662ftiming measurements\u76f8\u5173\u7684hardware\uff08\u4e3b\u8981\u5305\u62ecClock and Timer Circuits\uff09\u4ee5\u53caOS kernel\u4e2d\u7531timing measurement\u9a71\u52a8\u7684\u91cd\u8981\u7684\u6d3b\u52a8\uff08\u4e0b\u9762\u4f1a\u6709\u4ecb\u7ecd\uff09\uff0c\u6b63\u5982\u672c\u7ae0\u5f00\u5934\u6240\u8ff0\uff1a Countless computerized activities are driven by timing measurements OS kernel\u7684\u4f17\u591a\u6838\u5fc3activity\u662fdriven by timing measurements\uff0c\u6b63\u59826.2. The Linux Timekeeping Architecture\u4e2d\u6240\u603b\u7ed3\u7684\uff1a Updates the time elapsed since system startup Updates the time and date Determines, for every CPU, how long the current process has been running, and preempts it if it has exceeded the time allocated to it. The allocation of time slots (also called \"quanta\") is discussed in Chapter 7. NOTE: \u8fd9\u4e2a\u6d3b\u52a8\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662fOS\u5b9e\u73b0 Time-sharing \uff0c\u8fdb\u800c\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5728linux kernel\u7684\u5b9e\u73b0\u4e2d\uff0c\u5b83\u7684\u5165\u53e3\u51fd\u6570\u662f scheduler_tick \uff0c\u641c\u7d22\u8fd9\u4e2a\u51fd\u6570\uff0c\u53ef\u4ee5\u67e5\u8be2\u5230\u975e\u5e38\u591a\u5173\u4e8e\u5b83\u7684\u5206\u6790\u3002 \u672c\u4e66\u4e2d\u5173\u4e8e\u8fd9\u4e2a\u51fd\u6570\u7684\u7ae0\u8282\uff1a 6.4. Updating System Statistics 7.4. Functions Used by the Scheduler Updates resource usage statistics. Checks whether the interval of time associated with each software timer (see the later section \"Software Timers and Delay Functions\") has elapsed. \u901a\u8fc7\u4e0a\u9762\u7684\u5185\u5bb9\u53ef\u4ee5\u770b\u5230\uff1atimer interrupt\u5bf9\u7cfb\u7edf\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u7cfb\u7edf\u7684heartbeat\uff0c\u5b83\u9a71\u52a8\u7740\u7cfb\u7edf\u7684\u8fd0\u8f6c\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u76f8\u540c\u7684\u7cfb\u7edf\u7684 Electric motor \uff08\u5185\u71c3\u673a\u8fd0\u8f6c\u5e26\u52a8\u6574\u4e2a\u7cfb\u7edf\u8fd0\u8f6c\u8d77\u6765\uff09\u3002","title":"Linux OS kernel is event-driven"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Linux-OS-kernel-is-event-driven/#system-callinterrupt","text":"\u4e0a\u9762\u4f7f\u7528\u7684\u662f\u201c\u76f8\u5f53\u4e8e\u201d\uff0c\u800c\u4e0d\u662f\u201c\u662f\u201d\uff0c\u8fd9\u662f\u56e0\u4e3a\u968f\u7740\u6280\u672f\u7684\u66f4\u65b0\u8fed\u4ee3\uff0c\u5b9e\u73b0system call\u7684assembly instruction\u4e5f\u5728\u8fdb\u884c\u66f4\u65b0\u8fed\u4ee3\uff0c\u53ef\u80fd\u539f\u6765\u4f7f\u7528\u7684\u4e2d\u65ad\u6307\u4ee4\uff08 int assembly instruction\uff09\u4f1a\u66ff\u6362\u4e3a\u66f4\u52a0\u9ad8\u6548\u7684assembly instruction\u3002\u572810.3. Entering and Exiting a System Call\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u8bf4\u660e\uff0c\u5982\u4e0b\uff1a Applications can invoke a system call in two different ways: By executing the int $0x80 assembly language instruction; in older versions of the Linux kernel, this was the only way to switch from User Mode to Kernel Mode. By executing the sysenter assembly language instruction, introduced in the Intel Pentium II microprocessors; this instruction is now supported by the Linux 2.6 kernel. \u4f7f\u7528 int $0x80 \u7684\u65b9\u5f0f\u662finterrupt\uff0c\u4f7f\u7528 sysenter \u7684\u65b9\u5f0f\u5219\u4e0d\u662finterrupt\uff0c\u4f46\u662f\u5b83\u7684\u4f5c\u7528\u5176\u5b9e\u548cinterrupt\u975e\u5e38\u7c7b\u4f3c\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u770b\u505a\u662finterrupt\u3002 \u5173\u4e8e sysenter \uff0c\u53c2\u52a0\uff1a https://wiki.osdev.org/Sysenter \u4e0a\u9762\u63cf\u8ff0\u7684interrupt\u4e3b\u8981\u6765\u81ea\u4e8ehardware\uff0c\u5176\u5b9esystem call\u7684\u5b9e\u73b0\u4e5f\u662f\u4f9d\u8d56\u4e8einterrupt\u3002 \u5728chapter 4.2. Interrupts and Exceptions\u7684\u201cProgrammed exceptions\u201d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event (see Chapter 10).","title":"System call\u4e5f\u76f8\u5f53\u4e8einterrupt"},{"location":"Kernel/Guide/Linux-OS's-interaction-with-the-hardware/Linux-OS-kernel-is-event-driven/#_1","text":"\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230OS kernel\u7684\u6240\u6709activity\u5176\u5b9e\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662fevent-driven\u7684\uff1aOS kernel\u7ba1\u7406\u7740hardware\u3001process\uff0c\u5b83\u4f5c\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u4e2d\u95f4\u5c42\uff0c\u53ef\u4ee5\u8ba4\u4e3aOS\u7684\u6240\u6709\u7684activity\u90fd\u662f\u7531\u5b83\u4eec\u89e6\u53d1\u7684\u3002 \u5efa\u7acb\u8fd9\u6837\u7684\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u5bf9\u4e8e\u540e\u9762\u8ba8\u8bbaOS kernel\u7684\u5b9e\u73b0\u601d\u8def\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002 \u6211\u4eec\u60ca\u559c\u7684\u53d1\u73b0\u7ad9\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u63cf\u8ff0\u672c\u8d28\u4e0a\u975e\u5e38\u7c7b\u4f3c\u7684\u4e8b\u52a1\u6709\u7740\u4e0d\u540c\u7684\u8bf4\u6cd5\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a Hardware Software Interrupt-driven Event-driven architecture / Event-driven programming Interrupt Event (computing) Interrupt handler / Interrupt service routine Event handler / Callback function \u5404\u79cdinterrupt\u5c31\u662f\u6240\u8c13\u7684event\u3002","title":"\u603b\u7ed3"},{"location":"Kernel/Guide/Linux-OS's-multitasking/00-Multitask/","text":"Computer multitasking # multitasking \u5373\u591a\u4efb\u52a1\uff0c\u662f\u73b0\u4ee3OS\u7684\u5fc5\u5907feature\u3002\u672c\u7ae0\u5c31\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff08\u5176\u5b9e\u5728\u4e0a\u7bc7\u4e2d\u5df2\u7ecf\u6d89\u53ca\u5230\u4e86\u8fd9\u4e9b\u5185\u5bb9\u4e86\uff09\u3002 \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7 Computer multitasking \u603b\u7ed3\u5730\u975e\u5e38\u597d\uff0c\u4e0b\u9762\u662f\u6211\u7684\u9605\u8bfb\u7b14\u8bb0\uff1a \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5e94\u8be5\u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85multitask\u7684\u53d1\u5c55\uff0cmultitask\u662f\u4e00\u4e2a\u5f88\u65e9\u63d0\u51fa\u7684 \u6982\u5ff5 \uff1a In computing , multitasking is the concurrent execution of multiple tasks \u663e\u7136\u8fd9\u4e2a\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662ftask\u7684concurrent\uff08\u5e76\u53d1\uff09\u6267\u884c\u3002\u81f3\u4e8etask\u6240\u6307\u4e3a\u4f55\uff1f\u662fprocess\uff08\u8fdb\u7a0b\uff09\u8fd8\u662fthread\uff08\u7ebf\u7a0b\uff09\uff1f\u4e0d\u540c\u7684\u5b9e\u73b0\u80af\u5b9a\u7b54\u6848\u5c31\u4e0d\u540c\u4e86\u3002\u5728\u65e9\u671f\uff0cthread\u8fd8\u6ca1\u6709\u51fa\u73b0\u7684\u65f6\u5019\uff0c\u663e\u7136task\u6240\u6307\u4e3aprocess\u3002\u4f46\u662f\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86thread\u7684\u6982\u5ff5\uff0c\u5982\u679cOS\u7684\u5b9e\u73b0\u652f\u6301\u7684thread\u7684\u8bdd\uff0c\u90a3\u4e48task\u5c31\u53ef\u80fd\u662f\u6307thread\u4e86\uff08\u663e\u7136task\u662f\u4e00\u79cd\u62bd\u8c61\u7684\u63cf\u8ff0\uff0c\u7c7b\u4f3c\u4e8ekernel control path\uff09\u3002 \u5728\u672c\u6587\u7684 Multithreading \u7ae0\u8282\u5c31\u8bf4\u660e\u4e86\u8fd9\u79cd\u6f14\u8fdb\uff1a\u4eceprocess\u5230thread\u3002\u8fd9\u4e00\u6bb5\u7684\u8bba\u8ff0\u662f\u6bd4\u8f83\u597d\u7684\uff0c\u5b83\u8bf4\u660e\u4e86thread\u7684\u4ef7\u503c\u6240\u5728\u3002\u5728\u672c\u6587\u7684\u5f00\u5934\u4e5f\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\uff1a Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program. \u53e6\u5916\u4e00\u4e2a\u5173\u4e8emultitask\u9700\u8981\u8fdb\u884c\u5f3a\u8c03\u7684\u662f\uff1amultitask\u662foperating system\u5c42\u7684\u6982\u5ff5\uff0c\u5728hardware\u5c42\u6ca1\u6709multitask\u7684\u6982\u5ff5\uff0c\u6240\u4ee5multitask\u7531OS\u5382\u5546\u5b9e\u73b0\uff0c\u5728hardware\u5c42\u6bd4\u5982CPU\u538b\u6839\u5c31\u6ca1\u6709\u8fd9\u6837\u7684\u6982\u5ff5\u3002\u4e0d\u8fc7CPU\u5382\u5546\u80af\u5b9a\u4f1a\u4e3aOS\u63d0\u4f9b\u4fbf\u4e8e\u5b9e\u73b0multitask\u7684\u786c\u4ef6\u652f\u6301\uff0c\u6bd4\u5982\u63d0\u4f9b\u4e00\u4e9b\u4e13\u95e8\u7684\u6307\u4ee4\u7b49\u3002 \u5173\u4e8e multitasking \u5b9e\u73b0\u7684\u4e00\u4e9b\u601d\u8003 # OS\u4e3a\u4e86\u652f\u6301 multitasking \u90fd\u4f1a\u884d\u751f\u51fa\u4e00\u4e9b\u5217\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5b9e\u73b0 multitasking \u5f80\u5f80\u9700\u8981hardware\u548cOS\u540c\u65f6\u652f\u6301\uff1a \u95ee\u9898\u4e00\uff1a\u6b63\u5982 multitasking \u7684\u5b9a\u4e49\u6240\u652f\u6301\uff0cOS\u4e2d\u4f1a\u5b58\u5728\u591a\u4e2atask\uff0c\u90a3\u5982\u4f55\u4fdd\u8bc1task\u4e4b\u95f4\u5f7c\u6b64\u7684\u9694\u79bb\u3001\u4e92\u4e0d\u4fb5\u72af\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728\u672c\u6587\u7684 Memory protection \u8fdb\u884c\u4e86\u8bf4\u660e\uff0c\u5176\u5b9e\u6700\u6839\u672c\u7684\u63aa\u65bd\u662f\u6bcf\u4e2aprocess\u90fd\u6709\u81ea\u5df1\u7684 address space \u3002 \u95ee\u9898\u4e8c\uff1aoperating system's scheduler \u5982\u4f55\u5b9e\u73b0\u6765\u652f\u6301 multitasking \uff1f scheduler \u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u5728\u6b64\u6211\u4eec\u4ec5\u4ec5\u8ba8\u8bba scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u6839\u636e scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u53ef\u4ee5\u5c06 multitasking \u5206\u4e3a\u5982\u4e0b\u4e24\u79cd\uff1a pre-emptive multitasking cooperative multitasking \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u662f\u666e\u904d\u5b58\u5728\u7684\uff0c\u4e24\u8005\u5404\u6709\u5343\u79cb\uff0cOS\u7684\u5b9e\u73b0\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5176\u4e2d\u4efb\u610f\u4e00\u79cd\u3002 \u65e0\u8bba\u54ea\u79cd multitasking \uff0c\u5728\u8fdb\u884c\u8c03\u5ea6\u7684\u65f6\u5019\uff0c\u90fd\u6d89\u53ca context switch \u3002 Task\u662fcontrol path # \u5728 Control-path-&-Context-&-Context-switch \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5c06task\u5f52\u5165\u4e86control path\u7684\u8303\u8f74\uff0c\u5728\u4e0b\u4e00\u7ae0\u4e2d\uff0c\u5c06\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\u3002 Linux OS\u7684\u5b9e\u73b0 # \u5728\u672c\u4e66\u4e2d\uff0c\u5176\u5b9e\u5e76\u6ca1\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u6765\u63cf\u8ff0linux OS\u4e2dmultitask\u7684\u5b9e\u73b0\uff0c\u800c\u662f\u5206\u6563\u5728\u591a\u4e2a\u7ae0\u8282\u3002\u6240\u4ee5\u5728\u6b64\u5bf9linux OS\u7684multitask\u7684\u5b9e\u73b0\u8fdb\u884c\u7efc\u8ff0\u3002 \u4f7f\u7528task model\u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Multitask"},{"location":"Kernel/Guide/Linux-OS's-multitasking/00-Multitask/#computer-multitasking","text":"multitasking \u5373\u591a\u4efb\u52a1\uff0c\u662f\u73b0\u4ee3OS\u7684\u5fc5\u5907feature\u3002\u672c\u7ae0\u5c31\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff08\u5176\u5b9e\u5728\u4e0a\u7bc7\u4e2d\u5df2\u7ecf\u6d89\u53ca\u5230\u4e86\u8fd9\u4e9b\u5185\u5bb9\u4e86\uff09\u3002 \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7 Computer multitasking \u603b\u7ed3\u5730\u975e\u5e38\u597d\uff0c\u4e0b\u9762\u662f\u6211\u7684\u9605\u8bfb\u7b14\u8bb0\uff1a \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5e94\u8be5\u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85multitask\u7684\u53d1\u5c55\uff0cmultitask\u662f\u4e00\u4e2a\u5f88\u65e9\u63d0\u51fa\u7684 \u6982\u5ff5 \uff1a In computing , multitasking is the concurrent execution of multiple tasks \u663e\u7136\u8fd9\u4e2a\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662ftask\u7684concurrent\uff08\u5e76\u53d1\uff09\u6267\u884c\u3002\u81f3\u4e8etask\u6240\u6307\u4e3a\u4f55\uff1f\u662fprocess\uff08\u8fdb\u7a0b\uff09\u8fd8\u662fthread\uff08\u7ebf\u7a0b\uff09\uff1f\u4e0d\u540c\u7684\u5b9e\u73b0\u80af\u5b9a\u7b54\u6848\u5c31\u4e0d\u540c\u4e86\u3002\u5728\u65e9\u671f\uff0cthread\u8fd8\u6ca1\u6709\u51fa\u73b0\u7684\u65f6\u5019\uff0c\u663e\u7136task\u6240\u6307\u4e3aprocess\u3002\u4f46\u662f\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86thread\u7684\u6982\u5ff5\uff0c\u5982\u679cOS\u7684\u5b9e\u73b0\u652f\u6301\u7684thread\u7684\u8bdd\uff0c\u90a3\u4e48task\u5c31\u53ef\u80fd\u662f\u6307thread\u4e86\uff08\u663e\u7136task\u662f\u4e00\u79cd\u62bd\u8c61\u7684\u63cf\u8ff0\uff0c\u7c7b\u4f3c\u4e8ekernel control path\uff09\u3002 \u5728\u672c\u6587\u7684 Multithreading \u7ae0\u8282\u5c31\u8bf4\u660e\u4e86\u8fd9\u79cd\u6f14\u8fdb\uff1a\u4eceprocess\u5230thread\u3002\u8fd9\u4e00\u6bb5\u7684\u8bba\u8ff0\u662f\u6bd4\u8f83\u597d\u7684\uff0c\u5b83\u8bf4\u660e\u4e86thread\u7684\u4ef7\u503c\u6240\u5728\u3002\u5728\u672c\u6587\u7684\u5f00\u5934\u4e5f\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\uff1a Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program. \u53e6\u5916\u4e00\u4e2a\u5173\u4e8emultitask\u9700\u8981\u8fdb\u884c\u5f3a\u8c03\u7684\u662f\uff1amultitask\u662foperating system\u5c42\u7684\u6982\u5ff5\uff0c\u5728hardware\u5c42\u6ca1\u6709multitask\u7684\u6982\u5ff5\uff0c\u6240\u4ee5multitask\u7531OS\u5382\u5546\u5b9e\u73b0\uff0c\u5728hardware\u5c42\u6bd4\u5982CPU\u538b\u6839\u5c31\u6ca1\u6709\u8fd9\u6837\u7684\u6982\u5ff5\u3002\u4e0d\u8fc7CPU\u5382\u5546\u80af\u5b9a\u4f1a\u4e3aOS\u63d0\u4f9b\u4fbf\u4e8e\u5b9e\u73b0multitask\u7684\u786c\u4ef6\u652f\u6301\uff0c\u6bd4\u5982\u63d0\u4f9b\u4e00\u4e9b\u4e13\u95e8\u7684\u6307\u4ee4\u7b49\u3002","title":"Computer multitasking"},{"location":"Kernel/Guide/Linux-OS's-multitasking/00-Multitask/#multitasking","text":"OS\u4e3a\u4e86\u652f\u6301 multitasking \u90fd\u4f1a\u884d\u751f\u51fa\u4e00\u4e9b\u5217\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5b9e\u73b0 multitasking \u5f80\u5f80\u9700\u8981hardware\u548cOS\u540c\u65f6\u652f\u6301\uff1a \u95ee\u9898\u4e00\uff1a\u6b63\u5982 multitasking \u7684\u5b9a\u4e49\u6240\u652f\u6301\uff0cOS\u4e2d\u4f1a\u5b58\u5728\u591a\u4e2atask\uff0c\u90a3\u5982\u4f55\u4fdd\u8bc1task\u4e4b\u95f4\u5f7c\u6b64\u7684\u9694\u79bb\u3001\u4e92\u4e0d\u4fb5\u72af\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728\u672c\u6587\u7684 Memory protection \u8fdb\u884c\u4e86\u8bf4\u660e\uff0c\u5176\u5b9e\u6700\u6839\u672c\u7684\u63aa\u65bd\u662f\u6bcf\u4e2aprocess\u90fd\u6709\u81ea\u5df1\u7684 address space \u3002 \u95ee\u9898\u4e8c\uff1aoperating system's scheduler \u5982\u4f55\u5b9e\u73b0\u6765\u652f\u6301 multitasking \uff1f scheduler \u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u5728\u6b64\u6211\u4eec\u4ec5\u4ec5\u8ba8\u8bba scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u6839\u636e scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u53ef\u4ee5\u5c06 multitasking \u5206\u4e3a\u5982\u4e0b\u4e24\u79cd\uff1a pre-emptive multitasking cooperative multitasking \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u662f\u666e\u904d\u5b58\u5728\u7684\uff0c\u4e24\u8005\u5404\u6709\u5343\u79cb\uff0cOS\u7684\u5b9e\u73b0\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5176\u4e2d\u4efb\u610f\u4e00\u79cd\u3002 \u65e0\u8bba\u54ea\u79cd multitasking \uff0c\u5728\u8fdb\u884c\u8c03\u5ea6\u7684\u65f6\u5019\uff0c\u90fd\u6d89\u53ca context switch \u3002","title":"\u5173\u4e8emultitasking\u5b9e\u73b0\u7684\u4e00\u4e9b\u601d\u8003"},{"location":"Kernel/Guide/Linux-OS's-multitasking/00-Multitask/#taskcontrol-path","text":"\u5728 Control-path-&-Context-&-Context-switch \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5c06task\u5f52\u5165\u4e86control path\u7684\u8303\u8f74\uff0c\u5728\u4e0b\u4e00\u7ae0\u4e2d\uff0c\u5c06\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\u3002","title":"Task\u662fcontrol path"},{"location":"Kernel/Guide/Linux-OS's-multitasking/00-Multitask/#linux-os","text":"\u5728\u672c\u4e66\u4e2d\uff0c\u5176\u5b9e\u5e76\u6ca1\u6709\u4e13\u95e8\u7684\u7ae0\u8282\u6765\u63cf\u8ff0linux OS\u4e2dmultitask\u7684\u5b9e\u73b0\uff0c\u800c\u662f\u5206\u6563\u5728\u591a\u4e2a\u7ae0\u8282\u3002\u6240\u4ee5\u5728\u6b64\u5bf9linux OS\u7684multitask\u7684\u5b9e\u73b0\u8fdb\u884c\u7efc\u8ff0\u3002 \u4f7f\u7528task model\u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Linux OS\u7684\u5b9e\u73b0"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/","text":"Process mode: run time environment # \u672c\u6587\u7684\u5185\u5bb9\u57fa\u4e8e\uff1a \u4e0a\u4e00\u7bc7 \u9f99\u4e66 Chapter 7 Run-Time Environments \u7ef4\u57fa\u767e\u79d1ABI \u672c\u6587\u5c06\u5bf9process\u7684run-time\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u5bf9process model\u8fdb\u884c\u66f4\u52a0\u6df1\u5165\u7684\u5206\u6790\u3002 \u672c\u6587\u7684\u6807\u9898\u662f\u53c2\u8003\u81ea\u9f99\u4e66 Chapter 7 Run-Time Environments \uff0c\u672c\u6587\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u57fa\u4e8e\u9f99\u4e66 Chapter 7 Run-Time Environments \u7684\u5185\u5bb9\uff0c\u5728\u5b83\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6269\u5145\u3002\u9f99\u4e66\u6b63\u5982\u5176\u540d\uff08\u539f\u6587\u539f\u540d\uff09\uff0c\u5b83\u6240\u8ff0\u7684\u662f\u539f\u7406\uff0c\u5b83\u6240\u8bb2\u8ff0\u7684\u662f \u6982\u5ff5\u6a21\u578b \uff08\u662f\u4e00\u4e2a\u7b80\u5316\u7684\u6a21\u578b\uff0c\u6ca1\u6709\u8003\u8651multi-thread\u7b49\uff09\uff0c\u5b9e\u9645\u7684\u5b9e\u73b0\u80af\u5b9a\u662f\u57fa\u4e8e\u8be5\u6982\u5ff5\u6a21\u578b\u7684\uff0c\u4f46\u662f\u9700\u8981\u8003\u8651\u7684\u5f88\u591a\u5176\u4ed6\u5143\u7d20\uff0c\u5982\uff1a multi-thread ......\uff08TODO:\u8fd8\u8981\u4e00\u4e9b\u5176\u4ed6\u7684\u56e0\u7d20\uff09 \u672c\u6587\u6240\u6269\u5145\u7684\u662f\uff1a \u4e00\u4e9b\u5b9e\u73b0\u76f8\u5173\u7684\u7ec6\u8282\uff0c\u5982ABI\u7b49 \u6dfb\u52a0multi-thread\u76f8\u5173\u7684\u5185\u5bb9 \u603b\u7684\u6765\u8bf4\uff0c\u672c\u6587\u7684\u76ee\u6807\u662f\u5bf9process\u7684run-time\u8fdb\u884c\u5206\u6790\uff0c\u6784\u5efa\u8d77\u66f4\u52a0\u5b8c\u6574\u7684process model\u3002 Thread run model: function as user-defined action # Thread\u662f\u8c03\u5ea6\u5355\u4f4d\uff0c\u5373\u6bcf\u4e2athread\u90fd\u80fd\u591f\u72ec\u7acb\u6267\u884c\uff0c\u90a3thread\u662f\u5982\u4f55\u6267\u884c\u7684\u5462\uff1f\u8fd9\u5c31\u662f\u672c\u8282\u6240\u8981\u8fdb\u884c\u8ba8\u8bba\u7684\u3002 \u5728\u6587\u7ae0 Unit \u4e2d\uff0c\u901a\u8fc7\u5206\u6790\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86thread\u7684unit of user-defined action \u662ffunction\u3002 Languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack. Each time a procedure is called, space for its local variables is pushed onto a stack, and when the procedure terminates, that space is popped off the stack. As we shall see, this arrangement not only allows space to be shared by procedure calls whose durations do not overlap in time, but it allows us to compile code for a procedure in such a way that the relative addresses of its nonlocal variables are always the same, regardless of the sequence of procedure calls. Function\uff08\u5305\u62ec\u6210\u5458\u51fd\u6570\uff09\u662f\u5f88\u591a\u73b0\u4ee3programming language\u90fd\u4f1a\u63d0\u4f9b\u7684\u4e00\u4e2a\u6982\u5ff5\uff08\u53c2\u89c1\u6587\u7ae0 Abstraction \uff09\uff0c\u6bd4\u5982 C++ \u3001python\uff0c\u5bf9\u4e8eSQL\u8fd9\u79cd\u8bed\u8a00\u662f\u4e0d\u5b58\u5728\u7684\u3002 Thread\u7684unit of user-defined action \u662ffunction\uff0c\u90a3function\u7684\u6267\u884c\u9700\u8981\u54ea\u4e9b\u914d\u7f6e\u5462\uff1f\u7b54\u6848\u5982\u4e0b\uff1a Call stack Program counter Stack pointer \u663e\u7136\uff0c\u6bcf\u4e2athread\u90fd\u9700\u8981\u6709\u81ea\u5df1\u7684\u72ec\u7acb\u7684\u4e00\u4efd\u8fd9\u6837\u7684\u914d\u7f6e\uff0cthread\u7684 thread control block \u9700\u8981\u4fdd\u5b58\u8fd9\u4e9b\u5185\u5bb9\u3002 \u6bcf\u4e2athread\u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u72ec\u7acb\u7684call stack\uff0cfunction\u7684\u8fd0\u884c\u90fd\u662f\u53d1\u751f\u5728call stack\u4e0a\uff0c\u6bcf\u6b21\u8c03\u7528function\uff0c\u5219\u5165\u6808\uff0c \u51fd\u6570\u8fd0\u884c\u7ed3\u675f\uff0c\u5219\u51fa\u6808\uff0c\u8fd9\u5c31\u662fthread\u7684\u8fd0\u884c\u6a21\u578b\u3002 call stack # TODO \u6b64\u6bb5\u5bf9call stack\u76f8\u5173\u7684ABI\u8fdb\u884c\u63cf\u8ff0\uff0c\u7531\u6b64\u5f15\u51fa\u63a7\u5236\u6d41\u3001\u4f20\u53c2\u3001\u7b49\u7b49\u4e00\u7cfb\u5217\u95ee\u9898\u3002 \u5173\u4e8e\u51fd\u6570\u8c03\u7528\uff0c\u4e0b\u9762\u5185\u5bb9\u662f\u9700\u8981\u8fdb\u884c\u8865\u5145\u7684\uff1a Entry point Function prologue Housekeeping (computing) Subroutine linux OS process model\u7684\u5b9e\u73b0 # \u4e4b\u524d\u6211\u4e00\u76f4\u6709\u4e00\u4e2a\u7591\u95ee\u5c31\u662f\uff1a\u4e00\u4e2aprocess\u7684\u6240\u6709\u7684thread\u90fd\u5171\u4eab\u8be5process\u7684address space\uff0c\u800c\u6bcf\u4e2athread\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 call stack \uff0c\u5e76\u4e14call stack\u662f\u5411\u4e0b\u751f\u957f\u7684\uff0c\u5f53\u65f6\u6211\u5c31\u975e\u5e38\u7591\u60d1\uff0c\u8fd9\u8981\u5982\u4f55\u5b9e\u73b0\u5440\uff1f\u4eca\u5929\u5728\u9605\u8bfb Call stack \u3001 Stack register \u7684\u65f6\u5019\uff0c\u6211\u6709\u4e86\u5982\u4e0b\u7684\u8ba4\u77e5\uff1a \u51fd\u6570\u8c03\u7528\u6240\u4f7f\u7528\u7684\u662fJMP\u6307\u4ee4 x86\u6709segment register\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u6307\u5b9acall stack \u5176\u5b9ecall stack\u5c31\u662f\u4e00\u7247\u5185\u5b58\u533a\u57df\u800c\u5df2\uff0c\u53ea\u8981\u6307\u5b9a\u4e00\u7247\u5185\u5b58\u533a\u57df\u4f5c\u4e3acall stack\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528calling convention\u6765\u5b9e\u73b0\u51fd\u6570\u8c03\u7528\u4e86\u3002\u5b9e\u73b0\u51fd\u6570\u8c03\u7528\u3001\u6267\u884c\u7684\u6307\u4ee4\u662f\u4e0e\u8fd9\u7247\u5185\u5b58\u533a\u57df\u5728\u4f55\u5904\u65e0\u5173\u7684\uff0c\u6240\u4ee5\u7528\u6237\u662f\u53ef\u4ee5\u6307\u5b9a\u4efb\u610f\u7684\u3001\u5408\u6cd5\u7684\u5185\u5b58\u533a\u57df\u6765\u4f5c\u4e3acall stack\u7684\u3002 \u6240\u4ee5\u6211\u5c31\u53bb\u770b\u4e86 pthread_create \u7684\u6587\u6863\uff0c\u5176\u4e2d\u662f\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\u7684\uff1a On Linux/x86-32, the default stack size for a new thread is 2 megabytes. Under the NPTL threading implementation, if the RLIMIT_STACK soft resource limit at the time the program started has any value other than \"unlimited\", then it determines the default stack size of new threads. Using pthread_attr_setstacksize (3), the stack size attribute can be explicitly set in the attr argument used to create a thread, in order to obtain a stack size other than the default. \u5373\u65b0\u521b\u5efa\u7684thread\u7684\u9ed8\u8ba4\u7684call stack\u7684\u5927\u5c0f\u9ed8\u8ba4\u662f2M\uff0c\u8fd9\u8bf4\u660e\u662f\u53ef\u4ee5\u7531\u7528\u6237\u4e86\u6765\u6307\u5b9a\u65b0\u521b\u5efa\u7684thread\u7684call stack\u7684\uff0c\u6211\u4eec\u77e5\u9053\uff0c pthread_create \u6700\u7ec8\u662f\u901a\u8fc7\u8c03\u7528 clone(2) \uff0c\u8be5\u51fd\u6570\u7684\u7b2c\u4e8c\u4e2a\u5165\u53c2\u5c31\u662f\u7531\u7528\u6237\u6765\u6307\u5b9a\u8be5lightweight process\u7684call stack\u7684\u3002 \u770b\u5230\u4e86\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c \u5176\u5b9e\u6211\u53c8\u60f3\u5230\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4e00\u4e2a\u51fd\u6570\uff0c\u5982\u679c\u58f0\u660e\u7684\u81ea\u52a8\u53d8\u91cf\u5927\u5c0f\u8d85\u8fc7\u4e86call stack\u7684\u5927\u5c0f\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53c2\u89c1\uff1a https://www.cnblogs.com/zmlctt/p/3987181.html https://blog.csdn.net/zDavid_2018/article/details/89255630 \u7ef4\u57fa\u767e\u79d1\u7684 Stack overflow \u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u9f99\u4e66\u7684chapter 7\u603b\u7ed3\u7684\u662f\u975e\u5e38\u597d\u7684\u3002 Process run model # \u6709\u4e86\u524d\u9762\u7684thread run model\uff0c\u90a3\u4e48process\u7684run model\u5c31\u76f8\u5bf9\u6bd4\u8f83\u597d\u5206\u6790\u4e86\u3002\u663e\u7136\u4e00\u4e2aprocess\u6709\u591a\u4e2aprocess\u7ec4\u6210\uff0c\u591a\u4e2athread\u72ec\u7acb\u8fdb\u884c\u8fd0\u884c\uff0c\u5171\u4eabprocess\u7684resource\u3002 \u5982\u4f55\u6765\u5b9e\u73b0\uff1f # \u5982\u4f55\u5229\u7528\u786c\u4ef6\u6765\u5b9e\u73b0\u4e0a\u8ff0model\u3002 \u672c\u6587\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002\u672c\u6587\u63cf\u8ff0process\u7684\u8fd0\u884c\u6a21\u578b\u3002 \u7f16\u5199\u601d\u8def\uff1a\u4ece\u8fdb\u7a0b\u7684\u8fd0\u884c\u5f62\u6001\u4f5c\u4e3a\u5207\u5165\u70b9\uff1a \u76ee\u524d\u7684\u6240\u6709\u7684hight level programming language\u90fd\u5c06\u51fd\u6570\u4f5c\u4e3a\u7a0b\u5e8f\u4e2duser-defined action\u7684\u5355\u4f4d\uff08\u5728hardware\u5c42\uff0c\u663e\u7136user-defined action\u7684\u5355\u4f4d\u662finstruction\uff09\uff0c\u8fd9\u53ef\u4ee5\u4f5c\u4e3a\u7a0b\u5e8f\u7684\u8fd0\u884c\u6a21\u578b\uff08\u5bf9\u4e8e\u8be5\u8fd0\u884c\u6a21\u578b\u662f\u8fd8\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4fee\u6b63\u7684\uff0c\u5b83\u7684\u6700\u5c0f\u7c92\u5ea6\u7684user-defined action\u5176\u5b9e\u662f\u8bed\u53e5\uff0c\u4f46\u662f\u8fd9\u79cd\u8fd0\u884c\u6a21\u578b\u662f\u66f4\u52a0\u7b26\u5408call stack\u7684\uff09\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u8fd0\u884c\u6a21\u578b\uff0c\u4f7f\u7528\u7684\u7ed3\u6784\u662fstack\uff0ccall stack\u3002\u7531\u6b64\u5c31\u5f15\u51fa\u4e86\u4e00\u4e9b\u5217\u7684\u95ee\u9898\uff1a\u51fd\u6570\u4f20\u53c2\u5982\u4f55\u5b9e\u73b0\u7b49\u7b49\uff0c\u7531\u6b64\u5c31\u5f15\u51fa\u4e86calling convention\u3002 \u5176\u5b9e\u6211\u7684\u8fd9\u4e2a\u63cf\u8ff0\u601d\u8def\u662f\u548c\u9f99\u4e66\u76847.2 Stack Allocation of Space\u7684\u63cf\u8ff0\u601d\u8def\u7c7b\u4f3c\u7684\u3002 \u5c06\u6240\u6709\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\u96c6\u4e2d\u5230\u8fd9\u91cc\u6765\u8fdb\u884c\uff0c\u5305\u62ec\uff1a ABI\u3002 \u9f99\u4e66chapter 7\u6240\u8bba\u8ff0\u7684concept model\u3002 \u672c\u7ae0\u7684\u5185\u5bb9\u662f\u4e3b\u8981\u6e90\u81ea\u9f99\u4e66\u7684 Chapter 7 Run-Time Environments \uff0c\u539f\u6587\u5185\u5bb9\u662f\u975e\u5e38\u597d\u7684\uff0c\u4e3a\u6211\u4eec\u6e05\u6670\u5730\u52fe\u753b\u51fa\u4e86process\u7684memory\u3001runtime\u7684concept model\u3002 \u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684memory model # \u9f99\u4e66\uff0cOS\u4e66\u3001\u7ef4\u57fa\u767e\u79d1 \u903b\u8f91\u4e0e\u5b9e\u73b0\u3002 \u9f99\u4e66\u6b63\u5982\u5176\u540d\uff0c\u5b83\u6240\u8ff0\u7684\u662f\u539f\u7406\uff0c\u6240\u4ee5\u5b83\u6240\u8bb2\u8ff0\u7684\u662f\u6982\u5ff5\u6a21\u578b\u3002 stack\u662fprocess\u6d3b\u52a8\u7684\u573a\u6240\uff0c\u6240\u4ee5\u5b83\u662fmemory management\u7684\u5173\u952e\u6240\u5728\u3002 call stack \u53c8\u79f0\u4e3a control stack\uff0c\u6240\u4ee5\u5b83\u4e5f\u4f53\u73b0\u4e86\u5b83\u4e0e program counter \uff0c flow of control \u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u5f80\u66f4\u52a0\u5bbd\u6cdb\u6765\u6240\uff0c\u5176\u5b9e\u662fapplication binary interface\uff0c\u56e0\u4e3aprograming language\u7684\u5f88\u591a\u4e1c\u897f\u6700\u7ec8\u90fd\u9700\u8981\u7ffb\u8bd1\u4e3a\u6307\u4ee4\uff0c\u800capplication binary interface\u5219\u662f\u8fd9\u7c7b\u573a\u666f\u7684\u603b\u4f53\u63cf\u8ff0\u3002 \u51fd\u6570\u8c03\u7528\u5bf9\u5e94\u7684\u662fJMP\u6307\u4ee4\uff0c\u90a3\u58f0\u660e\u4e00\u4e2a int \u7c7b\u578b\u7684\u53d8\u91cf\u5bf9\u5e94\u7684\u662f\u4ec0\u4e48\u6307\u4ee4\u5462\uff1f\u4e0e\u6b64\u7c7b\u4f3c\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1a\u51fd\u6570\u8c03\u7528\u7684\u65f6\u5019\uff0c\u9700\u8981\u5206\u914d\u6808\u7a7a\u95f4\uff0c\u90a3\u8fd9\u662f\u5982\u4f55\u5b9e\u73b0\u7684\uff1f push \u6307\u4ee4\u5c31\u53ef\u4ee5\u5b9e\u73b0 process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u8981\u6d3b\u52a8\u5176\u5b9e\u5c31\u662f\u4e0d\u65ad\u5730\u51fd\u6570\u8c03\u7528\uff0c\u6240\u4ee5\u641e\u6e05\u695a\u51fd\u6570\u8c03\u7528\u7684\u8fc7\u7a0b\u5bf9\u7406\u89e3process\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002\u9f99\u4e66\u7684chapter 7\u5c31\u662f\u4ecb\u7ecd\u6b64\u7684\u975e\u5e38\u597d\u7684\u5185\u5bb9\u3002\u8fd9\u4e9b\u5185\u5bb9\u6211\u89c9\u5f97\u5168\u90e8\u90fd\u6574\u7406\u5230OS book\u4e2d\u53bb\u3002 \u8fdb\u7a0b\u8fd0\u884c\u5f62\u6001 # \u8fdb\u7a0b\u5b8c\u5168\u662f\u57fa\u4e8efunction\u7684\u8fd0\u884c\u6a21\u5f0f\uff0c\u5b83\u7684\u6240\u6709\u6d3b\u52a8\u90fd\u53d1\u751f\u5728call stack\u4e0a\u3002 \u5728\u8fdb\u5165\u51fd\u6570\u4e4b\u524d\uff0c\u5982\u4f55\u5f97\u77e5\u8981\u7533\u8bf7\u591a\u5c11\u6808\u7a7a\u95f4\uff1f\u5e94\u8be5\u4e0d\u662f\u63d0\u524d\u4e00\u6b21\u6027\u7533\u8bf7\u8be5\u51fd\u6570\u6240\u9700\u8981\u7684\u6240\u6709\u7684\u6808\u7a7a\u95f4\uff0c\u800c\u662f\u8fd0\u884c\u5230\u8be5\u6307\u4ee4\u7684\u65f6\u5019\uff0c\u624d\u5728\u6808\u4e0a\u5206\u914d\u7a7a\u95f4\u3002\u8fd9\u8ba9\u6211\u60f3\u5230\u4e86stored-program\u601d\u60f3\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\u5728\u9f99\u4e66\u76847.2 Stack Allocation of Space\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Almost all compilers for languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack. Each time a procedure is called, space for its local variables is pushed onto a stack, and when the procedure terminates, that space is popped off the stack.","title":"Process-mode-run-time-environment"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#process-mode-run-time-environment","text":"\u672c\u6587\u7684\u5185\u5bb9\u57fa\u4e8e\uff1a \u4e0a\u4e00\u7bc7 \u9f99\u4e66 Chapter 7 Run-Time Environments \u7ef4\u57fa\u767e\u79d1ABI \u672c\u6587\u5c06\u5bf9process\u7684run-time\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u5bf9process model\u8fdb\u884c\u66f4\u52a0\u6df1\u5165\u7684\u5206\u6790\u3002 \u672c\u6587\u7684\u6807\u9898\u662f\u53c2\u8003\u81ea\u9f99\u4e66 Chapter 7 Run-Time Environments \uff0c\u672c\u6587\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u57fa\u4e8e\u9f99\u4e66 Chapter 7 Run-Time Environments \u7684\u5185\u5bb9\uff0c\u5728\u5b83\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6269\u5145\u3002\u9f99\u4e66\u6b63\u5982\u5176\u540d\uff08\u539f\u6587\u539f\u540d\uff09\uff0c\u5b83\u6240\u8ff0\u7684\u662f\u539f\u7406\uff0c\u5b83\u6240\u8bb2\u8ff0\u7684\u662f \u6982\u5ff5\u6a21\u578b \uff08\u662f\u4e00\u4e2a\u7b80\u5316\u7684\u6a21\u578b\uff0c\u6ca1\u6709\u8003\u8651multi-thread\u7b49\uff09\uff0c\u5b9e\u9645\u7684\u5b9e\u73b0\u80af\u5b9a\u662f\u57fa\u4e8e\u8be5\u6982\u5ff5\u6a21\u578b\u7684\uff0c\u4f46\u662f\u9700\u8981\u8003\u8651\u7684\u5f88\u591a\u5176\u4ed6\u5143\u7d20\uff0c\u5982\uff1a multi-thread ......\uff08TODO:\u8fd8\u8981\u4e00\u4e9b\u5176\u4ed6\u7684\u56e0\u7d20\uff09 \u672c\u6587\u6240\u6269\u5145\u7684\u662f\uff1a \u4e00\u4e9b\u5b9e\u73b0\u76f8\u5173\u7684\u7ec6\u8282\uff0c\u5982ABI\u7b49 \u6dfb\u52a0multi-thread\u76f8\u5173\u7684\u5185\u5bb9 \u603b\u7684\u6765\u8bf4\uff0c\u672c\u6587\u7684\u76ee\u6807\u662f\u5bf9process\u7684run-time\u8fdb\u884c\u5206\u6790\uff0c\u6784\u5efa\u8d77\u66f4\u52a0\u5b8c\u6574\u7684process model\u3002","title":"Process mode: run time environment"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#thread-run-model-function-as-user-defined-action","text":"Thread\u662f\u8c03\u5ea6\u5355\u4f4d\uff0c\u5373\u6bcf\u4e2athread\u90fd\u80fd\u591f\u72ec\u7acb\u6267\u884c\uff0c\u90a3thread\u662f\u5982\u4f55\u6267\u884c\u7684\u5462\uff1f\u8fd9\u5c31\u662f\u672c\u8282\u6240\u8981\u8fdb\u884c\u8ba8\u8bba\u7684\u3002 \u5728\u6587\u7ae0 Unit \u4e2d\uff0c\u901a\u8fc7\u5206\u6790\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86thread\u7684unit of user-defined action \u662ffunction\u3002 Languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack. Each time a procedure is called, space for its local variables is pushed onto a stack, and when the procedure terminates, that space is popped off the stack. As we shall see, this arrangement not only allows space to be shared by procedure calls whose durations do not overlap in time, but it allows us to compile code for a procedure in such a way that the relative addresses of its nonlocal variables are always the same, regardless of the sequence of procedure calls. Function\uff08\u5305\u62ec\u6210\u5458\u51fd\u6570\uff09\u662f\u5f88\u591a\u73b0\u4ee3programming language\u90fd\u4f1a\u63d0\u4f9b\u7684\u4e00\u4e2a\u6982\u5ff5\uff08\u53c2\u89c1\u6587\u7ae0 Abstraction \uff09\uff0c\u6bd4\u5982 C++ \u3001python\uff0c\u5bf9\u4e8eSQL\u8fd9\u79cd\u8bed\u8a00\u662f\u4e0d\u5b58\u5728\u7684\u3002 Thread\u7684unit of user-defined action \u662ffunction\uff0c\u90a3function\u7684\u6267\u884c\u9700\u8981\u54ea\u4e9b\u914d\u7f6e\u5462\uff1f\u7b54\u6848\u5982\u4e0b\uff1a Call stack Program counter Stack pointer \u663e\u7136\uff0c\u6bcf\u4e2athread\u90fd\u9700\u8981\u6709\u81ea\u5df1\u7684\u72ec\u7acb\u7684\u4e00\u4efd\u8fd9\u6837\u7684\u914d\u7f6e\uff0cthread\u7684 thread control block \u9700\u8981\u4fdd\u5b58\u8fd9\u4e9b\u5185\u5bb9\u3002 \u6bcf\u4e2athread\u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u72ec\u7acb\u7684call stack\uff0cfunction\u7684\u8fd0\u884c\u90fd\u662f\u53d1\u751f\u5728call stack\u4e0a\uff0c\u6bcf\u6b21\u8c03\u7528function\uff0c\u5219\u5165\u6808\uff0c \u51fd\u6570\u8fd0\u884c\u7ed3\u675f\uff0c\u5219\u51fa\u6808\uff0c\u8fd9\u5c31\u662fthread\u7684\u8fd0\u884c\u6a21\u578b\u3002","title":"Thread run model:  function as user-defined action"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#call-stack","text":"TODO \u6b64\u6bb5\u5bf9call stack\u76f8\u5173\u7684ABI\u8fdb\u884c\u63cf\u8ff0\uff0c\u7531\u6b64\u5f15\u51fa\u63a7\u5236\u6d41\u3001\u4f20\u53c2\u3001\u7b49\u7b49\u4e00\u7cfb\u5217\u95ee\u9898\u3002 \u5173\u4e8e\u51fd\u6570\u8c03\u7528\uff0c\u4e0b\u9762\u5185\u5bb9\u662f\u9700\u8981\u8fdb\u884c\u8865\u5145\u7684\uff1a Entry point Function prologue Housekeeping (computing) Subroutine","title":"call stack"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#linux-os-process-model","text":"\u4e4b\u524d\u6211\u4e00\u76f4\u6709\u4e00\u4e2a\u7591\u95ee\u5c31\u662f\uff1a\u4e00\u4e2aprocess\u7684\u6240\u6709\u7684thread\u90fd\u5171\u4eab\u8be5process\u7684address space\uff0c\u800c\u6bcf\u4e2athread\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 call stack \uff0c\u5e76\u4e14call stack\u662f\u5411\u4e0b\u751f\u957f\u7684\uff0c\u5f53\u65f6\u6211\u5c31\u975e\u5e38\u7591\u60d1\uff0c\u8fd9\u8981\u5982\u4f55\u5b9e\u73b0\u5440\uff1f\u4eca\u5929\u5728\u9605\u8bfb Call stack \u3001 Stack register \u7684\u65f6\u5019\uff0c\u6211\u6709\u4e86\u5982\u4e0b\u7684\u8ba4\u77e5\uff1a \u51fd\u6570\u8c03\u7528\u6240\u4f7f\u7528\u7684\u662fJMP\u6307\u4ee4 x86\u6709segment register\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u6307\u5b9acall stack \u5176\u5b9ecall stack\u5c31\u662f\u4e00\u7247\u5185\u5b58\u533a\u57df\u800c\u5df2\uff0c\u53ea\u8981\u6307\u5b9a\u4e00\u7247\u5185\u5b58\u533a\u57df\u4f5c\u4e3acall stack\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528calling convention\u6765\u5b9e\u73b0\u51fd\u6570\u8c03\u7528\u4e86\u3002\u5b9e\u73b0\u51fd\u6570\u8c03\u7528\u3001\u6267\u884c\u7684\u6307\u4ee4\u662f\u4e0e\u8fd9\u7247\u5185\u5b58\u533a\u57df\u5728\u4f55\u5904\u65e0\u5173\u7684\uff0c\u6240\u4ee5\u7528\u6237\u662f\u53ef\u4ee5\u6307\u5b9a\u4efb\u610f\u7684\u3001\u5408\u6cd5\u7684\u5185\u5b58\u533a\u57df\u6765\u4f5c\u4e3acall stack\u7684\u3002 \u6240\u4ee5\u6211\u5c31\u53bb\u770b\u4e86 pthread_create \u7684\u6587\u6863\uff0c\u5176\u4e2d\u662f\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\u7684\uff1a On Linux/x86-32, the default stack size for a new thread is 2 megabytes. Under the NPTL threading implementation, if the RLIMIT_STACK soft resource limit at the time the program started has any value other than \"unlimited\", then it determines the default stack size of new threads. Using pthread_attr_setstacksize (3), the stack size attribute can be explicitly set in the attr argument used to create a thread, in order to obtain a stack size other than the default. \u5373\u65b0\u521b\u5efa\u7684thread\u7684\u9ed8\u8ba4\u7684call stack\u7684\u5927\u5c0f\u9ed8\u8ba4\u662f2M\uff0c\u8fd9\u8bf4\u660e\u662f\u53ef\u4ee5\u7531\u7528\u6237\u4e86\u6765\u6307\u5b9a\u65b0\u521b\u5efa\u7684thread\u7684call stack\u7684\uff0c\u6211\u4eec\u77e5\u9053\uff0c pthread_create \u6700\u7ec8\u662f\u901a\u8fc7\u8c03\u7528 clone(2) \uff0c\u8be5\u51fd\u6570\u7684\u7b2c\u4e8c\u4e2a\u5165\u53c2\u5c31\u662f\u7531\u7528\u6237\u6765\u6307\u5b9a\u8be5lightweight process\u7684call stack\u7684\u3002 \u770b\u5230\u4e86\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c \u5176\u5b9e\u6211\u53c8\u60f3\u5230\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4e00\u4e2a\u51fd\u6570\uff0c\u5982\u679c\u58f0\u660e\u7684\u81ea\u52a8\u53d8\u91cf\u5927\u5c0f\u8d85\u8fc7\u4e86call stack\u7684\u5927\u5c0f\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53c2\u89c1\uff1a https://www.cnblogs.com/zmlctt/p/3987181.html https://blog.csdn.net/zDavid_2018/article/details/89255630 \u7ef4\u57fa\u767e\u79d1\u7684 Stack overflow \u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u9f99\u4e66\u7684chapter 7\u603b\u7ed3\u7684\u662f\u975e\u5e38\u597d\u7684\u3002","title":"linux OS process model\u7684\u5b9e\u73b0"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#process-run-model","text":"\u6709\u4e86\u524d\u9762\u7684thread run model\uff0c\u90a3\u4e48process\u7684run model\u5c31\u76f8\u5bf9\u6bd4\u8f83\u597d\u5206\u6790\u4e86\u3002\u663e\u7136\u4e00\u4e2aprocess\u6709\u591a\u4e2aprocess\u7ec4\u6210\uff0c\u591a\u4e2athread\u72ec\u7acb\u8fdb\u884c\u8fd0\u884c\uff0c\u5171\u4eabprocess\u7684resource\u3002","title":"Process run model"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#_1","text":"\u5982\u4f55\u5229\u7528\u786c\u4ef6\u6765\u5b9e\u73b0\u4e0a\u8ff0model\u3002 \u672c\u6587\u5904\u4e8e\u8349\u7a3f\u72b6\u6001\u3002\u672c\u6587\u63cf\u8ff0process\u7684\u8fd0\u884c\u6a21\u578b\u3002 \u7f16\u5199\u601d\u8def\uff1a\u4ece\u8fdb\u7a0b\u7684\u8fd0\u884c\u5f62\u6001\u4f5c\u4e3a\u5207\u5165\u70b9\uff1a \u76ee\u524d\u7684\u6240\u6709\u7684hight level programming language\u90fd\u5c06\u51fd\u6570\u4f5c\u4e3a\u7a0b\u5e8f\u4e2duser-defined action\u7684\u5355\u4f4d\uff08\u5728hardware\u5c42\uff0c\u663e\u7136user-defined action\u7684\u5355\u4f4d\u662finstruction\uff09\uff0c\u8fd9\u53ef\u4ee5\u4f5c\u4e3a\u7a0b\u5e8f\u7684\u8fd0\u884c\u6a21\u578b\uff08\u5bf9\u4e8e\u8be5\u8fd0\u884c\u6a21\u578b\u662f\u8fd8\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4fee\u6b63\u7684\uff0c\u5b83\u7684\u6700\u5c0f\u7c92\u5ea6\u7684user-defined action\u5176\u5b9e\u662f\u8bed\u53e5\uff0c\u4f46\u662f\u8fd9\u79cd\u8fd0\u884c\u6a21\u578b\u662f\u66f4\u52a0\u7b26\u5408call stack\u7684\uff09\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u8fd0\u884c\u6a21\u578b\uff0c\u4f7f\u7528\u7684\u7ed3\u6784\u662fstack\uff0ccall stack\u3002\u7531\u6b64\u5c31\u5f15\u51fa\u4e86\u4e00\u4e9b\u5217\u7684\u95ee\u9898\uff1a\u51fd\u6570\u4f20\u53c2\u5982\u4f55\u5b9e\u73b0\u7b49\u7b49\uff0c\u7531\u6b64\u5c31\u5f15\u51fa\u4e86calling convention\u3002 \u5176\u5b9e\u6211\u7684\u8fd9\u4e2a\u63cf\u8ff0\u601d\u8def\u662f\u548c\u9f99\u4e66\u76847.2 Stack Allocation of Space\u7684\u63cf\u8ff0\u601d\u8def\u7c7b\u4f3c\u7684\u3002 \u5c06\u6240\u6709\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\u96c6\u4e2d\u5230\u8fd9\u91cc\u6765\u8fdb\u884c\uff0c\u5305\u62ec\uff1a ABI\u3002 \u9f99\u4e66chapter 7\u6240\u8bba\u8ff0\u7684concept model\u3002 \u672c\u7ae0\u7684\u5185\u5bb9\u662f\u4e3b\u8981\u6e90\u81ea\u9f99\u4e66\u7684 Chapter 7 Run-Time Environments \uff0c\u539f\u6587\u5185\u5bb9\u662f\u975e\u5e38\u597d\u7684\uff0c\u4e3a\u6211\u4eec\u6e05\u6670\u5730\u52fe\u753b\u51fa\u4e86process\u7684memory\u3001runtime\u7684concept model\u3002","title":"\u5982\u4f55\u6765\u5b9e\u73b0\uff1f"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#memory-model","text":"\u9f99\u4e66\uff0cOS\u4e66\u3001\u7ef4\u57fa\u767e\u79d1 \u903b\u8f91\u4e0e\u5b9e\u73b0\u3002 \u9f99\u4e66\u6b63\u5982\u5176\u540d\uff0c\u5b83\u6240\u8ff0\u7684\u662f\u539f\u7406\uff0c\u6240\u4ee5\u5b83\u6240\u8bb2\u8ff0\u7684\u662f\u6982\u5ff5\u6a21\u578b\u3002 stack\u662fprocess\u6d3b\u52a8\u7684\u573a\u6240\uff0c\u6240\u4ee5\u5b83\u662fmemory management\u7684\u5173\u952e\u6240\u5728\u3002 call stack \u53c8\u79f0\u4e3a control stack\uff0c\u6240\u4ee5\u5b83\u4e5f\u4f53\u73b0\u4e86\u5b83\u4e0e program counter \uff0c flow of control \u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u5f80\u66f4\u52a0\u5bbd\u6cdb\u6765\u6240\uff0c\u5176\u5b9e\u662fapplication binary interface\uff0c\u56e0\u4e3aprograming language\u7684\u5f88\u591a\u4e1c\u897f\u6700\u7ec8\u90fd\u9700\u8981\u7ffb\u8bd1\u4e3a\u6307\u4ee4\uff0c\u800capplication binary interface\u5219\u662f\u8fd9\u7c7b\u573a\u666f\u7684\u603b\u4f53\u63cf\u8ff0\u3002 \u51fd\u6570\u8c03\u7528\u5bf9\u5e94\u7684\u662fJMP\u6307\u4ee4\uff0c\u90a3\u58f0\u660e\u4e00\u4e2a int \u7c7b\u578b\u7684\u53d8\u91cf\u5bf9\u5e94\u7684\u662f\u4ec0\u4e48\u6307\u4ee4\u5462\uff1f\u4e0e\u6b64\u7c7b\u4f3c\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1a\u51fd\u6570\u8c03\u7528\u7684\u65f6\u5019\uff0c\u9700\u8981\u5206\u914d\u6808\u7a7a\u95f4\uff0c\u90a3\u8fd9\u662f\u5982\u4f55\u5b9e\u73b0\u7684\uff1f push \u6307\u4ee4\u5c31\u53ef\u4ee5\u5b9e\u73b0 process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u8981\u6d3b\u52a8\u5176\u5b9e\u5c31\u662f\u4e0d\u65ad\u5730\u51fd\u6570\u8c03\u7528\uff0c\u6240\u4ee5\u641e\u6e05\u695a\u51fd\u6570\u8c03\u7528\u7684\u8fc7\u7a0b\u5bf9\u7406\u89e3process\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002\u9f99\u4e66\u7684chapter 7\u5c31\u662f\u4ecb\u7ecd\u6b64\u7684\u975e\u5e38\u597d\u7684\u5185\u5bb9\u3002\u8fd9\u4e9b\u5185\u5bb9\u6211\u89c9\u5f97\u5168\u90e8\u90fd\u6574\u7406\u5230OS book\u4e2d\u53bb\u3002","title":"\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684memory model"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-mode-run-time-environment/#_2","text":"\u8fdb\u7a0b\u5b8c\u5168\u662f\u57fa\u4e8efunction\u7684\u8fd0\u884c\u6a21\u5f0f\uff0c\u5b83\u7684\u6240\u6709\u6d3b\u52a8\u90fd\u53d1\u751f\u5728call stack\u4e0a\u3002 \u5728\u8fdb\u5165\u51fd\u6570\u4e4b\u524d\uff0c\u5982\u4f55\u5f97\u77e5\u8981\u7533\u8bf7\u591a\u5c11\u6808\u7a7a\u95f4\uff1f\u5e94\u8be5\u4e0d\u662f\u63d0\u524d\u4e00\u6b21\u6027\u7533\u8bf7\u8be5\u51fd\u6570\u6240\u9700\u8981\u7684\u6240\u6709\u7684\u6808\u7a7a\u95f4\uff0c\u800c\u662f\u8fd0\u884c\u5230\u8be5\u6307\u4ee4\u7684\u65f6\u5019\uff0c\u624d\u5728\u6808\u4e0a\u5206\u914d\u7a7a\u95f4\u3002\u8fd9\u8ba9\u6211\u60f3\u5230\u4e86stored-program\u601d\u60f3\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\u5728\u9f99\u4e66\u76847.2 Stack Allocation of Space\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Almost all compilers for languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack. Each time a procedure is called, space for its local variables is pushed onto a stack, and when the procedure terminates, that space is popped off the stack.","title":"\u8fdb\u7a0b\u8fd0\u884c\u5f62\u6001"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/","text":"Process model # Thread \u548c Process \u662f\u73b0\u4ee3OS\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5efa\u7acb\u8d77\u5b8c\u6574\u3001\u6b63\u786e\u7684process model\u5bf9\u4e8e\u5728linux OS\u4e0b\u8fdb\u884c\u5f00\u53d1\u3001\u7406\u89e3linux kernel\u7684\u5b9e\u73b0\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c \u672c\u6587\u6240\u8ff0\u7684process model\u662f\u4e00\u4e2a Conceptual model \uff0c\u4e5f\u53ef\u4ee5\u8bf4\u672c\u6587\u6240\u63cf\u8ff0\u7684process model\u662f\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u3002\u4e0d\u540c\u7684OS\u5bf9process model\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\u3002 Process model\u56fe\u793a\u5982\u4e0b\uff1a Process # \u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Process In computing, a process is the instance of a computer program that is being executed by one or many threads. It contains the program code and its activity. Depending on the operating system (OS), a process may be made up of multiple threads of execution that execute instructions concurrently . \u663e\u7136thread\u662fprocess\u7684\u201c\u6210\u5206\u201d\uff0c\u4e0b\u9762\u770b\u770bthread\u3002 Thread # \u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Thread (computing) In computer science , a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler . Multiple threads can exist within one process, executing concurrently and sharing resources \u6982\u62ecprocess model # \u7efc\u5408\u4e0a\u9762\u63cf\u8ff0\uff0c\u4ee5\u4e0b\u6211\u6240\u6982\u62ecProcess model\uff1a \u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0c\u57fa\u4e8e thread \u7684\u8c03\u5ea6\u3002\u4e00\u4e2a process \u53ef\u80fd\u7531\u591a\u4e2a thread \u7ec4\u6210\uff0c thread \u5171\u4eabprocess\u7684resource\u3001 \u5e76\u53d1 \u6267\u884c \u3002\u201d \u6ce8\u610f\uff1a\u4e0a\u8ff0\u6982\u62ec\u7684\u662f\u73b0\u4ee3\u5927\u591a\u6570OS\u7684process model\uff0c\u5e76\u975e\u6240\u6709OS\u7684process model\u90fd\u662f\u5982\u6b64\uff0c\u5b9e\u73b0\u4e0a\u662f\u5b58\u5728\u5dee\u5f02\u7684\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u867d\u7136\u7b80\u77ed\uff0c\u4f46\u662f\u8574\u542b\u7740\u4e30\u5bcc\u7684\u5185\u6db5\uff0c\u9700\u8981\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff1a \u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\u201d # \u8fd9\u6bb5\u8bdd\u610f\u5473\u7740\uff1aprocess\u662fOS\u7684\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0cprocess\u4e4b\u95f4\u662f\u5f7c\u6b64\u9694\u79bb\u7684\u3002 \u6ce8\u610f: \u5bf9\u4e8e\u4e00\u4e9b\u7279\u6b8a\u7684\u60c5\u51b5\uff0c\u5982process\u4e4b\u95f4\u5171\u4eabmemory\u7684\u60c5\u51b5\u9664\u5916\u3002 OS\u4e2d\u6709\u54ea\u4e9bresource\uff1fProcess\u9700\u8981\u54ea\u4e9bresource\uff1f # OS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0cresource\u5305\u62ec\uff1a Memory address space process\u5982\u4f55\u5b9e\u73b0\u9694\u79bb\uff1f # \u201c\u57fa\u4e8e thread \u7684\u8c03\u5ea6\u201d # \u8fd9\u6bb5\u8bdd\u610f\u5473\u7740thread\u662fOS\u7684\u8c03\u5ea6\u5355\u4f4d\u3002\u663e\u7136\u6bcf\u4e2athread\u53ef\u4ee5\u72ec\u7acb\u6267\u884c\uff0c\u5219\u6bcf\u4e2athread\u90fd\u6709\u6267\u884c\u7684\u5fc5\u5907\u6761\u4ef6\uff1a call stack process model\u7684\u6f14\u8fdb\u5386\u53f2 # \u5728 stanford CS 140: Operating Systems (Spring 2014) \u7684lecture\u4e2d\u603b\u7ed3\u4e86Evolution of operating system process model : Early operating systems supported a single process with a single thread at a time ( single tasking ). They ran batch jobs (one user at a time). Some early personal computer operating systems used single-tasking (e.g. MS-DOS), but these systems are almost unheard of today. By late 1970's most operating systems were multitasking systems: they supported multiple processes , but each process had only a single thread. In the 1990's most systems converted to multithreading : multiple threads within each process. \u663e\u7136\uff0c\u65e9\u671f\u7684\u65f6\u5019\uff0c\u5e76\u6ca1\u6709 multithreading : multiple threads within each process\uff0c\u6240\u4ee5\u65e9\u671f\u7684\u65f6\u5019multitasking\u7684task\u6240\u6307 processes \u3002\u800c\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u540e\u6765\u624d\u51fa\u73b0\u4e86 multithreading : multiple threads within each process\u3002 multithreading \u76f8\u8f83\u4e8e Multiprocessing \u4f18\u52bf\u662f\u4ec0\u4e48\uff1f # \u4e0d\u7981\u8981\u95ee\uff1a multithreading \u76f8\u8f83\u4e8e Multiprocessing \u4f18\u52bf\u662f\u4ec0\u4e48\uff1f \u8fd9\u4e2a\u95ee\u9898\uff0c\u5728 Computer multitasking \u7684 Multithreading \u7ae0\u8282\u7ed9\u51fa\u4e86\u7b54\u6848\u89e3\u7b54\u3002 Threads vs. processes # \u663e\u7136operating system\u4e3a\u4e86\u652f\u6301multiple thread\uff0c\u5c31\u5fc5\u987b\u8981\u8ba9\u6bcf\u4e2athread\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 call stack \uff1b\u5728Wikipedia\u7684 Thread control block \u4e2d\u5c31\u8c08\u53ca\u6bcf\u4e2athread\u90fd\u6709\u4e00\u4e2a Stack pointer \uff0c\u800c Process control block \u4e2d\uff0c\u53ef\u80fd\u5c31\u4e0d\u9700\u8981 Stack pointer \u4e86\uff1b process\u7684\u4e00\u7cfb\u5217\u95ee\u9898 # \u751f # \u521b\u5efaprocess \u5360\u7528\u4e86\u54ea\u4e9b\u8d44\u6e90 # \u5982\u4f55\u6765\u63a7\u5236process # \u5982\u4f55\u9650\u5236\u8d44\u6e90 # process\u4e4b\u95f4\u7684\u5173\u7cfb # process\u4e4b\u95f4\u5982\u4f55\u8fdb\u884c\u6c9f\u901a # \u65f6\u7a7a\u7684\u89d2\u5ea6 # process\u7684\u72b6\u6001 # \u6b7b #","title":"Process-model"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process-model","text":"Thread \u548c Process \u662f\u73b0\u4ee3OS\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5efa\u7acb\u8d77\u5b8c\u6574\u3001\u6b63\u786e\u7684process model\u5bf9\u4e8e\u5728linux OS\u4e0b\u8fdb\u884c\u5f00\u53d1\u3001\u7406\u89e3linux kernel\u7684\u5b9e\u73b0\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c \u672c\u6587\u6240\u8ff0\u7684process model\u662f\u4e00\u4e2a Conceptual model \uff0c\u4e5f\u53ef\u4ee5\u8bf4\u672c\u6587\u6240\u63cf\u8ff0\u7684process model\u662f\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u3002\u4e0d\u540c\u7684OS\u5bf9process model\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\u3002 Process model\u56fe\u793a\u5982\u4e0b\uff1a","title":"Process model"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process","text":"\u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Process In computing, a process is the instance of a computer program that is being executed by one or many threads. It contains the program code and its activity. Depending on the operating system (OS), a process may be made up of multiple threads of execution that execute instructions concurrently . \u663e\u7136thread\u662fprocess\u7684\u201c\u6210\u5206\u201d\uff0c\u4e0b\u9762\u770b\u770bthread\u3002","title":"Process"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#thread","text":"\u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Thread (computing) In computer science , a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler . Multiple threads can exist within one process, executing concurrently and sharing resources","title":"Thread"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process-model_1","text":"\u7efc\u5408\u4e0a\u9762\u63cf\u8ff0\uff0c\u4ee5\u4e0b\u6211\u6240\u6982\u62ecProcess model\uff1a \u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0c\u57fa\u4e8e thread \u7684\u8c03\u5ea6\u3002\u4e00\u4e2a process \u53ef\u80fd\u7531\u591a\u4e2a thread \u7ec4\u6210\uff0c thread \u5171\u4eabprocess\u7684resource\u3001 \u5e76\u53d1 \u6267\u884c \u3002\u201d \u6ce8\u610f\uff1a\u4e0a\u8ff0\u6982\u62ec\u7684\u662f\u73b0\u4ee3\u5927\u591a\u6570OS\u7684process model\uff0c\u5e76\u975e\u6240\u6709OS\u7684process model\u90fd\u662f\u5982\u6b64\uff0c\u5b9e\u73b0\u4e0a\u662f\u5b58\u5728\u5dee\u5f02\u7684\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u867d\u7136\u7b80\u77ed\uff0c\u4f46\u662f\u8574\u542b\u7740\u4e30\u5bcc\u7684\u5185\u6db5\uff0c\u9700\u8981\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff1a","title":"\u6982\u62ecprocess model"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#osprocessresource","text":"\u8fd9\u6bb5\u8bdd\u610f\u5473\u7740\uff1aprocess\u662fOS\u7684\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0cprocess\u4e4b\u95f4\u662f\u5f7c\u6b64\u9694\u79bb\u7684\u3002 \u6ce8\u610f: \u5bf9\u4e8e\u4e00\u4e9b\u7279\u6b8a\u7684\u60c5\u51b5\uff0c\u5982process\u4e4b\u95f4\u5171\u4eabmemory\u7684\u60c5\u51b5\u9664\u5916\u3002","title":"\u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\u201d"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#osresourceprocessresource","text":"OS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0cresource\u5305\u62ec\uff1a Memory address space","title":"OS\u4e2d\u6709\u54ea\u4e9bresource\uff1fProcess\u9700\u8981\u54ea\u4e9bresource\uff1f"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process_1","text":"","title":"process\u5982\u4f55\u5b9e\u73b0\u9694\u79bb\uff1f"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#thread_1","text":"\u8fd9\u6bb5\u8bdd\u610f\u5473\u7740thread\u662fOS\u7684\u8c03\u5ea6\u5355\u4f4d\u3002\u663e\u7136\u6bcf\u4e2athread\u53ef\u4ee5\u72ec\u7acb\u6267\u884c\uff0c\u5219\u6bcf\u4e2athread\u90fd\u6709\u6267\u884c\u7684\u5fc5\u5907\u6761\u4ef6\uff1a call stack","title":"\u201c\u57fa\u4e8ethread\u7684\u8c03\u5ea6\u201d"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process-model_2","text":"\u5728 stanford CS 140: Operating Systems (Spring 2014) \u7684lecture\u4e2d\u603b\u7ed3\u4e86Evolution of operating system process model : Early operating systems supported a single process with a single thread at a time ( single tasking ). They ran batch jobs (one user at a time). Some early personal computer operating systems used single-tasking (e.g. MS-DOS), but these systems are almost unheard of today. By late 1970's most operating systems were multitasking systems: they supported multiple processes , but each process had only a single thread. In the 1990's most systems converted to multithreading : multiple threads within each process. \u663e\u7136\uff0c\u65e9\u671f\u7684\u65f6\u5019\uff0c\u5e76\u6ca1\u6709 multithreading : multiple threads within each process\uff0c\u6240\u4ee5\u65e9\u671f\u7684\u65f6\u5019multitasking\u7684task\u6240\u6307 processes \u3002\u800c\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u540e\u6765\u624d\u51fa\u73b0\u4e86 multithreading : multiple threads within each process\u3002","title":"process model\u7684\u6f14\u8fdb\u5386\u53f2"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#multithreadingmultiprocessing","text":"\u4e0d\u7981\u8981\u95ee\uff1a multithreading \u76f8\u8f83\u4e8e Multiprocessing \u4f18\u52bf\u662f\u4ec0\u4e48\uff1f \u8fd9\u4e2a\u95ee\u9898\uff0c\u5728 Computer multitasking \u7684 Multithreading \u7ae0\u8282\u7ed9\u51fa\u4e86\u7b54\u6848\u89e3\u7b54\u3002","title":"multithreading\u76f8\u8f83\u4e8eMultiprocessing\u4f18\u52bf\u662f\u4ec0\u4e48\uff1f"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#threads-vs-processes","text":"\u663e\u7136operating system\u4e3a\u4e86\u652f\u6301multiple thread\uff0c\u5c31\u5fc5\u987b\u8981\u8ba9\u6bcf\u4e2athread\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 call stack \uff1b\u5728Wikipedia\u7684 Thread control block \u4e2d\u5c31\u8c08\u53ca\u6bcf\u4e2athread\u90fd\u6709\u4e00\u4e2a Stack pointer \uff0c\u800c Process control block \u4e2d\uff0c\u53ef\u80fd\u5c31\u4e0d\u9700\u8981 Stack pointer \u4e86\uff1b","title":"Threads vs. processes"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process_2","text":"","title":"process\u7684\u4e00\u7cfb\u5217\u95ee\u9898"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#_1","text":"\u521b\u5efaprocess","title":"\u751f"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#_2","text":"","title":"\u5360\u7528\u4e86\u54ea\u4e9b\u8d44\u6e90"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process_3","text":"","title":"\u5982\u4f55\u6765\u63a7\u5236process"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#_3","text":"","title":"\u5982\u4f55\u9650\u5236\u8d44\u6e90"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process_4","text":"","title":"process\u4e4b\u95f4\u7684\u5173\u7cfb"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process_5","text":"","title":"process\u4e4b\u95f4\u5982\u4f55\u8fdb\u884c\u6c9f\u901a"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#_4","text":"","title":"\u65f6\u7a7a\u7684\u89d2\u5ea6"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#process_6","text":"","title":"process\u7684\u72b6\u6001"},{"location":"Kernel/Guide/Linux-OS's-multitasking/01-Process-model/#_5","text":"","title":"\u6b7b"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/","text":"Linux OS implementation of process model # \u5728\u672c\u4e66\u7684chapter 1.1. Linux Versus Other Unix-Like Kernels\u5bf9linux OS\u4e2dprocess model\u7684\u5b9e\u73b0\u601d\u8def\u8fdb\u884c\u4e86\u6982\u62ec\uff1a Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes , which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads , Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone(2) system call. Process\u548cThread\u7684\u6982\u5ff5\u5728\u524d\u9762\u7684\u7ae0\u8282\u5df2\u7ecf\u63cf\u8ff0\u4e86\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6982\u5ff5\uff1a lightweight processes (LWP)\uff0c lightweight processes \u662f\u4e00\u4e2a\u5b9e\u73b0\u5c42\u9762\u7684\u6982\u5ff5\uff0c\u800cProcess\u548cThread\u662f\u6807\u51c6\u5b9a\u4e49\u7684\u6982\u5ff5\u3002 \u7ed3\u5408\u524d\u9762\u7ae0\u8282\u5173\u4e8eprocess model\u7684\u63cf\u8ff0\u548c\u4e0a\u9762\u8fd9\u6bb5\u5173\u4e8elinux OS\u4e2dprocess model\u5b9e\u73b0\u6982\u8ff0\uff0c\u53ef\u4ee5\u603b\u7ed3\uff1a Linux OS\u7684kernel scheduling entity\u662f lightweight processes linux OS\u901a\u8fc7\u5b83\u7684 lightweight processes \u6765\u5b9e\u73b0process model\u7684\uff1blinux OS\u4e2d\uff0clight weight process\u5bf9\u5e94\u7684\u662f\u6807\u51c6\u7684thread\uff0clinux OS\u4e2d\uff0c\u4e00\u4e2aprocess\u7531n\uff08n>=1\uff09\u4e2alight weight process\u7ec4\u6210\uff08\u663e\u7136\uff0c\u5f53n\u4e3a1\u65f6\uff0c\u4e00\u4e2aprocess\u53ea\u6709\u4e00\u4e2alightweight process\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u901a\u5e38\u6240\u8bf4\u7684single-thread process\uff09\u3002 \u5728\u672c\u4e66chapter 3.1. Processes, Lightweight Processes, and Threads\u4e2d\u5b9a\u4e49\u4e86 thread group \u7684\u6982\u5ff5\uff0c thread group \u76f8\u5f53\u4e8eprocess\u3002 \u5728\u672c\u4e66chapter 3.1. Processes, Lightweight Processes, and Threads\u4e2d\u63d0\u51fa\uff1a\u5bf9\u4e8e\u9762\u5411process\u7684system call\uff0cthread group\u8981\u201cact as a whole\u201d\u5373\u8868\u793a\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u8fd9\u4e9bsystem call\u5305\u62ec\uff1a getpid \u3001 kill \u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u672c\u4e66\u4e2d\u6709\u65f6\u5019\u4f1a\u5c06lightweight process\u7b80\u79f0\u4e3aprocess\uff0c\u6bd4\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\u8fd9\u53e5\uff1a A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. \u6240\u4ee5\u5728\u672c\u4e66\u4e2d\uff0cprocess\u4e0d\u4e00\u5b9a\u6307\u7684\u662f\u6807\u51c6\u7684process\uff0c\u6709\u7684\u65f6\u5019\u6307\u7684\u662flightweight process\uff1b\u5728\u672c\u4e66\u4e2d\uff0c\u4f7f\u7528 thread group \u6765\u8868\u793a\u6807\u51c6\u7684process\u3002\u8bb0\u4f4f\u8fd9\u4e00\u70b9\uff0c\u5426\u5219\u672c\u4e66\u4e2d\u7684\u5f88\u591a\u5730\u65b9\u90fd\u4f1a\u641e\u6df7\u6dc6\u3002 \u5173\u4e8elightweight process\uff0c\u53c2\u89c1\uff1a Light-weight process \u66f4\u52a0\u6df1\u5165\u7684\u5206\u6790 # \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u544a\u8bc9\u4e86\u6211\u4eec\uff0clightweight process\u662f\u7531nonstandard clone(2) system call\u521b\u5efa\u3002\u6807\u51c6\u7ed9\u51fa\u7684\u521b\u5efaprocess\u7684api\u662f fork(2) \uff0cPOSIX\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u521b\u5efathread\u7684api\u662f pthread_create(3) \uff0c\u4e0b\u9762\u901a\u8fc7linux\u7684man\u6765\u63a2\u7d22\u5b83\u4eec\u7684\u5b9e\u73b0\u7ec6\u8282\uff1a \u5728 PTHREADS(7) \u7684Linux implementations of POSIX threads\u7ae0\u8282\u7ed9\u51fa\u4e86linux\u4e2dPOSIX threads\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u8be6\u7ec6\u4fe1\u606f Over time, two threading implementations have been provided by the GNU C library on Linux: LinuxThreads \u200b This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. NPTL (Native POSIX Threads Library) \u200b This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity. Both threading implementations employ the Linux clone(2 ) system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex(2) system call. \u53ef\u4ee5\u770b\u5230\u65e0\u8bba\u91c7\u7528\u54ea\u79cd\u65b9\u5f0f\uff0c\u6700\u7ec8\u90fd\u662f\u4f9d\u8d56 clone(2) \u3002 \u5728 fork(2) \u7684 NOTES \u7ae0\u8282\u63cf\u8ff0\u4e86 fork \u7684\u5b9e\u73b0\u7ec6\u8282\uff1a Since version 2.3.3, rather than invoking the kernel's fork() system call, the glibc fork() wrapper that is provided as part of the NPTL threading implementation invokes clone(2) with flags that provide the same effect as the traditional system call. (A call to fork() is equivalent to a call to clone(2) specifying flags as just SIGCHLD.) The glibc wrapper invokes any fork handlers that have been established using pthread_atfork(3) . \u663e\u7136\uff0c fork(2) \u7684\u5b9e\u73b0\u4e5f\u662f\u4f9d\u8d56 clone(2) \u3002 linux kernel\u5982\u4f55\u5b9e\u73b0process\u4e0ethread # \u53c2\u89c13.1. Processes, Lightweight Processes, and Threads \u6211\u89c9\u5f97\u8981\u60f3\u89e3\u91ca\u597d\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u68b3\u7406\u4e00\u4e0blinux\u7684fork\uff0cclone\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5728 Fork (system call) \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u68b3\u7406\u5730\u975e\u5e38\u597d\u3002\u57283.4. Creating Processes\u4e2d\u4e5f\u6709\u76f8\u5173\u7684\u63cf\u8ff0\u3002 per-process kernel data structures # \u57283.4. Creating Processes\u4e2d\u63d0\u51fa\u4e86\u8fd9\u4e2a\u8bf4\u6cd5\uff0c\u5b83\u8ba9\u6211\u60f3\u8d77\u4e86\u4e24\u4ef6\u4e8b\u60c5\uff1a process\u4f5c\u4e3asystem resource\u5206\u914d\u5355\u4f4d\uff0c\u5b83\u6709\u54ea\u4e9bresource\u5462\uff1f\u663e\u7136\uff0c\u5b83\u7684\u6240\u6709\u7684resource\u90fd\u9700\u8981\u4f7f\u7528\u4e00\u4e2a kernel data structures\u6765\u8fdb\u884c\u63cf\u8ff0\u3002\u6709\u5fc5\u8981\u603b\u7ed3per-process\u7684resource\u4ee5\u53ca\u5bf9\u5e94\u7684kernel data structures\u3002\u4e0e\u6b64\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff0c\u8fd9\u4e9bresource\u54ea\u4e9b\u662fchild process\u53ef\u4ee5\u7ee7\u627f\u7684\uff0c\u54ea\u4e9b\u662f\u65e0\u6cd5\u7ee7\u627f\u7684\u3002 \u663e\u7136\uff0c\u591a\u4e2alightweight process\u662f\u53ef\u4ee5\u5171\u4eabper-process kernel data structure\u7684\uff08\u8fd9\u662f\u6807\u51c6\u89c4\u5b9a\u7684\uff09\uff0c\u8fd9\u79cd\u5171\u4eab\uff0c\u6211\u89c9\u5f97\u5b9e\u73b0\u4e0a\u5e94\u8be5\u4e5f\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c\u65e0\u975e\u5c31\u662f\u4f20\u5165\u4e00\u4e2a\u6307\u9488\u3002 LWP VS thread VS kernel thread? # \u5173\u4e8e\u672c\u6bb5\uff0c\u6709\u7591\u95ee\uff1aLWP VS thread VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy Linux OS lightweigh thread\u6240\u5171\u4eab\u7684 # \u6807\u51c6process ID See also # \u53c2\u89c1\u7ae0\u8282\uff1a 1.6.2. Process Implementation 1.6.4. Process Address Space","title":"Linux-OS-implementation-of-process-model-01"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#linux-os-implementation-of-process-model","text":"\u5728\u672c\u4e66\u7684chapter 1.1. Linux Versus Other Unix-Like Kernels\u5bf9linux OS\u4e2dprocess model\u7684\u5b9e\u73b0\u601d\u8def\u8fdb\u884c\u4e86\u6982\u62ec\uff1a Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes , which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads , Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone(2) system call. Process\u548cThread\u7684\u6982\u5ff5\u5728\u524d\u9762\u7684\u7ae0\u8282\u5df2\u7ecf\u63cf\u8ff0\u4e86\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6982\u5ff5\uff1a lightweight processes (LWP)\uff0c lightweight processes \u662f\u4e00\u4e2a\u5b9e\u73b0\u5c42\u9762\u7684\u6982\u5ff5\uff0c\u800cProcess\u548cThread\u662f\u6807\u51c6\u5b9a\u4e49\u7684\u6982\u5ff5\u3002 \u7ed3\u5408\u524d\u9762\u7ae0\u8282\u5173\u4e8eprocess model\u7684\u63cf\u8ff0\u548c\u4e0a\u9762\u8fd9\u6bb5\u5173\u4e8elinux OS\u4e2dprocess model\u5b9e\u73b0\u6982\u8ff0\uff0c\u53ef\u4ee5\u603b\u7ed3\uff1a Linux OS\u7684kernel scheduling entity\u662f lightweight processes linux OS\u901a\u8fc7\u5b83\u7684 lightweight processes \u6765\u5b9e\u73b0process model\u7684\uff1blinux OS\u4e2d\uff0clight weight process\u5bf9\u5e94\u7684\u662f\u6807\u51c6\u7684thread\uff0clinux OS\u4e2d\uff0c\u4e00\u4e2aprocess\u7531n\uff08n>=1\uff09\u4e2alight weight process\u7ec4\u6210\uff08\u663e\u7136\uff0c\u5f53n\u4e3a1\u65f6\uff0c\u4e00\u4e2aprocess\u53ea\u6709\u4e00\u4e2alightweight process\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u901a\u5e38\u6240\u8bf4\u7684single-thread process\uff09\u3002 \u5728\u672c\u4e66chapter 3.1. Processes, Lightweight Processes, and Threads\u4e2d\u5b9a\u4e49\u4e86 thread group \u7684\u6982\u5ff5\uff0c thread group \u76f8\u5f53\u4e8eprocess\u3002 \u5728\u672c\u4e66chapter 3.1. Processes, Lightweight Processes, and Threads\u4e2d\u63d0\u51fa\uff1a\u5bf9\u4e8e\u9762\u5411process\u7684system call\uff0cthread group\u8981\u201cact as a whole\u201d\u5373\u8868\u793a\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u8fd9\u4e9bsystem call\u5305\u62ec\uff1a getpid \u3001 kill \u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u672c\u4e66\u4e2d\u6709\u65f6\u5019\u4f1a\u5c06lightweight process\u7b80\u79f0\u4e3aprocess\uff0c\u6bd4\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\u8fd9\u53e5\uff1a A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. \u6240\u4ee5\u5728\u672c\u4e66\u4e2d\uff0cprocess\u4e0d\u4e00\u5b9a\u6307\u7684\u662f\u6807\u51c6\u7684process\uff0c\u6709\u7684\u65f6\u5019\u6307\u7684\u662flightweight process\uff1b\u5728\u672c\u4e66\u4e2d\uff0c\u4f7f\u7528 thread group \u6765\u8868\u793a\u6807\u51c6\u7684process\u3002\u8bb0\u4f4f\u8fd9\u4e00\u70b9\uff0c\u5426\u5219\u672c\u4e66\u4e2d\u7684\u5f88\u591a\u5730\u65b9\u90fd\u4f1a\u641e\u6df7\u6dc6\u3002 \u5173\u4e8elightweight process\uff0c\u53c2\u89c1\uff1a Light-weight process","title":"Linux OS implementation of process model"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#_1","text":"\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u544a\u8bc9\u4e86\u6211\u4eec\uff0clightweight process\u662f\u7531nonstandard clone(2) system call\u521b\u5efa\u3002\u6807\u51c6\u7ed9\u51fa\u7684\u521b\u5efaprocess\u7684api\u662f fork(2) \uff0cPOSIX\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u521b\u5efathread\u7684api\u662f pthread_create(3) \uff0c\u4e0b\u9762\u901a\u8fc7linux\u7684man\u6765\u63a2\u7d22\u5b83\u4eec\u7684\u5b9e\u73b0\u7ec6\u8282\uff1a \u5728 PTHREADS(7) \u7684Linux implementations of POSIX threads\u7ae0\u8282\u7ed9\u51fa\u4e86linux\u4e2dPOSIX threads\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u8be6\u7ec6\u4fe1\u606f Over time, two threading implementations have been provided by the GNU C library on Linux: LinuxThreads \u200b This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. NPTL (Native POSIX Threads Library) \u200b This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity. Both threading implementations employ the Linux clone(2 ) system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex(2) system call. \u53ef\u4ee5\u770b\u5230\u65e0\u8bba\u91c7\u7528\u54ea\u79cd\u65b9\u5f0f\uff0c\u6700\u7ec8\u90fd\u662f\u4f9d\u8d56 clone(2) \u3002 \u5728 fork(2) \u7684 NOTES \u7ae0\u8282\u63cf\u8ff0\u4e86 fork \u7684\u5b9e\u73b0\u7ec6\u8282\uff1a Since version 2.3.3, rather than invoking the kernel's fork() system call, the glibc fork() wrapper that is provided as part of the NPTL threading implementation invokes clone(2) with flags that provide the same effect as the traditional system call. (A call to fork() is equivalent to a call to clone(2) specifying flags as just SIGCHLD.) The glibc wrapper invokes any fork handlers that have been established using pthread_atfork(3) . \u663e\u7136\uff0c fork(2) \u7684\u5b9e\u73b0\u4e5f\u662f\u4f9d\u8d56 clone(2) \u3002","title":"\u66f4\u52a0\u6df1\u5165\u7684\u5206\u6790"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#linux-kernelprocessthread","text":"\u53c2\u89c13.1. Processes, Lightweight Processes, and Threads \u6211\u89c9\u5f97\u8981\u60f3\u89e3\u91ca\u597d\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u68b3\u7406\u4e00\u4e0blinux\u7684fork\uff0cclone\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5728 Fork (system call) \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u68b3\u7406\u5730\u975e\u5e38\u597d\u3002\u57283.4. Creating Processes\u4e2d\u4e5f\u6709\u76f8\u5173\u7684\u63cf\u8ff0\u3002","title":"linux kernel\u5982\u4f55\u5b9e\u73b0process\u4e0ethread"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#per-process-kernel-data-structures","text":"\u57283.4. Creating Processes\u4e2d\u63d0\u51fa\u4e86\u8fd9\u4e2a\u8bf4\u6cd5\uff0c\u5b83\u8ba9\u6211\u60f3\u8d77\u4e86\u4e24\u4ef6\u4e8b\u60c5\uff1a process\u4f5c\u4e3asystem resource\u5206\u914d\u5355\u4f4d\uff0c\u5b83\u6709\u54ea\u4e9bresource\u5462\uff1f\u663e\u7136\uff0c\u5b83\u7684\u6240\u6709\u7684resource\u90fd\u9700\u8981\u4f7f\u7528\u4e00\u4e2a kernel data structures\u6765\u8fdb\u884c\u63cf\u8ff0\u3002\u6709\u5fc5\u8981\u603b\u7ed3per-process\u7684resource\u4ee5\u53ca\u5bf9\u5e94\u7684kernel data structures\u3002\u4e0e\u6b64\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff0c\u8fd9\u4e9bresource\u54ea\u4e9b\u662fchild process\u53ef\u4ee5\u7ee7\u627f\u7684\uff0c\u54ea\u4e9b\u662f\u65e0\u6cd5\u7ee7\u627f\u7684\u3002 \u663e\u7136\uff0c\u591a\u4e2alightweight process\u662f\u53ef\u4ee5\u5171\u4eabper-process kernel data structure\u7684\uff08\u8fd9\u662f\u6807\u51c6\u89c4\u5b9a\u7684\uff09\uff0c\u8fd9\u79cd\u5171\u4eab\uff0c\u6211\u89c9\u5f97\u5b9e\u73b0\u4e0a\u5e94\u8be5\u4e5f\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c\u65e0\u975e\u5c31\u662f\u4f20\u5165\u4e00\u4e2a\u6307\u9488\u3002","title":"per-process kernel data structures"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#lwp-vs-thread-vs-kernel-thread","text":"\u5173\u4e8e\u672c\u6bb5\uff0c\u6709\u7591\u95ee\uff1aLWP VS thread VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy","title":"LWP VS thread VS kernel thread?"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#linux-os-lightweigh-thread","text":"\u6807\u51c6process ID","title":"Linux OS lightweigh thread\u6240\u5171\u4eab\u7684"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-01-Linux-OS-implementation-of-process-model/#see-also","text":"\u53c2\u89c1\u7ae0\u8282\uff1a 1.6.2. Process Implementation 1.6.4. Process Address Space","title":"See also"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-02-Linux-OS-implementation-of-process-model/","text":"What are Linux Processes, Threads, Light Weight Processes, and Process State # Linux has evolved a lot since its inception. It has become the most widely used operating system when in comes to servers and mission critical work. Though its not easy to understand Linux as a whole but there are aspects which are fundamental to Linux and worth understanding. In this article, we will discuss about Linux processes, threads and light weight processes and understand the difference between them. Towards the end, we will also discuss various states for Linux processes. Linux Processes # In a very basic form, Linux process can be visualized as running instance of a program. For example, just open a text editor on your Linux box and a text editor process will be born. Here is an example when I opened gedit on my machine : $ gedit & [1] 5560 $ ps -aef | grep gedit 1000 5560 2684 9 17:34 pts/0 00:00:00 gedit First command ( gedit &) opens gedit window while second ps command ( ps -aef | grep gedit ) checks if there is an associated process. In the result you can see that there is a process associated with gedit . Processes are fundamental to Linux as each and every work done by the OS is done in terms of and by the processes. Just think of anything and you will see that it is a process. This is because any work that is intended to be done requires system resources ( that are provided by kernel) and it is a process that is viewed by kernel as an entity to which it can provide system resources. Processes have priority based on which kernel context switches them. A process can be pre-empted if a process with higher priority is ready to be executed. For example, if a process is waiting for a system resource like some text from text file kept on disk then kernel can schedule a higher priority process and get back to the waiting process when data is available. This keeps the ball rolling for an operating system as a whole and gives user a feeling that tasks are being run in parallel. Processes can talk to other processes using Inter process communication methods and can share data using techniques like shared memory. In Linux, fork() is used to create new processes. These new processes are called as child processes and each child process initially shares all the segments like text, stack, heap etc until child tries to make any change to stack or heap. In case of any change, a separate copy of stack and heap segments are prepared for child so that changes remain child specific. The text segment is read-only so both parent and child share the same text segment. C fork function article explains more about fork(). Linux Threads vs Light Weight Processes # Threads in Linux are nothing but a flow of execution of the process. A process containing multiple execution flows is known as multi-threaded process. For a non multi-threaded process there is only execution flow that is the main execution flow and hence it is also known as single threaded process. For Linux kernel , there is no concept of thread . Each thread is viewed by kernel as a separate process but these processes are somewhat different from other normal processes. I will explain the difference in following paragraphs. Threads are often mixed with the term Light Weight Processes or LWPs. The reason dates back to those times when Linux supported threads at user level only . This means that even a multi-threaded application was viewed by kernel as a single process only. This posed big challenges for the library that managed these user level threads because it had to take care of cases that a thread execution did not hinder if any other thread issued a blocking call. Later on the implementation changed and processes were attached to each thread so that kernel can take care of them. But, as discussed earlier, Linux kernel does not see them as threads, each thread is viewed as a process inside kernel. These processes are known as light weight processes . The main difference between a light weight process (LWP) and a normal process is that LWPs share same address space and other resources like open files etc. As some resources are shared so these processes are considered to be light weight as compared to other normal processes and hence the name light weight processes. So, effectively we can say that threads and light weight processes are same. It\u2019s just that thread is a term that is used at user level while light weight process is a term used at kernel level. From implementation point of view, threads are created using functions exposed by POSIX compliant pthread library in Linux. Internally, the clone() function is used to create a normal as well as a light weight process . This means that to create a normal process fork() is used that further calls clone() with appropriate arguments while to create a thread or LWP, a function from pthread library calls clone() with relevant flags. So, the main difference is generated by using different flags that can be passed to clone() function. Read more about fork() and clone() on their respective man pages. How to Create Threads in Linux explains more about threads. Linux Process States # Life cycle of a normal Linux process seems pretty much like real life. Processes are born, share resources with parents for sometime, get their own copy of resources when they are ready to make changes, go through various states depending upon their priority and then finally die. In this section will will discuss various states of Linux processes : RUNNING \u2013 This state specifies that the process is either in execution or waiting to get executed. INTERRUPTIBLE \u2013 This state specifies that the process is waiting to get interrupted as it is in sleep mode and waiting for some action to happen that can wake this process up. The action can be a hardware interrupt, signal etc. UN-INTERRUPTIBLE \u2013 It is just like the INTERRUPTIBLE state, the only difference being that a process in this state cannot be waken up by delivering a signal. STOPPED \u2013 This state specifies that the process has been stopped. This may happen if a signal like SIGSTOP, SIGTTIN etc is delivered to the process. TRACED \u2013 This state specifies that the process is being debugged. Whenever the process is stopped by debugger (to help user debug the code) the process enters this state. ZOMBIE \u2013 This state specifies that the process is terminated but still hanging around in kernel process table because the parent of this process has still not fetched the termination status of this process. Parent uses wait() family of functions to fetch the termination status. DEAD \u2013 This state specifies that the process is terminated and entry is removed from process table. This state is achieved when the parent successfully fetches the termination status as explained in ZOMBIE state.","title":"Linux-OS-implementation-of-process-model-02"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-02-Linux-OS-implementation-of-process-model/#what-are-linux-processes-threads-light-weight-processes-and-process-state","text":"Linux has evolved a lot since its inception. It has become the most widely used operating system when in comes to servers and mission critical work. Though its not easy to understand Linux as a whole but there are aspects which are fundamental to Linux and worth understanding. In this article, we will discuss about Linux processes, threads and light weight processes and understand the difference between them. Towards the end, we will also discuss various states for Linux processes.","title":"What are Linux Processes, Threads, Light Weight Processes, and Process State"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-02-Linux-OS-implementation-of-process-model/#linux-processes","text":"In a very basic form, Linux process can be visualized as running instance of a program. For example, just open a text editor on your Linux box and a text editor process will be born. Here is an example when I opened gedit on my machine : $ gedit & [1] 5560 $ ps -aef | grep gedit 1000 5560 2684 9 17:34 pts/0 00:00:00 gedit First command ( gedit &) opens gedit window while second ps command ( ps -aef | grep gedit ) checks if there is an associated process. In the result you can see that there is a process associated with gedit . Processes are fundamental to Linux as each and every work done by the OS is done in terms of and by the processes. Just think of anything and you will see that it is a process. This is because any work that is intended to be done requires system resources ( that are provided by kernel) and it is a process that is viewed by kernel as an entity to which it can provide system resources. Processes have priority based on which kernel context switches them. A process can be pre-empted if a process with higher priority is ready to be executed. For example, if a process is waiting for a system resource like some text from text file kept on disk then kernel can schedule a higher priority process and get back to the waiting process when data is available. This keeps the ball rolling for an operating system as a whole and gives user a feeling that tasks are being run in parallel. Processes can talk to other processes using Inter process communication methods and can share data using techniques like shared memory. In Linux, fork() is used to create new processes. These new processes are called as child processes and each child process initially shares all the segments like text, stack, heap etc until child tries to make any change to stack or heap. In case of any change, a separate copy of stack and heap segments are prepared for child so that changes remain child specific. The text segment is read-only so both parent and child share the same text segment. C fork function article explains more about fork().","title":"Linux Processes"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-02-Linux-OS-implementation-of-process-model/#linux-threads-vs-light-weight-processes","text":"Threads in Linux are nothing but a flow of execution of the process. A process containing multiple execution flows is known as multi-threaded process. For a non multi-threaded process there is only execution flow that is the main execution flow and hence it is also known as single threaded process. For Linux kernel , there is no concept of thread . Each thread is viewed by kernel as a separate process but these processes are somewhat different from other normal processes. I will explain the difference in following paragraphs. Threads are often mixed with the term Light Weight Processes or LWPs. The reason dates back to those times when Linux supported threads at user level only . This means that even a multi-threaded application was viewed by kernel as a single process only. This posed big challenges for the library that managed these user level threads because it had to take care of cases that a thread execution did not hinder if any other thread issued a blocking call. Later on the implementation changed and processes were attached to each thread so that kernel can take care of them. But, as discussed earlier, Linux kernel does not see them as threads, each thread is viewed as a process inside kernel. These processes are known as light weight processes . The main difference between a light weight process (LWP) and a normal process is that LWPs share same address space and other resources like open files etc. As some resources are shared so these processes are considered to be light weight as compared to other normal processes and hence the name light weight processes. So, effectively we can say that threads and light weight processes are same. It\u2019s just that thread is a term that is used at user level while light weight process is a term used at kernel level. From implementation point of view, threads are created using functions exposed by POSIX compliant pthread library in Linux. Internally, the clone() function is used to create a normal as well as a light weight process . This means that to create a normal process fork() is used that further calls clone() with appropriate arguments while to create a thread or LWP, a function from pthread library calls clone() with relevant flags. So, the main difference is generated by using different flags that can be passed to clone() function. Read more about fork() and clone() on their respective man pages. How to Create Threads in Linux explains more about threads.","title":"Linux Threads vs Light Weight Processes"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-02-Linux-OS-implementation-of-process-model/#linux-process-states","text":"Life cycle of a normal Linux process seems pretty much like real life. Processes are born, share resources with parents for sometime, get their own copy of resources when they are ready to make changes, go through various states depending upon their priority and then finally die. In this section will will discuss various states of Linux processes : RUNNING \u2013 This state specifies that the process is either in execution or waiting to get executed. INTERRUPTIBLE \u2013 This state specifies that the process is waiting to get interrupted as it is in sleep mode and waiting for some action to happen that can wake this process up. The action can be a hardware interrupt, signal etc. UN-INTERRUPTIBLE \u2013 It is just like the INTERRUPTIBLE state, the only difference being that a process in this state cannot be waken up by delivering a signal. STOPPED \u2013 This state specifies that the process has been stopped. This may happen if a signal like SIGSTOP, SIGTTIN etc is delivered to the process. TRACED \u2013 This state specifies that the process is being debugged. Whenever the process is stopped by debugger (to help user debug the code) the process enters this state. ZOMBIE \u2013 This state specifies that the process is terminated but still hanging around in kernel process table because the parent of this process has still not fetched the termination status of this process. Parent uses wait() family of functions to fetch the termination status. DEAD \u2013 This state specifies that the process is terminated and entry is removed from process table. This state is achieved when the parent successfully fetches the termination status as explained in ZOMBIE state.","title":"Linux Process States"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-03-Linux-OS-implementation-of-process-model/","text":"What are Linux Processes, Threads, Light Weight Processes, and Process State # Linux has evolved a lot since its inception. It has become the most widely used operating system when in comes to servers and mission critical work. Though its not easy to understand Linux as a whole but there are aspects which are fundamental to Linux and worth understanding. In this article, we will discuss about Linux processes, threads and light weight processes and understand the difference between them. Towards the end, we will also discuss various states for Linux processes. Linux Processes # In a very basic form, Linux process can be visualized as running instance of a program. For example, just open a text editor on your Linux box and a text editor process will be born. Here is an example when I opened gedit on my machine : $ gedit & [1] 5560 $ ps -aef | grep gedit 1000 5560 2684 9 17:34 pts/0 00:00:00 gedit First command ( gedit &) opens gedit window while second ps command ( ps -aef | grep gedit ) checks if there is an associated process. In the result you can see that there is a process associated with gedit . Processes are fundamental to Linux as each and every work done by the OS is done in terms of and by the processes. Just think of anything and you will see that it is a process. This is because any work that is intended to be done requires system resources ( that are provided by kernel) and it is a process that is viewed by kernel as an entity to which it can provide system resources. Processes have priority based on which kernel context switches them. A process can be pre-empted if a process with higher priority is ready to be executed. For example, if a process is waiting for a system resource like some text from text file kept on disk then kernel can schedule a higher priority process and get back to the waiting process when data is available. This keeps the ball rolling for an operating system as a whole and gives user a feeling that tasks are being run in parallel. Processes can talk to other processes using Inter process communication methods and can share data using techniques like shared memory. In Linux, fork() is used to create new processes. These new processes are called as child processes and each child process initially shares all the segments like text, stack, heap etc until child tries to make any change to stack or heap. In case of any change, a separate copy of stack and heap segments are prepared for child so that changes remain child specific. The text segment is read-only so both parent and child share the same text segment. C fork function article explains more about fork(). Linux Threads vs Light Weight Processes # Threads in Linux are nothing but a flow of execution of the process. A process containing multiple execution flows is known as multi-threaded process. For a non multi-threaded process there is only execution flow that is the main execution flow and hence it is also known as single threaded process. For Linux kernel , there is no concept of thread . Each thread is viewed by kernel as a separate process but these processes are somewhat different from other normal processes. I will explain the difference in following paragraphs. Threads are often mixed with the term Light Weight Processes or LWPs. The reason dates back to those times when Linux supported threads at user level only . This means that even a multi-threaded application was viewed by kernel as a single process only. This posed big challenges for the library that managed these user level threads because it had to take care of cases that a thread execution did not hinder if any other thread issued a blocking call. Later on the implementation changed and processes were attached to each thread so that kernel can take care of them. But, as discussed earlier, Linux kernel does not see them as threads, each thread is viewed as a process inside kernel. These processes are known as light weight processes . The main difference between a light weight process (LWP) and a normal process is that LWPs share same address space and other resources like open files etc. As some resources are shared so these processes are considered to be light weight as compared to other normal processes and hence the name light weight processes. So, effectively we can say that threads and light weight processes are same. It\u2019s just that thread is a term that is used at user level while light weight process is a term used at kernel level. From implementation point of view, threads are created using functions exposed by POSIX compliant pthread library in Linux. Internally, the clone() function is used to create a normal as well as a light weight process . This means that to create a normal process fork() is used that further calls clone() with appropriate arguments while to create a thread or LWP, a function from pthread library calls clone() with relevant flags. So, the main difference is generated by using different flags that can be passed to clone() function. Read more about fork() and clone() on their respective man pages. How to Create Threads in Linux explains more about threads. Linux Process States # Life cycle of a normal Linux process seems pretty much like real life. Processes are born, share resources with parents for sometime, get their own copy of resources when they are ready to make changes, go through various states depending upon their priority and then finally die. In this section will will discuss various states of Linux processes : RUNNING \u2013 This state specifies that the process is either in execution or waiting to get executed. INTERRUPTIBLE \u2013 This state specifies that the process is waiting to get interrupted as it is in sleep mode and waiting for some action to happen that can wake this process up. The action can be a hardware interrupt, signal etc. UN-INTERRUPTIBLE \u2013 It is just like the INTERRUPTIBLE state, the only difference being that a process in this state cannot be waken up by delivering a signal. STOPPED \u2013 This state specifies that the process has been stopped. This may happen if a signal like SIGSTOP, SIGTTIN etc is delivered to the process. TRACED \u2013 This state specifies that the process is being debugged. Whenever the process is stopped by debugger (to help user debug the code) the process enters this state. ZOMBIE \u2013 This state specifies that the process is terminated but still hanging around in kernel process table because the parent of this process has still not fetched the termination status of this process. Parent uses wait() family of functions to fetch the termination status. DEAD \u2013 This state specifies that the process is terminated and entry is removed from process table. This state is achieved when the parent successfully fetches the termination status as explained in ZOMBIE state.","title":"Linux-OS-implementation-of-process-model-03"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-03-Linux-OS-implementation-of-process-model/#what-are-linux-processes-threads-light-weight-processes-and-process-state","text":"Linux has evolved a lot since its inception. It has become the most widely used operating system when in comes to servers and mission critical work. Though its not easy to understand Linux as a whole but there are aspects which are fundamental to Linux and worth understanding. In this article, we will discuss about Linux processes, threads and light weight processes and understand the difference between them. Towards the end, we will also discuss various states for Linux processes.","title":"What are Linux Processes, Threads, Light Weight Processes, and Process State"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-03-Linux-OS-implementation-of-process-model/#linux-processes","text":"In a very basic form, Linux process can be visualized as running instance of a program. For example, just open a text editor on your Linux box and a text editor process will be born. Here is an example when I opened gedit on my machine : $ gedit & [1] 5560 $ ps -aef | grep gedit 1000 5560 2684 9 17:34 pts/0 00:00:00 gedit First command ( gedit &) opens gedit window while second ps command ( ps -aef | grep gedit ) checks if there is an associated process. In the result you can see that there is a process associated with gedit . Processes are fundamental to Linux as each and every work done by the OS is done in terms of and by the processes. Just think of anything and you will see that it is a process. This is because any work that is intended to be done requires system resources ( that are provided by kernel) and it is a process that is viewed by kernel as an entity to which it can provide system resources. Processes have priority based on which kernel context switches them. A process can be pre-empted if a process with higher priority is ready to be executed. For example, if a process is waiting for a system resource like some text from text file kept on disk then kernel can schedule a higher priority process and get back to the waiting process when data is available. This keeps the ball rolling for an operating system as a whole and gives user a feeling that tasks are being run in parallel. Processes can talk to other processes using Inter process communication methods and can share data using techniques like shared memory. In Linux, fork() is used to create new processes. These new processes are called as child processes and each child process initially shares all the segments like text, stack, heap etc until child tries to make any change to stack or heap. In case of any change, a separate copy of stack and heap segments are prepared for child so that changes remain child specific. The text segment is read-only so both parent and child share the same text segment. C fork function article explains more about fork().","title":"Linux Processes"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-03-Linux-OS-implementation-of-process-model/#linux-threads-vs-light-weight-processes","text":"Threads in Linux are nothing but a flow of execution of the process. A process containing multiple execution flows is known as multi-threaded process. For a non multi-threaded process there is only execution flow that is the main execution flow and hence it is also known as single threaded process. For Linux kernel , there is no concept of thread . Each thread is viewed by kernel as a separate process but these processes are somewhat different from other normal processes. I will explain the difference in following paragraphs. Threads are often mixed with the term Light Weight Processes or LWPs. The reason dates back to those times when Linux supported threads at user level only . This means that even a multi-threaded application was viewed by kernel as a single process only. This posed big challenges for the library that managed these user level threads because it had to take care of cases that a thread execution did not hinder if any other thread issued a blocking call. Later on the implementation changed and processes were attached to each thread so that kernel can take care of them. But, as discussed earlier, Linux kernel does not see them as threads, each thread is viewed as a process inside kernel. These processes are known as light weight processes . The main difference between a light weight process (LWP) and a normal process is that LWPs share same address space and other resources like open files etc. As some resources are shared so these processes are considered to be light weight as compared to other normal processes and hence the name light weight processes. So, effectively we can say that threads and light weight processes are same. It\u2019s just that thread is a term that is used at user level while light weight process is a term used at kernel level. From implementation point of view, threads are created using functions exposed by POSIX compliant pthread library in Linux. Internally, the clone() function is used to create a normal as well as a light weight process . This means that to create a normal process fork() is used that further calls clone() with appropriate arguments while to create a thread or LWP, a function from pthread library calls clone() with relevant flags. So, the main difference is generated by using different flags that can be passed to clone() function. Read more about fork() and clone() on their respective man pages. How to Create Threads in Linux explains more about threads.","title":"Linux Threads vs Light Weight Processes"},{"location":"Kernel/Guide/Linux-OS's-multitasking/03-03-Linux-OS-implementation-of-process-model/#linux-process-states","text":"Life cycle of a normal Linux process seems pretty much like real life. Processes are born, share resources with parents for sometime, get their own copy of resources when they are ready to make changes, go through various states depending upon their priority and then finally die. In this section will will discuss various states of Linux processes : RUNNING \u2013 This state specifies that the process is either in execution or waiting to get executed. INTERRUPTIBLE \u2013 This state specifies that the process is waiting to get interrupted as it is in sleep mode and waiting for some action to happen that can wake this process up. The action can be a hardware interrupt, signal etc. UN-INTERRUPTIBLE \u2013 It is just like the INTERRUPTIBLE state, the only difference being that a process in this state cannot be waken up by delivering a signal. STOPPED \u2013 This state specifies that the process has been stopped. This may happen if a signal like SIGSTOP, SIGTTIN etc is delivered to the process. TRACED \u2013 This state specifies that the process is being debugged. Whenever the process is stopped by debugger (to help user debug the code) the process enters this state. ZOMBIE \u2013 This state specifies that the process is terminated but still hanging around in kernel process table because the parent of this process has still not fetched the termination status of this process. Parent uses wait() family of functions to fetch the termination status. DEAD \u2013 This state specifies that the process is terminated and entry is removed from process table. This state is achieved when the parent successfully fetches the termination status as explained in ZOMBIE state.","title":"Linux Process States"},{"location":"Kernel/Guide/Linux-OS's-multitasking/04-Thread/","text":"\u4e0d\u540c\u7684OS\u6709\u7740\u4e0d\u540c\u7684\u5b9e\u73b0\uff0c\u4f46\u662f\u5b83\u4eec\u80af\u5b9a\u90fd\u4f1a\u7b26\u5408\u6807\u51c6\u3002 \u6309\u7167\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\u6d41\u7a0b\u6765\u770b\uff0c\u5e94\u8be5\u662f\u9996\u5148\u6709\u8ba1\u7b97\u673a\u7406\u8bba\u5b66\u5bb6\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\uff0c\u7136\u540e\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u518d\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\u3002\u6240\u4ee5\u4ece\u6807\u51c6\u7684\u51fa\u73b0\u5230\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u5b9e\u73b0\u8fd9\u4e9b\u6807\u51c6\uff0c\u4e24\u8005\u4e4b\u95f4\u662f\u6709\u4e00\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u3002\u4e0d\u540c\u5382\u5546\u7684\u5bf9\u540c\u4e00\u6982\u5ff5/\u6807\u51c6\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u4e0d\u65ad\u5730\u6f14\u8fdb\u3002\u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u4e66\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb \u4e0b\u9762\u4ee5operating system\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u76ee\u524d\u5b58\u5728\u7740\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a user level thread\uff0c\u5e38\u79f0\u4e3auser thread kernel level thread \u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/ What is a user thread and a kernel thread? \u663e\u7136\uff0c\u5bf9\u4e8e\u6807\u51c6\u6240\u63d0\u51fa\u7684 Thread (computing) \uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5173\u4e8e\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Thread (computing) \u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002","title":"Thread"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/","text":"\u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0 # Thread (computing) \u548c Process (computing) \u662fsoftware engineer\u975e\u5e38\u719f\u7cfb\u7684\u6982\u5ff5\uff0c\u5b83\u4eec\u662f\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u4e24\u4e2a\u6982\u5ff5\uff0c\u6709\u7740\u51c6\u786e\u7684\u542b\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u4e5f\u662f\u975e\u5e38\u6e05\u695a\u7684\u3002\u6309\u7167\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\u6d41\u7a0b\u6765\u770b\uff0c\u5e94\u8be5\u662f\u9996\u5148\u6709\u8ba1\u7b97\u673a\u7406\u8bba\u5b66\u5bb6\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\uff0c\u7136\u540e\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u518d\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\u3002\u6240\u4ee5\u4ece\u6807\u51c6\u7684\u51fa\u73b0\u5230\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u5b9e\u73b0\u8fd9\u4e9b\u6807\u51c6\uff0c\u4e24\u8005\u4e4b\u95f4\u662f\u6709\u4e00\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u3002\u4e0d\u540c\u5382\u5546\u7684\u5bf9\u540c\u4e00\u6982\u5ff5/\u6807\u51c6\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u4e0d\u65ad\u5730\u6f14\u8fdb\u3002\u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u4e66\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb \u4e0b\u9762\u4ee5operating system\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u76ee\u524d\u5b58\u5728\u7740\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a user level thread\uff0c\u5e38\u79f0\u4e3auser thread kernel level thread \u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/ What is a user thread and a kernel thread? \u663e\u7136\uff0c\u5bf9\u4e8e\u6807\u51c6\u6240\u63d0\u51fa\u7684 Thread (computing) \uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5173\u4e8e\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Thread (computing) \u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002 \u7406\u89e3\u6807\u51c6 # \u63cf\u8ff0\u6807\u51c6\u7684process\u548cthread\u5b9a\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e9b\u9700\u8981\u7740\u91cd\u5f3a\u8c03\u7684\uff1a process\u662fOS\u7684\u6982\u5ff5\uff0c\u5728instruction\u5c42\u7ea7\u5e76\u6ca1\u6709process\u7684\u6982\u5ff5\u3002OS\u4f7f\u7528process\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b9e\u73b0 multitasking \uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528hardware\u3002process\u662fprogram\u7684\u6267\u884c\uff0c\u5b83\u662fOS\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0c\u4e0d\u540cprocess\u4e4b\u95f4\u7684\u8d44\u6e90\u9700\u8981\u5b8c\u5168\u9694\u79bb\uff08\u7279\u6b8a\u60c5\u51b5\u9664\u5916\uff09\uff0cOS\u4e2d\u7684\u6240\u6709process\u5171\u4eabOS\u6240\u7ba1\u7406\u7684hardware\u8d44\u6e90\u3002OS\u9700\u8981\u6e05\u695a\u5730\u77e5\u9053process\u548c\u8d44\u6e90\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5373\u4e00\u4e2aprocess\u62e5\u6709\u54ea\u4e9bresource\u3002 \u8865\u5145\u5185\u5bb9 # \u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u4e00\u4e9b\u5206\u6790\u5730\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\u3002 What the difference between lightweight process and thread? # I found an answer to the question here . But I don't understand some ideas in the answer. For instance, lightweight process is said to share its logical address space with other processes. What does it mean? I can understand the same situation with 2 threads: both of them share one address space, so both of them can read any variables from bss segment (for example). But we've got a lot of different processes with different bss sections and I dunno how to share all of them. A # From MSDN, Threads and Processes : Processes exist in the operating system and correspond to what users see as programs or applications. A thread, on the other hand, exists within a process. For this reason, threads are sometimes referred to as light-weight processes. Each process consists of one or more threads. A # I am not sure that answers are correct here, so let me post my version. There is a difference between process - LWP (lightweight process) and user thread . I will leave process definition aside since that's more or less known and focus on LWP vs user threads . LWP is what essentially are called today threads . Originally, user thread meant a thread that is managed by the application itself and the kernel does not know anything about it. LWP , on the other hand, is a unit of scheduling and execution by the kernel . Example: Let's assume that system has 3 other processes running and scheduling is round-robin without priorities. And you have 1 processor/core. Option 1 . You have 2 user threads using one LWP. That means that from OS perspective you have ONE scheduling unit. Totally there are 4 LWP running (3 others + 1 yours). Your LWP gets 1/4 of total CPU time and since you have 2 user threads, each of them gets 1/8 of total CPU time (depends on your implementation) Option2 . You have 2 LWP. From OS perspective, you have TWO scheduling units. Totally there are 5 LWP running. Your LWP gets 1/5 of total CPU time EACH and your application get's 2/5 of CPU. Another rough difference - LWP has pid (process id), user threads do not. For some reason, naming got little messed and we refer to LWP as threads. There are definitely more differences, but please, refer to slides.http://www.cosc.brocku.ca/Offerings/4P13/slides/threads.ppt EDIT: After posting, I found a good article that explains everything in more details and is in better English than I write. http://www.thegeekstuff.com/2013/11/linux-process-and-threads/ What is the difference between LWP and threads? # This explains the difference between LWP-Process-Thread: A light-weight process (LWP) is a means of achieving multitasking. In contrast to a regular (full-blown) process, an LWP shares all (or most of) its logical address space and system resources with other process(es) \uff08\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684process\u6240\u6307\u7684\u662flight weight process\uff0c\u800c\u4e0d\u662f\u6211\u4eec\u5bfb\u5e38\u610f\u4e49\u7684process\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f20\u7edf\u610f\u4e49\u7684process\u4f7f\u7528full-blown process\u6765\u8868\u793a\uff09; in contrast to a thread, a light-weight process has its own private process identifier and parenthood relationships with other processes. Moreover, while a thread can either be managed at the application level or by the kernel , an LWP is always managed by the kernel and it is scheduled as a regular process. One significant example of a kernel that supports LWPs is the Linux kernel . On most systems, a light-weight process also differs from a full-blown process , in that it only consists of the bare minimum execution context and accounting information that is needed by the scheduler, hence the term light-weight . Generally, a process \uff08full-blown process\uff09 refers to an instance of a program, while an LWP represents a thread of execution of a program (indeed, LWP s can be conveniently used to implement thread s, if the underlying kernel does not directly support them). Since a thread of execution does not need as much state information as a process, a light-weight process does not carry such information. As a consequence of the fact that LWPs share most of their resources with other LWPs, they are unsuitable for certain applications, where multiple full-blown processes are needed, e.g. to avoid memory leaks (a process can be replaced by another one) or to achieve privilege separation (processes can run under other credentials and have other permissions). Using multiple processes also allows the application to more easily survive if a process of the pool crashes or is exploited. What are the relations between processes, kernel threads, lightweight processes and user threads in Unix? [closed] #","title":"\u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#_1","text":"Thread (computing) \u548c Process (computing) \u662fsoftware engineer\u975e\u5e38\u719f\u7cfb\u7684\u6982\u5ff5\uff0c\u5b83\u4eec\u662f\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u4e24\u4e2a\u6982\u5ff5\uff0c\u6709\u7740\u51c6\u786e\u7684\u542b\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u4e5f\u662f\u975e\u5e38\u6e05\u695a\u7684\u3002\u6309\u7167\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\u6d41\u7a0b\u6765\u770b\uff0c\u5e94\u8be5\u662f\u9996\u5148\u6709\u8ba1\u7b97\u673a\u7406\u8bba\u5b66\u5bb6\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\uff0c\u7136\u540e\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u518d\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\u3002\u6240\u4ee5\u4ece\u6807\u51c6\u7684\u51fa\u73b0\u5230\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u5b9e\u73b0\u8fd9\u4e9b\u6807\u51c6\uff0c\u4e24\u8005\u4e4b\u95f4\u662f\u6709\u4e00\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u3002\u4e0d\u540c\u5382\u5546\u7684\u5bf9\u540c\u4e00\u6982\u5ff5/\u6807\u51c6\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u4e0d\u65ad\u5730\u6f14\u8fdb\u3002\u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u4e66\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb \u4e0b\u9762\u4ee5operating system\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u76ee\u524d\u5b58\u5728\u7740\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a user level thread\uff0c\u5e38\u79f0\u4e3auser thread kernel level thread \u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/ What is a user thread and a kernel thread? \u663e\u7136\uff0c\u5bf9\u4e8e\u6807\u51c6\u6240\u63d0\u51fa\u7684 Thread (computing) \uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5173\u4e8e\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Thread (computing) \u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002","title":"\u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#_2","text":"\u63cf\u8ff0\u6807\u51c6\u7684process\u548cthread\u5b9a\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e9b\u9700\u8981\u7740\u91cd\u5f3a\u8c03\u7684\uff1a process\u662fOS\u7684\u6982\u5ff5\uff0c\u5728instruction\u5c42\u7ea7\u5e76\u6ca1\u6709process\u7684\u6982\u5ff5\u3002OS\u4f7f\u7528process\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b9e\u73b0 multitasking \uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528hardware\u3002process\u662fprogram\u7684\u6267\u884c\uff0c\u5b83\u662fOS\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0c\u4e0d\u540cprocess\u4e4b\u95f4\u7684\u8d44\u6e90\u9700\u8981\u5b8c\u5168\u9694\u79bb\uff08\u7279\u6b8a\u60c5\u51b5\u9664\u5916\uff09\uff0cOS\u4e2d\u7684\u6240\u6709process\u5171\u4eabOS\u6240\u7ba1\u7406\u7684hardware\u8d44\u6e90\u3002OS\u9700\u8981\u6e05\u695a\u5730\u77e5\u9053process\u548c\u8d44\u6e90\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5373\u4e00\u4e2aprocess\u62e5\u6709\u54ea\u4e9bresource\u3002","title":"\u7406\u89e3\u6807\u51c6"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#_3","text":"\u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u4e00\u4e9b\u5206\u6790\u5730\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\u3002","title":"\u8865\u5145\u5185\u5bb9"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#what-the-difference-between-lightweight-process-and-thread","text":"I found an answer to the question here . But I don't understand some ideas in the answer. For instance, lightweight process is said to share its logical address space with other processes. What does it mean? I can understand the same situation with 2 threads: both of them share one address space, so both of them can read any variables from bss segment (for example). But we've got a lot of different processes with different bss sections and I dunno how to share all of them.","title":"What the difference between lightweight process and thread?"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#a","text":"From MSDN, Threads and Processes : Processes exist in the operating system and correspond to what users see as programs or applications. A thread, on the other hand, exists within a process. For this reason, threads are sometimes referred to as light-weight processes. Each process consists of one or more threads.","title":"A"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#a_1","text":"I am not sure that answers are correct here, so let me post my version. There is a difference between process - LWP (lightweight process) and user thread . I will leave process definition aside since that's more or less known and focus on LWP vs user threads . LWP is what essentially are called today threads . Originally, user thread meant a thread that is managed by the application itself and the kernel does not know anything about it. LWP , on the other hand, is a unit of scheduling and execution by the kernel . Example: Let's assume that system has 3 other processes running and scheduling is round-robin without priorities. And you have 1 processor/core. Option 1 . You have 2 user threads using one LWP. That means that from OS perspective you have ONE scheduling unit. Totally there are 4 LWP running (3 others + 1 yours). Your LWP gets 1/4 of total CPU time and since you have 2 user threads, each of them gets 1/8 of total CPU time (depends on your implementation) Option2 . You have 2 LWP. From OS perspective, you have TWO scheduling units. Totally there are 5 LWP running. Your LWP gets 1/5 of total CPU time EACH and your application get's 2/5 of CPU. Another rough difference - LWP has pid (process id), user threads do not. For some reason, naming got little messed and we refer to LWP as threads. There are definitely more differences, but please, refer to slides.http://www.cosc.brocku.ca/Offerings/4P13/slides/threads.ppt EDIT: After posting, I found a good article that explains everything in more details and is in better English than I write. http://www.thegeekstuff.com/2013/11/linux-process-and-threads/","title":"A"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#what-is-the-difference-between-lwp-and-threads","text":"This explains the difference between LWP-Process-Thread: A light-weight process (LWP) is a means of achieving multitasking. In contrast to a regular (full-blown) process, an LWP shares all (or most of) its logical address space and system resources with other process(es) \uff08\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684process\u6240\u6307\u7684\u662flight weight process\uff0c\u800c\u4e0d\u662f\u6211\u4eec\u5bfb\u5e38\u610f\u4e49\u7684process\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f20\u7edf\u610f\u4e49\u7684process\u4f7f\u7528full-blown process\u6765\u8868\u793a\uff09; in contrast to a thread, a light-weight process has its own private process identifier and parenthood relationships with other processes. Moreover, while a thread can either be managed at the application level or by the kernel , an LWP is always managed by the kernel and it is scheduled as a regular process. One significant example of a kernel that supports LWPs is the Linux kernel . On most systems, a light-weight process also differs from a full-blown process , in that it only consists of the bare minimum execution context and accounting information that is needed by the scheduler, hence the term light-weight . Generally, a process \uff08full-blown process\uff09 refers to an instance of a program, while an LWP represents a thread of execution of a program (indeed, LWP s can be conveniently used to implement thread s, if the underlying kernel does not directly support them). Since a thread of execution does not need as much state information as a process, a light-weight process does not carry such information. As a consequence of the fact that LWPs share most of their resources with other LWPs, they are unsuitable for certain applications, where multiple full-blown processes are needed, e.g. to avoid memory leaks (a process can be replaced by another one) or to achieve privilege separation (processes can run under other credentials and have other permissions). Using multiple processes also allows the application to more easily survive if a process of the pool crashes or is exploited.","title":"What is the difference between LWP and threads?"},{"location":"Kernel/Guide/Linux-OS's-multitasking/VS-process-VS-thread-VS-lightweight-process/#what-are-the-relations-between-processes-kernel-threads-lightweight-processes-and-user-threads-in-unix-closed","text":"","title":"What are the relations between processes, kernel threads, lightweight processes and user threads in Unix? [closed]"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/","text":"\u5173\u4e8e\u672c\u7ae0 # \u5728 \u6307\u4ee4\u5c42 \u538b\u6839\u5c31\u6ca1\u6709\u51fd\u6570\u3001\u51fd\u6570\u53c2\u6570\u7b49\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u90fd\u662fhigh level program language\u5efa\u7acb\u8d77\u6765\u7684\uff0c\u5b83\u4eec\u7684\u5b9e\u73b0\u90fd\u662f\u5efa\u7acb\u5728 \u6307\u4ee4\u5c42 \u4e4b\u4e0a\uff0c\u8fde\u63a5\u51fd\u6570\u8c03\u7528\u548c\u6307\u4ee4\u5c42\u7684\u6b63\u662f\u6240\u8c13\u7684calling convention\uff0c\u5b83\u7531compiler\u6765\u5b9e\u73b0\u7684\u3002 \u672c\u7ae0\u6240\u5c06\u63a2\u7d22\u5404\u79cd\u5728high level program language\u4e2d\u5efa\u7acb\u7684\u6982\u5ff5\u662f\u5982\u4f55\u901a\u8fc7\u6307\u4ee4\u6765\u8fdb\u884c\u5b9e\u73b0\u7684\u3002 \u5982\u4f55\u5b9e\u73b0\u63a7\u5236\u6d41 # \u6240\u8c13\u7684 Control flow \u5176\u5b9e\u5c31\u662f program counter # \u6211\u4eec\u5e38\u5e38\u542c\u5230Control flow\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Control flow \u5bf9\u5b83\u7684\u603b\u7ed3\u662f\u975e\u5e38\u5168\u9762\u7684\uff0c\u4ecehigh-level programming language\u7ea7\u522b\uff08\u5728high-level programming language\u4e2d\u6709control flow statement\uff0c\u6bd4\u5982return\u3001goto\u7b49\uff09\uff0c\u5230 machine language \u7ea7\u522b\uff08\u8fd9\u662f\u6700\u5e95\u5c42\u4e86\uff1b\u4ee5x86 \u4e3a\u4f8b\uff0c JMP \u6307\u4ee4\uff0c\u66f4\u591a\u53c2\u89c1 X86 Assembly/Control Flow \uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7684\u662f machine language \u7ea7\u522b\uff0c\u6b63\u5982\u5176\u6240\u603b\u7ed3\u7684\uff1a At the level of machine language or assembly language , control flow instructions usually work by altering the program counter . For some central processing units (CPUs), the only control flow instructions available are conditional or unconditional branch instructions, also termed jumps. CPU\u7684 program counter \u9ed8\u8ba4\u884c\u4e3a\u662f\uff1a\u81ea\u52a01\u7684\uff0c\u6240\u4ee5\u7a0b\u5e8f\u9ed8\u8ba4\u662f\u987a\u5e8f\u6267\u884c\u5373\u53ef\uff08\u7f16\u8bd1\u5668\u7f16\u8bd1\u751f\u6210\u7684machine language program\u5176\u5b9e\u662f\u987a\u5e8f\u7684\uff09\uff0c\u901a\u8fc7control flow instruction\uff0c\u53ef\u7528\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5404\u79cd\u6267\u884cflow\u3002 \u4e00\u4e2a\u4f8b\u5b50\u662f\u5728OS\u4e66\u76844.1. The Role of Interrupt Signals As the name suggests, interrupt signals provide a way to divert the processor to code outside the normal flow of control . When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter. \u6b63\u5728\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u770b\u5f85\u672c\u8d28\u4e0a\u76f8\u540c\u7684\u4e8b\u60c5\uff0c\u5728program language\u5c42\uff0c\u6211\u4eec\u628a\u5b83\u53eb\u505aflow of control\uff0c\u5728\u6307\u4ee4\u5c42\uff0c\u6211\u4eec\u5b83\u5176\u5b9e\u662fprogram counter\u3002 \u5982\u4f55\u5b9e\u73b0\u51fd\u6570\u8c03\u7528 #","title":"Introduction"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/#_1","text":"\u5728 \u6307\u4ee4\u5c42 \u538b\u6839\u5c31\u6ca1\u6709\u51fd\u6570\u3001\u51fd\u6570\u53c2\u6570\u7b49\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u90fd\u662fhigh level program language\u5efa\u7acb\u8d77\u6765\u7684\uff0c\u5b83\u4eec\u7684\u5b9e\u73b0\u90fd\u662f\u5efa\u7acb\u5728 \u6307\u4ee4\u5c42 \u4e4b\u4e0a\uff0c\u8fde\u63a5\u51fd\u6570\u8c03\u7528\u548c\u6307\u4ee4\u5c42\u7684\u6b63\u662f\u6240\u8c13\u7684calling convention\uff0c\u5b83\u7531compiler\u6765\u5b9e\u73b0\u7684\u3002 \u672c\u7ae0\u6240\u5c06\u63a2\u7d22\u5404\u79cd\u5728high level program language\u4e2d\u5efa\u7acb\u7684\u6982\u5ff5\u662f\u5982\u4f55\u901a\u8fc7\u6307\u4ee4\u6765\u8fdb\u884c\u5b9e\u73b0\u7684\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/#_2","text":"","title":"\u5982\u4f55\u5b9e\u73b0\u63a7\u5236\u6d41"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/#control-flowprogram-counter","text":"\u6211\u4eec\u5e38\u5e38\u542c\u5230Control flow\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Control flow \u5bf9\u5b83\u7684\u603b\u7ed3\u662f\u975e\u5e38\u5168\u9762\u7684\uff0c\u4ecehigh-level programming language\u7ea7\u522b\uff08\u5728high-level programming language\u4e2d\u6709control flow statement\uff0c\u6bd4\u5982return\u3001goto\u7b49\uff09\uff0c\u5230 machine language \u7ea7\u522b\uff08\u8fd9\u662f\u6700\u5e95\u5c42\u4e86\uff1b\u4ee5x86 \u4e3a\u4f8b\uff0c JMP \u6307\u4ee4\uff0c\u66f4\u591a\u53c2\u89c1 X86 Assembly/Control Flow \uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7684\u662f machine language \u7ea7\u522b\uff0c\u6b63\u5982\u5176\u6240\u603b\u7ed3\u7684\uff1a At the level of machine language or assembly language , control flow instructions usually work by altering the program counter . For some central processing units (CPUs), the only control flow instructions available are conditional or unconditional branch instructions, also termed jumps. CPU\u7684 program counter \u9ed8\u8ba4\u884c\u4e3a\u662f\uff1a\u81ea\u52a01\u7684\uff0c\u6240\u4ee5\u7a0b\u5e8f\u9ed8\u8ba4\u662f\u987a\u5e8f\u6267\u884c\u5373\u53ef\uff08\u7f16\u8bd1\u5668\u7f16\u8bd1\u751f\u6210\u7684machine language program\u5176\u5b9e\u662f\u987a\u5e8f\u7684\uff09\uff0c\u901a\u8fc7control flow instruction\uff0c\u53ef\u7528\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5404\u79cd\u6267\u884cflow\u3002 \u4e00\u4e2a\u4f8b\u5b50\u662f\u5728OS\u4e66\u76844.1. The Role of Interrupt Signals As the name suggests, interrupt signals provide a way to divert the processor to code outside the normal flow of control . When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter. \u6b63\u5728\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u770b\u5f85\u672c\u8d28\u4e0a\u76f8\u540c\u7684\u4e8b\u60c5\uff0c\u5728program language\u5c42\uff0c\u6211\u4eec\u628a\u5b83\u53eb\u505aflow of control\uff0c\u5728\u6307\u4ee4\u5c42\uff0c\u6211\u4eec\u5b83\u5176\u5b9e\u662fprogram counter\u3002","title":"\u6240\u8c13\u7684Control flow\u5176\u5b9e\u5c31\u662fprogram counter"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/#_3","text":"","title":"\u5982\u4f55\u5b9e\u73b0\u51fd\u6570\u8c03\u7528"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/","text":"Application binary interface # In computer software , an application binary interface ( ABI ) is an interface between two binary program modules ; often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. An ABI defines how data structures or computational routines are accessed in machine code , which is a low-level, hardware-dependent format; in contrast, an API defines this access in source code , which is a relatively high-level, hardware-independent, often human-readable format. A common aspect of an ABI is the calling convention , which determines how data is provided as input to or read as output from computational routines; examples are the x86 calling conventions . Adhering to an ABI (which may or may not be officially standardized) is usually the job of a compiler , operating system, or library author; however, an application programmer may have to deal with an ABI directly when writing a program in a mix of programming languages, which can be achieved by using foreign function calls . Description # ABIs cover details such as: a processor instruction set (with details like register file structure, stack organization, memory access types, ...) the sizes, layouts, and alignments of basic data types that the processor can directly access the calling convention , which controls how functions ' arguments are passed and return values are retrieved; for example, whether all parameters are passed on the stack or some are passed in registers , which registers are used for which function parameters, and whether the first function parameter passed on the stack is pushed first or last onto the stack how an application should make system calls to the operating system and, if the ABI specifies direct system calls rather than procedure calls to system call stubs, the system call numbers and in the case of a complete operating system ABI, the binary format of object files , program libraries and so on. Complete ABIs # A complete ABI, such as the Intel Binary Compatibility Standard (iBCS), allows a program from one operating system supporting that ABI to run without modifications on any other such system, provided that necessary shared libraries are present, and similar prerequisites are fulfilled. Other[ which? ] ABIs standardize details such as the C++ name mangling , exception propagation, and calling convention between compilers on the same platform, but do not require cross-platform compatibility. Embedded ABIs # An embedded-application binary interface (EABI) specifies standard conventions for file formats , data types, register usage, stack frame organization, and function parameter passing of an embedded software program, for use with an embedded operating system . Compilers that support the EABI create object code that is compatible with code generated by other such compilers, allowing developers to link libraries generated with one compiler with object code generated with another compiler. Developers writing their own assembly language code may also interface with assembly generated by a compliant compiler. EABIs are designed to optimize for performance within the limited resources of an embedded system. Therefore, EABIs omit most abstractions that are made between kernel and user code in complex operating systems. For example, dynamic linking is avoided to allow smaller executables and faster loading, fixed register usage allows more compact stacks and kernel calls, and running the application in privileged mode allows direct access to custom hardware operation without the indirection of calling a device driver. [ 4] The choice of EABI can affect performance.[ 5] [ 6] Widely used EABIs include PowerPC ,[ 4] ARM EABI2[ 7] and MIPS EABI.[ 8] Difference between API and ABI # Q: I am new to linux system programming and I came across API and ABI while reading Linux System Programming . Definition of API : An API defines the interfaces by which one piece of software communicates with another at the source level. Definition of ABI : Whereas an API defines a source interface, an ABI defines the low-level binary interface between two or more pieces of software on a particular architecture. It defines how an application interacts with itself, how an application interacts with the kernel , and how an application interacts with libraries . How can a program communicate at a source level ? What is a source level ? Is it related to source code in anyway? Or the source of the library gets included in the main program ? The only difference I know is API is mostly used by programmers and ABI is mostly used by compiler. A: by source level they mean something like include file to expose function definitions \u2013 Anycorn A : API: Application Program Interface # This is the set of public types/variables/functions that you expose from your application/library. In C/C++ this is what you expose in the header files that you ship with the application. ABI: Application Binary Interface # This is how the compiler builds an application. It defines things (but is not limited to): How parameters are passed to functions (registers/stack). Who cleans parameters from the stack (caller/callee). Where the return value is placed for return. How exceptions propagate.","title":"Application-binary-interface"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#application-binary-interface","text":"In computer software , an application binary interface ( ABI ) is an interface between two binary program modules ; often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. An ABI defines how data structures or computational routines are accessed in machine code , which is a low-level, hardware-dependent format; in contrast, an API defines this access in source code , which is a relatively high-level, hardware-independent, often human-readable format. A common aspect of an ABI is the calling convention , which determines how data is provided as input to or read as output from computational routines; examples are the x86 calling conventions . Adhering to an ABI (which may or may not be officially standardized) is usually the job of a compiler , operating system, or library author; however, an application programmer may have to deal with an ABI directly when writing a program in a mix of programming languages, which can be achieved by using foreign function calls .","title":"Application binary interface"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#description","text":"ABIs cover details such as: a processor instruction set (with details like register file structure, stack organization, memory access types, ...) the sizes, layouts, and alignments of basic data types that the processor can directly access the calling convention , which controls how functions ' arguments are passed and return values are retrieved; for example, whether all parameters are passed on the stack or some are passed in registers , which registers are used for which function parameters, and whether the first function parameter passed on the stack is pushed first or last onto the stack how an application should make system calls to the operating system and, if the ABI specifies direct system calls rather than procedure calls to system call stubs, the system call numbers and in the case of a complete operating system ABI, the binary format of object files , program libraries and so on.","title":"Description"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#complete-abis","text":"A complete ABI, such as the Intel Binary Compatibility Standard (iBCS), allows a program from one operating system supporting that ABI to run without modifications on any other such system, provided that necessary shared libraries are present, and similar prerequisites are fulfilled. Other[ which? ] ABIs standardize details such as the C++ name mangling , exception propagation, and calling convention between compilers on the same platform, but do not require cross-platform compatibility.","title":"Complete ABIs"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#embedded-abis","text":"An embedded-application binary interface (EABI) specifies standard conventions for file formats , data types, register usage, stack frame organization, and function parameter passing of an embedded software program, for use with an embedded operating system . Compilers that support the EABI create object code that is compatible with code generated by other such compilers, allowing developers to link libraries generated with one compiler with object code generated with another compiler. Developers writing their own assembly language code may also interface with assembly generated by a compliant compiler. EABIs are designed to optimize for performance within the limited resources of an embedded system. Therefore, EABIs omit most abstractions that are made between kernel and user code in complex operating systems. For example, dynamic linking is avoided to allow smaller executables and faster loading, fixed register usage allows more compact stacks and kernel calls, and running the application in privileged mode allows direct access to custom hardware operation without the indirection of calling a device driver. [ 4] The choice of EABI can affect performance.[ 5] [ 6] Widely used EABIs include PowerPC ,[ 4] ARM EABI2[ 7] and MIPS EABI.[ 8]","title":"Embedded ABIs"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#difference-between-api-and-abi","text":"Q: I am new to linux system programming and I came across API and ABI while reading Linux System Programming . Definition of API : An API defines the interfaces by which one piece of software communicates with another at the source level. Definition of ABI : Whereas an API defines a source interface, an ABI defines the low-level binary interface between two or more pieces of software on a particular architecture. It defines how an application interacts with itself, how an application interacts with the kernel , and how an application interacts with libraries . How can a program communicate at a source level ? What is a source level ? Is it related to source code in anyway? Or the source of the library gets included in the main program ? The only difference I know is API is mostly used by programmers and ABI is mostly used by compiler. A: by source level they mean something like include file to expose function definitions \u2013 Anycorn A :","title":"Difference between API and ABI"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#api-application-program-interface","text":"This is the set of public types/variables/functions that you expose from your application/library. In C/C++ this is what you expose in the header files that you ship with the application.","title":"API: Application Program Interface"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Application-binary-interface/#abi-application-binary-interface","text":"This is how the compiler builds an application. It defines things (but is not limited to): How parameters are passed to functions (registers/stack). Who cleans parameters from the stack (caller/callee). Where the return value is placed for return. How exceptions propagate.","title":"ABI: Application Binary Interface"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Book-x86-Disassembly/","text":"\u5173\u4e8e\u672c\u4e66 # x86 calling conventions","title":"Introduction"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Book-x86-Disassembly/#_1","text":"x86 calling conventions","title":"\u5173\u4e8e\u672c\u4e66"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/","text":"Calling Conventions Demystified Introduction C calling convention (__cdecl) Standard calling convention (__stdcall) Fast calling convention (__fastcall) Thiscall Conclusion Calling Conventions Demystified # Introduction # During the long, hard, but yet beautiful process of learning C++ programming for Windows, you have probably been curious about the strange specifiers that sometime appear in front of function declarations, like __cdecl , __stdcall , __fastcall , WINAPI , etc. After looking through MSDN, or some other reference, you probably found out that these specifiers specify the calling conventions for functions. In this article, I will try to explain different calling conventions used by Visual C++ (and probably other Windows C/C++ compilers). I emphasize that above mentioned specifiers are Microsoft-specific, and that you should not use them if you want to write portable code. So, what are the calling conventions? When a function is called, the arguments are typically passed to it, and the return value is retrieved. A calling convention describes how the arguments are passed and values returned by functions. It also specifies how the function names are decorated. Is it really necessary to understand the calling conventions to write good C/C++ programs? Not at all. However, it may be helpful with debugging. Also, it is necessary for linking C/C++ with assembly code. To understand this article, you will need to have some very basic knowledge of assembly programming. No matter which calling convention is used, the following things will happen: All arguments are widened to 4 bytes (on Win32, of course), and put into appropriate memory locations. These locations are typically on the stack, but may also be in registers; this is specified by calling conventions . Program execution jumps to the address of the called function . Inside the function, registers ESI , EDI , EBX , and EBP are saved on the stack. The part of code that performs these operations is called function prolog (\u51fd\u6570\u5f00\u573a) and usually is generated by the compiler. The function-specific code is executed, and the return value is placed into the EAX register. Registers ESI, EDI, EBX, and EBP are restored from the stack. The piece of code that does this is called function epilog (\u51fd\u6570\u6536\u573a), and as with the function prolog, in most cases the compiler generates it. Arguments are removed from the stack. This operation is called stack cleanup and may be performed either inside the called function or by the caller, depending on the calling convention used. As an example for the calling conventions (except for this ), we are going to use a simple function: Hide Copy Code int sumExample (int a, int b) { return a + b; } The call to this function will look like this: Hide Copy Code int c = sum (2, 3); For __cdecl , __stdcall , and __fastcall calling conventions, I compiled the example code as C (not C++). The function name decorations , mentioned later in the article, apply to the C decoration schema. C++ name decorations are beyond the scope of this article. C calling convention ( __cdecl ) # This convention is the default for C/C++ programs (compiler option /Gd). If a project is set to use some other calling convention, we can still declare a function to use __cdecl : Hide Copy Code int __cdecl sumExample (int a, int b); The main characteristics of __cdecl calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the caller. Function name is decorated by prefixing it with an underscore character '_' . Now, take a look at an example of a __cdecl call: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample ; // cleanup the stack by adding the size of the arguments to ESP register add esp,8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The called function is shown below: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0C0h push ebx push esi push edi lea edi,[ebp-0C0h] mov ecx,30h mov eax,0CCCCCCCCh rep stos dword ptr [edi] ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret Standard calling convention ( __stdcall ) # This convention is usually used to call Win32 API functions. In fact, WINAPI is nothing but another name for __stdcall : Hide Copy Code #define WINAPI __stdcall We can explicitly declare a function to use the __stdcall convention: Hide Copy Code int __stdcall sumExample (int a, int b); Also, we can use the compiler option /Gz to specify __stdcall for all functions not explicitly declared with some other calling convention. The main characteristics of __stdcall calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the called function. Function name is decorated by prepending an underscore character and appending a '@' character and the number of bytes of stack space required. The example follows: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The function code is shown below: Hide Copy Code ; // function prolog goes here (the same code as in the __cdecl example) ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog goes here (the same code as in the __cdecl example) ; // cleanup the stack and return ret 8 Because the stack is cleaned by the called function, the __stdcall calling convention creates smaller executables than __cdecl , in which the code for stack cleanup must be generated for each function call. On the other hand, functions with the variable number of arguments (like printf() ) must use __cdecl , because only the caller knows the number of arguments in each function call; therefore only the caller can perform the stack cleanup. Fast calling convention (__fastcall) # Fast calling convention indicates that the arguments should be placed in registers, rather than on the stack, whenever possible. This reduces the cost of a function call, because operations with registers are faster than with the stack. We can explicitly declare a function to use the __fastcall convention as shown: Hide Copy Code int __fastcall sumExample (int a, int b); We can also use the compiler option /Gr to specify __fastcall for all functions not explicitly declared with some other calling convention. The main characteristics of __fastcall calling convention are: The first two function arguments that require 32 bits or less are placed into registers ECX and EDX. The rest of them are pushed on the stack from right to left. Arguments are popped from the stack by the called function. Function name is decorated by by prepending a '@' character and appending a '@' and the number of bytes (decimal) of space required by the arguments. Note: Microsoft have reserved the right to change the registers for passing the arguments in future compiler versions. Here goes an example: Hide Copy Code ; // put the arguments in the registers EDX and ECX mov edx,3 mov ecx,2 ; // call the function call @fastcallSum@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax Function code: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0D8h push ebx push esi push edi push ecx lea edi,[ebp-0D8h] mov ecx,36h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-14h],edx mov dword ptr [ebp-8],ecx ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ;// function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret How fast is this calling convention, comparing to __cdecl and __stdcall ? Find out for yourselves. Set the compiler option /Gr , and compare the execution time. I didn't find __fastcall to be any faster than other calling conventons, but you may come to different conclusions. Thiscall # Thiscall is the default calling convention for calling member functions of C++ classes (except for those with a variable number of arguments). The main characteristics of thiscall calling convention are: Arguments are passed from right to left, and placed on the stack. this is placed in ECX . Stack cleanup is performed by the called function. The example for this calling convention had to be a little different. First, the code is compiled as C++, and not C. Second, we have a struct with a member function, instead of a global function. Hide Copy Code struct CSum { int sum ( int a, int b) {return a+b;} }; The assembly code for the function call looks like this: Hide Copy Code push 3 push 2 lea ecx,[sumObj] call ?sum@CSum@@QAEHHH@Z ; CSum::sum mov dword ptr [s4],eax The function itself is given below: Hide Copy Code push ebp mov ebp,esp sub esp,0CCh push ebx push esi push edi push ecx lea edi,[ebp-0CCh] mov ecx,33h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-8],ecx mov eax,dword ptr [a] add eax,dword ptr [b] pop edi pop esi pop ebx mov esp,ebp pop ebp ret 8 Now, what happens if we have a member function with a variable number of arguments? In that case, __cdecl is used, and this is pushed onto the stack last. Conclusion # To cut a long story short, we'll outline the main differences between the calling conventions: __cdecl is the default calling convention for C and C++ programs. The advantage of this calling convetion is that it allows functions with a variable number of arguments to be used. The disadvantage is that it creates larger executables. __stdcall is used to call Win32 API functions. It does not allow functions to have a variable number of arguments. __fastcall attempts to put arguments in registers, rather than on the stack, thus making function calls faster. Thiscall calling convention is the default calling convention used by C++ member functions that do not use variable arguments. In most cases, this is all you'll ever need to know about the calling conventions.","title":"Calling-Conventions-Demystified"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#calling-conventions-demystified","text":"","title":"Calling Conventions Demystified"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#introduction","text":"During the long, hard, but yet beautiful process of learning C++ programming for Windows, you have probably been curious about the strange specifiers that sometime appear in front of function declarations, like __cdecl , __stdcall , __fastcall , WINAPI , etc. After looking through MSDN, or some other reference, you probably found out that these specifiers specify the calling conventions for functions. In this article, I will try to explain different calling conventions used by Visual C++ (and probably other Windows C/C++ compilers). I emphasize that above mentioned specifiers are Microsoft-specific, and that you should not use them if you want to write portable code. So, what are the calling conventions? When a function is called, the arguments are typically passed to it, and the return value is retrieved. A calling convention describes how the arguments are passed and values returned by functions. It also specifies how the function names are decorated. Is it really necessary to understand the calling conventions to write good C/C++ programs? Not at all. However, it may be helpful with debugging. Also, it is necessary for linking C/C++ with assembly code. To understand this article, you will need to have some very basic knowledge of assembly programming. No matter which calling convention is used, the following things will happen: All arguments are widened to 4 bytes (on Win32, of course), and put into appropriate memory locations. These locations are typically on the stack, but may also be in registers; this is specified by calling conventions . Program execution jumps to the address of the called function . Inside the function, registers ESI , EDI , EBX , and EBP are saved on the stack. The part of code that performs these operations is called function prolog (\u51fd\u6570\u5f00\u573a) and usually is generated by the compiler. The function-specific code is executed, and the return value is placed into the EAX register. Registers ESI, EDI, EBX, and EBP are restored from the stack. The piece of code that does this is called function epilog (\u51fd\u6570\u6536\u573a), and as with the function prolog, in most cases the compiler generates it. Arguments are removed from the stack. This operation is called stack cleanup and may be performed either inside the called function or by the caller, depending on the calling convention used. As an example for the calling conventions (except for this ), we are going to use a simple function: Hide Copy Code int sumExample (int a, int b) { return a + b; } The call to this function will look like this: Hide Copy Code int c = sum (2, 3); For __cdecl , __stdcall , and __fastcall calling conventions, I compiled the example code as C (not C++). The function name decorations , mentioned later in the article, apply to the C decoration schema. C++ name decorations are beyond the scope of this article.","title":"Introduction"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#c-calling-convention-__cdecl","text":"This convention is the default for C/C++ programs (compiler option /Gd). If a project is set to use some other calling convention, we can still declare a function to use __cdecl : Hide Copy Code int __cdecl sumExample (int a, int b); The main characteristics of __cdecl calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the caller. Function name is decorated by prefixing it with an underscore character '_' . Now, take a look at an example of a __cdecl call: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample ; // cleanup the stack by adding the size of the arguments to ESP register add esp,8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The called function is shown below: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0C0h push ebx push esi push edi lea edi,[ebp-0C0h] mov ecx,30h mov eax,0CCCCCCCCh rep stos dword ptr [edi] ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret","title":"C calling convention (__cdecl)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#standard-calling-convention-__stdcall","text":"This convention is usually used to call Win32 API functions. In fact, WINAPI is nothing but another name for __stdcall : Hide Copy Code #define WINAPI __stdcall We can explicitly declare a function to use the __stdcall convention: Hide Copy Code int __stdcall sumExample (int a, int b); Also, we can use the compiler option /Gz to specify __stdcall for all functions not explicitly declared with some other calling convention. The main characteristics of __stdcall calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the called function. Function name is decorated by prepending an underscore character and appending a '@' character and the number of bytes of stack space required. The example follows: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The function code is shown below: Hide Copy Code ; // function prolog goes here (the same code as in the __cdecl example) ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog goes here (the same code as in the __cdecl example) ; // cleanup the stack and return ret 8 Because the stack is cleaned by the called function, the __stdcall calling convention creates smaller executables than __cdecl , in which the code for stack cleanup must be generated for each function call. On the other hand, functions with the variable number of arguments (like printf() ) must use __cdecl , because only the caller knows the number of arguments in each function call; therefore only the caller can perform the stack cleanup.","title":"Standard calling convention (__stdcall)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#fast-calling-convention-__fastcall","text":"Fast calling convention indicates that the arguments should be placed in registers, rather than on the stack, whenever possible. This reduces the cost of a function call, because operations with registers are faster than with the stack. We can explicitly declare a function to use the __fastcall convention as shown: Hide Copy Code int __fastcall sumExample (int a, int b); We can also use the compiler option /Gr to specify __fastcall for all functions not explicitly declared with some other calling convention. The main characteristics of __fastcall calling convention are: The first two function arguments that require 32 bits or less are placed into registers ECX and EDX. The rest of them are pushed on the stack from right to left. Arguments are popped from the stack by the called function. Function name is decorated by by prepending a '@' character and appending a '@' and the number of bytes (decimal) of space required by the arguments. Note: Microsoft have reserved the right to change the registers for passing the arguments in future compiler versions. Here goes an example: Hide Copy Code ; // put the arguments in the registers EDX and ECX mov edx,3 mov ecx,2 ; // call the function call @fastcallSum@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax Function code: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0D8h push ebx push esi push edi push ecx lea edi,[ebp-0D8h] mov ecx,36h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-14h],edx mov dword ptr [ebp-8],ecx ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ;// function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret How fast is this calling convention, comparing to __cdecl and __stdcall ? Find out for yourselves. Set the compiler option /Gr , and compare the execution time. I didn't find __fastcall to be any faster than other calling conventons, but you may come to different conclusions.","title":"Fast calling convention (__fastcall)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#thiscall","text":"Thiscall is the default calling convention for calling member functions of C++ classes (except for those with a variable number of arguments). The main characteristics of thiscall calling convention are: Arguments are passed from right to left, and placed on the stack. this is placed in ECX . Stack cleanup is performed by the called function. The example for this calling convention had to be a little different. First, the code is compiled as C++, and not C. Second, we have a struct with a member function, instead of a global function. Hide Copy Code struct CSum { int sum ( int a, int b) {return a+b;} }; The assembly code for the function call looks like this: Hide Copy Code push 3 push 2 lea ecx,[sumObj] call ?sum@CSum@@QAEHHH@Z ; CSum::sum mov dword ptr [s4],eax The function itself is given below: Hide Copy Code push ebp mov ebp,esp sub esp,0CCh push ebx push esi push edi push ecx lea edi,[ebp-0CCh] mov ecx,33h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-8],ecx mov eax,dword ptr [a] add eax,dword ptr [b] pop edi pop esi pop ebx mov esp,ebp pop ebp ret 8 Now, what happens if we have a member function with a variable number of arguments? In that case, __cdecl is used, and this is pushed onto the stack last.","title":"Thiscall"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#conclusion","text":"To cut a long story short, we'll outline the main differences between the calling conventions: __cdecl is the default calling convention for C and C++ programs. The advantage of this calling convetion is that it allows functions with a variable number of arguments to be used. The disadvantage is that it creates larger executables. __stdcall is used to call Win32 API functions. It does not allow functions to have a variable number of arguments. __fastcall attempts to put arguments in registers, rather than on the stack, thus making function calls faster. Thiscall calling convention is the default calling convention used by C++ member functions that do not use variable arguments. In most cases, this is all you'll ever need to know about the calling conventions.","title":"Conclusion"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/","text":"Calling convention # In computer science , a calling convention is an implementation-level (low-level) scheme for how subroutines receive parameters from their caller and how they return a result. Differences in various implementations include where parameters, return values , return addresses and scope links are placed, and how the tasks of preparing for a function call and restoring the environment afterward are divided between the caller and the callee(\u4ee5\u53ca\u5982\u4f55\u5728\u8c03\u7528\u8005\u548c\u88ab\u8c03\u7528\u8005\u4e4b\u95f4\u5212\u5206\u51c6\u5907\u51fd\u6570\u8c03\u7528\u548c\u6062\u590d\u73af\u5883\u7684\u4efb\u52a1). Calling conventions may be related to a particular programming language's evaluation strategy but most often are not considered part of it (or vice versa), as the evaluation strategy is usually defined on a higher abstraction level and seen as a part of the language rather than as a low-level implementation detail of a particular language's compiler . Variations # Calling conventions may differ in: Where parameters, return values and return addresses are placed (in registers , on the call stack , a mix of both, or in other memory structures) The order in which actual arguments for formal parameters are passed (or the parts of a large or complex argument) How a (possibly long or complex) return value is delivered from the callee back to the caller (on the stack, in a register, or within the heap) How the task of setting up for and cleaning up after a function call is divided between the caller and the callee Whether and how metadata describing the arguments is passed Where the previous value of the frame pointer is stored, which is used to restore the frame pointer when the routine ends (in the stack frame, or in some register) Where any static scope links for the routine's non-local data access are placed (typically at one or more positions in the stack frame, but sometimes in a general register, or, for some architectures, in special-purpose registers) How local variables are allocated can sometimes also be part of the calling convention (when the caller allocates for the callee) In some cases, differences also include the following: Conventions on which registers may be directly used by the callee, without being preserved (otherwise regarded as an ABI detail) Which registers are considered to be volatile and, if volatile, need not be restored by the callee (often regarded as an ABI detail) Compiler variation # Although some [ which? ] languages actually may specify this partially in the programming language specification (or in some pivotal implementation), different implementations of such languages (i.e. different compilers ) may typically still use various calling conventions , often selectable. Reasons for this are performance, frequent adaptation to the conventions of other popular languages (with or without technical reasons), and restrictions or conventions imposed by various \"platforms\" (combinations of CPU architectures and operating systems ). Architecture variation # CPU architectures always have more than one possible calling convention[ why? ]. With many general-purpose registers and other features, the potential number of calling conventions is large, although some[ which? ] architectures are formally specified to use only one calling convention, supplied by the architect. x86 (32-bit) # Main article: x86 calling conventions The x86 architecture is used with many different calling conventions. Due to the small number of architectural registers, the x86 calling conventions mostly pass arguments on the stack, while the return value (or a pointer to it) is passed in a register. Some conventions use registers for the first few parameters, which may improve performance for short and simple leaf-routines very frequently invoked (i.e. routines that do not call other routines and do not have to be reentrant ). Example call: push EAX ; pass some register result push byte[EBP+20] ; pass some memory variable (FASM/TASM syntax) push 3 ; pass some constant call calc ; the returned result is now in EAX Typical callee structure: ( some or all (except ret) of the instructions below may be optimized away in simple procedures ) calc: push EBP ; save old frame pointer mov EBP,ESP ; get new frame pointer sub ESP,localsize ; reserve place for locals . . ; perform calculations, leave result in EAX . mov ESP,EBP ; free space for locals pop EBP ; restore old frame pointer ret paramsize ; free parameter space and return ARM (A32) # The standard 32-bit ARM calling convention allocates the 15 general-purpose registers as: r14 is the link register. (The BL instruction, used in a subroutine call, stores the return address in this register). r13 is the stack pointer. (The Push/Pop instructions in \"Thumb\" operating mode use this register only). r12 is the Intra-Procedure-call scratch register. r4 to r11: used to hold local variables. r0 to r3: used to hold argument values passed to a subroutine, and also hold results returned from a subroutine. The 16th register, r15, is the program counter. If the type of value returned is too large to fit in r0 to r3, or whose size cannot be determined statically at compile time, then the caller must allocate space for that value at run time, and pass a pointer to that space in r0. Subroutines must preserve the contents of r4 to r11 and the stack pointer. (Perhaps by saving them to the stack in the function prologue, then using them as scratch space, then restoring them from the stack in the function epilogue). In particular, subroutines that call other subroutines must save the return address in the link register r14 to the stack before calling those other subroutines. However, such subroutines do not need to return that value to r14\u2014they merely need to load that value into r15, the program counter, to return. The ARM calling convention mandates using a full-descending stack.[ 1] This calling convention causes a \"typical\" ARM subroutine to In the prologue, push r4 to r11 to the stack, and push the return address in r14, to the stack. (This can be done with a single STM instruction). copy any passed arguments (in r0 to r3) to the local scratch registers (r4 to r11). allocate other local variables to the remaining local scratch registers (r4 to r11). do calculations and call other subroutines as necessary using BL, assuming r0 to r3, r12 and r14 will not be preserved. put the result in r0 In the epilogue, pull r4 to r11 from the stack, and pull the return address to the program counter r15. (This can be done with a single LDM instruction). See also # Calling Conventions #","title":"Calling-convention"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#calling-convention","text":"In computer science , a calling convention is an implementation-level (low-level) scheme for how subroutines receive parameters from their caller and how they return a result. Differences in various implementations include where parameters, return values , return addresses and scope links are placed, and how the tasks of preparing for a function call and restoring the environment afterward are divided between the caller and the callee(\u4ee5\u53ca\u5982\u4f55\u5728\u8c03\u7528\u8005\u548c\u88ab\u8c03\u7528\u8005\u4e4b\u95f4\u5212\u5206\u51c6\u5907\u51fd\u6570\u8c03\u7528\u548c\u6062\u590d\u73af\u5883\u7684\u4efb\u52a1). Calling conventions may be related to a particular programming language's evaluation strategy but most often are not considered part of it (or vice versa), as the evaluation strategy is usually defined on a higher abstraction level and seen as a part of the language rather than as a low-level implementation detail of a particular language's compiler .","title":"Calling convention"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#variations","text":"Calling conventions may differ in: Where parameters, return values and return addresses are placed (in registers , on the call stack , a mix of both, or in other memory structures) The order in which actual arguments for formal parameters are passed (or the parts of a large or complex argument) How a (possibly long or complex) return value is delivered from the callee back to the caller (on the stack, in a register, or within the heap) How the task of setting up for and cleaning up after a function call is divided between the caller and the callee Whether and how metadata describing the arguments is passed Where the previous value of the frame pointer is stored, which is used to restore the frame pointer when the routine ends (in the stack frame, or in some register) Where any static scope links for the routine's non-local data access are placed (typically at one or more positions in the stack frame, but sometimes in a general register, or, for some architectures, in special-purpose registers) How local variables are allocated can sometimes also be part of the calling convention (when the caller allocates for the callee) In some cases, differences also include the following: Conventions on which registers may be directly used by the callee, without being preserved (otherwise regarded as an ABI detail) Which registers are considered to be volatile and, if volatile, need not be restored by the callee (often regarded as an ABI detail)","title":"Variations"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#compiler-variation","text":"Although some [ which? ] languages actually may specify this partially in the programming language specification (or in some pivotal implementation), different implementations of such languages (i.e. different compilers ) may typically still use various calling conventions , often selectable. Reasons for this are performance, frequent adaptation to the conventions of other popular languages (with or without technical reasons), and restrictions or conventions imposed by various \"platforms\" (combinations of CPU architectures and operating systems ).","title":"Compiler variation"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#architecture-variation","text":"CPU architectures always have more than one possible calling convention[ why? ]. With many general-purpose registers and other features, the potential number of calling conventions is large, although some[ which? ] architectures are formally specified to use only one calling convention, supplied by the architect.","title":"Architecture variation"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#x86-32-bit","text":"Main article: x86 calling conventions The x86 architecture is used with many different calling conventions. Due to the small number of architectural registers, the x86 calling conventions mostly pass arguments on the stack, while the return value (or a pointer to it) is passed in a register. Some conventions use registers for the first few parameters, which may improve performance for short and simple leaf-routines very frequently invoked (i.e. routines that do not call other routines and do not have to be reentrant ). Example call: push EAX ; pass some register result push byte[EBP+20] ; pass some memory variable (FASM/TASM syntax) push 3 ; pass some constant call calc ; the returned result is now in EAX Typical callee structure: ( some or all (except ret) of the instructions below may be optimized away in simple procedures ) calc: push EBP ; save old frame pointer mov EBP,ESP ; get new frame pointer sub ESP,localsize ; reserve place for locals . . ; perform calculations, leave result in EAX . mov ESP,EBP ; free space for locals pop EBP ; restore old frame pointer ret paramsize ; free parameter space and return","title":"x86 (32-bit)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#arm-a32","text":"The standard 32-bit ARM calling convention allocates the 15 general-purpose registers as: r14 is the link register. (The BL instruction, used in a subroutine call, stores the return address in this register). r13 is the stack pointer. (The Push/Pop instructions in \"Thumb\" operating mode use this register only). r12 is the Intra-Procedure-call scratch register. r4 to r11: used to hold local variables. r0 to r3: used to hold argument values passed to a subroutine, and also hold results returned from a subroutine. The 16th register, r15, is the program counter. If the type of value returned is too large to fit in r0 to r3, or whose size cannot be determined statically at compile time, then the caller must allocate space for that value at run time, and pass a pointer to that space in r0. Subroutines must preserve the contents of r4 to r11 and the stack pointer. (Perhaps by saving them to the stack in the function prologue, then using them as scratch space, then restoring them from the stack in the function epilogue). In particular, subroutines that call other subroutines must save the return address in the link register r14 to the stack before calling those other subroutines. However, such subroutines do not need to return that value to r14\u2014they merely need to load that value into r15, the program counter, to return. The ARM calling convention mandates using a full-descending stack.[ 1] This calling convention causes a \"typical\" ARM subroutine to In the prologue, push r4 to r11 to the stack, and push the return address in r14, to the stack. (This can be done with a single STM instruction). copy any passed arguments (in r0 to r3) to the local scratch registers (r4 to r11). allocate other local variables to the remaining local scratch registers (r4 to r11). do calculations and call other subroutines as necessary using BL, assuming r0 to r3, r12 and r14 will not be preserved. put the result in r0 In the epilogue, pull r4 to r11 from the stack, and pull the return address to the program counter r15. (This can be done with a single LDM instruction).","title":"ARM (A32)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#see-also","text":"","title":"See also"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-convention/Calling-convention/#calling-conventions","text":"","title":"Calling Conventions"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack-summary/","text":"call stack of recursion function how to using user stack to replace the call stack of recursion function call stack of recursion function # \u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u6808\u53ef\u4ee5\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u53c2\u89c1\u300a recursion-analysis-and-representation.md \u300b how to using user stack to replace the call stack of recursion function # \u5982\u4f55\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6808\u6765\u66ff\u4ee3call stack\uff0c\u53c2\u89c1\u300a recursion-to-iteration.md \u300b","title":"Call-stack-summary"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack-summary/#call-stack-of-recursion-function","text":"\u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u6808\u53ef\u4ee5\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u53c2\u89c1\u300a recursion-analysis-and-representation.md \u300b","title":"call stack of recursion function"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack-summary/#how-to-using-user-stack-to-replace-the-call-stack-of-recursion-function","text":"\u5982\u4f55\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6808\u6765\u66ff\u4ee3call stack\uff0c\u53c2\u89c1\u300a recursion-to-iteration.md \u300b","title":"how to using user stack to replace the call stack of recursion function"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack/","text":"Call stack # In computer science , a call stack is a stack data structure that stores information about the active subroutines of a computer program . This kind of stack is also known as an execution stack , program stack , control stack , run-time stack , or machine stack , and is often shortened to just \"the stack\". Although maintenance of the call stack is important for the proper functioning of most software , the details are normally hidden and automatic in high-level programming languages . Many computer instruction sets provide special instructions for manipulating stacks. NOTE: \u6bd4\u5982 x86\u7684PUSH \u548c POP\u6307\u4ee4 \u3002 A call stack is used for several related purposes, but the main reason for having one is to keep track of the point to which each active subroutine should return control when it finishes executing(\u53c2\u89c1 call trace ). An active subroutine is one that has been called but is yet to complete execution after which control should be handed back to the point of call. Such activations of subroutines may be nested to any level (recursive as a special case), hence the stack structure. If, for example, a subroutine DrawSquare calls a subroutine DrawLine from four different places, DrawLine must know where to return when its execution completes. To accomplish this, the address following the call instruction , the return address , is pushed onto the call stack with each call(\u53c2\u89c1procedure's prologue and epilogue \uff09. Description # Since the call stack is organized as a stack , the caller pushes the return address onto the stack, and the called subroutine, when it finishes, pulls or pops the return address off the call stack and transfers control to that address. If a called subroutine calls on yet another subroutine, it will push another return address onto the call stack, and so on, with the information stacking up and unstacking as the program dictates. If the pushing consumes all of the space allocated for the call stack, an error called a stack overflow occurs, generally causing the program to crash . Adding a subroutine's entry to the call stack is sometimes called \"winding\"; conversely, removing entries is \"unwinding\". There is usually exactly one call stack associated with a running program (or more accurately, with each task or thread of a process ), although additional stacks may be created for signal handling or cooperative multitasking (as with setcontext ). Since there is only one in this important context, it can be referred to as the stack (implicitly, \"of the task\"); however, in the Forth programming language the data stack or parameter stack is accessed more explicitly than the call stack and is commonly referred to as the stack (see below). In high-level programming languages , the specifics of the call stack are usually hidden from the programmer. They are given access only to a set of functions, and not the memory on the stack itself. This is an example of abstraction . Most assembly languages , on the other hand, require programmers to be involved with manipulating the stack. The actual details of the stack in a programming language depend upon the compiler , operating system , and the available instruction set . Functions of the call stack # NOTE: \u539f\u6587\u4e2d\u672c\u6bb5\u7684\u201c return state \u201d\u6982\u5ff5\u9700\u8981\u6ce8\u610f\uff0c\u5b83\u5305\u542b\u4e86return address\u3002","title":"Call-stack"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack/#call-stack","text":"In computer science , a call stack is a stack data structure that stores information about the active subroutines of a computer program . This kind of stack is also known as an execution stack , program stack , control stack , run-time stack , or machine stack , and is often shortened to just \"the stack\". Although maintenance of the call stack is important for the proper functioning of most software , the details are normally hidden and automatic in high-level programming languages . Many computer instruction sets provide special instructions for manipulating stacks. NOTE: \u6bd4\u5982 x86\u7684PUSH \u548c POP\u6307\u4ee4 \u3002 A call stack is used for several related purposes, but the main reason for having one is to keep track of the point to which each active subroutine should return control when it finishes executing(\u53c2\u89c1 call trace ). An active subroutine is one that has been called but is yet to complete execution after which control should be handed back to the point of call. Such activations of subroutines may be nested to any level (recursive as a special case), hence the stack structure. If, for example, a subroutine DrawSquare calls a subroutine DrawLine from four different places, DrawLine must know where to return when its execution completes. To accomplish this, the address following the call instruction , the return address , is pushed onto the call stack with each call(\u53c2\u89c1procedure's prologue and epilogue \uff09.","title":"Call stack"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack/#description","text":"Since the call stack is organized as a stack , the caller pushes the return address onto the stack, and the called subroutine, when it finishes, pulls or pops the return address off the call stack and transfers control to that address. If a called subroutine calls on yet another subroutine, it will push another return address onto the call stack, and so on, with the information stacking up and unstacking as the program dictates. If the pushing consumes all of the space allocated for the call stack, an error called a stack overflow occurs, generally causing the program to crash . Adding a subroutine's entry to the call stack is sometimes called \"winding\"; conversely, removing entries is \"unwinding\". There is usually exactly one call stack associated with a running program (or more accurately, with each task or thread of a process ), although additional stacks may be created for signal handling or cooperative multitasking (as with setcontext ). Since there is only one in this important context, it can be referred to as the stack (implicitly, \"of the task\"); however, in the Forth programming language the data stack or parameter stack is accessed more explicitly than the call stack and is commonly referred to as the stack (see below). In high-level programming languages , the specifics of the call stack are usually hidden from the programmer. They are given access only to a set of functions, and not the memory on the stack itself. This is an example of abstraction . Most assembly languages , on the other hand, require programmers to be involved with manipulating the stack. The actual details of the stack in a programming language depend upon the compiler , operating system , and the available instruction set .","title":"Description"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Function-Call/Call-stack/Call-stack/#functions-of-the-call-stack","text":"NOTE: \u539f\u6587\u4e2d\u672c\u6bb5\u7684\u201c return state \u201d\u6982\u5ff5\u9700\u8981\u6ce8\u610f\uff0c\u5b83\u5305\u542b\u4e86return address\u3002","title":"Functions of the call stack"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Library(computing)/","text":"Library (computing) # Illustration of an application which uses libvorbisfile to play an Ogg Vorbis file In computer science , a library is a collection of non-volatile resources used by computer programs , often for software development . These may include configuration data, documentation, help data, message templates, pre-written code and subroutines , classes , values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets . A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system. Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library. The value of a library lies in the reuse of the behavior. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion, and ease the distribution of the code. The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases . If the code of the library is accessed during the build of the invoking program, then the library is called a static library .[ 1] An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at run time ). A dynamic library can be loaded and linked when preparing a program for execution, by the linker . Alternatively, in the middle of execution, an application may explicitly request that a module be loaded . Most compiled languages have a standard library although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have commoditized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries. Linking # Main articles: Link time and Linker (computing) Libraries are important in the program linking or binding process, which resolves references known as links or symbols to library modules. The linking process is usually automatically done by a linker or binder program that searches a set of libraries and other modules in a given order. Usually it is not considered an error if a link target can be found multiple times in a given set of libraries. Linking may be done when an executable file is created, or whenever the program is used at run time . The references being resolved may be addresses(\u5730\u5740) for jumps and other routine calls. They may be in the main program, or in one module depending upon another. They are resolved into fixed or relocatable addresses (from a common base) by allocating runtime memory for the memory segments of each module referenced. Some programming languages may use a feature called smart linking whereby the linker is aware of or integrated with the compiler, such that the linker knows how external references are used, and code in a library that is never actually used , even though internally referenced, can be discarded from the compiled application. For example, a program that only uses integers for arithmetic, or does no arithmetic operations at all, can exclude floating-point library routines. This smart-linking feature can lead to smaller application file sizes and reduced memory usage. Relocation # Main article: Relocation (computer science) Some references in a program or library module are stored in a relative or symbolic form which cannot be resolved until all code and libraries are assigned final static addresses . Relocation is the process of adjusting these references, and is done either by the linker or the loader . In general, relocation cannot be done to individual libraries themselves because the addresses in memory may vary depending on the program using them and other libraries they are combined with. Position-independent code avoids references to absolute addresses and therefore does not require relocation. See also # Program Library HOWTO https://stackoverflow.com/questions/480764/linux-error-while-loading-shared-libraries-cannot-open-shared-object-file-no-s","title":"Library(computing)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Library(computing)/#library-computing","text":"Illustration of an application which uses libvorbisfile to play an Ogg Vorbis file In computer science , a library is a collection of non-volatile resources used by computer programs , often for software development . These may include configuration data, documentation, help data, message templates, pre-written code and subroutines , classes , values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets . A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system. Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library. The value of a library lies in the reuse of the behavior. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion, and ease the distribution of the code. The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases . If the code of the library is accessed during the build of the invoking program, then the library is called a static library .[ 1] An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at run time ). A dynamic library can be loaded and linked when preparing a program for execution, by the linker . Alternatively, in the middle of execution, an application may explicitly request that a module be loaded . Most compiled languages have a standard library although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have commoditized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.","title":"Library (computing)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Library(computing)/#linking","text":"Main articles: Link time and Linker (computing) Libraries are important in the program linking or binding process, which resolves references known as links or symbols to library modules. The linking process is usually automatically done by a linker or binder program that searches a set of libraries and other modules in a given order. Usually it is not considered an error if a link target can be found multiple times in a given set of libraries. Linking may be done when an executable file is created, or whenever the program is used at run time . The references being resolved may be addresses(\u5730\u5740) for jumps and other routine calls. They may be in the main program, or in one module depending upon another. They are resolved into fixed or relocatable addresses (from a common base) by allocating runtime memory for the memory segments of each module referenced. Some programming languages may use a feature called smart linking whereby the linker is aware of or integrated with the compiler, such that the linker knows how external references are used, and code in a library that is never actually used , even though internally referenced, can be discarded from the compiled application. For example, a program that only uses integers for arithmetic, or does no arithmetic operations at all, can exclude floating-point library routines. This smart-linking feature can lead to smaller application file sizes and reduced memory usage.","title":"Linking"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Library(computing)/#relocation","text":"Main article: Relocation (computer science) Some references in a program or library module are stored in a relative or symbolic form which cannot be resolved until all code and libraries are assigned final static addresses . Relocation is the process of adjusting these references, and is done either by the linker or the loader . In general, relocation cannot be done to individual libraries themselves because the addresses in memory may vary depending on the program using them and other libraries they are combined with. Position-independent code avoids references to absolute addresses and therefore does not require relocation.","title":"Relocation"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Library(computing)/#see-also","text":"Program Library HOWTO https://stackoverflow.com/questions/480764/linux-error-while-loading-shared-libraries-cannot-open-shared-object-file-no-s","title":"See also"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/","text":"Position Independent Code (PIC) in shared libraries Some problems of load-time relocation PIC - introduction Key insight #1 - offset between text and data sections Key insight #2 - making an IP-relative offset work on x86 The Global Offset Table (GOT) PIC with data references through GOT - an example Function calls in PIC The lazy binding optimization The Procedure Linkage Table (PLT) PIC with function calls through PLT and GOT - an example Controlling if and when the resolution is done by the loader The costs of PIC Conclusion Position Independent Code (PIC) in shared libraries # I've described the need for special handling of shared libraries while loading them into the process's address space in a previous article . Briefly, when the linker creates a shared library, it doesn't know in advance where it might be loaded. This creates a problem for the data and code references within the library, which should be somehow made to point to the correct memory locations. There are two main approaches to solve this problem in Linux ELF shared libraries: Load-time relocation Position independent code (PIC) Load-time relocation was already covered . Here, I want to explain the second approach - PIC. I originally planned to focus on both x86 and x64 (a.k.a. x86-64) in this article, but as it grew longer and longer I decided it won't be practical. So, it will explain only how PIC works on x86, picking this older architecture specifically because (unlike x64) it wasn't designed with PIC in mind, so implementing PIC on it is a bit trickier. A future (hopefully much shorter) article will build upon the foundation of this one to explain how PIC is implemented on x64. Some problems of load-time relocation # As we've seen in the previous article, load-time relocation is a fairly straightforward method, and it works. PIC, however, is much more popular nowadays, and is usually the recommended method of building shared libraries. Why is this so? Load-time relocation has a couple of problems: it takes time to perform, and it makes the text section of the library non-shareable. First, the performance problem. If a shared library was linked with load-time relocation entries, it will take some time to actually perform these relocations when the application is loaded. You may think that the cost shouldn't be too large - after all, the loader doesn't have to scan through the whole text section - it should only look at the relocation entries. But if a complex piece of software loads multiple large shared libraries at start-up, and each shared library must first have its load-time relocations applied, these costs can build up and result in a noticeable delay in the start-up time of the application. Second, the non-shareable text section problem, which is somewhat more serious. One of the main points of having shared libraries in the first place, is saving RAM. Some common shared libraries are used by multiple applications. If the text section (where the code is) of the shared library can only be loaded into memory once (and then mapped into the virtual memories of many processes), considerable amounts of RAM can be saved. But this is not possible with load-time relocation, since when using this technique the text section has to be modified at load-time to apply the relocations. Therefore, for each application that loaded this shared library, it will have to be wholly placed in RAM again [ 1] . Different applications won't be able to really share it. Moreover, having a writable text section (it must be kept writable, to allow the dynamic loader to perform the relocations) poses a security risk, making it easier to exploit the application. As we'll see in this article, PIC mostly mitigates these problems. PIC - introduction # The idea behind PIC is simple - add an additional level of indirection to all global data and function references in the code. By cleverly utilizing some artifacts of the linking and loading processes, it's possible to make the text section of the shared library truly position independent , in the sense that it can be easily mapped into different memory addresses without needing to change one bit. In the next few sections I will explain in detail how this feat is achieved. Key insight #1 - offset between text and data sections # One of the key insights on which PIC relies is the offset between the text and data sections, known to the linker at link-time . When the linker combines several object files together, it collects their sections (for example, all text sections get unified into a single large text section). Therefore, the linker knows both about the sizes of the sections and about their relative locations. For example, the text section may be immediately followed by the data section, so the offset from any given instruction in the text section to the beginning of the data section is just the size of the text section minus the offset of the instruction from the beginning of the text section - and both these quantities are known to the linker. In the diagram above, the code section was loaded into some address (unknown at link-time) 0xXXXX0000 (the X-es literally mean \"don't care\"), and the data section right after it at offset 0xXXXXF000. Then, if some instruction at offset 0x80 in the code section wants to reference stuff in the data section, the linker knows the relative offset (0xEF80 in this case) and can encode it in the instruction. Note that it wouldn't matter if another section was placed between the code and data sections, or if the data section preceded the code section. Since the linker knows the sizes of all sections and decides where to place them, the insight holds. Key insight #2 - making an IP-relative offset work on x86 # The above is only useful if we can actually put the relative offset to work. But data references (i.e. in the mov instruction) on x86 require absolute addresses. So, what can we do? If we have a relative address and need an absolute address, what's missing is the value of the instruction pointer (since, by definition, the relative address is relative to the instruction's location). There's no instruction to obtain the value of the instruction pointer on x86, but we can use a simple trick to get it. Here's some assembly pseudo-code that demonstrates it: call TMPLABEL TMPLABEL: pop ebx What happens here is: The CPU executes call TMPLABEL , which causes it to save the address of the next instruction (the pop ebx ) on stack and jump to the label. Since the instruction at the label is pop ebx , it gets executed next. It pops a value from the stack into ebx . But this value is the address of the instruction itself, so ebx now effectively contains the value of the instruction pointer. The Global Offset Table (GOT) # With this at hand, we can finally get to the implementation of position-independent data addressing on x86. It is accomplished by means of a \"global offset table\", or in short GOT. A GOT is simply a table of addresses, residing in the data section. Suppose some instruction in the code section wants to refer to a variable. Instead of referring to it directly by absolute address (which would require a relocation), it refers to an entry in the GOT. Since the GOT is in a known place in the data section, this reference is relative and known to the linker. The GOT entry, in turn, will contain the absolute address of the variable: In pseudo-assembly, we replace an absolute addressing instruction: ; Place the value of the variable in edx mov edx, [ADDR_OF_VAR] With displacement addressing from a register, along with an extra indirection: ; 1. Somehow get the address of the GOT into ebx lea ebx, ADDR_OF_GOT ; 2. Suppose ADDR_OF_VAR is stored at offset 0x10 ; in the GOT. Then this will place ADDR_OF_VAR ; into edx. mov edx, DWORD PTR [ebx + 0x10] ; 3. Finally, access the variable and place its ; value into edx. mov edx, DWORD PTR [edx] So, we've gotten rid of a relocation in the code section by redirecting variable references through the GOT. But we've also created a relocation in the data section. Why? Because the GOT still has to contain the absolute address of the variable for the scheme described above to work. So what have we gained? A lot, it turns out. A relocation in the data section is much less problematic than one in the code section, for two reasons (which directly address the two main problems of load-time relocation of code described in the beginning of the article): Relocations in the code section are required per variable reference , while in the GOT we only need to relocate once per variable . There are likely much more references to variables than variables, so this is more efficient. The data section is writable and not shared between processes anyway, so adding relocations to it does no harm. Moving relocations from the code section, however, allows to make it read-only and share it between processes. PIC with data references through GOT - an example # I will now show a complete example that demonstrates the mechanics of PIC: int myglob = 42; int ml_func(int a, int b) { return myglob + a + b; } This chunk of code will be compiled into a shared library (using the -fpic and -shared flags as appropriate) named libmlpic_dataonly.so . Let's take a look at its disassembly, focusing on the ml_func function: 0000043c <ml_func>: 43c: 55 push ebp 43d: 89 e5 mov ebp,esp 43f: e8 16 00 00 00 call 45a <__i686.get_pc_thunk.cx> 444: 81 c1 b0 1b 00 00 add ecx,0x1bb0 44a: 8b 81 f0 ff ff ff mov eax,DWORD PTR [ecx-0x10] 450: 8b 00 mov eax,DWORD PTR [eax] 452: 03 45 08 add eax,DWORD PTR [ebp+0x8] 455: 03 45 0c add eax,DWORD PTR [ebp+0xc] 458: 5d pop ebp 459: c3 ret 0000045a <__i686.get_pc_thunk.cx>: 45a: 8b 0c 24 mov ecx,DWORD PTR [esp] 45d: c3 ret I'm going to refer to instructions by their addresses (the left-most number in the disassembly). This address is the offset from the load address of the shared library. At 43f , the address of the next instruction is placed into ecx , by means of the technique described in the \"key insight #2\" section above. At 444 , a known constant offset from the instruction to the place where the GOT is located is added to ecx . So ecx now serves as a base pointer to GOT. At 44a , a value is taken from [ecx - 0x10] , which is a GOT entry, and placed into eax . This is the address of myglob . At 450 the indirection is done, and the value of myglob is placed into eax . Later the parameters a and b are added to myglob and the value is returned (by keeping it in eax ). We can also query the shared library with readelf -S to see where the GOT section was placed: Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al <snip> [19] .got PROGBITS 00001fe4 000fe4 000010 04 WA 0 0 4 [20] .got.plt PROGBITS 00001ff4 000ff4 000014 04 WA 0 0 4 <snip> Let's do some math to check the computation done by the compiler to find myglob . As I mentioned above, the call to __i686.get_pc_thunk.cx places the address of the next instruction into ecx . That address is 0x444 [ 2] . The next instruction then adds 0x1bb0 to it, and the result in ecx is going to be 0x1ff4 . Finally, to actually obtain the GOT entry holding the address of myglob , displacement addressing is used - [ecx - 0x10] , so the entry is at 0x1fe4 , which is the first entry in the GOT according to the section header. Why there's another section whose name starts with .got will be explained later in the article [ 3] . Note that the compiler chooses to point ecx to after the GOT and then use negative offsets to obtain entries. This is fine, as long as the math works out. And so far it does. There's something we're still missing, however. How does the address of myglob actually get into the GOT slot at 0x1fe4 ? Recall that I mentioned a relocation, so let's find it: > readelf -r libmlpic_dataonly.so Relocation section '.rel.dyn' at offset 0x2dc contains 5 entries: Offset Info Type Sym.Value Sym. Name 00002008 00000008 R_386_RELATIVE 00001fe4 00000406 R_386_GLOB_DAT 0000200c myglob <snip> Note the relocation section for myglob , pointing to address 0x1fe4 , as expected. The relocation is of type R_386_GLOB_DAT , which simply tells the dynamic loader - \"put the actual value of the symbol (i.e. its address) into that offset\". So everything works out nicely. All that's left is to check how it actually looks when the library is loaded. We can do this by writing a simple \"driver\" executable that links to libmlpic_dataonly.so and calls ml_func , and then running it through GDB. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func [...] (gdb) run Starting program: [...]pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_reloc_dataonly.c:5 5 return myglob + a + b; (gdb) set disassembly-flavor intel (gdb) disas ml_func Dump of assembler code for function ml_func: 0x0013143c <+0>: push ebp 0x0013143d <+1>: mov ebp,esp 0x0013143f <+3>: call 0x13145a <__i686.get_pc_thunk.cx> 0x00131444 <+8>: add ecx,0x1bb0 => 0x0013144a <+14>: mov eax,DWORD PTR [ecx-0x10] 0x00131450 <+20>: mov eax,DWORD PTR [eax] 0x00131452 <+22>: add eax,DWORD PTR [ebp+0x8] 0x00131455 <+25>: add eax,DWORD PTR [ebp+0xc] 0x00131458 <+28>: pop ebp 0x00131459 <+29>: ret End of assembler dump. (gdb) i registers eax 0x1 1 ecx 0x132ff4 1257460 [...] skipping output The debugger has entered ml_func , and stopped at IP 0x0013144a [ 4] . We see that ecx holds the value 0x132ff4 (which is the address of the instruction plus 0x1bb0 , as explained before). Note that at this point, at runtime, these are absolute addresses - the shared library has already been loaded into the address space of the process. So, the GOT entry for myglob is at [ecx - 0x10] . Let's check what's there: (gdb) x 0x132fe4 0x132fe4: 0x0013300c So, we'd expect 0x0013300c to be the address of myglob . Let's verify: (gdb) p &myglob $1 = (int *) 0x13300c Indeed, it is! Function calls in PIC # Alright, so this is how data addressing works in position independent code. But what about function calls? Theoretically, the exact same approach could work for function calls as well. Instead of call actually containing the address of the function to call, let it contain the address of a known GOT entry, and fill in that entry during loading. But this is not how function calls work in PIC. What actually happens is a bit more complicated. Before I explain how it's done, a few words about the motivation for such a mechanism. The lazy binding optimization # When a shared library refers to some function, the real address of that function is not known until load time. Resolving this address is called binding , and it's something the dynamic loader does when it loads the shared library into the process's memory space. This binding process is non-trivial, since the loader has to actually look up the function symbol in special tables [ 5] . So, resolving each function takes time. Not a lot of time, but it adds up since the amount of functions in libraries is typically much larger than the amount of global variables. Moreover, most of these resolutions are done in vain, because in a typical run of a program only a fraction of functions actually get called (think about various functions handling error and special conditions, which typically don't get called at all). So, to speed up this process, a clever lazy binding scheme was devised. \"Lazy\" is a generic name for a family of optimizations in computer programming, where work is delayed until the last moment when it's actually needed, with the intention of avoiding doing this work if its results are never required during a specific run of a program. Good examples of laziness are copy-on-write and lazy evaluation . This lazy binding scheme is attained by adding yet another level of indirection - the PLT. The Procedure Linkage Table (PLT) # The PLT is part of the executable text section, consisting of a set of entries (one for each external function the shared library calls). Each PLT entry is a short chunk of executable code. Instead of calling the function directly, the code calls an entry in the PLT, which then takes care to call the actual function. This arrangement is sometimes called a \" trampoline \". Each PLT entry also has a corresponding entry in the GOT which contains the actual offset to the function, but only when the dynamic loader resolves it. I know this is confusing, but hopefully it will be come clearer once I explain the details in the next few paragraphs and diagrams. As the previous section mentioned, PLTs allow lazy resolution of functions. When the shared library is first loaded, the function calls have not been resolved yet: Explanation: In the code, a function func is called. The compiler translates it to a call to func@plt , which is some N-th entry in the PLT. The PLT consists of a special first entry, followed by a bunch of identically structured entries, one for each function needing resolution. Each PLT entry but the first consists of these parts: A jump to a location which is specified in a corresponding GOT entry Preparation of arguments for a \"resolver\" routine Call to the resolver routine, which resides in the first entry of the PLT The first PLT entry is a call to a resolver routine, which is located in the dynamic loader itself [ 6] . This routine resolves the actual address of the function. More on its action a bit later. Before the function's actual address has been resolved, the Nth GOT entry just points to after the jump. This is why this arrow in the diagram is colored differently - it's not an actual jump, just a pointer. What happens when func is called for the first time is this: PLT[n] is called and jumps to the address pointed to in GOT[n] . This address points into PLT[n] itself, to the preparation of arguments for the resolver. The resolver is then called. The resolver performs resolution of the actual address of func , places its actual address into GOT[n] and calls func . After the first call, the diagram looks a bit differently: Note that GOT[n] now points to the actual func [ 7] instead of back into the PLT. So, when func is called again: PLT[n] is called and jumps to the address pointed to in GOT[n] . GOT[n] points to func , so this just transfers control to func . In other words, now func is being actually called, without going through the resolver, at the cost of one additional jump. That's all there is to it, really. This mechanism allows lazy resolution of functions, and no resolution at all for functions that aren't actually called. It also leaves the code/text section of the library completely position independent, since the only place where an absolute address is used is the GOT, which resides in the data section and will be relocated by the dynamic loader. Even the PLT itself is PIC, so it can live in the read-only text section. I didn't get into much details regarding the resolver, but it's really not important for our purpose here. The resolver is simply a chunk of low-level code in the loader that does symbol resolution. The arguments prepared for it in each PLT entry, along with a suitable relocation entry, help it know about the symbol that needs resolution and about the GOT entry to update. PIC with function calls through PLT and GOT - an example # Once again, to fortify the hard-learned theory with a practical demonstration, here's a complete example showing function call resolution using the mechanism described above. I'll be moving forward a bit faster this time. Here's the code for the shared library: int myglob = 42; int ml_util_func(int a) { return a + 1; } int ml_func(int a, int b) { int c = b + ml_util_func(a); myglob += c; return b + myglob; } This code will be compiled into libmlpic.so , and the focus is going to be on the call to ml_util_func from ml_func . Let's first disassemble ml_func : 00000477 <ml_func>: 477: 55 push ebp 478: 89 e5 mov ebp,esp 47a: 53 push ebx 47b: 83 ec 24 sub esp,0x24 47e: e8 e4 ff ff ff call 467 <__i686.get_pc_thunk.bx> 483: 81 c3 71 1b 00 00 add ebx,0x1b71 489: 8b 45 08 mov eax,DWORD PTR [ebp+0x8] 48c: 89 04 24 mov DWORD PTR [esp],eax 48f: e8 0c ff ff ff call 3a0 <ml_util_func@plt> <... snip more code> The interesting part is the call to ml_util_func@plt . Note also that the address of GOT is in ebx . Here's what ml_util_func@plt looks like (it's in an executable section called .plt ): 000003a0 <ml_util_func@plt>: 3a0: ff a3 14 00 00 00 jmp DWORD PTR [ebx+0x14] 3a6: 68 10 00 00 00 push 0x10 3ab: e9 c0 ff ff ff jmp 370 <_init+0x30> Recall that each PLT entry consists of three parts: A jump to an address specified in GOT (this is the jump to [ebx+0x14] ) Preparation of arguments for the resolver Call to the resolver The resolver (PLT entry 0) resides at address 0x370 , but it's of no interest to us here. What's more interesting is to see what the GOT contains. For that, we first have to do some math. The \"get IP\" trick in ml_func was done on address 0x483 , to which 0x1b71 is added. So the base of the GOT is at 0x1ff4 . We can take a peek at the GOT contents with readelf [ 8] : > readelf -x .got.plt libmlpic.so Hex dump of section '.got.plt': 0x00001ff4 241f0000 00000000 00000000 86030000 $............... 0x00002004 96030000 a6030000 ........ The GOT entry ml_util_func@plt looks at is at offset +0x14 , or 0x2008 . From above, the word at that location is 0x3a6 , which is the address of the push instruction in ml_util_func@plt . To help the dynamic loader do its job, a relocation entry is also added and specifies which place in the GOT to relocate for ml_util_func : > readelf -r libmlpic.so [...] snip output Relocation section '.rel.plt' at offset 0x328 contains 3 entries: Offset Info Type Sym.Value Sym. Name 00002000 00000107 R_386_JUMP_SLOT 00000000 __cxa_finalize 00002004 00000207 R_386_JUMP_SLOT 00000000 __gmon_start__ 00002008 00000707 R_386_JUMP_SLOT 0000046c ml_util_func The last line means that the dynamic loader should place the value (address) of symbol ml_util_func into 0x2008 (which, recall, is the GOT entry for this function). It would be interesting to see this GOT entry modification actually happen after the first call. Let's once again use GDB for the inspection. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func Breakpoint 1 at 0x80483c0 (gdb) run Starting program: /pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_main.c:10 10 int c = b + ml_util_func(a); (gdb) We're now before the first call to ml_util_func . Recall that GOT is pointed to by ebx in this code. Let's see what's in it: (gdb) i registers ebx ebx 0x132ff4 And the offset to the entry we need is at [ebx+0x14] : (gdb) x/w 0x133008 0x133008: 0x001313a6 Yep, the 0x3a6 ending, looks right. Now, let's step until after the call to ml_util_func and check again: (gdb) step ml_util_func (a=1) at ml_main.c:5 5 return a + 1; (gdb) x/w 0x133008 0x133008: 0x0013146c The value at 0x133008 was changed. Hence, 0x0013146c should be the real address of ml_util_func , placed in there by the dynamic loader: (gdb) p &ml_util_func $1 = (int (*)(int)) 0x13146c <ml_util_func> Just as expected. Controlling if and when the resolution is done by the loader # This would be a good place to mention that the process of lazy symbol resolution performed by the dynamic loader can be configured with some environment variables (and corresponding flags to ld when linking the shared library). This is sometimes useful for special performance requirements or debugging. The LD_BIND_NOW env var, when defined, tells the dynamic loader to always perform the resolution for all symbols at start-up time, and not lazily. You can easily verify this in action by setting this env var and re-running the previous sample with GDB. You'll see that the GOT entry for ml_util_func contains its real address even before the first call to the function. Conversely, the LD_BIND_NOT env var tells the dynamic loader not to update the GOT entry at all. Each call to an external function will then go through the dynamic loader and be resolved anew. The dynamic loader is configurable by other flags as well. I encourage you to go over man ld.so - it contains some interesting information. The costs of PIC # This article started by stating the problems of load-time relocation and how the PIC approach fixes them. But PIC is also not without problems. One immediately apparent cost is the extra indirection required for all external references to data and code in PIC. That's an extra memory load for each reference to a global variable, and for each call to a function. How problematic this is in practice depends on the compiler, the CPU architecture and the particular application. Another, less apparent cost, is the increased register usage required to implement PIC. In order to avoid locating the GOT too frequently, it makes sense for the compiler to generate code that keeps its address in a register (usually ebx ). But that ties down a whole register just for the sake of GOT. While not a big problem for RISC architectures that tend to have a lot of general purposes registers, it presents a performance problem for architectures like x86, which has a small amount of registers. PIC means having one general purpose register less, which adds up indirect costs since now more memory references have to be made. Conclusion # This article explained what position independent code is, and how it helps create shared libraries with shareable read-only text sections. There are some tradeoffs when choosing between PIC and its alternative (load-time relocation), and the eventual outcome really depends on a lot of factors, like the CPU architecture on which the program is going to run. That said, PIC is becoming more and more popular. Some non-Intel architectures like SPARC64 force PIC-only code for shared libraries, and many others (for example, ARM) include IP-relative addressing modes to make PIC more efficient. Both are true for the successor of x86, the x64 architecture. I will discuss PIC on x64 in a future article. The focus of this article, however, has not been on performance considerations or architectural decisions. My aim was to explain, given that PIC is used, how it works . If the explanation wasn't clear enough - please let me know in the comments and I will try to provide more information. [ 1] Unless all applications load this library into the exact same virtual memory address. But this usually isn't done on Linux. [ 2] 0x444 (and all other addresses mentioned in this computation) is relative to the load address of the shared library, which is unknown until an executable actually loads it at runtime. Note how it doesn't matter in the code since it only juggles relative addresses. [ 3] The astute reader may wonder why .got is a separate section at all. Didn't I just show in the diagrams that it's located in the data section? In practice, it is. I don't want to get into the distinction between ELF sections and segments here, since that would take use too far away from the point. But briefly, any number of \"data\" sections can be defined for a library and mapped into a read-write segment. This doesn't really matter, as long as the ELF file is organized correctly. Separating the data segment into different logical sections provides modularity and makes the linker's job easier. [ 4] Note that gdb skipped the part where ecx is assigned. That's because it's kind-of considered to be part of the function's prolog (the real reason is in the way gcc structures its debug info, of course). Several references to global data and functions are made inside a function, and a register pointing to GOT can serve all of them. [ 5] Shared library ELF objects actually come with special hash table sections for this purpose. [ 6] The dynamic loader on Linux is just another shared library which gets loaded into the address space of all running processes. [ 7] I placed func in a separate code section, although in theory this could be the same one where the call to func is made (i.e. in the same shared library). The \"extra credit\" section of this article has information about why a call to an external function in the same shared library needs PIC (or relocation) as well. [ 8] Recall that in the data reference example I promised to explain why there are apparently two GOT sections in the object: .got and .got.plt . Now it should become obvious that this is just to conveniently split the GOT entries required for global data from GOT entries required for the PLT. This is also why when the GOT offset is computed in functions, it points to .got.plt , which comes right after .got . This way, negative offsets lead us to .got , while positive offsets lead us to .got.plt . While convenient, such an arrangement is by no means compulsory. Both parts could be placed into a single .got section.","title":"Position-Independent-Code(PIC)-in-shared-libraries"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#position-independent-code-pic-in-shared-libraries","text":"I've described the need for special handling of shared libraries while loading them into the process's address space in a previous article . Briefly, when the linker creates a shared library, it doesn't know in advance where it might be loaded. This creates a problem for the data and code references within the library, which should be somehow made to point to the correct memory locations. There are two main approaches to solve this problem in Linux ELF shared libraries: Load-time relocation Position independent code (PIC) Load-time relocation was already covered . Here, I want to explain the second approach - PIC. I originally planned to focus on both x86 and x64 (a.k.a. x86-64) in this article, but as it grew longer and longer I decided it won't be practical. So, it will explain only how PIC works on x86, picking this older architecture specifically because (unlike x64) it wasn't designed with PIC in mind, so implementing PIC on it is a bit trickier. A future (hopefully much shorter) article will build upon the foundation of this one to explain how PIC is implemented on x64.","title":"Position Independent Code (PIC) in shared libraries"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#some-problems-of-load-time-relocation","text":"As we've seen in the previous article, load-time relocation is a fairly straightforward method, and it works. PIC, however, is much more popular nowadays, and is usually the recommended method of building shared libraries. Why is this so? Load-time relocation has a couple of problems: it takes time to perform, and it makes the text section of the library non-shareable. First, the performance problem. If a shared library was linked with load-time relocation entries, it will take some time to actually perform these relocations when the application is loaded. You may think that the cost shouldn't be too large - after all, the loader doesn't have to scan through the whole text section - it should only look at the relocation entries. But if a complex piece of software loads multiple large shared libraries at start-up, and each shared library must first have its load-time relocations applied, these costs can build up and result in a noticeable delay in the start-up time of the application. Second, the non-shareable text section problem, which is somewhat more serious. One of the main points of having shared libraries in the first place, is saving RAM. Some common shared libraries are used by multiple applications. If the text section (where the code is) of the shared library can only be loaded into memory once (and then mapped into the virtual memories of many processes), considerable amounts of RAM can be saved. But this is not possible with load-time relocation, since when using this technique the text section has to be modified at load-time to apply the relocations. Therefore, for each application that loaded this shared library, it will have to be wholly placed in RAM again [ 1] . Different applications won't be able to really share it. Moreover, having a writable text section (it must be kept writable, to allow the dynamic loader to perform the relocations) poses a security risk, making it easier to exploit the application. As we'll see in this article, PIC mostly mitigates these problems.","title":"Some problems of load-time relocation"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#pic-introduction","text":"The idea behind PIC is simple - add an additional level of indirection to all global data and function references in the code. By cleverly utilizing some artifacts of the linking and loading processes, it's possible to make the text section of the shared library truly position independent , in the sense that it can be easily mapped into different memory addresses without needing to change one bit. In the next few sections I will explain in detail how this feat is achieved.","title":"PIC - introduction"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#key-insight-1-offset-between-text-and-data-sections","text":"One of the key insights on which PIC relies is the offset between the text and data sections, known to the linker at link-time . When the linker combines several object files together, it collects their sections (for example, all text sections get unified into a single large text section). Therefore, the linker knows both about the sizes of the sections and about their relative locations. For example, the text section may be immediately followed by the data section, so the offset from any given instruction in the text section to the beginning of the data section is just the size of the text section minus the offset of the instruction from the beginning of the text section - and both these quantities are known to the linker. In the diagram above, the code section was loaded into some address (unknown at link-time) 0xXXXX0000 (the X-es literally mean \"don't care\"), and the data section right after it at offset 0xXXXXF000. Then, if some instruction at offset 0x80 in the code section wants to reference stuff in the data section, the linker knows the relative offset (0xEF80 in this case) and can encode it in the instruction. Note that it wouldn't matter if another section was placed between the code and data sections, or if the data section preceded the code section. Since the linker knows the sizes of all sections and decides where to place them, the insight holds.","title":"Key insight #1 - offset between text and data sections"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#key-insight-2-making-an-ip-relative-offset-work-on-x86","text":"The above is only useful if we can actually put the relative offset to work. But data references (i.e. in the mov instruction) on x86 require absolute addresses. So, what can we do? If we have a relative address and need an absolute address, what's missing is the value of the instruction pointer (since, by definition, the relative address is relative to the instruction's location). There's no instruction to obtain the value of the instruction pointer on x86, but we can use a simple trick to get it. Here's some assembly pseudo-code that demonstrates it: call TMPLABEL TMPLABEL: pop ebx What happens here is: The CPU executes call TMPLABEL , which causes it to save the address of the next instruction (the pop ebx ) on stack and jump to the label. Since the instruction at the label is pop ebx , it gets executed next. It pops a value from the stack into ebx . But this value is the address of the instruction itself, so ebx now effectively contains the value of the instruction pointer.","title":"Key insight #2 - making an IP-relative offset work on x86"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-global-offset-table-got","text":"With this at hand, we can finally get to the implementation of position-independent data addressing on x86. It is accomplished by means of a \"global offset table\", or in short GOT. A GOT is simply a table of addresses, residing in the data section. Suppose some instruction in the code section wants to refer to a variable. Instead of referring to it directly by absolute address (which would require a relocation), it refers to an entry in the GOT. Since the GOT is in a known place in the data section, this reference is relative and known to the linker. The GOT entry, in turn, will contain the absolute address of the variable: In pseudo-assembly, we replace an absolute addressing instruction: ; Place the value of the variable in edx mov edx, [ADDR_OF_VAR] With displacement addressing from a register, along with an extra indirection: ; 1. Somehow get the address of the GOT into ebx lea ebx, ADDR_OF_GOT ; 2. Suppose ADDR_OF_VAR is stored at offset 0x10 ; in the GOT. Then this will place ADDR_OF_VAR ; into edx. mov edx, DWORD PTR [ebx + 0x10] ; 3. Finally, access the variable and place its ; value into edx. mov edx, DWORD PTR [edx] So, we've gotten rid of a relocation in the code section by redirecting variable references through the GOT. But we've also created a relocation in the data section. Why? Because the GOT still has to contain the absolute address of the variable for the scheme described above to work. So what have we gained? A lot, it turns out. A relocation in the data section is much less problematic than one in the code section, for two reasons (which directly address the two main problems of load-time relocation of code described in the beginning of the article): Relocations in the code section are required per variable reference , while in the GOT we only need to relocate once per variable . There are likely much more references to variables than variables, so this is more efficient. The data section is writable and not shared between processes anyway, so adding relocations to it does no harm. Moving relocations from the code section, however, allows to make it read-only and share it between processes.","title":"The Global Offset Table (GOT)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#pic-with-data-references-through-got-an-example","text":"I will now show a complete example that demonstrates the mechanics of PIC: int myglob = 42; int ml_func(int a, int b) { return myglob + a + b; } This chunk of code will be compiled into a shared library (using the -fpic and -shared flags as appropriate) named libmlpic_dataonly.so . Let's take a look at its disassembly, focusing on the ml_func function: 0000043c <ml_func>: 43c: 55 push ebp 43d: 89 e5 mov ebp,esp 43f: e8 16 00 00 00 call 45a <__i686.get_pc_thunk.cx> 444: 81 c1 b0 1b 00 00 add ecx,0x1bb0 44a: 8b 81 f0 ff ff ff mov eax,DWORD PTR [ecx-0x10] 450: 8b 00 mov eax,DWORD PTR [eax] 452: 03 45 08 add eax,DWORD PTR [ebp+0x8] 455: 03 45 0c add eax,DWORD PTR [ebp+0xc] 458: 5d pop ebp 459: c3 ret 0000045a <__i686.get_pc_thunk.cx>: 45a: 8b 0c 24 mov ecx,DWORD PTR [esp] 45d: c3 ret I'm going to refer to instructions by their addresses (the left-most number in the disassembly). This address is the offset from the load address of the shared library. At 43f , the address of the next instruction is placed into ecx , by means of the technique described in the \"key insight #2\" section above. At 444 , a known constant offset from the instruction to the place where the GOT is located is added to ecx . So ecx now serves as a base pointer to GOT. At 44a , a value is taken from [ecx - 0x10] , which is a GOT entry, and placed into eax . This is the address of myglob . At 450 the indirection is done, and the value of myglob is placed into eax . Later the parameters a and b are added to myglob and the value is returned (by keeping it in eax ). We can also query the shared library with readelf -S to see where the GOT section was placed: Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al <snip> [19] .got PROGBITS 00001fe4 000fe4 000010 04 WA 0 0 4 [20] .got.plt PROGBITS 00001ff4 000ff4 000014 04 WA 0 0 4 <snip> Let's do some math to check the computation done by the compiler to find myglob . As I mentioned above, the call to __i686.get_pc_thunk.cx places the address of the next instruction into ecx . That address is 0x444 [ 2] . The next instruction then adds 0x1bb0 to it, and the result in ecx is going to be 0x1ff4 . Finally, to actually obtain the GOT entry holding the address of myglob , displacement addressing is used - [ecx - 0x10] , so the entry is at 0x1fe4 , which is the first entry in the GOT according to the section header. Why there's another section whose name starts with .got will be explained later in the article [ 3] . Note that the compiler chooses to point ecx to after the GOT and then use negative offsets to obtain entries. This is fine, as long as the math works out. And so far it does. There's something we're still missing, however. How does the address of myglob actually get into the GOT slot at 0x1fe4 ? Recall that I mentioned a relocation, so let's find it: > readelf -r libmlpic_dataonly.so Relocation section '.rel.dyn' at offset 0x2dc contains 5 entries: Offset Info Type Sym.Value Sym. Name 00002008 00000008 R_386_RELATIVE 00001fe4 00000406 R_386_GLOB_DAT 0000200c myglob <snip> Note the relocation section for myglob , pointing to address 0x1fe4 , as expected. The relocation is of type R_386_GLOB_DAT , which simply tells the dynamic loader - \"put the actual value of the symbol (i.e. its address) into that offset\". So everything works out nicely. All that's left is to check how it actually looks when the library is loaded. We can do this by writing a simple \"driver\" executable that links to libmlpic_dataonly.so and calls ml_func , and then running it through GDB. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func [...] (gdb) run Starting program: [...]pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_reloc_dataonly.c:5 5 return myglob + a + b; (gdb) set disassembly-flavor intel (gdb) disas ml_func Dump of assembler code for function ml_func: 0x0013143c <+0>: push ebp 0x0013143d <+1>: mov ebp,esp 0x0013143f <+3>: call 0x13145a <__i686.get_pc_thunk.cx> 0x00131444 <+8>: add ecx,0x1bb0 => 0x0013144a <+14>: mov eax,DWORD PTR [ecx-0x10] 0x00131450 <+20>: mov eax,DWORD PTR [eax] 0x00131452 <+22>: add eax,DWORD PTR [ebp+0x8] 0x00131455 <+25>: add eax,DWORD PTR [ebp+0xc] 0x00131458 <+28>: pop ebp 0x00131459 <+29>: ret End of assembler dump. (gdb) i registers eax 0x1 1 ecx 0x132ff4 1257460 [...] skipping output The debugger has entered ml_func , and stopped at IP 0x0013144a [ 4] . We see that ecx holds the value 0x132ff4 (which is the address of the instruction plus 0x1bb0 , as explained before). Note that at this point, at runtime, these are absolute addresses - the shared library has already been loaded into the address space of the process. So, the GOT entry for myglob is at [ecx - 0x10] . Let's check what's there: (gdb) x 0x132fe4 0x132fe4: 0x0013300c So, we'd expect 0x0013300c to be the address of myglob . Let's verify: (gdb) p &myglob $1 = (int *) 0x13300c Indeed, it is!","title":"PIC with data references through GOT - an example"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#function-calls-in-pic","text":"Alright, so this is how data addressing works in position independent code. But what about function calls? Theoretically, the exact same approach could work for function calls as well. Instead of call actually containing the address of the function to call, let it contain the address of a known GOT entry, and fill in that entry during loading. But this is not how function calls work in PIC. What actually happens is a bit more complicated. Before I explain how it's done, a few words about the motivation for such a mechanism.","title":"Function calls in PIC"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-lazy-binding-optimization","text":"When a shared library refers to some function, the real address of that function is not known until load time. Resolving this address is called binding , and it's something the dynamic loader does when it loads the shared library into the process's memory space. This binding process is non-trivial, since the loader has to actually look up the function symbol in special tables [ 5] . So, resolving each function takes time. Not a lot of time, but it adds up since the amount of functions in libraries is typically much larger than the amount of global variables. Moreover, most of these resolutions are done in vain, because in a typical run of a program only a fraction of functions actually get called (think about various functions handling error and special conditions, which typically don't get called at all). So, to speed up this process, a clever lazy binding scheme was devised. \"Lazy\" is a generic name for a family of optimizations in computer programming, where work is delayed until the last moment when it's actually needed, with the intention of avoiding doing this work if its results are never required during a specific run of a program. Good examples of laziness are copy-on-write and lazy evaluation . This lazy binding scheme is attained by adding yet another level of indirection - the PLT.","title":"The lazy binding optimization"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-procedure-linkage-table-plt","text":"The PLT is part of the executable text section, consisting of a set of entries (one for each external function the shared library calls). Each PLT entry is a short chunk of executable code. Instead of calling the function directly, the code calls an entry in the PLT, which then takes care to call the actual function. This arrangement is sometimes called a \" trampoline \". Each PLT entry also has a corresponding entry in the GOT which contains the actual offset to the function, but only when the dynamic loader resolves it. I know this is confusing, but hopefully it will be come clearer once I explain the details in the next few paragraphs and diagrams. As the previous section mentioned, PLTs allow lazy resolution of functions. When the shared library is first loaded, the function calls have not been resolved yet: Explanation: In the code, a function func is called. The compiler translates it to a call to func@plt , which is some N-th entry in the PLT. The PLT consists of a special first entry, followed by a bunch of identically structured entries, one for each function needing resolution. Each PLT entry but the first consists of these parts: A jump to a location which is specified in a corresponding GOT entry Preparation of arguments for a \"resolver\" routine Call to the resolver routine, which resides in the first entry of the PLT The first PLT entry is a call to a resolver routine, which is located in the dynamic loader itself [ 6] . This routine resolves the actual address of the function. More on its action a bit later. Before the function's actual address has been resolved, the Nth GOT entry just points to after the jump. This is why this arrow in the diagram is colored differently - it's not an actual jump, just a pointer. What happens when func is called for the first time is this: PLT[n] is called and jumps to the address pointed to in GOT[n] . This address points into PLT[n] itself, to the preparation of arguments for the resolver. The resolver is then called. The resolver performs resolution of the actual address of func , places its actual address into GOT[n] and calls func . After the first call, the diagram looks a bit differently: Note that GOT[n] now points to the actual func [ 7] instead of back into the PLT. So, when func is called again: PLT[n] is called and jumps to the address pointed to in GOT[n] . GOT[n] points to func , so this just transfers control to func . In other words, now func is being actually called, without going through the resolver, at the cost of one additional jump. That's all there is to it, really. This mechanism allows lazy resolution of functions, and no resolution at all for functions that aren't actually called. It also leaves the code/text section of the library completely position independent, since the only place where an absolute address is used is the GOT, which resides in the data section and will be relocated by the dynamic loader. Even the PLT itself is PIC, so it can live in the read-only text section. I didn't get into much details regarding the resolver, but it's really not important for our purpose here. The resolver is simply a chunk of low-level code in the loader that does symbol resolution. The arguments prepared for it in each PLT entry, along with a suitable relocation entry, help it know about the symbol that needs resolution and about the GOT entry to update.","title":"The Procedure Linkage Table (PLT)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#pic-with-function-calls-through-plt-and-got-an-example","text":"Once again, to fortify the hard-learned theory with a practical demonstration, here's a complete example showing function call resolution using the mechanism described above. I'll be moving forward a bit faster this time. Here's the code for the shared library: int myglob = 42; int ml_util_func(int a) { return a + 1; } int ml_func(int a, int b) { int c = b + ml_util_func(a); myglob += c; return b + myglob; } This code will be compiled into libmlpic.so , and the focus is going to be on the call to ml_util_func from ml_func . Let's first disassemble ml_func : 00000477 <ml_func>: 477: 55 push ebp 478: 89 e5 mov ebp,esp 47a: 53 push ebx 47b: 83 ec 24 sub esp,0x24 47e: e8 e4 ff ff ff call 467 <__i686.get_pc_thunk.bx> 483: 81 c3 71 1b 00 00 add ebx,0x1b71 489: 8b 45 08 mov eax,DWORD PTR [ebp+0x8] 48c: 89 04 24 mov DWORD PTR [esp],eax 48f: e8 0c ff ff ff call 3a0 <ml_util_func@plt> <... snip more code> The interesting part is the call to ml_util_func@plt . Note also that the address of GOT is in ebx . Here's what ml_util_func@plt looks like (it's in an executable section called .plt ): 000003a0 <ml_util_func@plt>: 3a0: ff a3 14 00 00 00 jmp DWORD PTR [ebx+0x14] 3a6: 68 10 00 00 00 push 0x10 3ab: e9 c0 ff ff ff jmp 370 <_init+0x30> Recall that each PLT entry consists of three parts: A jump to an address specified in GOT (this is the jump to [ebx+0x14] ) Preparation of arguments for the resolver Call to the resolver The resolver (PLT entry 0) resides at address 0x370 , but it's of no interest to us here. What's more interesting is to see what the GOT contains. For that, we first have to do some math. The \"get IP\" trick in ml_func was done on address 0x483 , to which 0x1b71 is added. So the base of the GOT is at 0x1ff4 . We can take a peek at the GOT contents with readelf [ 8] : > readelf -x .got.plt libmlpic.so Hex dump of section '.got.plt': 0x00001ff4 241f0000 00000000 00000000 86030000 $............... 0x00002004 96030000 a6030000 ........ The GOT entry ml_util_func@plt looks at is at offset +0x14 , or 0x2008 . From above, the word at that location is 0x3a6 , which is the address of the push instruction in ml_util_func@plt . To help the dynamic loader do its job, a relocation entry is also added and specifies which place in the GOT to relocate for ml_util_func : > readelf -r libmlpic.so [...] snip output Relocation section '.rel.plt' at offset 0x328 contains 3 entries: Offset Info Type Sym.Value Sym. Name 00002000 00000107 R_386_JUMP_SLOT 00000000 __cxa_finalize 00002004 00000207 R_386_JUMP_SLOT 00000000 __gmon_start__ 00002008 00000707 R_386_JUMP_SLOT 0000046c ml_util_func The last line means that the dynamic loader should place the value (address) of symbol ml_util_func into 0x2008 (which, recall, is the GOT entry for this function). It would be interesting to see this GOT entry modification actually happen after the first call. Let's once again use GDB for the inspection. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func Breakpoint 1 at 0x80483c0 (gdb) run Starting program: /pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_main.c:10 10 int c = b + ml_util_func(a); (gdb) We're now before the first call to ml_util_func . Recall that GOT is pointed to by ebx in this code. Let's see what's in it: (gdb) i registers ebx ebx 0x132ff4 And the offset to the entry we need is at [ebx+0x14] : (gdb) x/w 0x133008 0x133008: 0x001313a6 Yep, the 0x3a6 ending, looks right. Now, let's step until after the call to ml_util_func and check again: (gdb) step ml_util_func (a=1) at ml_main.c:5 5 return a + 1; (gdb) x/w 0x133008 0x133008: 0x0013146c The value at 0x133008 was changed. Hence, 0x0013146c should be the real address of ml_util_func , placed in there by the dynamic loader: (gdb) p &ml_util_func $1 = (int (*)(int)) 0x13146c <ml_util_func> Just as expected.","title":"PIC with function calls through PLT and GOT - an example"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#controlling-if-and-when-the-resolution-is-done-by-the-loader","text":"This would be a good place to mention that the process of lazy symbol resolution performed by the dynamic loader can be configured with some environment variables (and corresponding flags to ld when linking the shared library). This is sometimes useful for special performance requirements or debugging. The LD_BIND_NOW env var, when defined, tells the dynamic loader to always perform the resolution for all symbols at start-up time, and not lazily. You can easily verify this in action by setting this env var and re-running the previous sample with GDB. You'll see that the GOT entry for ml_util_func contains its real address even before the first call to the function. Conversely, the LD_BIND_NOT env var tells the dynamic loader not to update the GOT entry at all. Each call to an external function will then go through the dynamic loader and be resolved anew. The dynamic loader is configurable by other flags as well. I encourage you to go over man ld.so - it contains some interesting information.","title":"Controlling if and when the resolution is done by the loader"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-costs-of-pic","text":"This article started by stating the problems of load-time relocation and how the PIC approach fixes them. But PIC is also not without problems. One immediately apparent cost is the extra indirection required for all external references to data and code in PIC. That's an extra memory load for each reference to a global variable, and for each call to a function. How problematic this is in practice depends on the compiler, the CPU architecture and the particular application. Another, less apparent cost, is the increased register usage required to implement PIC. In order to avoid locating the GOT too frequently, it makes sense for the compiler to generate code that keeps its address in a register (usually ebx ). But that ties down a whole register just for the sake of GOT. While not a big problem for RISC architectures that tend to have a lot of general purposes registers, it presents a performance problem for architectures like x86, which has a small amount of registers. PIC means having one general purpose register less, which adds up indirect costs since now more memory references have to be made.","title":"The costs of PIC"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#conclusion","text":"This article explained what position independent code is, and how it helps create shared libraries with shareable read-only text sections. There are some tradeoffs when choosing between PIC and its alternative (load-time relocation), and the eventual outcome really depends on a lot of factors, like the CPU architecture on which the program is going to run. That said, PIC is becoming more and more popular. Some non-Intel architectures like SPARC64 force PIC-only code for shared libraries, and many others (for example, ARM) include IP-relative addressing modes to make PIC more efficient. Both are true for the successor of x86, the x64 architecture. I will discuss PIC on x64 in a future article. The focus of this article, however, has not been on performance considerations or architectural decisions. My aim was to explain, given that PIC is used, how it works . If the explanation wasn't clear enough - please let me know in the comments and I will try to provide more information. [ 1] Unless all applications load this library into the exact same virtual memory address. But this usually isn't done on Linux. [ 2] 0x444 (and all other addresses mentioned in this computation) is relative to the load address of the shared library, which is unknown until an executable actually loads it at runtime. Note how it doesn't matter in the code since it only juggles relative addresses. [ 3] The astute reader may wonder why .got is a separate section at all. Didn't I just show in the diagrams that it's located in the data section? In practice, it is. I don't want to get into the distinction between ELF sections and segments here, since that would take use too far away from the point. But briefly, any number of \"data\" sections can be defined for a library and mapped into a read-write segment. This doesn't really matter, as long as the ELF file is organized correctly. Separating the data segment into different logical sections provides modularity and makes the linker's job easier. [ 4] Note that gdb skipped the part where ecx is assigned. That's because it's kind-of considered to be part of the function's prolog (the real reason is in the way gcc structures its debug info, of course). Several references to global data and functions are made inside a function, and a register pointing to GOT can serve all of them. [ 5] Shared library ELF objects actually come with special hash table sections for this purpose. [ 6] The dynamic loader on Linux is just another shared library which gets loaded into the address space of all running processes. [ 7] I placed func in a separate code section, although in theory this could be the same one where the call to func is made (i.e. in the same shared library). The \"extra credit\" section of this article has information about why a call to an external function in the same shared library needs PIC (or relocation) as well. [ 8] Recall that in the data reference example I promised to explain why there are apparently two GOT sections in the object: .got and .got.plt . Now it should become obvious that this is just to conveniently split the GOT entries required for global data from GOT entries required for the PLT. This is also why when the GOT offset is computed in functions, it points to .got.plt , which comes right after .got . This way, negative offsets lead us to .got , while positive offsets lead us to .got.plt . While convenient, such an arrangement is by no means compulsory. Both parts could be placed into a single .got section.","title":"Conclusion"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-independent-code/","text":"Position-independent code # In computing , position-independent code [ 1] ( PIC [ 1] ) or position-independent executable ( PIE ) is a body of machine code that, being placed somewhere in the primary memory , executes properly regardless of its absolute address (\u65e0\u8bba\u5176\u7edd\u5bf9\u5730\u5740\u5982\u4f55\uff0c\u5b83\u90fd\u88ab\u653e\u7f6e\u5728\u4e3b\u5b58\u50a8\u5668\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e). PIC is commonly used for shared libraries , so that the same library code can be loaded in a location in each program address space where it will not overlap any other uses of memory (for example, other shared libraries). PIC was also used on older computer systems lacking an MMU ,[ 2] so that the operating system could keep applications away from each other even within the single address space of an MMU-less system. Position-independent code can be executed at any memory address without modification. This differs from absolute code ,[ 1] which must be loaded at a specific location to function correctly,[ 1] and load-time locatable (LTL) code,[ 1] in which a linker or program loader modifies a program before execution so it can be run only from a particular memory location.[ 1] Generating position-independent code is often the default behavior for compilers , but they may place restrictions on the use of some language features, such as disallowing use of absolute addresses (position-independent code has to use relative addressing ). Instructions that refer directly to specific memory addresses sometimes execute faster, and replacing them with equivalent relative-addressing instructions may result in slightly slower execution, although modern processors make the difference practically negligible.[ 3]","title":"Position-independent-code"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Position-independent-code/Position-independent-code/#position-independent-code","text":"In computing , position-independent code [ 1] ( PIC [ 1] ) or position-independent executable ( PIE ) is a body of machine code that, being placed somewhere in the primary memory , executes properly regardless of its absolute address (\u65e0\u8bba\u5176\u7edd\u5bf9\u5730\u5740\u5982\u4f55\uff0c\u5b83\u90fd\u88ab\u653e\u7f6e\u5728\u4e3b\u5b58\u50a8\u5668\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e). PIC is commonly used for shared libraries , so that the same library code can be loaded in a location in each program address space where it will not overlap any other uses of memory (for example, other shared libraries). PIC was also used on older computer systems lacking an MMU ,[ 2] so that the operating system could keep applications away from each other even within the single address space of an MMU-less system. Position-independent code can be executed at any memory address without modification. This differs from absolute code ,[ 1] which must be loaded at a specific location to function correctly,[ 1] and load-time locatable (LTL) code,[ 1] in which a linker or program loader modifies a program before execution so it can be run only from a particular memory location.[ 1] Generating position-independent code is often the default behavior for compilers , but they may place restrictions on the use of some language features, such as disallowing use of absolute addresses (position-independent code has to use relative addressing ). Instructions that refer directly to specific memory addresses sometimes execute faster, and replacing them with equivalent relative-addressing instructions may result in slightly slower execution, although modern processors make the difference practically negligible.[ 3]","title":"Position-independent code"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/","text":"Relocation (computing) Segmentation Relocation table Unix-like systems Relocation procedure Example G53OPS - Operating Systems Relocation and Protection What does 'relocation' mean? Relocation (computing) # Relocation is the process of assigning load addresses for position-dependent code and data of a program and adjusting the code and data to reflect the assigned addresses .[ 1] [ 2] Prior to the advent of multiprocess systems, and still in many embedded systems the addresses for objects were absolute starting at a known location, often zero. Since multiprocessing systems dynamically link and switch between programs it became necessary to be able to relocate objects using position-independent code . A linker usually performs relocation in conjunction with symbol resolution , the process of searching files and libraries to replace symbolic references or names of libraries with actual usable addresses in memory before running a program. Relocation is typically done by the linker at link time , but it can also be done at load time by a relocating loader , or at run time by the running program itself. Some architectures avoid relocation entirely by deferring address assignment to run time; this is known as zero address arithmetic .[ which? ] THINKING : \u7f16\u8bd1\u751f\u6210\u7684executable\uff0c\u5b83\u4eec\u4e5f\u662f\u6709\u5730\u5740\u7a7a\u95f4\u7684 Segmentation # Object files are segmented into various memory segment types. Example segments include code segment(.text) , initialized data segment(.data) , uninitialized data segment(.bss ), or others.[ clarification needed ] Relocation table # The relocation table is a list of pointers created by the translator (a compiler or assembler ) and stored in the object or executable file. Each entry in the table, or \"fixup\", is a pointer to an absolute address in the object code that must be changed when the loader relocates the program so that it will refer to the correct location. Fixups are designed to support relocation of the program as a complete unit. In some cases, each fixup in the table is itself relative to a base address of zero, so the fixups themselves must be changed as the loader moves through the table.[ 3] In some architectures a fixup that crosses certain boundaries (such as a segment boundary) or that is not aligned on a word boundary is illegal and flagged as an error by the linker.[ 4] SUMMARY : \u663e\u7136\uff0c\u6bcf\u4e2aexecutable\u90fd\u5305\u542b\u4e00\u4e2arelocation table\u3002 Unix-like systems # The Executable and Linkable Format (ELF) executable format and shared library format used by most Unix-like systems allows several types of relocation to be defined.[ 5] Relocation procedure # The linker reads segment information and relocation tables in the object files and performs relocation by: merging all segments of common type into a single segment of that type assigning unique run time addresses to each section and each symbol, giving all code (functions) and data (global variables) unique run time addresses referring to the relocation table to modify[ why? ] symbols so that they point to the correct[ clarification needed ] run time addresses. Example # The following example uses Donald Knuth 's MIX architecture and MIXAL assembly language. The principles are the same for any architecture, though the details will change. (A) Program SUBR is compiled to produce object file (B), shown as both machine code and assembler. The compiler may start the compiled code at an arbitrary location, often location zero as shown. Location 13 contains the machine code for the jump instruction to statement ST in location 5. (C) If SUBR is later linked with other code it may be stored at a location other than zero. In this example the linker places it at location 120. The address in the jump instruction, which is now at location 133, must be relocated to point to the new location of the code for statement ST , now 125. [1 61 shown in the instruction is the MIX machine code representation of 125]. (D) When the program is loaded into memory to run it may be loaded at some location other than the one assigned by the linker. This example shows SUBR now at location 300. The address in the jump instruction, now at 313, needs to be relocated again so that it points to the updated location of ST , 305. [4 49 is the MIX machine representation of 305]. G53OPS - Operating Systems # Relocation and Protection # As soon as we introduce multiprogramming we have two problems that we need to address. Relocation : When a program is run it does not know in advance what location it will be loaded at. Therefore, the program cannot simply generate static addresses (e.g. from jump instructions). Instead, they must be made relative to where the program has been loaded. SUMMARY : \u5728\u7f16\u8bd1\u9636\u6bb5\uff0c\u7f16\u8bd1\u5668\u662f\u65e0\u6cd5\u5f97\u77e5\u5176\u751f\u6210\u7684executable\u5728\u8fd0\u884c\u65f6\u7684location\u7684\uff0c\u56e0\u6b64\u7f16\u8bd1\u5668\u751f\u6210\u7684executable\u4e0d\u80fd\u591f\u4f7f\u7528static address\uff0c\u5b83\u53ea\u80fd\u591f\u4f7f\u7528relative address\uff1b\u7f16\u8bd1\u5668\u5b9e\u73b0relative address\u7684\u65b9\u5f0f\u662f\u4f7f\u7528symbol\u3002\u8fd9\u662f\u4f7f\u7528relocation\u7684\u539f\u56e0\u3002 Protection : Once you can have two programs in memory at the same time there is a danger that one program can write to the address space of another program. This is obviously dangerous and should be avoided. In order to cater(\u8fce\u5408) for relocation we could make the loader modify all the relevant addresses as the binary file is loaded. The OS/360 worked in this way but the scheme suffers from the following problems \u00b7 The program cannot be moved, after it has been loaded without going through the same process. \u00b7 Using this scheme does not help the protection problem as the program can still generate illegal addresses (maybe by using absolute addressing). \u00b7 The program needs to have some sort of map that tells the loader which addresses need to be modified. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4ecb\u7ecd\u7684\u5185\u5bb9\u6307\u51fa\u4e86\u662f\u7531 loader \u6765\u6267\u884crelocation\uff0c\u4ee5\u53ca\u6267\u884c\u7684\u65f6\u673a\u3002\u5b83\u8fd8\u6d89\u53ca\u5230\u4e86relocation\u7684\u4e00\u4e9b\u5b9e\u73b0\u65b9\u6cd5\uff0c\u5982\u5f15\u5165relocation table\u3002 A solution, which solves both the relocation and protection problem is to equip(\u914d\u5907) the machine with two registers called the base and limit registers. The base register stores the start address of the partition and the limit register holds the length of the partition. Any address that is generated by the program has the base register added to it. In addition, all addresses are checked to ensure they are within the range of the partition. An additional benefit of this scheme is that if a program is moved within memory, only its base register needs to be amended. This is obviously a lot quicker than having to modify every address reference within the program. The IBM PC uses a scheme similar to this, although it does not have a limit register. What does 'relocation' mean? #","title":"Relocation(computing)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#relocation-computing","text":"Relocation is the process of assigning load addresses for position-dependent code and data of a program and adjusting the code and data to reflect the assigned addresses .[ 1] [ 2] Prior to the advent of multiprocess systems, and still in many embedded systems the addresses for objects were absolute starting at a known location, often zero. Since multiprocessing systems dynamically link and switch between programs it became necessary to be able to relocate objects using position-independent code . A linker usually performs relocation in conjunction with symbol resolution , the process of searching files and libraries to replace symbolic references or names of libraries with actual usable addresses in memory before running a program. Relocation is typically done by the linker at link time , but it can also be done at load time by a relocating loader , or at run time by the running program itself. Some architectures avoid relocation entirely by deferring address assignment to run time; this is known as zero address arithmetic .[ which? ] THINKING : \u7f16\u8bd1\u751f\u6210\u7684executable\uff0c\u5b83\u4eec\u4e5f\u662f\u6709\u5730\u5740\u7a7a\u95f4\u7684","title":"Relocation (computing)"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#segmentation","text":"Object files are segmented into various memory segment types. Example segments include code segment(.text) , initialized data segment(.data) , uninitialized data segment(.bss ), or others.[ clarification needed ]","title":"Segmentation"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#relocation-table","text":"The relocation table is a list of pointers created by the translator (a compiler or assembler ) and stored in the object or executable file. Each entry in the table, or \"fixup\", is a pointer to an absolute address in the object code that must be changed when the loader relocates the program so that it will refer to the correct location. Fixups are designed to support relocation of the program as a complete unit. In some cases, each fixup in the table is itself relative to a base address of zero, so the fixups themselves must be changed as the loader moves through the table.[ 3] In some architectures a fixup that crosses certain boundaries (such as a segment boundary) or that is not aligned on a word boundary is illegal and flagged as an error by the linker.[ 4] SUMMARY : \u663e\u7136\uff0c\u6bcf\u4e2aexecutable\u90fd\u5305\u542b\u4e00\u4e2arelocation table\u3002","title":"Relocation table"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#unix-like-systems","text":"The Executable and Linkable Format (ELF) executable format and shared library format used by most Unix-like systems allows several types of relocation to be defined.[ 5]","title":"Unix-like systems"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#relocation-procedure","text":"The linker reads segment information and relocation tables in the object files and performs relocation by: merging all segments of common type into a single segment of that type assigning unique run time addresses to each section and each symbol, giving all code (functions) and data (global variables) unique run time addresses referring to the relocation table to modify[ why? ] symbols so that they point to the correct[ clarification needed ] run time addresses.","title":"Relocation procedure"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#example","text":"The following example uses Donald Knuth 's MIX architecture and MIXAL assembly language. The principles are the same for any architecture, though the details will change. (A) Program SUBR is compiled to produce object file (B), shown as both machine code and assembler. The compiler may start the compiled code at an arbitrary location, often location zero as shown. Location 13 contains the machine code for the jump instruction to statement ST in location 5. (C) If SUBR is later linked with other code it may be stored at a location other than zero. In this example the linker places it at location 120. The address in the jump instruction, which is now at location 133, must be relocated to point to the new location of the code for statement ST , now 125. [1 61 shown in the instruction is the MIX machine code representation of 125]. (D) When the program is loaded into memory to run it may be loaded at some location other than the one assigned by the linker. This example shows SUBR now at location 300. The address in the jump instruction, now at 313, needs to be relocated again so that it points to the updated location of ST , 305. [4 49 is the MIX machine representation of 305].","title":"Example"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#g53ops-operating-systems","text":"","title":"G53OPS - Operating Systems"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#relocation-and-protection","text":"As soon as we introduce multiprogramming we have two problems that we need to address. Relocation : When a program is run it does not know in advance what location it will be loaded at. Therefore, the program cannot simply generate static addresses (e.g. from jump instructions). Instead, they must be made relative to where the program has been loaded. SUMMARY : \u5728\u7f16\u8bd1\u9636\u6bb5\uff0c\u7f16\u8bd1\u5668\u662f\u65e0\u6cd5\u5f97\u77e5\u5176\u751f\u6210\u7684executable\u5728\u8fd0\u884c\u65f6\u7684location\u7684\uff0c\u56e0\u6b64\u7f16\u8bd1\u5668\u751f\u6210\u7684executable\u4e0d\u80fd\u591f\u4f7f\u7528static address\uff0c\u5b83\u53ea\u80fd\u591f\u4f7f\u7528relative address\uff1b\u7f16\u8bd1\u5668\u5b9e\u73b0relative address\u7684\u65b9\u5f0f\u662f\u4f7f\u7528symbol\u3002\u8fd9\u662f\u4f7f\u7528relocation\u7684\u539f\u56e0\u3002 Protection : Once you can have two programs in memory at the same time there is a danger that one program can write to the address space of another program. This is obviously dangerous and should be avoided. In order to cater(\u8fce\u5408) for relocation we could make the loader modify all the relevant addresses as the binary file is loaded. The OS/360 worked in this way but the scheme suffers from the following problems \u00b7 The program cannot be moved, after it has been loaded without going through the same process. \u00b7 Using this scheme does not help the protection problem as the program can still generate illegal addresses (maybe by using absolute addressing). \u00b7 The program needs to have some sort of map that tells the loader which addresses need to be modified. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4ecb\u7ecd\u7684\u5185\u5bb9\u6307\u51fa\u4e86\u662f\u7531 loader \u6765\u6267\u884crelocation\uff0c\u4ee5\u53ca\u6267\u884c\u7684\u65f6\u673a\u3002\u5b83\u8fd8\u6d89\u53ca\u5230\u4e86relocation\u7684\u4e00\u4e9b\u5b9e\u73b0\u65b9\u6cd5\uff0c\u5982\u5f15\u5165relocation table\u3002 A solution, which solves both the relocation and protection problem is to equip(\u914d\u5907) the machine with two registers called the base and limit registers. The base register stores the start address of the partition and the limit register holds the length of the partition. Any address that is generated by the program has the base register added to it. In addition, all addresses are checked to ensure they are within the range of the partition. An additional benefit of this scheme is that if a program is moved within memory, only its base register needs to be amended. This is obviously a lot quicker than having to modify every address reference within the program. The IBM PC uses a scheme similar to this, although it does not have a limit register.","title":"Relocation and Protection"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Library/Relocation/Relocation(computing)/#what-does-relocation-mean","text":"","title":"What does 'relocation' mean?"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/","text":"Name mangling Examples C C++ Simple example[edit] Complex example[edit] How different compilers mangle the same functions[edit] Handling of C symbols when linking from C++ Standardised name mangling in C++ Real-world effects of C++ name mangling Demangle via c++filt Demangle via builtin GCC ABI Python Name mangling # In compiler construction, name mangling (also called name decoration ) is a technique used to solve various problems caused by the need to resolve unique names for programming entities in many modern programming languages . It provides a way of encoding additional information in the name of a function , structure , class or another datatype in order to pass more semantic information from the compilers to linkers . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u4f7f\u7528name mangling\u7684\u610f\u56fe The need arises where the language allows different entities to be named with the same identifier as long as they occupy a different namespace (where a namespace is typically defined by a module, class, or explicit namespace directive) or have different signatures (such as function overloading ). Any object code produced by compilers is usually linked with other pieces of object code (produced by the same or another compiler) by a type of program called a linker . The linker needs a great deal of information on each program entity. For example, to correctly link a function it needs its name, the number of arguments and their types, and so on. Examples # C # Although name mangling is not generally required or used by languages that do not support function overloading (such as C and classic Pascal), they use it in some cases to provide additional information about a function. For example, compilers targeted at Microsoft Windows platforms support a variety of calling conventions , which determine the manner in which parameters are sent to subroutines and results returned. Because the different calling conventions are not compatible with one another, compilers mangle symbols with codes detailing which convention should be used to call the specific routine. SUMMARY : encode calling convention in the name The mangling scheme was established by Microsoft, and has been informally followed by other compilers including Digital Mars, Borland, and GNU GCC, when compiling code for the Windows platforms. The scheme even applies to other languages, such as Pascal , D , Delphi , Fortran , and C# . This allows subroutines written in those languages to call, or be called by, existing Windows libraries using a calling convention different from their default. When compiling the following C examples: int _cdecl f (int x) { return 0; } int _stdcall g (int y) { return 0; } int _fastcall h (int z) { return 0; } 32 bit compilers emit, respectively: _f _g@4 @h@4 In the stdcall and fastcall mangling schemes, the function is encoded as _**name**@**X** and @**name**@**X** respectively, where X is the number of bytes, in decimal, of the argument(s) in the parameter list (including those passed in registers, for fastcall). In the case of cdecl , the function name is merely prefixed by an underscore. The 64-bit convention on Windows (Microsoft C) has no leading underscore. This difference may in some rare cases lead to unresolved externals when porting such code to 64 bits. For example, Fortran code can use 'alias' to link against a C method by name as follows: SUBROUTINE f() !DEC$ ATTRIBUTES C, ALIAS:'_f' :: f END SUBROUTINE This will compile and link fine under 32 bits, but generate an unresolved external '_f' under 64 bits. One workaround for this is not to use 'alias' at all (in which the method names typically need to be capitalized in C and Fortran). Another is to use the BIND option: SUBROUTINE f() BIND(C,NAME=\"f\") END SUBROUTINE C++ # C++ compilers are the most widespread users of name mangling . The first C++ compilers were implemented as translators to C source code, which would then be compiled by a C compiler to object code; because of this, symbol names had to conform to C identifier rules. Even later, with the emergence of compilers which produced machine code or assembly directly, the system's linker generally did not support C++ symbols, and mangling was still required. The C++ language does not define a standard decoration scheme, so each compiler uses its own. C++ also has complex language features, such as classes , templates , namespaces , and operator overloading , that alter the meaning of specific symbols based on context or usage. Meta-data about these features can be disambiguated(\u6d88\u9664) by mangling (decorating) the name of a symbol . Because the name-mangling systems for such features are not standardized across compilers, few linkers can link object code that was produced by different compilers. Simple example[ edit ] # A single C++ translation unit might define two functions named f() : int f (void) { return 1; } int f (int) { return 0; } void g (void) { int i = f(), j = f(0); } These are distinct functions, with no relation to each other apart from the name. The C++ compiler therefore will encode the type information in the symbol name, the result being something resembling: int __f_v (void) { return 1; } int __f_i (int) { return 0; } void __g_v (void) { int i = __f_v(), j = __f_i(0); } Even though its name is unique, g() is still mangled: name mangling applies to all symbols. Complex example[ edit ] # The mangled symbols in this example, in the comments below the respective identifier name, are those produced by the GNU GCC 3.x compilers: namespace wikipedia { class article { public: std::string format (void); /* = _ZN9wikipedia7article6formatEv */ bool print_to (std::ostream&); /* = _ZN9wikipedia7article8print_toERSo */ class wikilink { public: wikilink (std::string const& name); /* = _ZN9wikipedia7article8wikilinkC1ERKSs */ }; }; } All mangled symbols begin with _Z (note that an identifier beginning with an underscore followed by a capital is a reserved identifier in C, so conflict with user identifiers is avoided); for nested names (including both namespaces and classes), this is followed by N , then a series of pairs (the length being the length of the next identifier), and finally E . For example, wikipedia::article::format becomes _ZN9Wikipedia7article6formatE For functions, this is then followed by the type information; as format() is a void function, this is simply v ; hence: _ZN9Wikipedia7article6formatEv For print_to , the standard type std::ostream (which is a typedef for std::basic_ostream<char, std::char_traits<char> > ) is used, which has the special alias So ; a reference to this type is therefore RSo , with the complete name for the function being: _ZN9Wikipedia7article8print_toERSo How different compilers mangle the same functions[ edit ] # There isn't a standard scheme by which even trivial C++ identifiers are mangled, and consequently different compilers (or even different versions of the same compiler, or the same compiler on different platforms) mangle public symbols in radically different (and thus totally incompatible) ways. Consider how different C++ compilers mangle the same functions: Compiler void h(int) void h(int, char) void h(void) Intel C++ 8.0 for Linux _Z1hi _Z1hic _Z1hv HP aC++ A.05.55 IA-64 IAR EWARM C++ 5.4 ARM GCC 3. x and higher Clang 1. x and higher[ 1] IAR EWARM C++ 7.4 ARM _Z<number>hi _Z<number>hic _Z<number>hv GCC 2.9 x h__Fi h__Fic h__Fv HP aC++ A.03.45 PA-RISC Microsoft Visual C++ v6-v10 ( mangling details ) ?h@@YAXH@Z ?h@@YAXHD@Z ?h@@YAXXZ Digital Mars C++ Borland C++ v3.1 @h$qi @h$qizc @h$qv OpenVMS C++ V6.5 (ARM mode) H__XI H__XIC H__XV OpenVMS C++ V6.5 (ANSI mode) CXX$__7H__FIC26CDH77 CXX$__7H__FV2CB06E8 OpenVMS C++ X7.1 IA-64 CXX$_Z1HI2DSQ26A CXX$_Z1HIC2NP3LI4 CXX$_Z1HV0BCA19V SunPro CC __1cBh6Fi_v_ __1cBh6Fic_v_ __1cBh6F_v_ Tru64 C++ V6.5 (ARM mode) h__Xi h__Xic h__Xv Tru64 C++ V6.5 (ANSI mode) __7h__Fi __7h__Fic __7h__Fv Watcom C++ 10.6 W?h$n(i)v W?h$n(ia)v W?h$n()v Notes: The Compaq C++ compiler on OpenVMS VAX and Alpha (but not IA-64) and Tru64 has two name mangling schemes. The original, pre-standard scheme is known as ARM model, and is based on the name mangling described in the C++ Annotated Reference Manual (ARM). With the advent of new features in standard C++, particularly templates , the ARM scheme became more and more unsuitable \u2014 it could not encode certain function types, or produced identical mangled names for different functions. It was therefore replaced by the newer \"ANSI\" model, which supported all ANSI template features, but was not backwards compatible. On IA-64, a standard Application Binary Interface (ABI) exists (see external links ), which defines (among other things) a standard name-mangling scheme, and which is used by all the IA-64 compilers. GNU GCC 3. x , in addition, has adopted the name mangling scheme defined in this standard for use on other, non-Intel platforms. The Visual Studio and Windows SDK include the program undname which prints the C-style function prototype for a given mangled name. On Microsoft Windows, the Intel compiler[ 2] and Clang [ 3] uses the Visual C++ name mangling for compatibility. For the IAR EWARM C++ 7.4 ARM compiler the best way to determine the name of a function is to compile with the assembler output turned on and to look at the output in the \".s\" file thus generated. Handling of C symbols when linking from C++ # The job of the common C++ idiom: #ifdef __cplusplus extern \"C\" { #endif /* ... */ #ifdef __cplusplus } #endif is to ensure that the symbols within are \"unmangled\" \u2013 that the compiler emits(\u521b\u9020\uff0c\u751f\u4ea7\u51fa) a binary file with their names undecorated, as a C compiler would do. As C language definitions are unmangled, the C++ compiler needs to avoid mangling references to these identifiers. For example, the standard strings library, <string.h> usually contains something resembling: #ifdef __cplusplus extern \"C\" { #endif void *memset (void *, int, size_t); char *strcat (char *, const char *); int strcmp (const char *, const char *); char *strcpy (char *, const char *); #ifdef __cplusplus } #endif Thus, code such as: if (strcmp(argv[1], \"-x\") == 0) strcpy(a, argv[2]); else memset (a, 0, sizeof(a)); uses the correct, unmangled strcmp and memset . If the extern had not been used, the (SunPro) C++ compiler would produce code equivalent to: if (__1cGstrcmp6Fpkc1_i_(argv[1], \"-x\") == 0) __1cGstrcpy6Fpcpkc_0_(a, argv[2]); else __1cGmemset6FpviI_0_ (a, 0, sizeof(a)); Since those symbols do not exist in the C runtime library ( e.g. libc ), link errors would result. Standardised name mangling in C++ # Though it would seem that standardised name mangling in the C++ language would lead to greater interoperability between compiler implementations, such a standardization by itself would not suffice to guarantee C++ compiler interoperability and it might even create a false impression that interoperability is possible and safe when it isn't. Name mangling is only one of several application binary interface (ABI) details that need to be decided and observed by a C++ implementation. Other ABI aspects like exception handling , virtual table layout, structure and stack frame padding , etc. also cause differing C++ implementations to be incompatible. Further, requiring a particular form of mangling would cause issues for systems where implementation limits (e.g., length of symbols) dictate a particular mangling scheme. A standardised requirement for name mangling would also prevent an implementation where mangling was not required at all \u2014 for example, a linker which understood the C++ language. The C++ standard therefore does not attempt to standardise name mangling. On the contrary, the Annotated C++ Reference Manual (also known as ARM , ISBN 0-201-51459-1 , section 7.2.1c) actively encourages the use of different mangling schemes to prevent linking when other aspects of the ABI, such as exception handling and virtual table layout, are incompatible. Nevertheless, as detailed in the section above, on some platforms[ 4] the full C++ ABI has been standardized, including name mangling. Real-world effects of C++ name mangling # Because C++ symbols are routinely exported from DLL and shared object files, the name mangling scheme is not merely a compiler-internal matter. Different compilers (or different versions of the same compiler, in many cases) produce such binaries under different name decoration schemes, meaning that symbols are frequently unresolved if the compilers used to create the library and the program using it employed different schemes. For example, if a system with multiple C++ compilers installed (e.g., GNU GCC and the OS vendor's compiler) wished to install the Boost C++ Libraries , it would have to be compiled multiple times (once for GCC and once for the vendor compiler). It is good for safety purposes that compilers producing incompatible object codes (codes based on different ABIs, regarding e.g., classes and exceptions) use different name mangling schemes. This guarantees that these incompatibilities are detected at the linking phase, not when executing the software (which could lead to obscure(\u6a21\u7cca\u7684\uff0c\u9690\u6666\u7684) bugs and serious stability issues). For this reason name decoration is an important aspect of any C++-related ABI . Demangle via c++filt # $ c++filt _ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_ Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const Demangle via builtin GCC ABI # #include <stdio.h> #include <stdlib.h> #include <cxxabi.h> int main() { const char *mangled_name = \"_ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_\"; char *demangled_name; int status = -1; demangled_name = abi::__cxa_demangle(mangled_name, NULL, NULL, &status); printf(\"Demangled: %s\\n\", demangled_name); free(demangled_name); return 0; } Output: Demangled: Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const Python # In Python , mangling is used for \"private\" class members which are designated as such by giving them a name with two leading underscores and no more than one trailing underscore. For example, __thing will be mangled, as will ___thing and __thing_ , but __thing__ and __thing___ will not. Python's runtime does not restrict access to such members, the mangling only prevents name collisions if a derived class defines a member with the same name. On encountering name mangled attributes, Python transforms these names by prepending a single underscore and the name of the enclosing class, for example: >>> class Test(object): ... def __mangled_name(self): ... pass ... def normal_name(self): ... pass >>> t = Test() >>> [attr for attr in dir(t) if 'name' in attr] ['_Test__mangled_name', 'normal_name']","title":"Name-mangling"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#name-mangling","text":"In compiler construction, name mangling (also called name decoration ) is a technique used to solve various problems caused by the need to resolve unique names for programming entities in many modern programming languages . It provides a way of encoding additional information in the name of a function , structure , class or another datatype in order to pass more semantic information from the compilers to linkers . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u4f7f\u7528name mangling\u7684\u610f\u56fe The need arises where the language allows different entities to be named with the same identifier as long as they occupy a different namespace (where a namespace is typically defined by a module, class, or explicit namespace directive) or have different signatures (such as function overloading ). Any object code produced by compilers is usually linked with other pieces of object code (produced by the same or another compiler) by a type of program called a linker . The linker needs a great deal of information on each program entity. For example, to correctly link a function it needs its name, the number of arguments and their types, and so on.","title":"Name mangling"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#examples","text":"","title":"Examples"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#c","text":"Although name mangling is not generally required or used by languages that do not support function overloading (such as C and classic Pascal), they use it in some cases to provide additional information about a function. For example, compilers targeted at Microsoft Windows platforms support a variety of calling conventions , which determine the manner in which parameters are sent to subroutines and results returned. Because the different calling conventions are not compatible with one another, compilers mangle symbols with codes detailing which convention should be used to call the specific routine. SUMMARY : encode calling convention in the name The mangling scheme was established by Microsoft, and has been informally followed by other compilers including Digital Mars, Borland, and GNU GCC, when compiling code for the Windows platforms. The scheme even applies to other languages, such as Pascal , D , Delphi , Fortran , and C# . This allows subroutines written in those languages to call, or be called by, existing Windows libraries using a calling convention different from their default. When compiling the following C examples: int _cdecl f (int x) { return 0; } int _stdcall g (int y) { return 0; } int _fastcall h (int z) { return 0; } 32 bit compilers emit, respectively: _f _g@4 @h@4 In the stdcall and fastcall mangling schemes, the function is encoded as _**name**@**X** and @**name**@**X** respectively, where X is the number of bytes, in decimal, of the argument(s) in the parameter list (including those passed in registers, for fastcall). In the case of cdecl , the function name is merely prefixed by an underscore. The 64-bit convention on Windows (Microsoft C) has no leading underscore. This difference may in some rare cases lead to unresolved externals when porting such code to 64 bits. For example, Fortran code can use 'alias' to link against a C method by name as follows: SUBROUTINE f() !DEC$ ATTRIBUTES C, ALIAS:'_f' :: f END SUBROUTINE This will compile and link fine under 32 bits, but generate an unresolved external '_f' under 64 bits. One workaround for this is not to use 'alias' at all (in which the method names typically need to be capitalized in C and Fortran). Another is to use the BIND option: SUBROUTINE f() BIND(C,NAME=\"f\") END SUBROUTINE","title":"C"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#c_1","text":"C++ compilers are the most widespread users of name mangling . The first C++ compilers were implemented as translators to C source code, which would then be compiled by a C compiler to object code; because of this, symbol names had to conform to C identifier rules. Even later, with the emergence of compilers which produced machine code or assembly directly, the system's linker generally did not support C++ symbols, and mangling was still required. The C++ language does not define a standard decoration scheme, so each compiler uses its own. C++ also has complex language features, such as classes , templates , namespaces , and operator overloading , that alter the meaning of specific symbols based on context or usage. Meta-data about these features can be disambiguated(\u6d88\u9664) by mangling (decorating) the name of a symbol . Because the name-mangling systems for such features are not standardized across compilers, few linkers can link object code that was produced by different compilers.","title":"C++"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#simple-exampleedit","text":"A single C++ translation unit might define two functions named f() : int f (void) { return 1; } int f (int) { return 0; } void g (void) { int i = f(), j = f(0); } These are distinct functions, with no relation to each other apart from the name. The C++ compiler therefore will encode the type information in the symbol name, the result being something resembling: int __f_v (void) { return 1; } int __f_i (int) { return 0; } void __g_v (void) { int i = __f_v(), j = __f_i(0); } Even though its name is unique, g() is still mangled: name mangling applies to all symbols.","title":"Simple example[edit]"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#complex-exampleedit","text":"The mangled symbols in this example, in the comments below the respective identifier name, are those produced by the GNU GCC 3.x compilers: namespace wikipedia { class article { public: std::string format (void); /* = _ZN9wikipedia7article6formatEv */ bool print_to (std::ostream&); /* = _ZN9wikipedia7article8print_toERSo */ class wikilink { public: wikilink (std::string const& name); /* = _ZN9wikipedia7article8wikilinkC1ERKSs */ }; }; } All mangled symbols begin with _Z (note that an identifier beginning with an underscore followed by a capital is a reserved identifier in C, so conflict with user identifiers is avoided); for nested names (including both namespaces and classes), this is followed by N , then a series of pairs (the length being the length of the next identifier), and finally E . For example, wikipedia::article::format becomes _ZN9Wikipedia7article6formatE For functions, this is then followed by the type information; as format() is a void function, this is simply v ; hence: _ZN9Wikipedia7article6formatEv For print_to , the standard type std::ostream (which is a typedef for std::basic_ostream<char, std::char_traits<char> > ) is used, which has the special alias So ; a reference to this type is therefore RSo , with the complete name for the function being: _ZN9Wikipedia7article8print_toERSo","title":"Complex example[edit]"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#how-different-compilers-mangle-the-same-functionsedit","text":"There isn't a standard scheme by which even trivial C++ identifiers are mangled, and consequently different compilers (or even different versions of the same compiler, or the same compiler on different platforms) mangle public symbols in radically different (and thus totally incompatible) ways. Consider how different C++ compilers mangle the same functions: Compiler void h(int) void h(int, char) void h(void) Intel C++ 8.0 for Linux _Z1hi _Z1hic _Z1hv HP aC++ A.05.55 IA-64 IAR EWARM C++ 5.4 ARM GCC 3. x and higher Clang 1. x and higher[ 1] IAR EWARM C++ 7.4 ARM _Z<number>hi _Z<number>hic _Z<number>hv GCC 2.9 x h__Fi h__Fic h__Fv HP aC++ A.03.45 PA-RISC Microsoft Visual C++ v6-v10 ( mangling details ) ?h@@YAXH@Z ?h@@YAXHD@Z ?h@@YAXXZ Digital Mars C++ Borland C++ v3.1 @h$qi @h$qizc @h$qv OpenVMS C++ V6.5 (ARM mode) H__XI H__XIC H__XV OpenVMS C++ V6.5 (ANSI mode) CXX$__7H__FIC26CDH77 CXX$__7H__FV2CB06E8 OpenVMS C++ X7.1 IA-64 CXX$_Z1HI2DSQ26A CXX$_Z1HIC2NP3LI4 CXX$_Z1HV0BCA19V SunPro CC __1cBh6Fi_v_ __1cBh6Fic_v_ __1cBh6F_v_ Tru64 C++ V6.5 (ARM mode) h__Xi h__Xic h__Xv Tru64 C++ V6.5 (ANSI mode) __7h__Fi __7h__Fic __7h__Fv Watcom C++ 10.6 W?h$n(i)v W?h$n(ia)v W?h$n()v Notes: The Compaq C++ compiler on OpenVMS VAX and Alpha (but not IA-64) and Tru64 has two name mangling schemes. The original, pre-standard scheme is known as ARM model, and is based on the name mangling described in the C++ Annotated Reference Manual (ARM). With the advent of new features in standard C++, particularly templates , the ARM scheme became more and more unsuitable \u2014 it could not encode certain function types, or produced identical mangled names for different functions. It was therefore replaced by the newer \"ANSI\" model, which supported all ANSI template features, but was not backwards compatible. On IA-64, a standard Application Binary Interface (ABI) exists (see external links ), which defines (among other things) a standard name-mangling scheme, and which is used by all the IA-64 compilers. GNU GCC 3. x , in addition, has adopted the name mangling scheme defined in this standard for use on other, non-Intel platforms. The Visual Studio and Windows SDK include the program undname which prints the C-style function prototype for a given mangled name. On Microsoft Windows, the Intel compiler[ 2] and Clang [ 3] uses the Visual C++ name mangling for compatibility. For the IAR EWARM C++ 7.4 ARM compiler the best way to determine the name of a function is to compile with the assembler output turned on and to look at the output in the \".s\" file thus generated.","title":"How different compilers mangle the same functions[edit]"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#handling-of-c-symbols-when-linking-from-c","text":"The job of the common C++ idiom: #ifdef __cplusplus extern \"C\" { #endif /* ... */ #ifdef __cplusplus } #endif is to ensure that the symbols within are \"unmangled\" \u2013 that the compiler emits(\u521b\u9020\uff0c\u751f\u4ea7\u51fa) a binary file with their names undecorated, as a C compiler would do. As C language definitions are unmangled, the C++ compiler needs to avoid mangling references to these identifiers. For example, the standard strings library, <string.h> usually contains something resembling: #ifdef __cplusplus extern \"C\" { #endif void *memset (void *, int, size_t); char *strcat (char *, const char *); int strcmp (const char *, const char *); char *strcpy (char *, const char *); #ifdef __cplusplus } #endif Thus, code such as: if (strcmp(argv[1], \"-x\") == 0) strcpy(a, argv[2]); else memset (a, 0, sizeof(a)); uses the correct, unmangled strcmp and memset . If the extern had not been used, the (SunPro) C++ compiler would produce code equivalent to: if (__1cGstrcmp6Fpkc1_i_(argv[1], \"-x\") == 0) __1cGstrcpy6Fpcpkc_0_(a, argv[2]); else __1cGmemset6FpviI_0_ (a, 0, sizeof(a)); Since those symbols do not exist in the C runtime library ( e.g. libc ), link errors would result.","title":"Handling of C symbols when linking from C++"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#standardised-name-mangling-in-c","text":"Though it would seem that standardised name mangling in the C++ language would lead to greater interoperability between compiler implementations, such a standardization by itself would not suffice to guarantee C++ compiler interoperability and it might even create a false impression that interoperability is possible and safe when it isn't. Name mangling is only one of several application binary interface (ABI) details that need to be decided and observed by a C++ implementation. Other ABI aspects like exception handling , virtual table layout, structure and stack frame padding , etc. also cause differing C++ implementations to be incompatible. Further, requiring a particular form of mangling would cause issues for systems where implementation limits (e.g., length of symbols) dictate a particular mangling scheme. A standardised requirement for name mangling would also prevent an implementation where mangling was not required at all \u2014 for example, a linker which understood the C++ language. The C++ standard therefore does not attempt to standardise name mangling. On the contrary, the Annotated C++ Reference Manual (also known as ARM , ISBN 0-201-51459-1 , section 7.2.1c) actively encourages the use of different mangling schemes to prevent linking when other aspects of the ABI, such as exception handling and virtual table layout, are incompatible. Nevertheless, as detailed in the section above, on some platforms[ 4] the full C++ ABI has been standardized, including name mangling.","title":"Standardised name mangling in C++"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#real-world-effects-of-c-name-mangling","text":"Because C++ symbols are routinely exported from DLL and shared object files, the name mangling scheme is not merely a compiler-internal matter. Different compilers (or different versions of the same compiler, in many cases) produce such binaries under different name decoration schemes, meaning that symbols are frequently unresolved if the compilers used to create the library and the program using it employed different schemes. For example, if a system with multiple C++ compilers installed (e.g., GNU GCC and the OS vendor's compiler) wished to install the Boost C++ Libraries , it would have to be compiled multiple times (once for GCC and once for the vendor compiler). It is good for safety purposes that compilers producing incompatible object codes (codes based on different ABIs, regarding e.g., classes and exceptions) use different name mangling schemes. This guarantees that these incompatibilities are detected at the linking phase, not when executing the software (which could lead to obscure(\u6a21\u7cca\u7684\uff0c\u9690\u6666\u7684) bugs and serious stability issues). For this reason name decoration is an important aspect of any C++-related ABI .","title":"Real-world effects of C++ name mangling"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#demangle-via-cfilt","text":"$ c++filt _ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_ Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const","title":"Demangle via c++filt"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#demangle-via-builtin-gcc-abi","text":"#include <stdio.h> #include <stdlib.h> #include <cxxabi.h> int main() { const char *mangled_name = \"_ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_\"; char *demangled_name; int status = -1; demangled_name = abi::__cxa_demangle(mangled_name, NULL, NULL, &status); printf(\"Demangled: %s\\n\", demangled_name); free(demangled_name); return 0; } Output: Demangled: Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const","title":"Demangle via builtin GCC ABI"},{"location":"Kernel/Guide/Linux-OS's-multitasking/ABI/Name-mangling/Name-mangling/#python","text":"In Python , mangling is used for \"private\" class members which are designated as such by giving them a name with two leading underscores and no more than one trailing underscore. For example, __thing will be mangled, as will ___thing and __thing_ , but __thing__ and __thing___ will not. Python's runtime does not restrict access to such members, the mangling only prevents name collisions if a derived class defines a member with the same name. On encountering name mangled attributes, Python transforms these names by prepending a single underscore and the name of the enclosing class, for example: >>> class Test(object): ... def __mangled_name(self): ... pass ... def normal_name(self): ... pass >>> t = Test() >>> [attr for attr in dir(t) if 'name' in attr] ['_Test__mangled_name', 'normal_name']","title":"Python"},{"location":"Kernel/Guide/Memory-management/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u4e3b\u8981\u63cf\u8ff0\u73b0\u4ee3linux OS\u7684Memory management\uff0c\u4e3b\u8981\u53c2\u8003\u7684\u7ef4\u57fa\u767e\u79d1 Memory management \u3002\u7ef4\u57fa\u767e\u79d1\u603b\u7ed3\u4e86\u975e\u5e38\u591a\u7684Memory management\u6280\u672f\uff0c\u672c\u7ae0\u6240\u5173\u6ce8\u7684\u662f\u73b0\u4ee3OS\u666e\u904d\u4f7f\u7528\u7684 Paged virtual memory \u6280\u672f\uff08 Memory segmentation \u88ab\u89c6\u4e3alegacy \u4e86\uff09\u3002","title":"Introduction"},{"location":"Kernel/Guide/Memory-management/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u63cf\u8ff0\u73b0\u4ee3linux OS\u7684Memory management\uff0c\u4e3b\u8981\u53c2\u8003\u7684\u7ef4\u57fa\u767e\u79d1 Memory management \u3002\u7ef4\u57fa\u767e\u79d1\u603b\u7ed3\u4e86\u975e\u5e38\u591a\u7684Memory management\u6280\u672f\uff0c\u672c\u7ae0\u6240\u5173\u6ce8\u7684\u662f\u73b0\u4ee3OS\u666e\u904d\u4f7f\u7528\u7684 Paged virtual memory \u6280\u672f\uff08 Memory segmentation \u88ab\u89c6\u4e3alegacy \u4e86\uff09\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Guide/Memory-management/C-dynamic-memory-allocation/","text":"C dynamic memory allocation C dynamic memory allocation #","title":"C dynamic memory allocation"},{"location":"Kernel/Guide/Memory-management/C-dynamic-memory-allocation/#c-dynamic-memory-allocation","text":"","title":"C dynamic memory allocation"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/","text":"Difference Between Paging and Segmentation in OS # The memory management in the operating system is an essential functionality, which allows the allocation of memory to the processes for execution and deallocates the memory when the process is no longer needed. In this article, we will discuss two memory management schemes paging and segmentation . The basic difference between paging and segmentation is that, \u201cpage\u201d is a fixed-sized block whereas, a \u201csegment\u201d is a variable-sized block. NOTE : \u5176\u5b9e\uff0c\u8fd9\u4e2a\u5dee\u522b\u5c31\u51b3\u5b9a\u4e86segment\u662f\u9700\u8981\u8bb0\u5f55\u4e0b\u5b83\u7684\u6bcf\u4e2ablock\u7684size\u7684\uff0c\u800cpage\u663e\u7136\u4e0d\u9700\u8981 We will discuss some more differences between Paging and Segmentation with the help of comparison chart shown below. Content: Paging Vs Segmentation # Comparison Chart Definition Key Differences Conclusion Comparison Chart # BASIS FOR COMPARISON PAGING SEGMENTATION Basic A page is of fixed block size. A segment is of variable size. Fragmentation\uff08\u788e\u7247\uff09 Paging may lead to internal fragmentation . Segmentation may lead to external fragmentation . Address The user specified address is divided by CPU into a page number and offset . The user specifies each address by two quantities a segment number and the offset (Segment limit). Size The hardware decides the page size . The segment size is specified by the user. Table Paging involves a page table that contains base address of each page. Segmentation involves the segment table that contains segment number and offset (segment length). Definition of Paging # Paging is a memory management scheme . Paging allows a process to be stored in a memory in a non-contiguous manner. Storing process in a non-contiguous manner solves the problem of external fragmentation . For implementing paging the physical and logical memory spaces are divided into the same fixed-sized blocks. These fixed-sized blocks of physical memory are called frames , and the fixed-sized blocks of logical memory are called pages . SUMMARY :\u6ce8\u610f pages \u548c frames \u7684\u542b\u4e49\u662f\u4e0d\u540c\u7684\uff1b When a process needs to be executed the process pages from logical memory space are loaded into the frames of physical memory address space. Now the address generated by CPU for accessing the frame is divided into two parts i.e. page number and page offset . The page table uses page number as an index; each process has its separate page table that maps logical address to the physical address . The page table contains base address of the page stored in the frame of physical memory space. The base address defined by page table is combined with the page offset to define the frame number in physical memory where the page is stored. Definition of Segmentation # Like Paging, Segmentation is also a memory management scheme . It supports the user\u2019s view of the memory. The process is divided into the variable size segments and loaded to the logical memory address space . The logical address space is the collection of variable size segments. Each segment has its name and length . For the execution, the segments from logical memory space are loaded to the physical memory space . The address specified by the user contain two quantities the segment name and the Offset . The segments are numbered and referred by the segment number instead of segment name. This segment number is used as an index in the segment table , and offset value decides the length or limit of the segment. The segment number and the offset together combinely generates the address of the segment in the physical memory space. Key Differences Between Paging and Segmentation # The basic difference between paging and segmentation is that a page is always of fixed block size whereas, a segment is of variable size . Paging may lead to internal fragmentation as the page is of fixed block size, but it may happen that the process does not acquire the entire block size which will generate the internal fragment in memory. The segmentation may lead to external fragmentation as the memory is filled with the variable sized blocks. In paging the user only provides a single integer as the address which is divided by the hardware into a page number and Offset . On the other hands, in segmentation the user specifies the address in two quantities i.e. segment number and offset . The size of the page is decided or specified by the hardware . On the other hands, the size of the segment is specified by the user . In paging, the page table maps the logical address to the physical address , and it contains base address of each page stored in the frames of physical memory space. However, in segmentation, the segment table maps the logical address to the physical address , and it contains segment number and offset (segment limit). Conclusion: # Paging and segmentation both are the memory management schemes . Paging allows the memory to be divided into fixed sized block whereas the segmentation, divides the memory space into segments of the variable block size . Where the paging leads to internal fragmentation the segmentation leads to external fragmentation .","title":"Difference-Between-Paging-and-Segmentation-in-OS"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#difference-between-paging-and-segmentation-in-os","text":"The memory management in the operating system is an essential functionality, which allows the allocation of memory to the processes for execution and deallocates the memory when the process is no longer needed. In this article, we will discuss two memory management schemes paging and segmentation . The basic difference between paging and segmentation is that, \u201cpage\u201d is a fixed-sized block whereas, a \u201csegment\u201d is a variable-sized block. NOTE : \u5176\u5b9e\uff0c\u8fd9\u4e2a\u5dee\u522b\u5c31\u51b3\u5b9a\u4e86segment\u662f\u9700\u8981\u8bb0\u5f55\u4e0b\u5b83\u7684\u6bcf\u4e2ablock\u7684size\u7684\uff0c\u800cpage\u663e\u7136\u4e0d\u9700\u8981 We will discuss some more differences between Paging and Segmentation with the help of comparison chart shown below.","title":"Difference Between Paging and Segmentation in OS"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#content-paging-vs-segmentation","text":"Comparison Chart Definition Key Differences Conclusion","title":"Content: Paging Vs Segmentation"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#comparison-chart","text":"BASIS FOR COMPARISON PAGING SEGMENTATION Basic A page is of fixed block size. A segment is of variable size. Fragmentation\uff08\u788e\u7247\uff09 Paging may lead to internal fragmentation . Segmentation may lead to external fragmentation . Address The user specified address is divided by CPU into a page number and offset . The user specifies each address by two quantities a segment number and the offset (Segment limit). Size The hardware decides the page size . The segment size is specified by the user. Table Paging involves a page table that contains base address of each page. Segmentation involves the segment table that contains segment number and offset (segment length).","title":"Comparison Chart"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#definition-of-paging","text":"Paging is a memory management scheme . Paging allows a process to be stored in a memory in a non-contiguous manner. Storing process in a non-contiguous manner solves the problem of external fragmentation . For implementing paging the physical and logical memory spaces are divided into the same fixed-sized blocks. These fixed-sized blocks of physical memory are called frames , and the fixed-sized blocks of logical memory are called pages . SUMMARY :\u6ce8\u610f pages \u548c frames \u7684\u542b\u4e49\u662f\u4e0d\u540c\u7684\uff1b When a process needs to be executed the process pages from logical memory space are loaded into the frames of physical memory address space. Now the address generated by CPU for accessing the frame is divided into two parts i.e. page number and page offset . The page table uses page number as an index; each process has its separate page table that maps logical address to the physical address . The page table contains base address of the page stored in the frame of physical memory space. The base address defined by page table is combined with the page offset to define the frame number in physical memory where the page is stored.","title":"Definition of Paging"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#definition-of-segmentation","text":"Like Paging, Segmentation is also a memory management scheme . It supports the user\u2019s view of the memory. The process is divided into the variable size segments and loaded to the logical memory address space . The logical address space is the collection of variable size segments. Each segment has its name and length . For the execution, the segments from logical memory space are loaded to the physical memory space . The address specified by the user contain two quantities the segment name and the Offset . The segments are numbered and referred by the segment number instead of segment name. This segment number is used as an index in the segment table , and offset value decides the length or limit of the segment. The segment number and the offset together combinely generates the address of the segment in the physical memory space.","title":"Definition of Segmentation"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#key-differences-between-paging-and-segmentation","text":"The basic difference between paging and segmentation is that a page is always of fixed block size whereas, a segment is of variable size . Paging may lead to internal fragmentation as the page is of fixed block size, but it may happen that the process does not acquire the entire block size which will generate the internal fragment in memory. The segmentation may lead to external fragmentation as the memory is filled with the variable sized blocks. In paging the user only provides a single integer as the address which is divided by the hardware into a page number and Offset . On the other hands, in segmentation the user specifies the address in two quantities i.e. segment number and offset . The size of the page is decided or specified by the hardware . On the other hands, the size of the segment is specified by the user . In paging, the page table maps the logical address to the physical address , and it contains base address of each page stored in the frames of physical memory space. However, in segmentation, the segment table maps the logical address to the physical address , and it contains segment number and offset (segment limit).","title":"Key Differences Between Paging and Segmentation"},{"location":"Kernel/Guide/Memory-management/Difference-Between-Paging-and-Segmentation-in-OS/#conclusion","text":"Paging and segmentation both are the memory management schemes . Paging allows the memory to be divided into fixed sized block whereas the segmentation, divides the memory space into segments of the variable block size . Where the paging leads to internal fragmentation the segmentation leads to external fragmentation .","title":"Conclusion:"},{"location":"Kernel/Guide/Memory-management/Memory-management/","text":"Memory management # Memory management is a form of resource management applied to computer memory . The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. This is critical to any advanced computer system where more than a single process might be underway\uff08\u8fd0\u884c\u4e2d\uff09 at any time. Several methods have been devised that increase the effectiveness of memory management. Virtual memory systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the size of the virtual address space beyond the available amount of RAM using paging or swapping to secondary storage . The quality of the virtual memory manager can have an extensive effect on overall system performance. Details # Memory management within an address space is generally categorized as either automatic memory management, usually involving garbage collection , or manual memory management . NOTE: Dynamic memory allocation \u6240\u5bf9\u5e94\u7684\u662f manual memory management \u3002 Automatic variables \u6240\u5bf9\u5e94\u7684\u662fautomatic memory management\u3002 Dynamic memory allocation # See also: C dynamic memory allocation Implementations # Fixed-size blocks allocation # Main article: Memory pool Buddy blocks # Further information: Buddy memory allocation Slab allocation # Main article: Slab allocation Stack allocation # Main article: Stack-based memory allocation Automatic variables # Main article: Automatic variable In many programming language implementations, all variables declared within a procedure (subroutine, or function) are local to that function; the runtime environment for the program automatically allocates memory for these variables on program execution entry to the procedure, and automatically releases that memory when the procedure is exited. Special declarations may allow local variables to retain values between invocations of the procedure, or may allow local variables to be accessed by other procedures. The automatic allocation of local variables makes recursion possible, to a depth limited by available memory. Garbage collection # Main article: Garbage collection (computer science) Systems with virtual memory # Main articles: Memory protection and Shared memory (interprocess communication) Virtual memory","title":"Memory-management"},{"location":"Kernel/Guide/Memory-management/Memory-management/#memory-management","text":"Memory management is a form of resource management applied to computer memory . The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. This is critical to any advanced computer system where more than a single process might be underway\uff08\u8fd0\u884c\u4e2d\uff09 at any time. Several methods have been devised that increase the effectiveness of memory management. Virtual memory systems separate the memory addresses used by a process from actual physical addresses, allowing separation of processes and increasing the size of the virtual address space beyond the available amount of RAM using paging or swapping to secondary storage . The quality of the virtual memory manager can have an extensive effect on overall system performance.","title":"Memory management"},{"location":"Kernel/Guide/Memory-management/Memory-management/#details","text":"Memory management within an address space is generally categorized as either automatic memory management, usually involving garbage collection , or manual memory management . NOTE: Dynamic memory allocation \u6240\u5bf9\u5e94\u7684\u662f manual memory management \u3002 Automatic variables \u6240\u5bf9\u5e94\u7684\u662fautomatic memory management\u3002","title":"Details"},{"location":"Kernel/Guide/Memory-management/Memory-management/#dynamic-memory-allocation","text":"See also: C dynamic memory allocation","title":"Dynamic memory allocation"},{"location":"Kernel/Guide/Memory-management/Memory-management/#implementations","text":"","title":"Implementations"},{"location":"Kernel/Guide/Memory-management/Memory-management/#fixed-size-blocks-allocation","text":"Main article: Memory pool","title":"Fixed-size blocks allocation"},{"location":"Kernel/Guide/Memory-management/Memory-management/#buddy-blocks","text":"Further information: Buddy memory allocation","title":"Buddy blocks"},{"location":"Kernel/Guide/Memory-management/Memory-management/#slab-allocation","text":"Main article: Slab allocation","title":"Slab allocation"},{"location":"Kernel/Guide/Memory-management/Memory-management/#stack-allocation","text":"Main article: Stack-based memory allocation","title":"Stack allocation"},{"location":"Kernel/Guide/Memory-management/Memory-management/#automatic-variables","text":"Main article: Automatic variable In many programming language implementations, all variables declared within a procedure (subroutine, or function) are local to that function; the runtime environment for the program automatically allocates memory for these variables on program execution entry to the procedure, and automatically releases that memory when the procedure is exited. Special declarations may allow local variables to retain values between invocations of the procedure, or may allow local variables to be accessed by other procedures. The automatic allocation of local variables makes recursion possible, to a depth limited by available memory.","title":"Automatic variables"},{"location":"Kernel/Guide/Memory-management/Memory-management/#garbage-collection","text":"Main article: Garbage collection (computer science)","title":"Garbage collection"},{"location":"Kernel/Guide/Memory-management/Memory-management/#systems-with-virtual-memory","text":"Main articles: Memory protection and Shared memory (interprocess communication) Virtual memory","title":"Systems with virtual memory"},{"location":"Kernel/Guide/Memory-management/Memory-protection/","text":"Memory protection # Memory protection is a way to control memory access rights on a computer, and is a part of most modern instruction set architectures and operating systems . The main purpose of memory protection is to prevent a process from accessing memory that has not been allocated to it. This prevents a bug or malware (\u6076\u610f\u8f6f\u4ef6) within a process from affecting other processes, or the operating system itself. Protection may encompass(\u73af\u7ed5) all accesses to a specified area of memory, write accesses, or attempts to execute the contents of the area. An attempt to access unowned memory results in a hardware fault , called a segmentation fault or storage violation exception, generally causing abnormal termination of the offending process. Memory protection for computer security includes additional techniques such as address space layout randomization and executable space protection . Methods # Segmentation # Segmentation NOTE: \u8fd9\u79cd\u65b9\u5f0f\u73b0\u4ee3OS\u4ee5\u53ca\u5f88\u5c11\u91c7\u7528\u4e86\u3002 Paged virtual memory # Main article: Paged virtual memory NOTE: \u8fd9\u79cd\u65b9\u5f0f\u662f\u76ee\u524d\u91c7\u7528\u6700\u591a\u7684\u3002","title":"Memory-protection"},{"location":"Kernel/Guide/Memory-management/Memory-protection/#memory-protection","text":"Memory protection is a way to control memory access rights on a computer, and is a part of most modern instruction set architectures and operating systems . The main purpose of memory protection is to prevent a process from accessing memory that has not been allocated to it. This prevents a bug or malware (\u6076\u610f\u8f6f\u4ef6) within a process from affecting other processes, or the operating system itself. Protection may encompass(\u73af\u7ed5) all accesses to a specified area of memory, write accesses, or attempts to execute the contents of the area. An attempt to access unowned memory results in a hardware fault , called a segmentation fault or storage violation exception, generally causing abnormal termination of the offending process. Memory protection for computer security includes additional techniques such as address space layout randomization and executable space protection .","title":"Memory protection"},{"location":"Kernel/Guide/Memory-management/Memory-protection/#methods","text":"","title":"Methods"},{"location":"Kernel/Guide/Memory-management/Memory-protection/#segmentation","text":"Segmentation NOTE: \u8fd9\u79cd\u65b9\u5f0f\u73b0\u4ee3OS\u4ee5\u53ca\u5f88\u5c11\u91c7\u7528\u4e86\u3002","title":"Segmentation"},{"location":"Kernel/Guide/Memory-management/Memory-protection/#paged-virtual-memory","text":"Main article: Paged virtual memory NOTE: \u8fd9\u79cd\u65b9\u5f0f\u662f\u76ee\u524d\u91c7\u7528\u6700\u591a\u7684\u3002","title":"Paged virtual memory"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/","text":"Paging # In computer operating systems , paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory . In this scheme, the operating system retrieves data from secondary storage in same-size blocks called pages . Paging is an important part of virtual memory implementations in modern operating systems, using secondary storage to let programs exceed the size of available physical memory . For simplicity, main memory is called \"RAM\" (an acronym of \" random-access memory \") and secondary storage is called \"disk\" (a shorthand for \" hard disk drive \"), but the concepts do not depend on whether these terms apply literally to a specific computer system. Page faults # Main article: Page fault When a process tries to reference a page not currently present in RAM, the processor treats this invalid memory reference as a page fault and transfers control from the program to the operating system. The operating system must: Determine the location of the data on disk. Obtain an empty page frame in RAM to use as a container for the data. Load the requested data into the available page frame. Update the page table to refer to the new page frame. Return control to the program, transparently retrying the instruction that caused the page fault. When all page frames are in use, the operating system must select a page frame to reuse for the page the program now needs. If the evicted page frame was dynamically allocated by a program to hold data, or if a program modified it since it was read into RAM (in other words, if it has become \"dirty\"), it must be written out to disk before being freed. If a program later references the evicted page, another page fault occurs and the page must be read back into RAM. The method the operating system uses to select the page frame to reuse, which is its page replacement algorithm , is important to efficiency. The operating system predicts the page frame least likely to be needed soon, often through the least recently used (LRU) algorithm or an algorithm based on the program's working set . To further increase responsiveness, paging systems may predict which pages will be needed soon, preemptively loading them into RAM before a program references them. Page replacement techniques # Main articles: Page replacement algorithm and Demand paging Sharing # In multi-programming or in a multi-user environment, many users may execute the same program, written so that its code and data are in separate pages. To minimize RAM use, all users share a single copy of the program. Each process's page table is set up so that the pages that address code point to the single shared copy, while the pages that address data point to different physical pages for each process. Different programs might also use the same libraries. To save space, only one copy of the shared library is loaded into physical memory. Programs which use the same library have virtual addresses that map to the same pages (which contain the library's code and data). When programs want to modify the library's code, they use copy-on-write , so memory is only allocated when needed. Shared memory is an efficient way of communication between programs. Programs can share pages in memory, and then write and read to exchange data. Implementations # Unix and Unix-like systems # Unix systems, and other Unix-like operating systems, use the term \" swap \" to describe both the act of moving memory pages between RAM and disk, and the region of a disk the pages are stored on. In some of those systems, it is common to dedicate an entire partition of a hard disk to swapping. These partitions are called swap partitions \uff08\u4ea4\u6362\u533a\uff09. Many systems have an entire hard drive dedicated to swapping, separate from the data drive(s), containing only a swap partition. A hard drive dedicated to swapping is called a \"swap drive\" or a \"scratch drive\" or a \" scratch disk \". Some of those systems only support swapping to a swap partition; others also support swapping to files. NOTE: swap \u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u4ea4\u6362\u533a Linux # The Linux kernel supports a virtually unlimited number of swap backends (devices or files), and also supports assignment of backend priorities. When the kernel needs to swap pages out of physical memory, it uses the highest-priority backend with available free space. If multiple swap backends are assigned the same priority, they are used in a round-robin fashion (which is somewhat similar to RAID 0 storage layouts), providing improved performance as long as the underlying devices can be efficiently accessed in parallel.","title":"Paging"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#paging","text":"In computer operating systems , paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory . In this scheme, the operating system retrieves data from secondary storage in same-size blocks called pages . Paging is an important part of virtual memory implementations in modern operating systems, using secondary storage to let programs exceed the size of available physical memory . For simplicity, main memory is called \"RAM\" (an acronym of \" random-access memory \") and secondary storage is called \"disk\" (a shorthand for \" hard disk drive \"), but the concepts do not depend on whether these terms apply literally to a specific computer system.","title":"Paging"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#page-faults","text":"Main article: Page fault When a process tries to reference a page not currently present in RAM, the processor treats this invalid memory reference as a page fault and transfers control from the program to the operating system. The operating system must: Determine the location of the data on disk. Obtain an empty page frame in RAM to use as a container for the data. Load the requested data into the available page frame. Update the page table to refer to the new page frame. Return control to the program, transparently retrying the instruction that caused the page fault. When all page frames are in use, the operating system must select a page frame to reuse for the page the program now needs. If the evicted page frame was dynamically allocated by a program to hold data, or if a program modified it since it was read into RAM (in other words, if it has become \"dirty\"), it must be written out to disk before being freed. If a program later references the evicted page, another page fault occurs and the page must be read back into RAM. The method the operating system uses to select the page frame to reuse, which is its page replacement algorithm , is important to efficiency. The operating system predicts the page frame least likely to be needed soon, often through the least recently used (LRU) algorithm or an algorithm based on the program's working set . To further increase responsiveness, paging systems may predict which pages will be needed soon, preemptively loading them into RAM before a program references them.","title":"Page faults"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#page-replacement-techniques","text":"Main articles: Page replacement algorithm and Demand paging","title":"Page replacement techniques"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#sharing","text":"In multi-programming or in a multi-user environment, many users may execute the same program, written so that its code and data are in separate pages. To minimize RAM use, all users share a single copy of the program. Each process's page table is set up so that the pages that address code point to the single shared copy, while the pages that address data point to different physical pages for each process. Different programs might also use the same libraries. To save space, only one copy of the shared library is loaded into physical memory. Programs which use the same library have virtual addresses that map to the same pages (which contain the library's code and data). When programs want to modify the library's code, they use copy-on-write , so memory is only allocated when needed. Shared memory is an efficient way of communication between programs. Programs can share pages in memory, and then write and read to exchange data.","title":"Sharing"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#implementations","text":"","title":"Implementations"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#unix-and-unix-like-systems","text":"Unix systems, and other Unix-like operating systems, use the term \" swap \" to describe both the act of moving memory pages between RAM and disk, and the region of a disk the pages are stored on. In some of those systems, it is common to dedicate an entire partition of a hard disk to swapping. These partitions are called swap partitions \uff08\u4ea4\u6362\u533a\uff09. Many systems have an entire hard drive dedicated to swapping, separate from the data drive(s), containing only a swap partition. A hard drive dedicated to swapping is called a \"swap drive\" or a \"scratch drive\" or a \" scratch disk \". Some of those systems only support swapping to a swap partition; others also support swapping to files. NOTE: swap \u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u4ea4\u6362\u533a","title":"Unix and Unix-like systems"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Paging/#linux","text":"The Linux kernel supports a virtually unlimited number of swap backends (devices or files), and also supports assignment of backend priorities. When the kernel needs to swap pages out of physical memory, it uses the highest-priority backend with available free space. If multiple swap backends are assigned the same priority, they are used in a round-robin fashion (which is somewhat similar to RAID 0 storage layouts), providing improved performance as long as the underlying devices can be efficiently accessed in parallel.","title":"Linux"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Unix-system-page-size/","text":"how is page size determined in virtual address space? # Linux uses a virtual memory system where all of the addresses are virtual addresses and not physical addresses . These virtual addresses are converted into physical addresses by the processor. To make this translation easier, virtual and physical memory are divided into pages . Each of these pages is given a unique number; the page frame number. Some page sizes can be 2 KB, 4 KB, etc. But how is this page size number determined? Is it influenced by the size of the architecture? For example, a 32-bit bus will have 4 GB address space. A # You can find out a system's default page size by querying its configuration via the getconf command: $ getconf PAGE_SIZE 4096 or $ getconf PAGESIZE 4096 NOTE: The above units are typically in bytes, so the 4096 equates to 4096 bytes or 4kB. This is hardwired in the Linux kernel's source here: Example # $ more /usr/src/kernels/3.13.9-100.fc19.x86_64/include/asm-generic/page.h ... ... /* PAGE_SHIFT determines the page size */ #define PAGE_SHIFT 12 #ifdef __ASSEMBLY__ #define PAGE_SIZE (1 << PAGE_SHIFT) #else #define PAGE_SIZE (1UL << PAGE_SHIFT) #endif #define PAGE_MASK (~(PAGE_SIZE-1)) How does shifting give you 4096? # When you shift bits, you're performing a binary multiplication by 2. So in effect a shifting of bits to the left ( 1 << PAGE_SHIFT ) is doing the multiplication of 2^12 = 4096. $ echo \"2^12\" | bc 4096","title":"Unix-system-page-size"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Unix-system-page-size/#how-is-page-size-determined-in-virtual-address-space","text":"Linux uses a virtual memory system where all of the addresses are virtual addresses and not physical addresses . These virtual addresses are converted into physical addresses by the processor. To make this translation easier, virtual and physical memory are divided into pages . Each of these pages is given a unique number; the page frame number. Some page sizes can be 2 KB, 4 KB, etc. But how is this page size number determined? Is it influenced by the size of the architecture? For example, a 32-bit bus will have 4 GB address space.","title":"how is page size determined in virtual address space?"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Unix-system-page-size/#a","text":"You can find out a system's default page size by querying its configuration via the getconf command: $ getconf PAGE_SIZE 4096 or $ getconf PAGESIZE 4096 NOTE: The above units are typically in bytes, so the 4096 equates to 4096 bytes or 4kB. This is hardwired in the Linux kernel's source here:","title":"A"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Unix-system-page-size/#example","text":"$ more /usr/src/kernels/3.13.9-100.fc19.x86_64/include/asm-generic/page.h ... ... /* PAGE_SHIFT determines the page size */ #define PAGE_SHIFT 12 #ifdef __ASSEMBLY__ #define PAGE_SIZE (1 << PAGE_SHIFT) #else #define PAGE_SIZE (1UL << PAGE_SHIFT) #endif #define PAGE_MASK (~(PAGE_SIZE-1))","title":"Example"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Unix-system-page-size/#how-does-shifting-give-you-4096","text":"When you shift bits, you're performing a binary multiplication by 2. So in effect a shifting of bits to the left ( 1 << PAGE_SHIFT ) is doing the multiplication of 2^12 = 4096. $ echo \"2^12\" | bc 4096","title":"How does shifting give you 4096?"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/","text":"Virtual memory # NOTE: Virtual memory \u6280\u672f\u7684\u76f8\u5173\u6982\u5ff5\u90fd\u4f1a\u5728\u540d\u79f0\u524d\u9762\u52a0\u4e0a\u201cvirtual\"\u4fee\u9970\u8bcd\uff0c\u6bd4\u5982\uff1a virtual addresses virtual address spaces In computing , virtual memory (also virtual storage ) is a memory management technique that provides an \"idealized abstraction of the storage resources that are actually available on a given machine\" which \"creates the illusion to users of a very large (main) memory.\" The computer's operating system , using a combination of hardware and software, maps memory addresses used by a program, called virtual addresses , into physical addresses in computer memory . NOTE: \u7531OS\u6765\u6267\u884c\u4e0a\u8ff0\u6620\u5c04\u3002 \u53c2\u89c1 \u8fd9\u7bc7\u6587\u7ae0 \uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u4e00\u53e5\uff1a Virtual address When we say virtual address, that refers to a location in a process's address space. In other words, a pointer ! \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u4e00\u76f4\u4f7f\u7528\u7684pointer\uff0c\u5b9e\u9645\u662fvirtual address\uff1b Main storage , as seen by a process or task, appears as a contiguous address space or collection of contiguous segments . The operating system manages virtual address spaces and the assignment of real memory to virtual memory . Address translation hardware in the CPU, often referred to as a memory management unit or MMU , automatically translates virtual addresses to physical addresses . Software within the operating system may extend these capabilities to provide a virtual address space that can exceed the capacity of real memory and thus reference more memory than is physically present in the computer. NOTE: \u53c2\u89c1\uff1a how is page size determined in virtual address space? The primary benefits of virtual memory include freeing applications from having to manage a shared memory space, increased security due to memory isolation , and being able to conceptually use more memory than might be physically available, using the technique of paging . NOTE: memory isolation\u6307\u7684\u662f\u6bcf\u4e2aprocess\u6709\u4e00\u4e2a\u72ec\u7acb\u7684 virtual address space \u3002 Virtual memory combines active RAM and inactive memory on DASD \uff08Direct-access storage device\uff09 to form a large range of contiguous addresses. Properties # Virtual memory makes application programming easier by hiding fragmentation (\u788e\u7247\uff09 of physical memory; by delegating to the kernel the burden\uff08\u8d23\u4efb\uff09 of managing the memory hierarchy (eliminating the need for the program to handle overlays explicitly); and, when each process is run in its own dedicated address space, by obviating\uff08\u6d88\u9664\uff09 the need to relocate program code or to access memory with relative addressing . Memory virtualization can be considered a generalization of the concept of virtual memory. Paged virtual memory # Nearly all current implementations of virtual memory divide a virtual address space into pages , blocks of contiguous virtual memory addresses. Pages on contemporary systems are usually at least 4 kilobytes in size; systems with large virtual address ranges or amounts of real memory generally use larger page sizes. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u6307\u5c06 virtual address space \u6309\u7167 pages \u8fdb\u884c\u5212\u5206\uff1b Page tables # Page tables are used to translate the virtual addresses seen by the application into physical addresses used by the hardware to process instructions; such hardware that handles this specific translation is often known as the memory management unit . Each entry in the page table holds a flag indicating whether the corresponding page is in real memory or not. If it is in real memory, the page table entry will contain the real memory address at which the page is stored. When a reference is made to a page by the hardware, if the page table entry for the page indicates that it is not currently in real memory, the hardware raises a page fault exception , invoking the paging supervisor component of the operating system . NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6709\u4e00\u53e5\u975e\u5e38\u91cd\u8981\u7684\uff1a virtual addresses seen by application while physical addresses use by hardware\uff1b\u5373process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u6240\u4f7f\u7528\u7684\u662fvirtual address\u3002 Systems can have one page table for the whole system separate page tables for each application and segment a tree of page tables for large segments or some combination of these. If there is only one page table, different applications running at the same time use different parts of a single range of virtual addresses. If there are multiple page or segment tables, there are multiple virtual address spaces and concurrent applications with separate page tables redirect to different real addresses. Some earlier systems with smaller real memory sizes, such as the SDS 940 , used page registers instead of page tables in memory for address translation. Paging supervisor # This part of the operating system creates and manages page tables . If the hardware raises a page fault exception , the paging supervisor accesses secondary storage, returns the page that has the virtual address that resulted in the page fault, updates the page tables to reflect the physical location of the virtual address and tells the translation mechanism to restart the request. When all physical memory is already in use, the paging supervisor must free a page in primary storage to hold the swapped-in page. The supervisor uses one of a variety of page replacement algorithms such as least recently used to determine which page to free. Pinned pages # Operating systems have memory areas that are pinned (never swapped to secondary storage). Other terms used are locked , fixed , or wired pages. For example, interrupt mechanisms rely on an array of pointers to their handlers, such as I/O completion and page fault . If the pages containing these pointers or the code that they invoke were pageable, interrupt-handling would become far more complex and time-consuming, particularly in the case of page fault interruptions. Hence, some part of the page table structures is not pageable. Segmented virtual memory # NOTE: \u8fd9\u79cd\u5e76\u975e\u4e3b\u6d41\u7684\uff0cpass\u6389\u3002 See also # Page table Page (computer memory) Paging Virtual Addresses","title":"Virtual-memory"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#virtual-memory","text":"NOTE: Virtual memory \u6280\u672f\u7684\u76f8\u5173\u6982\u5ff5\u90fd\u4f1a\u5728\u540d\u79f0\u524d\u9762\u52a0\u4e0a\u201cvirtual\"\u4fee\u9970\u8bcd\uff0c\u6bd4\u5982\uff1a virtual addresses virtual address spaces In computing , virtual memory (also virtual storage ) is a memory management technique that provides an \"idealized abstraction of the storage resources that are actually available on a given machine\" which \"creates the illusion to users of a very large (main) memory.\" The computer's operating system , using a combination of hardware and software, maps memory addresses used by a program, called virtual addresses , into physical addresses in computer memory . NOTE: \u7531OS\u6765\u6267\u884c\u4e0a\u8ff0\u6620\u5c04\u3002 \u53c2\u89c1 \u8fd9\u7bc7\u6587\u7ae0 \uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u4e00\u53e5\uff1a Virtual address When we say virtual address, that refers to a location in a process's address space. In other words, a pointer ! \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u4e00\u76f4\u4f7f\u7528\u7684pointer\uff0c\u5b9e\u9645\u662fvirtual address\uff1b Main storage , as seen by a process or task, appears as a contiguous address space or collection of contiguous segments . The operating system manages virtual address spaces and the assignment of real memory to virtual memory . Address translation hardware in the CPU, often referred to as a memory management unit or MMU , automatically translates virtual addresses to physical addresses . Software within the operating system may extend these capabilities to provide a virtual address space that can exceed the capacity of real memory and thus reference more memory than is physically present in the computer. NOTE: \u53c2\u89c1\uff1a how is page size determined in virtual address space? The primary benefits of virtual memory include freeing applications from having to manage a shared memory space, increased security due to memory isolation , and being able to conceptually use more memory than might be physically available, using the technique of paging . NOTE: memory isolation\u6307\u7684\u662f\u6bcf\u4e2aprocess\u6709\u4e00\u4e2a\u72ec\u7acb\u7684 virtual address space \u3002 Virtual memory combines active RAM and inactive memory on DASD \uff08Direct-access storage device\uff09 to form a large range of contiguous addresses.","title":"Virtual memory"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#properties","text":"Virtual memory makes application programming easier by hiding fragmentation (\u788e\u7247\uff09 of physical memory; by delegating to the kernel the burden\uff08\u8d23\u4efb\uff09 of managing the memory hierarchy (eliminating the need for the program to handle overlays explicitly); and, when each process is run in its own dedicated address space, by obviating\uff08\u6d88\u9664\uff09 the need to relocate program code or to access memory with relative addressing . Memory virtualization can be considered a generalization of the concept of virtual memory.","title":"Properties"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#paged-virtual-memory","text":"Nearly all current implementations of virtual memory divide a virtual address space into pages , blocks of contiguous virtual memory addresses. Pages on contemporary systems are usually at least 4 kilobytes in size; systems with large virtual address ranges or amounts of real memory generally use larger page sizes. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u6307\u5c06 virtual address space \u6309\u7167 pages \u8fdb\u884c\u5212\u5206\uff1b","title":"Paged virtual memory"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#page-tables","text":"Page tables are used to translate the virtual addresses seen by the application into physical addresses used by the hardware to process instructions; such hardware that handles this specific translation is often known as the memory management unit . Each entry in the page table holds a flag indicating whether the corresponding page is in real memory or not. If it is in real memory, the page table entry will contain the real memory address at which the page is stored. When a reference is made to a page by the hardware, if the page table entry for the page indicates that it is not currently in real memory, the hardware raises a page fault exception , invoking the paging supervisor component of the operating system . NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6709\u4e00\u53e5\u975e\u5e38\u91cd\u8981\u7684\uff1a virtual addresses seen by application while physical addresses use by hardware\uff1b\u5373process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u6240\u4f7f\u7528\u7684\u662fvirtual address\u3002 Systems can have one page table for the whole system separate page tables for each application and segment a tree of page tables for large segments or some combination of these. If there is only one page table, different applications running at the same time use different parts of a single range of virtual addresses. If there are multiple page or segment tables, there are multiple virtual address spaces and concurrent applications with separate page tables redirect to different real addresses. Some earlier systems with smaller real memory sizes, such as the SDS 940 , used page registers instead of page tables in memory for address translation.","title":"Page tables"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#paging-supervisor","text":"This part of the operating system creates and manages page tables . If the hardware raises a page fault exception , the paging supervisor accesses secondary storage, returns the page that has the virtual address that resulted in the page fault, updates the page tables to reflect the physical location of the virtual address and tells the translation mechanism to restart the request. When all physical memory is already in use, the paging supervisor must free a page in primary storage to hold the swapped-in page. The supervisor uses one of a variety of page replacement algorithms such as least recently used to determine which page to free.","title":"Paging supervisor"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#pinned-pages","text":"Operating systems have memory areas that are pinned (never swapped to secondary storage). Other terms used are locked , fixed , or wired pages. For example, interrupt mechanisms rely on an array of pointers to their handlers, such as I/O completion and page fault . If the pages containing these pointers or the code that they invoke were pageable, interrupt-handling would become far more complex and time-consuming, particularly in the case of page fault interruptions. Hence, some part of the page table structures is not pageable.","title":"Pinned pages"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#segmented-virtual-memory","text":"NOTE: \u8fd9\u79cd\u5e76\u975e\u4e3b\u6d41\u7684\uff0cpass\u6389\u3002","title":"Segmented virtual memory"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-memory/#see-also","text":"Page table Page (computer memory) Paging Virtual Addresses","title":"See also"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space/","text":"Virtual address space # In computing , a virtual address space ( VAS ) or address space is the set of ranges of virtual addresses that an operating system makes available to a process. The range of virtual addresses usually starts at a low address and can extend to the highest address allowed by the computer's instruction set architecture and supported by the operating system 's pointer size implementation, which can be 4 bytes for 32-bit or 8 bytes for 64-bit OS versions. This provides several benefits, one of which is security through process isolation assuming each process is given a separate address space . NOTE: process isolation\u662f\u975e\u5e38\u6709\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u5f53OS\u4e2d\u8fd0\u884c\u591a\u4e2aprocess\u7684\u65f6\u5019\uff0cOS\u5c31\u9700\u8981\u8fdb\u884c\u8c03\u5ea6\uff0c\u56e0\u6b64\u5c31\u6709\u53ef\u80fd\u6682\u505c\u67d0\u4e2aprocess\u7684\u6267\u884c\u800c\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2aprocess\uff1b\u53ef\u80fd\u8fc7\u6765\u4e00\u4e9b\u65f6\u95f4\u540e\uff0c\u518dresume\u4e4b\u524d\u6682\u505c\u7684process\uff1b\u53ef\u4ee5\u770b\u5230\uff0c\u4e3a\u4e86\u8fbe\u5230\u4f7fprocess\u53ef\u4e2d\u65ad\uff0c\u4e0d\u540cprocess\u4e4b\u95f4\u7684isolation\u975e\u5e38\u91cd\u8981\uff0c\u88ab\u4e2d\u65ad\u7684process\u7684address space\u5e94\u5f53\u8981\u514d\u6536\u5176\u4ed6\u7684\u6b63\u5728running\u7684process\u7684\u5f71\u54cd\uff1b Example # When a new application on a 32-bit OS is executed, the process has a 4 GiB VAS: each one of the memory addresses (from 0 to $2^{32} \u2212 1$) in that space can have a single byte as a value. Initially, none of them have values ('-' represents no value). Using or setting values in such a VAS would cause a memory exception . SUMMARY : \u4e0a\u8ff032-bit\u6307\u7684\u662fOS\u7684Word size 0 4 GiB VAS |----------------------------------------------| Then the application's executable file is mapped into the VAS. Addresses in the process VAS are mapped to bytes in the exe file. The OS manages the mapping: 0 4 GiB VAS |---vvvvvvv------------------------------------| mapping |-----| file bytes app.exe The v's are values from bytes in the mapped file . Then, required DLL files are mapped (this includes custom libraries as well as system ones such as kernel32.dll and user32.dll ): 0 4 GiB VAS |---vvvvvvv----vvvvvv---vvvv-------------------| mapping ||||||| |||||| |||| file bytes app.exe kernel user The process then starts executing bytes in the exe file. However, the only way the process can use or set '-' values in its VAS is to ask the OS to map them to bytes from a file . A common way to use VAS memory in this way is to map it to the page file . The page file is a single file, but multiple distinct sets of contiguous bytes can be mapped into a VAS: 0 4 GiB VAS |---vvvvvvv----vvvvvv---vvvv----vv---v----vvv--| mapping ||||||| |||||| |||| || | ||| file bytes app.exe kernel user system_page_file And different parts of the page file can map into the VAS of different processes: 0 4 GiB VAS 1 |---vvvv-------vvvvvv---vvvv----vv---v----vvv--| mapping |||| |||||| |||| || | ||| file bytes app1 app2 kernel user system_page_file mapping |||| |||||| |||| || | VAS 2 |--------vvvv--vvvvvv---vvvv-------vv---v------| On Microsoft Windows 32-bit, by default, only 2 GiB are made available to processes for their own use.[ 2] The other 2 GiB are used by the operating system. On later 32-bit editions of Microsoft Windows it is possible to extend the user-mode virtual address space to 3 GiB while only 1 GiB is left for kernel-mode virtual address space by marking the programs as IMAGE_FILE_LARGE_ADDRESS_AWARE and enabling the /3GB switch in the boot.ini file. On Microsoft Windows 64-bit, in a process running an executable that was linked with /LARGEADDRESSAWARE:NO, the operating system artificially limits the user mode portion of the process's virtual address space to 2 GiB. This applies to both 32- and 64-bit executables.[ 5] [ 6] Processes running executables that were linked with the /LARGEADDRESSAWARE:YES option, which is the default for 64-bit Visual Studio 2010 and later, have access to more than 2 GiB of virtual address space: Up to 4 GiB for 32-bit executables, up to 8 TiB for 64-bit executables in Windows through Windows 8, and up to 128 TiB for 64-bit executables in Windows 8.1 and later. Allocating memory via C 's malloc establishes the page file as the backing store for any new virtual address space. However, a process can also explicitly map file bytes. Linux # For x86 CPUs, Linux 32-bit allows splitting the user and kernel address ranges in different ways: 3G/1G user/kernel (default), 1G/3G user/kernel or 2G/2G user/kernel .","title":"Virtual-address-space"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space/#virtual-address-space","text":"In computing , a virtual address space ( VAS ) or address space is the set of ranges of virtual addresses that an operating system makes available to a process. The range of virtual addresses usually starts at a low address and can extend to the highest address allowed by the computer's instruction set architecture and supported by the operating system 's pointer size implementation, which can be 4 bytes for 32-bit or 8 bytes for 64-bit OS versions. This provides several benefits, one of which is security through process isolation assuming each process is given a separate address space . NOTE: process isolation\u662f\u975e\u5e38\u6709\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u5f53OS\u4e2d\u8fd0\u884c\u591a\u4e2aprocess\u7684\u65f6\u5019\uff0cOS\u5c31\u9700\u8981\u8fdb\u884c\u8c03\u5ea6\uff0c\u56e0\u6b64\u5c31\u6709\u53ef\u80fd\u6682\u505c\u67d0\u4e2aprocess\u7684\u6267\u884c\u800c\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2aprocess\uff1b\u53ef\u80fd\u8fc7\u6765\u4e00\u4e9b\u65f6\u95f4\u540e\uff0c\u518dresume\u4e4b\u524d\u6682\u505c\u7684process\uff1b\u53ef\u4ee5\u770b\u5230\uff0c\u4e3a\u4e86\u8fbe\u5230\u4f7fprocess\u53ef\u4e2d\u65ad\uff0c\u4e0d\u540cprocess\u4e4b\u95f4\u7684isolation\u975e\u5e38\u91cd\u8981\uff0c\u88ab\u4e2d\u65ad\u7684process\u7684address space\u5e94\u5f53\u8981\u514d\u6536\u5176\u4ed6\u7684\u6b63\u5728running\u7684process\u7684\u5f71\u54cd\uff1b","title":"Virtual address space"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space/#example","text":"When a new application on a 32-bit OS is executed, the process has a 4 GiB VAS: each one of the memory addresses (from 0 to $2^{32} \u2212 1$) in that space can have a single byte as a value. Initially, none of them have values ('-' represents no value). Using or setting values in such a VAS would cause a memory exception . SUMMARY : \u4e0a\u8ff032-bit\u6307\u7684\u662fOS\u7684Word size 0 4 GiB VAS |----------------------------------------------| Then the application's executable file is mapped into the VAS. Addresses in the process VAS are mapped to bytes in the exe file. The OS manages the mapping: 0 4 GiB VAS |---vvvvvvv------------------------------------| mapping |-----| file bytes app.exe The v's are values from bytes in the mapped file . Then, required DLL files are mapped (this includes custom libraries as well as system ones such as kernel32.dll and user32.dll ): 0 4 GiB VAS |---vvvvvvv----vvvvvv---vvvv-------------------| mapping ||||||| |||||| |||| file bytes app.exe kernel user The process then starts executing bytes in the exe file. However, the only way the process can use or set '-' values in its VAS is to ask the OS to map them to bytes from a file . A common way to use VAS memory in this way is to map it to the page file . The page file is a single file, but multiple distinct sets of contiguous bytes can be mapped into a VAS: 0 4 GiB VAS |---vvvvvvv----vvvvvv---vvvv----vv---v----vvv--| mapping ||||||| |||||| |||| || | ||| file bytes app.exe kernel user system_page_file And different parts of the page file can map into the VAS of different processes: 0 4 GiB VAS 1 |---vvvv-------vvvvvv---vvvv----vv---v----vvv--| mapping |||| |||||| |||| || | ||| file bytes app1 app2 kernel user system_page_file mapping |||| |||||| |||| || | VAS 2 |--------vvvv--vvvvvv---vvvv-------vv---v------| On Microsoft Windows 32-bit, by default, only 2 GiB are made available to processes for their own use.[ 2] The other 2 GiB are used by the operating system. On later 32-bit editions of Microsoft Windows it is possible to extend the user-mode virtual address space to 3 GiB while only 1 GiB is left for kernel-mode virtual address space by marking the programs as IMAGE_FILE_LARGE_ADDRESS_AWARE and enabling the /3GB switch in the boot.ini file. On Microsoft Windows 64-bit, in a process running an executable that was linked with /LARGEADDRESSAWARE:NO, the operating system artificially limits the user mode portion of the process's virtual address space to 2 GiB. This applies to both 32- and 64-bit executables.[ 5] [ 6] Processes running executables that were linked with the /LARGEADDRESSAWARE:YES option, which is the default for 64-bit Visual Studio 2010 and later, have access to more than 2 GiB of virtual address space: Up to 4 GiB for 32-bit executables, up to 8 TiB for 64-bit executables in Windows through Windows 8, and up to 128 TiB for 64-bit executables in Windows 8.1 and later. Allocating memory via C 's malloc establishes the page file as the backing store for any new virtual address space. However, a process can also explicitly map file bytes.","title":"Example"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space/#linux","text":"For x86 CPUs, Linux 32-bit allows splitting the user and kernel address ranges in different ways: 3G/1G user/kernel (default), 1G/3G user/kernel or 2G/2G user/kernel .","title":"Linux"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/","text":"\u5173\u4e8eVirtual address space\u7684\u601d\u8003 # \u4e00\u4e2aprocess\u7684virtual address space\u7684\u4e0a\u9650\uff1f # \u6211\u4e4b\u524d\u51fa\u73b0out-of-memory\u7684\u60c5\u51b5\u662f\uff1aprocess\u975e\u5e38\u9ad8\u9891\u5730\u6267\u884c\u4e00\u4e2a\u5b58\u5728memory leakage\u7684function\uff0c\u6700\u7ec8\u5bfc\u81f4out-of-memory\uff08\u8fd9\u4e2afunction\u4f7f\u7528\u7684\u662f malloc \u5f0f\u7684\uff0c\u6240\u4ee5\u5b83\u6240\u83b7\u5f97\u7684memory\u662f\u4f4d\u4e8eheap\u7684\uff0c\u90a3\u4f4d\u4e8eheap\u4e0a\u7684memory\u6ca1\u6709\u4e0a\u9650\u5417\uff1f\u5b83\u5982\u4f55\u80fd\u591f\u5bfc\u81f4out-of-memory\uff1f\uff09\u3002\u4e00\u4e2aprocess\u5982\u4f55\u80fd\u591f\u5c06\u4e00\u53f0\u673a\u5668\u7684memory\u8017\u5c3d\uff1f\u8fd9\u4e2a\u95ee\u9898\u8f6c\u6362\u4e00\u4e0b\u5c31\u662f\u96be\u9053\u4e00\u4e2aprocess\u7684address space\u6ca1\u6709\u4e0a\u9650\u5417? \u4e00\u4e2aprocess\u7684address space\u662f\u6709\u4e0a\u9650\u7684\uff0c\u8fd9\u5c31\u662fvirtual address\u7684\u957f\u5ea6\u51b3\u5b9a\u7684\uff0c\u6bd4\u598232bit\uff0c\u5219\u5b83\u7684\u4e0a\u9650\u5c31\u662f4G\uff08\u4ec5\u4ec5\u53ea\u67094G\uff0c\u8fd9\u53ef\u80fd\u5417\uff1f\uff09\u76f4\u89c2\u611f\u53d7virtual address\u7684\u957f\u5ea6\u5c31\u662fC\u4e2d\u7684\u6307\u9488\u7c7b\u578b\u7684\u957f\u5ea6\uff0c\u6b63\u5982\u5728 Virtual address space \u4e2d\u6240\u63cf\u8ff0\u7684\u3002 \u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1aCPU\u7684word size\u548cvirtual address length\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4e24\u8005\u80af\u5b9a\u4e0d\u662f\u4e00\u56de\u4e8b\u513f\u3002 \u6bcf\u4e2aprocess\u6709\u4e00\u4e2avirtual address space\uff0c\u4e0e\u8fd9\u4e2avirtual address space\u76f8\u5339\u914d\u7684\u662f\u4e00\u4e2apage table\uff0c\u8fd9\u4e2apage table\u8bb0\u5f55\u7740\u5f52\u5c5e\u4e8e\u5b83\u7684\u6240\u6709\u7684memory\uff0c\u90a3\u662f\u5426\u662f\u5728\u9884\u5148\u5c31\u5c06\u8fd9\u4e2apage table\u7684size\u7ed9\u56fa\u5b9a\u4e86\u5462\uff1f\u8fd8\u662f\u540e\u7eed\u5728process\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u4e2apage table\u7684\u5927\u5c0f\u4f1a\u6539\u53d8\uff1f \u4e0eout-of-memory\u76f8\u5173\u7684\u53e6\u5916\u4e00\u4e2a\u95ee\u9898\u662f\uff1astackoverflow\u3002 virtual address\u7684\u610f\u4e49 # lazy\uff0c\u5373\u65e0\u9700\u4e00\u6b21\u6027\u5206\u914d\u6574\u4e2avirtual address space\uff0c\u800c\u662f\u7528\u65f6\u518d\u5206\u914d\u3002 copy on write \u5176\u5b9e\u901a\u8fc7\u8fd9\u4e2a\u601d\u8003\u624d\u53d1\u73b0virtual memory\u7684\u91cd\u8981\u4ef7\u503c\u6240\u5728\uff0c\u5b83\u662f\u5b9e\u73b0demand page\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0\u6269\u5145memory\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0copy on write\u7684\u57fa\u7840\u3002 linux process virtual address space\u548cpage table\u4e4b\u95f4\u7684\u5173\u7cfb # \u5f53\u7136\u662f\u6709\u5dee\u522b\u7684\uff0cprocess virtual address space\u5219\u662f\u8868\u793a\u8fdb\u7a0b\u7684\u5730\u5740\u7a7a\u95f4\uff0c\u5176\u4e2d\u6240\u4f7f\u7528\u7684\u662fvirtual address\uff0cpage table\u662f\u8bb0\u5f55\u7740virtual address\u548cphysical address\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002 \u663e\u7136page table\u5bf9user\u662f\u900f\u660e\u7684\uff0c\u4f46\u662f\u6211\u4eec\u5374\u53ef\u4ee5\u8bbf\u95ee\u5230virtual address\uff0c\u901a\u8fc7pointer\u3002 \u63cf\u8ff0process virtual address space\u7684\u6570\u636e\u7ed3\u6784 # \u57289.2. The Memory Descriptor\u4e2d\u6307\u51fa\uff1a mm_struct \u63cf\u8ff0page table\u7684data structure # \u6bcf\u4e2avirtual address space\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684page table\uff0c\u90a3\u8fd9page table\u7684data structure\u662f\u5982\u4f55\u7684\uff1f \u5176\u5b9e\u8fd9\u5728Chapter 2. Memory Addressing\u4e2d\u5df2\u7ecf\u63cf\u8ff0\u4e86\u3002 \u4ecememory usage\u7684\u89d2\u5ea6\u6765\u5206\u6790process\u7684\u8fd0\u884c # \u8fdb\u7a0b\u7684\u8fd0\u884c\u4f34\u968f\u7740\u5185\u5b58\u7684\u4f7f\u7528\uff0c\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u8fd8\u6ca1\u6709\u4e00\u4e2a\u5b8c\u6574\u3001\u6e05\u6670\u7684\u8ba4\u77e5\u3002\u5982\u4f55\u65f6\u5206\u914dmemory\uff1f\u4f55\u65f6\u56de\u6536memory\uff1f \u5206\u914dmemory\u7684\u573a\u666f\uff1a \u51fd\u6570\u8c03\u7528 stored program \u5728context switch\u7684\u65f6\u5019\uff0cOS\u9700\u8981\u5c06process\u7684context\u4fdd\u5b58\u8d77\u6765 \u5176\u5b9e\u6211\u77e5\u9053\uff0c\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u7684\u66f4\u597d\u7684\u65b9\u5f0f\u662f\u9605\u8bfb\u8ba1\u7b97\u673a\u6267\u884c\u6307\u4ee4\u7684\u6d41\u7a0b\u3002 Address space # \u8fd9\u4e2a\u95ee\u9898\u662f\u7531\u524d\u9762\u7684\u5173\u4e8eprocess\u7684resource\u7684\u601d\u8003\u884d\u751f\u51fa\u6765\u7684\u3002Address space\u662f\u4e00\u4e2aprocess\u975e\u5e38\u91cd\u8981\u7684resource\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u662fprocess\u8fdb\u884c\u6d3b\u52a8\u7684\u7a7a\u95f4\u3002\u76ee\u524d\u7684OS\u90fd\u662f\u91c7\u7528\u7684virtual address\uff0c\u5373process\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u6240\u4f7f\u7528\u7684\u662fvirtual memory\uff0c\u6240\u4ee5\u4e5f\u53ef\u4ee5\u5c06Address space\u79f0\u4e3aVirtual address space\u3002\u5173\u4e8eprocess\u7684Virtual address space\uff0c\u6211\u6709\u5982\u4e0b\u7591\u95ee\uff1a Question: process\u4f7f\u7528virtual memory\uff0c\u5e76\u4e14\u4f7f\u7528\u57fa\u4e8epage\u7684memory management\uff0c\u90a3\u5b83\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u57fa\u4e8epage\u7684virtual memory\u5462\uff1f\u662f\u5206\u5272\u4e3a\u4e00\u4e2a\u4e00\u4e2a\u7684page\uff1f \u7ecf\u8fc7\u7b80\u5355\u7684\u601d\u8003\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u7f16\u8bd1\u5668\u5728\u7ed9\u751f\u6210\u4ee3\u7801\u7684\u65f6\u5019\u5176\u5b9e\u662f\u4e0d\u9700\u8981\u8003\u8651\u8fd9\u4e2a\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u662fOS\u5728\u8fd0\u884cprogram\u7684\u65f6\u5019\u6309\u7167page\u8fdb\u884cmemory management\uff0c\u65e0\u8bba\u7f16\u8bd1\u5668\u751f\u6210\u7684program\u662f\u600e\u6837\u7684\uff0c\u662fOS\u8d1f\u8d23\u5c06\u8fd9\u4e9bprogram\u88c5\u5165\u5230memory\u4e2d\uff0c\u8fd9\u4e00\u5207\u5bf9compiler\u800c\u8a00\u90fd\u662f\u900f\u660e\u7684\u3002 \u4f46\u662f\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u5ef6\u4f38\u4e00\u4e0b\uff1a\u6211\u4eec\u77e5\u9053\uff0c\u7f16\u8bd1\u5668\u751f\u6210\u7684\u4ee3\u7801\u80af\u5b9a\u662f\u9700\u8981\u9075\u5faaalignment\u7684\uff0c\u90a3\u8fd9\u5c31\u6d89\u53ca\u5230alignment\u548cpage size\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u5e94\u8be5\u53ea\u8981\u7b26\u5408alignment\uff0c\u90a3\u4e48\u5e94\u8be5\u5c31\u4e0d\u4f1a\u5b58\u5728\u4e00\u4e2a\u6570\u636e\u5b58\u50a8\u8de8\u8d8a\u4e86\u591a\u4e2apage\u7684\u60c5\u51b5\u4e86\u3002 Question: \u8fdb\u7a0b\u7684virtual address space\u90fd\u662f\u76f8\u540c\u7684\uff0c\u90a3virtual address\u662f\u5982\u4f55\u6620\u5c04\u5230physical memory address\u7684\u5462\uff1f \u65e2\u7136\u4f7f\u7528\u7684\u662fdemand page\uff0c\u4e5f\u5c31\u662f\u5728process\u8fd0\u884c\u7684\u65f6\u5019\u9700\u8981\u8bbf\u95ee\u8be5virtual memory\u7684\u65f6\u5019\uff0c\u624dallocate physical memory\u6216\u8005swap-in\uff0c\u624d\u5c06virtual address\u6620\u5c04\u5230physical memory\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4fdd\u5b58\u5230\u8be5process\u7684page table\u4e2d\u3002 \u5176\u5b9e\u901a\u8fc7\u8fd9\u4e2a\u601d\u8003\u624d\u53d1\u73b0virtual memory\u7684\u91cd\u8981\u4ef7\u503c\u6240\u5728\uff0c\u5b83\u662f\u5b9e\u73b0demand page\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0\u6269\u5145memory\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0copy on write\u7684\u57fa\u7840\u3002 Question: \u59821.6.8.4. Process virtual address space handling\u8282\u6240\u53d9\u8ff0\u7684 The kernel usually stores a process virtual address space as a list of memory area descriptors . \u5373\u6211\u4eec\u901a\u5e38\u5c06virtual address space\u5206\u5272\u4e3a\u591a\u5757\uff0c\u90a3\u662f\u5728\u4ec0\u4e48\u5730\u65b9\u5c06virtual address space\u5206\u5272\u4e3a\u5982\u4e0a\u6240\u8ff0\u7684a list of memory area descriptors \uff1f operating system\u91c7\u7528\u7684\u662fdemand paging\uff0c\u5e76\u4e14stack\u7684\u589e\u957f\u65b9\u5411\u548cheap\u7684\u589e\u957f\u65b9\u5411\u76f8\u53cd\uff0c\u90a3\u8fd9\u4e9b\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u8981\u60f3\u5b8c\u5168\u7406\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9605\u8bfbcalling convention\u3002\u6211\u89c9\u5f97process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5bf9call stack\u7684\u7ef4\u62a4\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6d3b\u52a8\uff0c\u6bcf\u6b21new\u4e00\u4e2a\u6808\u5e27\u90fd\u9700\u8981\u5206\u914d\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\u91cd\u8981\u624d\u80fd\u591f\u4fdd\u8bc1process\u8fd0\u884c\u4e0b\u53bb\u3002 \u53e6\u5916\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u4e3a\u4ec0\u4e48\u9700\u8981\u7533\u8bf7memory\uff1f \u5176\u5b9e\u5982\u679c\u8fd9\u4e2a\u7cfb\u7edf\u4e2d\u53ea\u6709\u4e00\u4e2a\u7a0b\u5e8f\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u60f3\u600e\u4e48\u6837\u4f7f\u7528memory\u5c31\u600e\u4e48\u6837\u4f7f\u7528memory\uff0c\u4f46\u662f\u95ee\u9898\u662f\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u662f\u9700\u8981\u652f\u6301\u591a\u4efb\u52a1\u7684\uff0c\u90a3\u5b83\u5c31\u9700\u8981\u505a\u597d\u4e0d\u540c\u7684process\u4e4b\u95f4\u7684\u9694\u79bb\uff0cA process\u4e0d\u80fd\u591f\u4f7f\u7528B process\u7684\u4e1c\u897f\u3002\u6240\u4ee5\uff0c\u6240\u6709\u7684process\u90fd\u5fc5\u987b\u8981\u5148\u60f3OS\u7533\u8bf7memory\uff0c\u7136\u540e\u624d\u80fd\u591f\u4f7f\u7528\uff0cOS\u4f1a\u8bb0\u4f4fmemory\u7684\u6240\u5c5e\uff0c\u8fd9\u6837\u5c31\u80fd\u591f\u4fdd\u8bc1\u4e0d\u51b2\u7a81\u4e86\u3002\u5176\u6b21\u662fprocess\u7684\u8fd0\u884c\u662f\u9700\u8981\u4e00\u5b9a\u7684memory space\u6765\u5b58\u653e\u5b83\u7684\u76f8\u5173\u7684\u6570\u636e\u7684\uff0c\u6bd4\u5982\u5728\u53d1\u751fcontext switch\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u5c06\u5b83\u7684context\u76f8\u5173\u7684\u6570\u636e\u90fd\u4fdd\u5b58\u5230\u5b83\u7684memory space\u4e2d\u6765\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fprocess\u7684call stack\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u9700\u8981memory space\u7684\u573a\u6240\u3002 Question: \u5982\u524d\u6240\u8ff0\uff0c\u6808\u4e5f\u662fvirtual address space\u7684\u6210\u5206\u4e4b\u4e00\uff0c\u6bcf\u4e2athread\u90fd\u6709\u5404\u81ea \u72ec\u7acb \u7684call stack\uff0c\u800c\u6240\u6709\u7684thread\u7406\u8bba\u4e0a\u90fd\u662f\u5171\u4eabprocess\u7684virtual address space\u7684\uff0c\u90a3\u8fd9\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u5176\u5b9e\u6700\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u662f\u67e5\u770b task_descriptor \u7684\u6210\u5458\u53d8\u91cf See also # Is kernel data segment in process address space? Anatomy of a Program in Memory Does every process have its own page table? How do I save space with inverted page tables? Page table Why one page table per process What are high memory and low memory on Linux?","title":"Virtual-memory-address-space-thinking"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#virtual-address-space","text":"","title":"\u5173\u4e8eVirtual address space\u7684\u601d\u8003"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#processvirtual-address-space","text":"\u6211\u4e4b\u524d\u51fa\u73b0out-of-memory\u7684\u60c5\u51b5\u662f\uff1aprocess\u975e\u5e38\u9ad8\u9891\u5730\u6267\u884c\u4e00\u4e2a\u5b58\u5728memory leakage\u7684function\uff0c\u6700\u7ec8\u5bfc\u81f4out-of-memory\uff08\u8fd9\u4e2afunction\u4f7f\u7528\u7684\u662f malloc \u5f0f\u7684\uff0c\u6240\u4ee5\u5b83\u6240\u83b7\u5f97\u7684memory\u662f\u4f4d\u4e8eheap\u7684\uff0c\u90a3\u4f4d\u4e8eheap\u4e0a\u7684memory\u6ca1\u6709\u4e0a\u9650\u5417\uff1f\u5b83\u5982\u4f55\u80fd\u591f\u5bfc\u81f4out-of-memory\uff1f\uff09\u3002\u4e00\u4e2aprocess\u5982\u4f55\u80fd\u591f\u5c06\u4e00\u53f0\u673a\u5668\u7684memory\u8017\u5c3d\uff1f\u8fd9\u4e2a\u95ee\u9898\u8f6c\u6362\u4e00\u4e0b\u5c31\u662f\u96be\u9053\u4e00\u4e2aprocess\u7684address space\u6ca1\u6709\u4e0a\u9650\u5417? \u4e00\u4e2aprocess\u7684address space\u662f\u6709\u4e0a\u9650\u7684\uff0c\u8fd9\u5c31\u662fvirtual address\u7684\u957f\u5ea6\u51b3\u5b9a\u7684\uff0c\u6bd4\u598232bit\uff0c\u5219\u5b83\u7684\u4e0a\u9650\u5c31\u662f4G\uff08\u4ec5\u4ec5\u53ea\u67094G\uff0c\u8fd9\u53ef\u80fd\u5417\uff1f\uff09\u76f4\u89c2\u611f\u53d7virtual address\u7684\u957f\u5ea6\u5c31\u662fC\u4e2d\u7684\u6307\u9488\u7c7b\u578b\u7684\u957f\u5ea6\uff0c\u6b63\u5982\u5728 Virtual address space \u4e2d\u6240\u63cf\u8ff0\u7684\u3002 \u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1aCPU\u7684word size\u548cvirtual address length\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4e24\u8005\u80af\u5b9a\u4e0d\u662f\u4e00\u56de\u4e8b\u513f\u3002 \u6bcf\u4e2aprocess\u6709\u4e00\u4e2avirtual address space\uff0c\u4e0e\u8fd9\u4e2avirtual address space\u76f8\u5339\u914d\u7684\u662f\u4e00\u4e2apage table\uff0c\u8fd9\u4e2apage table\u8bb0\u5f55\u7740\u5f52\u5c5e\u4e8e\u5b83\u7684\u6240\u6709\u7684memory\uff0c\u90a3\u662f\u5426\u662f\u5728\u9884\u5148\u5c31\u5c06\u8fd9\u4e2apage table\u7684size\u7ed9\u56fa\u5b9a\u4e86\u5462\uff1f\u8fd8\u662f\u540e\u7eed\u5728process\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u4e2apage table\u7684\u5927\u5c0f\u4f1a\u6539\u53d8\uff1f \u4e0eout-of-memory\u76f8\u5173\u7684\u53e6\u5916\u4e00\u4e2a\u95ee\u9898\u662f\uff1astackoverflow\u3002","title":"\u4e00\u4e2aprocess\u7684virtual address space\u7684\u4e0a\u9650\uff1f"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#virtual-address","text":"lazy\uff0c\u5373\u65e0\u9700\u4e00\u6b21\u6027\u5206\u914d\u6574\u4e2avirtual address space\uff0c\u800c\u662f\u7528\u65f6\u518d\u5206\u914d\u3002 copy on write \u5176\u5b9e\u901a\u8fc7\u8fd9\u4e2a\u601d\u8003\u624d\u53d1\u73b0virtual memory\u7684\u91cd\u8981\u4ef7\u503c\u6240\u5728\uff0c\u5b83\u662f\u5b9e\u73b0demand page\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0\u6269\u5145memory\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0copy on write\u7684\u57fa\u7840\u3002","title":"virtual address\u7684\u610f\u4e49"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#linux-process-virtual-address-spacepage-table","text":"\u5f53\u7136\u662f\u6709\u5dee\u522b\u7684\uff0cprocess virtual address space\u5219\u662f\u8868\u793a\u8fdb\u7a0b\u7684\u5730\u5740\u7a7a\u95f4\uff0c\u5176\u4e2d\u6240\u4f7f\u7528\u7684\u662fvirtual address\uff0cpage table\u662f\u8bb0\u5f55\u7740virtual address\u548cphysical address\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002 \u663e\u7136page table\u5bf9user\u662f\u900f\u660e\u7684\uff0c\u4f46\u662f\u6211\u4eec\u5374\u53ef\u4ee5\u8bbf\u95ee\u5230virtual address\uff0c\u901a\u8fc7pointer\u3002","title":"linux process virtual address space\u548cpage table\u4e4b\u95f4\u7684\u5173\u7cfb"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#process-virtual-address-space","text":"\u57289.2. The Memory Descriptor\u4e2d\u6307\u51fa\uff1a mm_struct","title":"\u63cf\u8ff0process virtual address space\u7684\u6570\u636e\u7ed3\u6784"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#page-tabledata-structure","text":"\u6bcf\u4e2avirtual address space\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684page table\uff0c\u90a3\u8fd9page table\u7684data structure\u662f\u5982\u4f55\u7684\uff1f \u5176\u5b9e\u8fd9\u5728Chapter 2. Memory Addressing\u4e2d\u5df2\u7ecf\u63cf\u8ff0\u4e86\u3002","title":"\u63cf\u8ff0page table\u7684data structure"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#memory-usageprocess","text":"\u8fdb\u7a0b\u7684\u8fd0\u884c\u4f34\u968f\u7740\u5185\u5b58\u7684\u4f7f\u7528\uff0c\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u8fd8\u6ca1\u6709\u4e00\u4e2a\u5b8c\u6574\u3001\u6e05\u6670\u7684\u8ba4\u77e5\u3002\u5982\u4f55\u65f6\u5206\u914dmemory\uff1f\u4f55\u65f6\u56de\u6536memory\uff1f \u5206\u914dmemory\u7684\u573a\u666f\uff1a \u51fd\u6570\u8c03\u7528 stored program \u5728context switch\u7684\u65f6\u5019\uff0cOS\u9700\u8981\u5c06process\u7684context\u4fdd\u5b58\u8d77\u6765 \u5176\u5b9e\u6211\u77e5\u9053\uff0c\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u7684\u66f4\u597d\u7684\u65b9\u5f0f\u662f\u9605\u8bfb\u8ba1\u7b97\u673a\u6267\u884c\u6307\u4ee4\u7684\u6d41\u7a0b\u3002","title":"\u4ecememory usage\u7684\u89d2\u5ea6\u6765\u5206\u6790process\u7684\u8fd0\u884c"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#address-space","text":"\u8fd9\u4e2a\u95ee\u9898\u662f\u7531\u524d\u9762\u7684\u5173\u4e8eprocess\u7684resource\u7684\u601d\u8003\u884d\u751f\u51fa\u6765\u7684\u3002Address space\u662f\u4e00\u4e2aprocess\u975e\u5e38\u91cd\u8981\u7684resource\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u662fprocess\u8fdb\u884c\u6d3b\u52a8\u7684\u7a7a\u95f4\u3002\u76ee\u524d\u7684OS\u90fd\u662f\u91c7\u7528\u7684virtual address\uff0c\u5373process\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u6240\u4f7f\u7528\u7684\u662fvirtual memory\uff0c\u6240\u4ee5\u4e5f\u53ef\u4ee5\u5c06Address space\u79f0\u4e3aVirtual address space\u3002\u5173\u4e8eprocess\u7684Virtual address space\uff0c\u6211\u6709\u5982\u4e0b\u7591\u95ee\uff1a Question: process\u4f7f\u7528virtual memory\uff0c\u5e76\u4e14\u4f7f\u7528\u57fa\u4e8epage\u7684memory management\uff0c\u90a3\u5b83\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u57fa\u4e8epage\u7684virtual memory\u5462\uff1f\u662f\u5206\u5272\u4e3a\u4e00\u4e2a\u4e00\u4e2a\u7684page\uff1f \u7ecf\u8fc7\u7b80\u5355\u7684\u601d\u8003\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u7f16\u8bd1\u5668\u5728\u7ed9\u751f\u6210\u4ee3\u7801\u7684\u65f6\u5019\u5176\u5b9e\u662f\u4e0d\u9700\u8981\u8003\u8651\u8fd9\u4e2a\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u662fOS\u5728\u8fd0\u884cprogram\u7684\u65f6\u5019\u6309\u7167page\u8fdb\u884cmemory management\uff0c\u65e0\u8bba\u7f16\u8bd1\u5668\u751f\u6210\u7684program\u662f\u600e\u6837\u7684\uff0c\u662fOS\u8d1f\u8d23\u5c06\u8fd9\u4e9bprogram\u88c5\u5165\u5230memory\u4e2d\uff0c\u8fd9\u4e00\u5207\u5bf9compiler\u800c\u8a00\u90fd\u662f\u900f\u660e\u7684\u3002 \u4f46\u662f\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u5ef6\u4f38\u4e00\u4e0b\uff1a\u6211\u4eec\u77e5\u9053\uff0c\u7f16\u8bd1\u5668\u751f\u6210\u7684\u4ee3\u7801\u80af\u5b9a\u662f\u9700\u8981\u9075\u5faaalignment\u7684\uff0c\u90a3\u8fd9\u5c31\u6d89\u53ca\u5230alignment\u548cpage size\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u5e94\u8be5\u53ea\u8981\u7b26\u5408alignment\uff0c\u90a3\u4e48\u5e94\u8be5\u5c31\u4e0d\u4f1a\u5b58\u5728\u4e00\u4e2a\u6570\u636e\u5b58\u50a8\u8de8\u8d8a\u4e86\u591a\u4e2apage\u7684\u60c5\u51b5\u4e86\u3002 Question: \u8fdb\u7a0b\u7684virtual address space\u90fd\u662f\u76f8\u540c\u7684\uff0c\u90a3virtual address\u662f\u5982\u4f55\u6620\u5c04\u5230physical memory address\u7684\u5462\uff1f \u65e2\u7136\u4f7f\u7528\u7684\u662fdemand page\uff0c\u4e5f\u5c31\u662f\u5728process\u8fd0\u884c\u7684\u65f6\u5019\u9700\u8981\u8bbf\u95ee\u8be5virtual memory\u7684\u65f6\u5019\uff0c\u624dallocate physical memory\u6216\u8005swap-in\uff0c\u624d\u5c06virtual address\u6620\u5c04\u5230physical memory\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4fdd\u5b58\u5230\u8be5process\u7684page table\u4e2d\u3002 \u5176\u5b9e\u901a\u8fc7\u8fd9\u4e2a\u601d\u8003\u624d\u53d1\u73b0virtual memory\u7684\u91cd\u8981\u4ef7\u503c\u6240\u5728\uff0c\u5b83\u662f\u5b9e\u73b0demand page\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0\u6269\u5145memory\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0copy on write\u7684\u57fa\u7840\u3002 Question: \u59821.6.8.4. Process virtual address space handling\u8282\u6240\u53d9\u8ff0\u7684 The kernel usually stores a process virtual address space as a list of memory area descriptors . \u5373\u6211\u4eec\u901a\u5e38\u5c06virtual address space\u5206\u5272\u4e3a\u591a\u5757\uff0c\u90a3\u662f\u5728\u4ec0\u4e48\u5730\u65b9\u5c06virtual address space\u5206\u5272\u4e3a\u5982\u4e0a\u6240\u8ff0\u7684a list of memory area descriptors \uff1f operating system\u91c7\u7528\u7684\u662fdemand paging\uff0c\u5e76\u4e14stack\u7684\u589e\u957f\u65b9\u5411\u548cheap\u7684\u589e\u957f\u65b9\u5411\u76f8\u53cd\uff0c\u90a3\u8fd9\u4e9b\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u8981\u60f3\u5b8c\u5168\u7406\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9605\u8bfbcalling convention\u3002\u6211\u89c9\u5f97process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5bf9call stack\u7684\u7ef4\u62a4\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6d3b\u52a8\uff0c\u6bcf\u6b21new\u4e00\u4e2a\u6808\u5e27\u90fd\u9700\u8981\u5206\u914d\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\u91cd\u8981\u624d\u80fd\u591f\u4fdd\u8bc1process\u8fd0\u884c\u4e0b\u53bb\u3002 \u53e6\u5916\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u4e3a\u4ec0\u4e48\u9700\u8981\u7533\u8bf7memory\uff1f \u5176\u5b9e\u5982\u679c\u8fd9\u4e2a\u7cfb\u7edf\u4e2d\u53ea\u6709\u4e00\u4e2a\u7a0b\u5e8f\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u60f3\u600e\u4e48\u6837\u4f7f\u7528memory\u5c31\u600e\u4e48\u6837\u4f7f\u7528memory\uff0c\u4f46\u662f\u95ee\u9898\u662f\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u662f\u9700\u8981\u652f\u6301\u591a\u4efb\u52a1\u7684\uff0c\u90a3\u5b83\u5c31\u9700\u8981\u505a\u597d\u4e0d\u540c\u7684process\u4e4b\u95f4\u7684\u9694\u79bb\uff0cA process\u4e0d\u80fd\u591f\u4f7f\u7528B process\u7684\u4e1c\u897f\u3002\u6240\u4ee5\uff0c\u6240\u6709\u7684process\u90fd\u5fc5\u987b\u8981\u5148\u60f3OS\u7533\u8bf7memory\uff0c\u7136\u540e\u624d\u80fd\u591f\u4f7f\u7528\uff0cOS\u4f1a\u8bb0\u4f4fmemory\u7684\u6240\u5c5e\uff0c\u8fd9\u6837\u5c31\u80fd\u591f\u4fdd\u8bc1\u4e0d\u51b2\u7a81\u4e86\u3002\u5176\u6b21\u662fprocess\u7684\u8fd0\u884c\u662f\u9700\u8981\u4e00\u5b9a\u7684memory space\u6765\u5b58\u653e\u5b83\u7684\u76f8\u5173\u7684\u6570\u636e\u7684\uff0c\u6bd4\u5982\u5728\u53d1\u751fcontext switch\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u5c06\u5b83\u7684context\u76f8\u5173\u7684\u6570\u636e\u90fd\u4fdd\u5b58\u5230\u5b83\u7684memory space\u4e2d\u6765\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fprocess\u7684call stack\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u9700\u8981memory space\u7684\u573a\u6240\u3002 Question: \u5982\u524d\u6240\u8ff0\uff0c\u6808\u4e5f\u662fvirtual address space\u7684\u6210\u5206\u4e4b\u4e00\uff0c\u6bcf\u4e2athread\u90fd\u6709\u5404\u81ea \u72ec\u7acb \u7684call stack\uff0c\u800c\u6240\u6709\u7684thread\u7406\u8bba\u4e0a\u90fd\u662f\u5171\u4eabprocess\u7684virtual address space\u7684\uff0c\u90a3\u8fd9\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u5176\u5b9e\u6700\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u662f\u67e5\u770b task_descriptor \u7684\u6210\u5458\u53d8\u91cf","title":"Address space"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-memory-address-space-thinking/#see-also","text":"Is kernel data segment in process address space? Anatomy of a Program in Memory Does every process have its own page table? How do I save space with inverted page tables? Page table Why one page table per process What are high memory and low memory on Linux?","title":"See also"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Data-segment/","text":"Data segment #","title":"Data-segment"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Data-segment/#data-segment","text":"","title":"Data segment"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Memory-layout-of-a-process/","text":"Virtual address space # memory layout of a process in linux TODO # https://inst.eecs.berkeley.edu/~cs161/sp15/slides/lec3-sw-vulns.pdf https://inst.eecs.berkeley.edu/ https://cpp.tech-academy.co.uk/memory-layout/ https://cpp.tech-academy.co.uk/ https://stackoverflow.com/questions/3080375/how-is-the-memory-layout-of-a-c-c-program Memory Layout of C Programs # memory layout of process with thread # https://stackoverflow.com/questions/18149218/the-memory-layout-of-a-multithreaded-process","title":"Memory-layout-of-a-process"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Memory-layout-of-a-process/#virtual-address-space","text":"","title":"Virtual address space"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Memory-layout-of-a-process/#memory-layout-of-a-process-in-linux-todo","text":"https://inst.eecs.berkeley.edu/~cs161/sp15/slides/lec3-sw-vulns.pdf https://inst.eecs.berkeley.edu/ https://cpp.tech-academy.co.uk/memory-layout/ https://cpp.tech-academy.co.uk/ https://stackoverflow.com/questions/3080375/how-is-the-memory-layout-of-a-c-c-program","title":"memory layout of a process in linux TODO"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Memory-layout-of-a-process/#memory-layout-of-c-programs","text":"","title":"Memory Layout of C Programs"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Layout-of-process/Memory-layout-of-a-process/#memory-layout-of-process-with-thread","text":"https://stackoverflow.com/questions/18149218/the-memory-layout-of-a-multithreaded-process","title":"memory layout of process with thread"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Unix-virtual-address-space/","text":"How does word size affect the amount of virtual address space available? # So, I should really know this stuff already, but I am starting to learn more about the lower levels of software development. I am currently reading Computer Systems: A Programmer's Perspective. by Bryant O'Hallaron. I am on chapter 2 and he is talking about the way data is represented internally. I am having trouble understanding something conceptually and I am sure that I'm about to make my ignorance lucid here. I understand that a \"word\" is just a set of bytes and that the word size is just how many bits wide the system bus is. But, he also says: \"the most important system parameter determined by the word size is the maximum size of the virtual address space . That is, for a machine with a w- bit word size, the virtual addresses can range from 0 to (2^w)-1, giving the program access to at most 2^w bytes \" I am both confused on the general relationship between the word size and the amount of addresses in the system and how the specific formula is w-bit word size=2^w bytes of memory available. I am really scratching my head here, can some one help me out? EDIT: I actually misinterpreted his definition of a word and consequently the definition of word size. What he really said was: Busses are typically designed to transfer fixed-sized chunks of bytes known as words . The number of bytes in a word (the word size ) is a fundamental system parameter that varies across systems. most machines today have word sizes of either 4 bytes(32 bits) or 8 bytes(64 bits). For the sake of our discussion here, we will assume a word size of 4 bytes, and we will assume that buses transfer only one word at a time. which is pretty much a catch-all for the cases discussed in the answers without having to go into detail. He also said he would be oversimplifying some things, perhaps in later sections he will go into more detail. A # The idea is that one word of memory can be used as an address in the address space (i.e., a word is wide enough to hold a pointer). If an address were larger than a word, addressing would require multiple sequential words. That's not impossible, but is unreasonably complicated (and likely slow). So, given an w -bit word, how many values can that word represent? How may addresses can it denote? Since any bit can take on two values, we end up with 2\u00b72\u00b72\u00b7\u2026\u00b72 = $2^w$ addresses. Addresses are counted starting by zero, so the highest address is $2^w$ - 1. Example using a three-bit word: There are $2^3$=8 possible addresses: bin: 000 001 010 011 100 101 110 111 dec: 0 1 2 3 4 5 6 7 The highest address is $2^3-1 = 8 - 1 = 7$. If the memory is an array of bytes and the address is an index to this array, then using a three-bit word we can only address eight bytes. Even if the array physically holds more bytes, we cannot reach them with a restricted index. Therefore, the amount of virtual memory is restricted by the word size . COMMENTS Ahh, I get it now, so to have 8GB ram on a 32 bit system would be pointless then. \u2013 Luke Dec 23 '14 at 18:36 1 Of course, counterexamples to the idea that word length and memory size are usually equal are common: the 6502 had an 8-bit word size but 16-bit addresses, the 8086 had 16-bit words and 20-bit addresses, the 80286 was likewise 16-bit but used 24-bit addresses, and the Pentium Pro was a 32-bit processor supporting 36-bit addresses. All of these are harder to use than the simple word = pointer scheme we've become used to, but architectures like this are not exactly rare, historically speaking. \u2013 Jules Dec 23 '14 at 18:42 @LukeP Yes, at 4GB or more a 64-bit system is a must. However, there's a technique called \u201c physical address extension \u201d that allows a 32-bit OS running on compatible hardware to handle more than 4GB of physical memory \u2013 although the limit on virtual memory still holds. \u2013 amon Dec 23 '14 at 18:43 @lukep - not exactly pointless, just that you'd have to jump through certain hoops to use the extra memory that might make it more hassle than it's e worth. Note that 32-bit versions of server variants of Windows, for example, can handle up to 64gb of memory, but end-user systems are limited to 4gb because a lot of things stop working properly when you try to extend the memory beyond the 4gb barrier. \u2013 Jules Dec 23 '14 at 18:46 Related reading: x86 memory segmentation , probably the one architecture people are likely to be familiar with that had a segmented memory model and all the hassles that came with it. \u2013 user22815 Dec 23 '14 at 21:35 A # I understand that a \"word\" is just a set of bytes and that the word size is just how many bits wide the system bus is. But, he also says: \"the most important system parameter determined by the word size is the maximum size of the virtual address space. That is, for a machine with a w-bit word size, the virtual addresses can range from 0 to (2^w)-1, giving the program access to at most 2^w bytes\" That's assuming a lot. The assumptions are not unreasonable, but holding them in a book pretending to introduce computer architecture to programmers seems a weakness of the book. A word is a sequence of bits with the width used for normal argument of integer computation. (And that's still assuming that the architecture doesn't define a word with another, smaller, size which was the word used in an ancestor of the architecture.) That is often the size of integer registers. Bus have often that size, but they may be smaller or larger and are not really used to define what is the word size. There is an important class of processors for which data addresses have the same size as word. That's not always true. There are other processors may have a smaller or bigger address space (this is somewhat out of fashion, the 8086 had 16-bit words but the addresses were 20-bit wide, the PDP-10 had 36-bit words but the addresses were 18-bit wide). There is an important class of processors for which each byte of memory has its own address. That's not always true. There are other processors which give addresses to words, not to bytes. That's also out of fashion for general purpose processors, but more specialized one like DSP are still doing that. So if you have a processor which is in those two classes, a word of width w bits may holds 2^w different value, each referencing one byte. This is a maximum for the virtual memory size. The processor may have architectural restrictions which prevent the use of all that space (reserving part for the OS, for IO), or it may have other restrictions (the data structures used to map the virtual memory to the physical one in 64-bit processors often are not able to map the whole 64-bit space). There are at least two instances of machines which ignored the high order byte of 32-bit addresses and programmers have taken advantage of that (using those bits for flags and other stuff) leading to a painful evolution when later one wanted to use the bits for addresses (IBM 360 and MC68000 as used on Mac). I actually misinterpreted his original definition. I'm editing the question to reflect his exact definition \u2013 Luke Dec 23 '14 at 18:46 1 @LukeP, that doesn't change much. Word size is a property of the ISA (Instruction Set Architecture). Bus sizes is a property of the implementation of the ISA (also called micro-architecture). A bus may be wider or smaller than a word, that's not important for the programmer. The same ISA may be implemented several times, with different bus size. \u2013 AProgrammer Dec 23 '14 at 18:54 but the bus is still going to be sending the same amount of information over, it just may have to be done in a different amount of \"words\" right? \u2013 Luke Dec 23 '14 at 18:59 Yes. You have to think about the ISA as the interface between software and hardware. And as most interface, its goal is to expose functionality and hide implementation. And buses (there may be several, of different width) are part of the implementation. \u2013 AProgrammer Dec 23 '14 at 20:44","title":"Unix-virtual-address-space"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Unix-virtual-address-space/#how-does-word-size-affect-the-amount-of-virtual-address-space-available","text":"So, I should really know this stuff already, but I am starting to learn more about the lower levels of software development. I am currently reading Computer Systems: A Programmer's Perspective. by Bryant O'Hallaron. I am on chapter 2 and he is talking about the way data is represented internally. I am having trouble understanding something conceptually and I am sure that I'm about to make my ignorance lucid here. I understand that a \"word\" is just a set of bytes and that the word size is just how many bits wide the system bus is. But, he also says: \"the most important system parameter determined by the word size is the maximum size of the virtual address space . That is, for a machine with a w- bit word size, the virtual addresses can range from 0 to (2^w)-1, giving the program access to at most 2^w bytes \" I am both confused on the general relationship between the word size and the amount of addresses in the system and how the specific formula is w-bit word size=2^w bytes of memory available. I am really scratching my head here, can some one help me out? EDIT: I actually misinterpreted his definition of a word and consequently the definition of word size. What he really said was: Busses are typically designed to transfer fixed-sized chunks of bytes known as words . The number of bytes in a word (the word size ) is a fundamental system parameter that varies across systems. most machines today have word sizes of either 4 bytes(32 bits) or 8 bytes(64 bits). For the sake of our discussion here, we will assume a word size of 4 bytes, and we will assume that buses transfer only one word at a time. which is pretty much a catch-all for the cases discussed in the answers without having to go into detail. He also said he would be oversimplifying some things, perhaps in later sections he will go into more detail.","title":"How does word size affect the amount of virtual address space available?"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Unix-virtual-address-space/#a","text":"The idea is that one word of memory can be used as an address in the address space (i.e., a word is wide enough to hold a pointer). If an address were larger than a word, addressing would require multiple sequential words. That's not impossible, but is unreasonably complicated (and likely slow). So, given an w -bit word, how many values can that word represent? How may addresses can it denote? Since any bit can take on two values, we end up with 2\u00b72\u00b72\u00b7\u2026\u00b72 = $2^w$ addresses. Addresses are counted starting by zero, so the highest address is $2^w$ - 1. Example using a three-bit word: There are $2^3$=8 possible addresses: bin: 000 001 010 011 100 101 110 111 dec: 0 1 2 3 4 5 6 7 The highest address is $2^3-1 = 8 - 1 = 7$. If the memory is an array of bytes and the address is an index to this array, then using a three-bit word we can only address eight bytes. Even if the array physically holds more bytes, we cannot reach them with a restricted index. Therefore, the amount of virtual memory is restricted by the word size . COMMENTS Ahh, I get it now, so to have 8GB ram on a 32 bit system would be pointless then. \u2013 Luke Dec 23 '14 at 18:36 1 Of course, counterexamples to the idea that word length and memory size are usually equal are common: the 6502 had an 8-bit word size but 16-bit addresses, the 8086 had 16-bit words and 20-bit addresses, the 80286 was likewise 16-bit but used 24-bit addresses, and the Pentium Pro was a 32-bit processor supporting 36-bit addresses. All of these are harder to use than the simple word = pointer scheme we've become used to, but architectures like this are not exactly rare, historically speaking. \u2013 Jules Dec 23 '14 at 18:42 @LukeP Yes, at 4GB or more a 64-bit system is a must. However, there's a technique called \u201c physical address extension \u201d that allows a 32-bit OS running on compatible hardware to handle more than 4GB of physical memory \u2013 although the limit on virtual memory still holds. \u2013 amon Dec 23 '14 at 18:43 @lukep - not exactly pointless, just that you'd have to jump through certain hoops to use the extra memory that might make it more hassle than it's e worth. Note that 32-bit versions of server variants of Windows, for example, can handle up to 64gb of memory, but end-user systems are limited to 4gb because a lot of things stop working properly when you try to extend the memory beyond the 4gb barrier. \u2013 Jules Dec 23 '14 at 18:46 Related reading: x86 memory segmentation , probably the one architecture people are likely to be familiar with that had a segmented memory model and all the hassles that came with it. \u2013 user22815 Dec 23 '14 at 21:35","title":"A"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Unix-virtual-address-space/#a_1","text":"I understand that a \"word\" is just a set of bytes and that the word size is just how many bits wide the system bus is. But, he also says: \"the most important system parameter determined by the word size is the maximum size of the virtual address space. That is, for a machine with a w-bit word size, the virtual addresses can range from 0 to (2^w)-1, giving the program access to at most 2^w bytes\" That's assuming a lot. The assumptions are not unreasonable, but holding them in a book pretending to introduce computer architecture to programmers seems a weakness of the book. A word is a sequence of bits with the width used for normal argument of integer computation. (And that's still assuming that the architecture doesn't define a word with another, smaller, size which was the word used in an ancestor of the architecture.) That is often the size of integer registers. Bus have often that size, but they may be smaller or larger and are not really used to define what is the word size. There is an important class of processors for which data addresses have the same size as word. That's not always true. There are other processors may have a smaller or bigger address space (this is somewhat out of fashion, the 8086 had 16-bit words but the addresses were 20-bit wide, the PDP-10 had 36-bit words but the addresses were 18-bit wide). There is an important class of processors for which each byte of memory has its own address. That's not always true. There are other processors which give addresses to words, not to bytes. That's also out of fashion for general purpose processors, but more specialized one like DSP are still doing that. So if you have a processor which is in those two classes, a word of width w bits may holds 2^w different value, each referencing one byte. This is a maximum for the virtual memory size. The processor may have architectural restrictions which prevent the use of all that space (reserving part for the OS, for IO), or it may have other restrictions (the data structures used to map the virtual memory to the physical one in 64-bit processors often are not able to map the whole 64-bit space). There are at least two instances of machines which ignored the high order byte of 32-bit addresses and programmers have taken advantage of that (using those bits for flags and other stuff) leading to a painful evolution when later one wanted to use the bits for addresses (IBM 360 and MC68000 as used on Mac). I actually misinterpreted his original definition. I'm editing the question to reflect his exact definition \u2013 Luke Dec 23 '14 at 18:46 1 @LukeP, that doesn't change much. Word size is a property of the ISA (Instruction Set Architecture). Bus sizes is a property of the implementation of the ISA (also called micro-architecture). A bus may be wider or smaller than a word, that's not important for the programmer. The same ISA may be implemented several times, with different bus size. \u2013 AProgrammer Dec 23 '14 at 18:54 but the bus is still going to be sending the same amount of information over, it just may have to be done in a different amount of \"words\" right? \u2013 Luke Dec 23 '14 at 18:59 Yes. You have to think about the ISA as the interface between software and hardware. And as most interface, its goal is to expose functionality and hide implementation. And buses (there may be several, of different width) are part of the implementation. \u2013 AProgrammer Dec 23 '14 at 20:44","title":"A"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Virtual-address-space-size-reading-list/","text":"\u201cOut Of Memory\u201d Does Not Refer to Physical Memory Is Virtual memory infinite? A A How much memory can a 64bit machine address at a time? \u201cOut Of Memory\u201d Does Not Refer to Physical Memory # Is Virtual memory infinite? # I have been asked in an interview if virtual memory is infinite? I answered saying that it is not infinite. Then the interviewer asked the explanation and what I suggested was that in windows we do have a manual way to configure virtual memory to a certain limit. I would like to know if Virtual memory is really Infinite? A # First of all, forget the idea that virtual memory is limited by the size of pointers on your machine. Virtual memory limits are not the same as addressing space . You can address more virtual memory than is available in your pointer-based address space using paging. Virtual memory upper limits are set by the OS: for example, on 32-bit Windows the limit is 16TB, and on 64-bit Windows the limit is 256TB. Virtual memory is also physically limited by the available disc space. For an excellent overview, which addresses various misconceptions, see the following: http://blogs.msdn.com/b/ericlippert/archive/2009/06/08/out-of-memory-does-not-refer-to-physical-memory.aspx COMMENTS Yes @stusmith i think you are right about You can address more virtual memory than is available in your pointer-based ,and it is whole idea of using virtual memory. \u2013 Amit Singh Tomar Jul 7 '11 at 10:42 1 Virtual memory is not exactly limited by disk space, it can be allocated but not mapped, so-called \"reserved virtual memory\". See msdn.microsoft.com/en-us/library/windows/desktop/\u2026 and MEM_RESERVE for explanation. \u2013 Sergey Alaev Apr 6 '17 at 9:50 A # At the very least, the size of virtual memory is limited by the size of pointers on given platform (unless it has near/far pointers and non-flat memory model). For example, you cannot address more than about 2^32 (4GB) of memory using single 32-bit pointer. In practice, the virtual memory must be backed up with something eventually -- like a pagefile on disk -- so the size of storage enforces a more practical limit. Sorry, that's just wrong. Look up \"PAE\" ( Physical Address Extension ) for example. \u2013 stusmith Jul 7 '11 at 10:18 1 @stusmith : A PAE-enabled Linux-kernel requires that the CPU also support PAE. So, it is limited by the computer architecture, right? \u2013 Priyank Bhatnagar Jul 7 '11 at 10:28 2 @logic_max: Yes, but a 32-bit Intel chip is capable of supporting PAE. Maybe a better way of putting it is: it is the lowest value of [chip support, OS limit, disk space] . Usually that lowest value is disk space. \u2013 stusmith Jul 7 '11 at 10:30 @stusmith I don't think that ARM, PowerPC, MIPS, or many other CPUs have PAE. Neither do older x86 CPUs. There is no right answer to this poor question. In fact, depending on how you count (for the system, for a process), the answer is different, but still not infinite. Down-vote anyone who says it is infinite. \u2013 artless noise May 30 '14 at 17:47 1 It is also not true that everything must be backed by disk. A large zero initialized array can all be backed by a single page of zeros (typically 4-8k). Any read from any address will show zero. On the first write to a page, a fault happens and then the page is allocated (needs disk or memory) and remapped. This can allow large sparse arrays. \u2013 artless noise Jun 2 '14 at 20:52 The sum of the virtual address spaces of all the processes running at once can be larger than what the 'user pointer' register can address - since the 'process id' is effectively part of the virtual address . In some cases these \"process-unique\" addresses actually point to the same PM (e.g. shared libraries, data shared after fork() ) - but even allowing for that, it's possible for the total amount of truly distinct VM pages to exceed what one process can address. \u2013 greggo Aug 13 '14 at 16:51 How much memory can a 64bit machine address at a time? #","title":"Virtual-address-space-size-reading-list"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Virtual-address-space-size-reading-list/#out-of-memory-does-not-refer-to-physical-memory","text":"","title":"\u201cOut Of Memory\u201d Does Not Refer to Physical Memory"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Virtual-address-space-size-reading-list/#is-virtual-memory-infinite","text":"I have been asked in an interview if virtual memory is infinite? I answered saying that it is not infinite. Then the interviewer asked the explanation and what I suggested was that in windows we do have a manual way to configure virtual memory to a certain limit. I would like to know if Virtual memory is really Infinite?","title":"Is Virtual memory infinite?"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Virtual-address-space-size-reading-list/#a","text":"First of all, forget the idea that virtual memory is limited by the size of pointers on your machine. Virtual memory limits are not the same as addressing space . You can address more virtual memory than is available in your pointer-based address space using paging. Virtual memory upper limits are set by the OS: for example, on 32-bit Windows the limit is 16TB, and on 64-bit Windows the limit is 256TB. Virtual memory is also physically limited by the available disc space. For an excellent overview, which addresses various misconceptions, see the following: http://blogs.msdn.com/b/ericlippert/archive/2009/06/08/out-of-memory-does-not-refer-to-physical-memory.aspx COMMENTS Yes @stusmith i think you are right about You can address more virtual memory than is available in your pointer-based ,and it is whole idea of using virtual memory. \u2013 Amit Singh Tomar Jul 7 '11 at 10:42 1 Virtual memory is not exactly limited by disk space, it can be allocated but not mapped, so-called \"reserved virtual memory\". See msdn.microsoft.com/en-us/library/windows/desktop/\u2026 and MEM_RESERVE for explanation. \u2013 Sergey Alaev Apr 6 '17 at 9:50","title":"A"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Virtual-address-space-size-reading-list/#a_1","text":"At the very least, the size of virtual memory is limited by the size of pointers on given platform (unless it has near/far pointers and non-flat memory model). For example, you cannot address more than about 2^32 (4GB) of memory using single 32-bit pointer. In practice, the virtual memory must be backed up with something eventually -- like a pagefile on disk -- so the size of storage enforces a more practical limit. Sorry, that's just wrong. Look up \"PAE\" ( Physical Address Extension ) for example. \u2013 stusmith Jul 7 '11 at 10:18 1 @stusmith : A PAE-enabled Linux-kernel requires that the CPU also support PAE. So, it is limited by the computer architecture, right? \u2013 Priyank Bhatnagar Jul 7 '11 at 10:28 2 @logic_max: Yes, but a 32-bit Intel chip is capable of supporting PAE. Maybe a better way of putting it is: it is the lowest value of [chip support, OS limit, disk space] . Usually that lowest value is disk space. \u2013 stusmith Jul 7 '11 at 10:30 @stusmith I don't think that ARM, PowerPC, MIPS, or many other CPUs have PAE. Neither do older x86 CPUs. There is no right answer to this poor question. In fact, depending on how you count (for the system, for a process), the answer is different, but still not infinite. Down-vote anyone who says it is infinite. \u2013 artless noise May 30 '14 at 17:47 1 It is also not true that everything must be backed by disk. A large zero initialized array can all be backed by a single page of zeros (typically 4-8k). Any read from any address will show zero. On the first write to a page, a fault happens and then the page is allocated (needs disk or memory) and remapped. This can allow large sparse arrays. \u2013 artless noise Jun 2 '14 at 20:52 The sum of the virtual address spaces of all the processes running at once can be larger than what the 'user pointer' register can address - since the 'process id' is effectively part of the virtual address . In some cases these \"process-unique\" addresses actually point to the same PM (e.g. shared libraries, data shared after fork() ) - but even allowing for that, it's possible for the total amount of truly distinct VM pages to exceed what one process can address. \u2013 greggo Aug 13 '14 at 16:51","title":"A"},{"location":"Kernel/Guide/Memory-management/Virtual-memory/Virtual-address-space/Virtual-address-space-size/Virtual-address-space-size-reading-list/#how-much-memory-can-a-64bit-machine-address-at-a-time","text":"","title":"How much memory can a 64bit machine address at a time?"},{"location":"Programming/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u4e3b\u8981\u8ba8\u8bbaprogramming in Linux OS\uff0c\u6240\u4ee5\u4e3b\u8981\u5173\u6ce8\u7684\u662fLinux OS\u63d0\u4f9b\u7684interface\u3002 man # TODO: \u5bf9Linux OS\u7684man\u8fdb\u884c\u4ecb\u7ecd\u3002 Linux OS\u5728\u591a\u4e2a\u5c42\u6b21\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7c7b\u4f3c\u529f\u80fd\u7684Interface\uff0c\u4e3b\u8981\u7531\u5982\u4e0b\u5c42\u6b21\uff1a system call\uff0c man(2) \u3001 man(3) user command\uff0c man(1) \u3001 man(8) user command\u5176\u5b9e\u4e5f\u662fkernel\u7684interface","title":"Introduction"},{"location":"Programming/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u8ba8\u8bbaprogramming in Linux OS\uff0c\u6240\u4ee5\u4e3b\u8981\u5173\u6ce8\u7684\u662fLinux OS\u63d0\u4f9b\u7684interface\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Programming/#man","text":"TODO: \u5bf9Linux OS\u7684man\u8fdb\u884c\u4ecb\u7ecd\u3002 Linux OS\u5728\u591a\u4e2a\u5c42\u6b21\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7c7b\u4f3c\u529f\u80fd\u7684Interface\uff0c\u4e3b\u8981\u7531\u5982\u4e0b\u5c42\u6b21\uff1a system call\uff0c man(2) \u3001 man(3) user command\uff0c man(1) \u3001 man(8) user command\u5176\u5b9e\u4e5f\u662fkernel\u7684interface","title":"man"},{"location":"Programming/01-01-Convention/Conventions/","text":"unix handler and start_rtn # FORWORD # \u4eca\u5929\u5728\u9605\u8bfbAPUE\u7684chapter 7.3 Process Termination\uff0c\u770b\u5176 atexit Function\uff0c\u901a\u8fc7\u6b64\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u6765\u6ce8\u518c exit handlers \uff0c\u8fd9\u79cd\u901a\u8fc7\u4f7f\u7528\u51fd\u6570\u6307\u9488\u6765\u4f5c\u4e3a\u53c2\u6570\u7684\u65b9\u5f0f\u5728Unix-like\u7684API\u4e2d\u975e\u5e38\u5e38\u89c1\uff0c\u5e76\u4e14\uff0c\u5b83\u4eec\u7684\u547d\u540d\u5f80\u5f80\u4e5f\u662f\u7c7b\u4f3c\u7684\uff0c\u6bd4\u5982\u5e38\u5e38\u5e26handler\u7b49\uff1b\u6240\u4ee5\u6211\u51b3\u5b9a\u8fdb\u884c\u6574\u7406\uff1b atexit and exit handler # \u53c2\u89c1 APUE chapter 7.3 Process Termination sigaction Function and signal handler # \u53c2\u89c1APUE 10.14 sigaction Function signal Function and signal handler # \u53c2\u89c1APUE 10.3 signal Function pthread_create and start_rtn # \u53c2\u89c1APUE 11.4 Thread Creation","title":"Conventions"},{"location":"Programming/01-01-Convention/Conventions/#unix-handler-and-start_rtn","text":"","title":"unix handler and start_rtn"},{"location":"Programming/01-01-Convention/Conventions/#forword","text":"\u4eca\u5929\u5728\u9605\u8bfbAPUE\u7684chapter 7.3 Process Termination\uff0c\u770b\u5176 atexit Function\uff0c\u901a\u8fc7\u6b64\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u6765\u6ce8\u518c exit handlers \uff0c\u8fd9\u79cd\u901a\u8fc7\u4f7f\u7528\u51fd\u6570\u6307\u9488\u6765\u4f5c\u4e3a\u53c2\u6570\u7684\u65b9\u5f0f\u5728Unix-like\u7684API\u4e2d\u975e\u5e38\u5e38\u89c1\uff0c\u5e76\u4e14\uff0c\u5b83\u4eec\u7684\u547d\u540d\u5f80\u5f80\u4e5f\u662f\u7c7b\u4f3c\u7684\uff0c\u6bd4\u5982\u5e38\u5e38\u5e26handler\u7b49\uff1b\u6240\u4ee5\u6211\u51b3\u5b9a\u8fdb\u884c\u6574\u7406\uff1b","title":"FORWORD"},{"location":"Programming/01-01-Convention/Conventions/#atexit-and-exit-handler","text":"\u53c2\u89c1 APUE chapter 7.3 Process Termination","title":"atexit and exit handler"},{"location":"Programming/01-01-Convention/Conventions/#sigaction-function-and-signal-handler","text":"\u53c2\u89c1APUE 10.14 sigaction Function","title":"sigaction Function and  signal handler"},{"location":"Programming/01-01-Convention/Conventions/#signal-function-and-signal-handler","text":"\u53c2\u89c1APUE 10.3 signal Function","title":"signal Function and  signal handler"},{"location":"Programming/01-01-Convention/Conventions/#pthread_create-and-start_rtn","text":"\u53c2\u89c1APUE 11.4 Thread Creation","title":"pthread_create  and start_rtn"},{"location":"Programming/01-02-Lib/organization-gnu/","text":"\u524d\u8a00 # \u5728Unix-like OS\u4e2d\u8fdb\u884cprogramming\uff0c\u5c31\u4e0d\u5f97\u4e0d\u638c\u63e1 GNU Project \u6240\u63d0\u4f9b\u7684\u4e00\u7cfb\u5217\u5de5\u5177\u4e86\u3002 GNU\u5b98\u7f51\uff1a GNU Operating System \u5206\u7c7b # GNU\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\uff08\u5f88\u591a\uff09software\uff0c\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e9bsoftware\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u5176\u5b98\u7f51\u7684 GNU Manuals Online \u9875\u9762\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5927\u5206\u7c7b\uff1a Archiving Audio Business and productivity Database Dictionaries Documentation translation Editors Education Email Fonts GNU organization Games Graphics Health Interface Internet applications Live communications Localization Mathematics Music Printing Science Security Software development Software libraries Spreadsheets System administration Telephony Text creation and manipulation Version control Video Web authoring Software development # \u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u8986\u76d6\u4e86\u975e\u5e38\u591a\u7684\u9886\u57df\u3002 \u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u800c\u8a00\uff0c\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u662f Software development \u8fd9\u4e00\u5927\u7c7b\u3002\u5176\u4e2d\u7684\u5de5\u5177\u5f88\u591a\u662f\u6211\u4eec\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u662f\u79bb\u4e0d\u5f00\u7684\u3002 GNU Build System # \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cgnu build system\u7684\u53e6\u5916\u4e00\u79cd\u8bf4\u6cd5\u662fautotools\uff1a \u5b98\u65b9\u4ecb\u7ecd\uff1a An Introduction to the Autotools \u7ec4\u6210 # GNU Autoconf # \u5b98\u7f51\uff1a Autoconf \u6587\u6863\uff1a GNU Autoconf - Creating Automatic Configuration Scripts \u529f\u80fd\uff1a\u521b\u5efa configure \u6587\u4ef6 \u5173\u4e8econfigure\u6587\u4ef6\uff0c\u53c2\u89c1 configure script \u4f7f\u7528configure\u6587\u4ef6\u6765\u521b\u5efamakefile\u6587\u4ef6 GNU Automake # \u5b98\u7f51\uff1a Automake \u6587\u6863\uff1a automake GNU Libtool # Gnulib # \u5b66\u4e60\u8d44\u6e90 # Autotools Mythbuster GNU toolchain #","title":"Introduction"},{"location":"Programming/01-02-Lib/organization-gnu/#_1","text":"\u5728Unix-like OS\u4e2d\u8fdb\u884cprogramming\uff0c\u5c31\u4e0d\u5f97\u4e0d\u638c\u63e1 GNU Project \u6240\u63d0\u4f9b\u7684\u4e00\u7cfb\u5217\u5de5\u5177\u4e86\u3002 GNU\u5b98\u7f51\uff1a GNU Operating System","title":"\u524d\u8a00"},{"location":"Programming/01-02-Lib/organization-gnu/#_2","text":"GNU\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\uff08\u5f88\u591a\uff09software\uff0c\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e9bsoftware\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u5176\u5b98\u7f51\u7684 GNU Manuals Online \u9875\u9762\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5927\u5206\u7c7b\uff1a Archiving Audio Business and productivity Database Dictionaries Documentation translation Editors Education Email Fonts GNU organization Games Graphics Health Interface Internet applications Live communications Localization Mathematics Music Printing Science Security Software development Software libraries Spreadsheets System administration Telephony Text creation and manipulation Version control Video Web authoring","title":"\u5206\u7c7b"},{"location":"Programming/01-02-Lib/organization-gnu/#software-development","text":"\u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u8986\u76d6\u4e86\u975e\u5e38\u591a\u7684\u9886\u57df\u3002 \u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u800c\u8a00\uff0c\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u662f Software development \u8fd9\u4e00\u5927\u7c7b\u3002\u5176\u4e2d\u7684\u5de5\u5177\u5f88\u591a\u662f\u6211\u4eec\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u662f\u79bb\u4e0d\u5f00\u7684\u3002","title":"Software development"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-build-system","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cgnu build system\u7684\u53e6\u5916\u4e00\u79cd\u8bf4\u6cd5\u662fautotools\uff1a \u5b98\u65b9\u4ecb\u7ecd\uff1a An Introduction to the Autotools","title":"GNU Build System"},{"location":"Programming/01-02-Lib/organization-gnu/#_3","text":"","title":"\u7ec4\u6210"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-autoconf","text":"\u5b98\u7f51\uff1a Autoconf \u6587\u6863\uff1a GNU Autoconf - Creating Automatic Configuration Scripts \u529f\u80fd\uff1a\u521b\u5efa configure \u6587\u4ef6 \u5173\u4e8econfigure\u6587\u4ef6\uff0c\u53c2\u89c1 configure script \u4f7f\u7528configure\u6587\u4ef6\u6765\u521b\u5efamakefile\u6587\u4ef6","title":"GNU Autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-automake","text":"\u5b98\u7f51\uff1a Automake \u6587\u6863\uff1a automake","title":"GNU Automake"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-libtool","text":"","title":"GNU Libtool"},{"location":"Programming/01-02-Lib/organization-gnu/#gnulib","text":"","title":"Gnulib"},{"location":"Programming/01-02-Lib/organization-gnu/#_4","text":"Autotools Mythbuster","title":"\u5b66\u4e60\u8d44\u6e90"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-toolchain","text":"","title":"GNU toolchain"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/","text":"Autoconf # 1 Introduction # Autoconf is a tool for producing shell scripts that automatically configure software source code packages to adapt to many kinds of Posix-like systems . The configuration scripts produced by Autoconf are independent of Autoconf when they are run, so their users do not need to have Autoconf. The configuration scripts produced by Autoconf require no manual user intervention when run; they do not normally even need an argument specifying the system type. Instead, they individually test for the presence of each feature that the software package they are for might need. (Before each check, they print a one-line message stating what they are checking for, so the user doesn't get too bored while waiting for the script to finish.) As a result, they deal well with systems that are hybrids or customized from the more common Posix variants. There is no need to maintain files that list the features supported by each release of each variant of Posix. For each software package that Autoconf is used with, it creates a configuration script from a template file that lists the system features that the package needs or can use. After the shell code to recognize and respond to a system feature has been written, Autoconf allows it to be shared by many software packages that can use (or need) that feature. If it later turns out that the shell code needs adjustment for some reason, it needs to be changed in only one place; all of the configuration scripts can be regenerated automatically to take advantage of the updated code. NOTE: Autoconf \u7684\u8f93\u5165\uff1atemplate file\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure.ac Autoconf \u7684\u8f93\u51fa\uff1aconfiguration script\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure 2 The GNU Build System # 2.1 Automake 2.2 Gnulib 2.3 Libtool 2.4 Pointers 3 Making configure Scripts # The configuration scripts that Autoconf produces are by convention called configure . When run, configure creates several files, replacing configuration parameters in them with appropriate values. The files that configure creates are: one or more Makefile files, usually one in each subdirectory of the package (see Makefile Substitutions ); optionally, a C header file, the name of which is configurable, containing #define directives (see Configuration Headers ); a shell script called config.status that, when run, recreates the files listed above (see config.status Invocation ); an optional shell script normally called config.cache (created when using \u2018 configure --config-cache \u2019) that saves the results of running many of the tests (see Cache Files ); a file called config.log containing any messages produced by compilers, to help debugging if configure makes a mistake. To create a configure script with Autoconf, you need to write an Autoconf input file configure.ac (or configure.in ) and run autoconf on it. If you write your own feature tests to supplement those that come with Autoconf, you might also write files called aclocal.m4 and acsite.m4 . If you use a C header file to contain #define directives, you might also run autoheader , and you can distribute the generated file config.h.in with the package. Here is a diagram showing how the files that can be used in configuration are produced. Programs that are executed are suffixed by \u2018 * \u2019. Optional files are enclosed in square brackets (\u2018 [] \u2019). autoconf and autoheader also read the installed Autoconf macro files (by reading autoconf.m4 ). Files used in preparing a software package for distribution, when using just Autoconf: your source files --> [autoscan*] --> [configure.scan] --> configure.ac configure.ac --. | .------> autoconf* -----> configure [aclocal.m4] --+---+ | `-----> [autoheader*] --> [config.h.in] [acsite.m4] ---' Makefile.in Additionally, if you use Automake, the following additional productions come into play: [acinclude.m4] --. | [local macros] --+--> aclocal* --> aclocal.m4 | configure.ac ----' configure.ac --. +--> automake* --> Makefile.in Makefile.am ---' 3.1 Writing configure.ac # To produce a configure script for a software package, create a file called configure.ac that contains invocations of the Autoconf macros that test the system features your package needs or can use. Autoconf macros already exist to check for many features; see Existing Tests , for their descriptions. For most other features, you can use Autoconf template macros to produce custom checks ; see Writing Tests , for information about them. For especially tricky or specialized features, configure.ac might need to contain some hand-crafted shell commands; see Portable Shell Programming . The autoscan program can give you a good start in writing configure.ac (see autoscan Invocation , for more information). Shell Script Compiler : Autoconf as solution of a problem Autoconf Language : Programming in Autoconf Autoconf Input Layout : Standard organization of configure.ac 3.1.1 A Shell Script Compiler # Just as for any other computer language, in order to properly program configure.ac in Autoconf you must understand what problem the language tries to address and how it does so. The problem Autoconf addresses is that the world is a mess. After all, you are using Autoconf in order to have your package compile easily on all sorts of different systems, some of them being extremely hostile. Autoconf itself bears the price for these differences: configure must run on all those systems, and thus configure must limit itself to their lowest common denominator of features. Naturally, you might then think of shell scripts; who needs autoconf? A set of properly written shell functions is enough to make it easy to write configure scripts by hand. Sigh! Unfortunately, even in 2008, where shells without any function support are far and few between, there are pitfalls to avoid when making use of them. Also, finding a Bourne shell that accepts shell functions is not trivial, even though there is almost always one on interesting porting targets. So, what is really needed is some kind of compiler, autoconf , that takes an Autoconf program, configure.ac , and transforms it into a portable shell script, configure . How does autoconf perform this task? There are two obvious possibilities: creating a brand new language or extending an existing one. The former option is attractive: all sorts of optimizations could easily be implemented in the compiler and many rigorous checks could be performed on the Autoconf program (e.g., rejecting any non-portable construct). Alternatively, you can extend an existing language, such as the sh (Bourne shell) language. Autoconf does the latter: it is a layer on top of sh . It was therefore most convenient to implement autoconf as a macro expander : a program that repeatedly performs macro expansions on text input, replacing macro calls with macro bodies and producing a pure sh script in the end. Instead of implementing a dedicated Autoconf macro expander , it is natural to use an existing general-purpose macro language , such as M4, and implement the extensions as a set of M4 macros. 3.1.2 The Autoconf Language # The Autoconf language differs from many other computer languages because it treats actual code the same as plain text. Whereas in C, for instance, data and instructions have different syntactic status, in Autoconf their status is rigorously the same. Therefore, we need a means to distinguish literal strings from text to be expanded: quotation. When calling macros that take arguments, there must not be any white space between the macro name and the open parenthesis. AC_INIT ([oops], [1.0]) # incorrect AC_INIT([hello], [1.0]) # good Arguments should be enclosed within the quote characters \u2018 [ \u2019 and \u2018 ] \u2019, and be separated by commas. Any leading blanks or newlines in arguments are ignored, unless they are quoted. You should always quote an argument that might contain a macro name, comma, parenthesis, or a leading blank or newline. This rule applies recursively for every macro call, including macros called from other macros. For more details on quoting rules, see Programming in M4 . For instance: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], [1], [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) is quoted properly. You may safely simplify its quotation to: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) because \u20181\u2019 cannot contain a macro call. Here, the argument of AC_MSG_ERROR must be quoted; otherwise, its comma would be interpreted as an argument separator. Also, the second and third arguments of \u2018 AC_CHECK_HEADER \u2019 must be quoted, since they contain macro calls. The three arguments \u2018 HAVE_STDIO_H \u2019, \u2018 stdio.h \u2019, and \u2018 Define to 1 if you have <stdio.h>. \u2019 do not need quoting, but if you unwisely defined a macro with a name like \u2018 Define \u2019 or \u2018 stdio \u2019 then they would need quoting. Cautious Autoconf users would keep the quotes, but many Autoconf users find such precautions annoying, and would rewrite the example as follows: AC_CHECK_HEADER(stdio.h, [AC_DEFINE(HAVE_STDIO_H, 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) 3.1.3 Standard configure.ac Layout # The order in which configure.ac calls the Autoconf macros is not important, with a few exceptions. Every configure.ac must contain a call to AC_INIT before the checks, and a call to AC_OUTPUT at the end (see Output ). Additionally, some macros rely on other macros having been called first, because they check previously set values of some variables to decide what to do. These macros are noted in the individual descriptions (see Existing Tests ), and they also warn you when configure is created if they are called out of order. To encourage consistency, here is a suggested order for calling the Autoconf macros. Generally speaking, the things near the end of this list are those that could depend on things earlier in it. For example, library functions could be affected by types and libraries. Autoconf requirements AC_INIT(package, version, bug-report-address) information on the package checks for programs checks for libraries checks for header files checks for types checks for structures checks for compiler characteristics checks for library functions checks for system services AC_CONFIG_FILES([file...]) AC_OUTPUT 3.2 Using autoscan to Create configure.ac # 3.3 Using ifnames to List Conditionals # 3.4 Using autoconf to Create configure 3.5 Using autoreconf to Update configure Scripts","title":"Autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#autoconf","text":"","title":"Autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#1-introduction","text":"Autoconf is a tool for producing shell scripts that automatically configure software source code packages to adapt to many kinds of Posix-like systems . The configuration scripts produced by Autoconf are independent of Autoconf when they are run, so their users do not need to have Autoconf. The configuration scripts produced by Autoconf require no manual user intervention when run; they do not normally even need an argument specifying the system type. Instead, they individually test for the presence of each feature that the software package they are for might need. (Before each check, they print a one-line message stating what they are checking for, so the user doesn't get too bored while waiting for the script to finish.) As a result, they deal well with systems that are hybrids or customized from the more common Posix variants. There is no need to maintain files that list the features supported by each release of each variant of Posix. For each software package that Autoconf is used with, it creates a configuration script from a template file that lists the system features that the package needs or can use. After the shell code to recognize and respond to a system feature has been written, Autoconf allows it to be shared by many software packages that can use (or need) that feature. If it later turns out that the shell code needs adjustment for some reason, it needs to be changed in only one place; all of the configuration scripts can be regenerated automatically to take advantage of the updated code. NOTE: Autoconf \u7684\u8f93\u5165\uff1atemplate file\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure.ac Autoconf \u7684\u8f93\u51fa\uff1aconfiguration script\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure","title":"1 Introduction"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#2-the-gnu-build-system","text":"2.1 Automake 2.2 Gnulib 2.3 Libtool 2.4 Pointers","title":"2 The GNU Build System"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#3-making-configure-scripts","text":"The configuration scripts that Autoconf produces are by convention called configure . When run, configure creates several files, replacing configuration parameters in them with appropriate values. The files that configure creates are: one or more Makefile files, usually one in each subdirectory of the package (see Makefile Substitutions ); optionally, a C header file, the name of which is configurable, containing #define directives (see Configuration Headers ); a shell script called config.status that, when run, recreates the files listed above (see config.status Invocation ); an optional shell script normally called config.cache (created when using \u2018 configure --config-cache \u2019) that saves the results of running many of the tests (see Cache Files ); a file called config.log containing any messages produced by compilers, to help debugging if configure makes a mistake. To create a configure script with Autoconf, you need to write an Autoconf input file configure.ac (or configure.in ) and run autoconf on it. If you write your own feature tests to supplement those that come with Autoconf, you might also write files called aclocal.m4 and acsite.m4 . If you use a C header file to contain #define directives, you might also run autoheader , and you can distribute the generated file config.h.in with the package. Here is a diagram showing how the files that can be used in configuration are produced. Programs that are executed are suffixed by \u2018 * \u2019. Optional files are enclosed in square brackets (\u2018 [] \u2019). autoconf and autoheader also read the installed Autoconf macro files (by reading autoconf.m4 ). Files used in preparing a software package for distribution, when using just Autoconf: your source files --> [autoscan*] --> [configure.scan] --> configure.ac configure.ac --. | .------> autoconf* -----> configure [aclocal.m4] --+---+ | `-----> [autoheader*] --> [config.h.in] [acsite.m4] ---' Makefile.in Additionally, if you use Automake, the following additional productions come into play: [acinclude.m4] --. | [local macros] --+--> aclocal* --> aclocal.m4 | configure.ac ----' configure.ac --. +--> automake* --> Makefile.in Makefile.am ---'","title":"3 Making configure Scripts"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#31-writing-configureac","text":"To produce a configure script for a software package, create a file called configure.ac that contains invocations of the Autoconf macros that test the system features your package needs or can use. Autoconf macros already exist to check for many features; see Existing Tests , for their descriptions. For most other features, you can use Autoconf template macros to produce custom checks ; see Writing Tests , for information about them. For especially tricky or specialized features, configure.ac might need to contain some hand-crafted shell commands; see Portable Shell Programming . The autoscan program can give you a good start in writing configure.ac (see autoscan Invocation , for more information). Shell Script Compiler : Autoconf as solution of a problem Autoconf Language : Programming in Autoconf Autoconf Input Layout : Standard organization of configure.ac","title":"3.1 Writing configure.ac"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#311-a-shell-script-compiler","text":"Just as for any other computer language, in order to properly program configure.ac in Autoconf you must understand what problem the language tries to address and how it does so. The problem Autoconf addresses is that the world is a mess. After all, you are using Autoconf in order to have your package compile easily on all sorts of different systems, some of them being extremely hostile. Autoconf itself bears the price for these differences: configure must run on all those systems, and thus configure must limit itself to their lowest common denominator of features. Naturally, you might then think of shell scripts; who needs autoconf? A set of properly written shell functions is enough to make it easy to write configure scripts by hand. Sigh! Unfortunately, even in 2008, where shells without any function support are far and few between, there are pitfalls to avoid when making use of them. Also, finding a Bourne shell that accepts shell functions is not trivial, even though there is almost always one on interesting porting targets. So, what is really needed is some kind of compiler, autoconf , that takes an Autoconf program, configure.ac , and transforms it into a portable shell script, configure . How does autoconf perform this task? There are two obvious possibilities: creating a brand new language or extending an existing one. The former option is attractive: all sorts of optimizations could easily be implemented in the compiler and many rigorous checks could be performed on the Autoconf program (e.g., rejecting any non-portable construct). Alternatively, you can extend an existing language, such as the sh (Bourne shell) language. Autoconf does the latter: it is a layer on top of sh . It was therefore most convenient to implement autoconf as a macro expander : a program that repeatedly performs macro expansions on text input, replacing macro calls with macro bodies and producing a pure sh script in the end. Instead of implementing a dedicated Autoconf macro expander , it is natural to use an existing general-purpose macro language , such as M4, and implement the extensions as a set of M4 macros.","title":"3.1.1 A Shell Script Compiler"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#312-the-autoconf-language","text":"The Autoconf language differs from many other computer languages because it treats actual code the same as plain text. Whereas in C, for instance, data and instructions have different syntactic status, in Autoconf their status is rigorously the same. Therefore, we need a means to distinguish literal strings from text to be expanded: quotation. When calling macros that take arguments, there must not be any white space between the macro name and the open parenthesis. AC_INIT ([oops], [1.0]) # incorrect AC_INIT([hello], [1.0]) # good Arguments should be enclosed within the quote characters \u2018 [ \u2019 and \u2018 ] \u2019, and be separated by commas. Any leading blanks or newlines in arguments are ignored, unless they are quoted. You should always quote an argument that might contain a macro name, comma, parenthesis, or a leading blank or newline. This rule applies recursively for every macro call, including macros called from other macros. For more details on quoting rules, see Programming in M4 . For instance: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], [1], [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) is quoted properly. You may safely simplify its quotation to: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) because \u20181\u2019 cannot contain a macro call. Here, the argument of AC_MSG_ERROR must be quoted; otherwise, its comma would be interpreted as an argument separator. Also, the second and third arguments of \u2018 AC_CHECK_HEADER \u2019 must be quoted, since they contain macro calls. The three arguments \u2018 HAVE_STDIO_H \u2019, \u2018 stdio.h \u2019, and \u2018 Define to 1 if you have <stdio.h>. \u2019 do not need quoting, but if you unwisely defined a macro with a name like \u2018 Define \u2019 or \u2018 stdio \u2019 then they would need quoting. Cautious Autoconf users would keep the quotes, but many Autoconf users find such precautions annoying, and would rewrite the example as follows: AC_CHECK_HEADER(stdio.h, [AC_DEFINE(HAVE_STDIO_H, 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])])","title":"3.1.2 The Autoconf Language"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#313-standard-configureac-layout","text":"The order in which configure.ac calls the Autoconf macros is not important, with a few exceptions. Every configure.ac must contain a call to AC_INIT before the checks, and a call to AC_OUTPUT at the end (see Output ). Additionally, some macros rely on other macros having been called first, because they check previously set values of some variables to decide what to do. These macros are noted in the individual descriptions (see Existing Tests ), and they also warn you when configure is created if they are called out of order. To encourage consistency, here is a suggested order for calling the Autoconf macros. Generally speaking, the things near the end of this list are those that could depend on things earlier in it. For example, library functions could be affected by types and libraries. Autoconf requirements AC_INIT(package, version, bug-report-address) information on the package checks for programs checks for libraries checks for header files checks for types checks for structures checks for compiler characteristics checks for library functions checks for system services AC_CONFIG_FILES([file...]) AC_OUTPUT","title":"3.1.3 Standard configure.ac Layout"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#32-using-autoscan-to-create-configureac","text":"","title":"3.2 Using autoscan to Create configure.ac"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#33-using-ifnames-to-list-conditionals","text":"3.4 Using autoconf to Create configure 3.5 Using autoreconf to Update configure Scripts","title":"3.3 Using ifnames to List Conditionals"},{"location":"Programming/01-02-Lib/organization-gnu/Automake/Automake/","text":"automake #","title":"Automake"},{"location":"Programming/01-02-Lib/organization-gnu/Automake/Automake/#automake","text":"","title":"automake"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/","text":"Autotools Mythbuster # Chapter 1. Configuring The Build \u2014 autoconf # Configuring the build consists of running a series of tests to identify the build environment and the presence of the required tools and libraries. It is a crucial step in allowing portability between different operating systems to detect this build environment system. In the autotools chain, this is done by the autoconf tool. The autoconf tool translates a configure.ac file, written in a mixture of m4 and shell scripting, into a configure POSIX shell script that executes the tests that determines what the build environment is. 1. M4sh # The language used to write the configure.ac is called M4sh , to make clear that it's based off both sh and the macro language M4 . 2. Canonical Systems # When using autoconf, there are three system definitions (or machine definitions ) that are used to identify the \u201cactors\u201d in the build process; each definition relates to a similarly-named variable which will be illustrated in detail later. These three definitions are: host ( CHOST ) The system that is going to run the software once it is built, which is the main actor. Once the software has been built, it will execute on this particular system. build ( CBUILD ) The system where the build process is being executed. For most uses this would be the same as the host system, but in case of cross-compilation the two obviously differ. target ( CTARGET ) The system against which the software being built will run on. This actor only exists, or rather has a meaning, when the software being built may interact specifically with a system that differs from the one it's being executed on (our host ). This is the case for compilers, debuggers, profilers and analyzers and other tools in general. To identify the current actors involved in the build process, autoconf provides three macros that take care of finding the so-called \u201ccanonical\u201d values (see Section 2.1, \u201cThe System Definition Tuples\u201d for their format): AC_CANONICAL_HOST , AC_CANONICAL_BUILD and AC_CANONICAL_TARGET . These three macros then provide to the configure script the sh variables with the name of the actor ( $host , $build and $target ), and three parameters with the same name to the configure script so that the user can override the default discovered values. The most basic autoconf based build systems won't need to know any of these values, at least directly. Some other tools, such as libtool, will require discovery of canonical systems by themselves. Since adding these macros unconditionally adds direct and indirect code to the configure script (and a dependency on the two support files config.sub and config.guess ); it is recommended not to call them unconditionally. It is actually quite easy to decide whether canonical system definitions are needed or not. We just have to look for the use of the related actor variable. For instance if the configure.ac script uses the $build variable, we would need to call AC_CANONICAL_BUILD to discover its value. If the system definition variables are used in a macro instead, we should use the AC_REQUIRE macro to ensure that they are executed before entering. Don't fear calling them in more than one place. See Section 6.2, \u201cOnce-Expansion\u201d for more details. One common mistake is to \u201cgo all the way\u201d and always use the AC_CANONICAL_TARGET macro, or its misnamed predecessor AC_CANONICAL_SYSTEM . This is particularly a problem; because most of the software will not have a target actor at all. This actor is only meaningful when the software that is being built manages data that is specific to a different system than the one it is being executed on (the host system). In practice, the only places where the target actor is meaningful are to the parts of a compile toolchain: assemblers, linkers, compilers, debuggers, profilers, analysers, \u2026 For the rest of the software, the presence of an extraneous --target option to configure is likely to just be confusing. Especially for software that processes the output of the script to identify some information about the package being built. 2.1. The System Definition Tuples # The system definitions used by autoconf (but also by other packages like GCC and Binutils) are simple tuples in the form of strings. These are designed to provide, in a format easy to parse with \u201cglob masks\u201d; the major details that describe a computer system . The number of elements in these tuples is variable, for some uses that only deal with very low-level code, there can be just a single element, the system architecture ( i386 , x86_64 , powerpc , \u2026); others will have two, defining either the operating system or, most often for definition pairs, the executable format ( elf , coff , \u2026). These two formats though are usually, only related to components of the toolchain and not to autoconf directly. The tuples commonly used with autoconf are triples and quadruples, which define three components: architecture , vendor and operating system . These three components usually map directly into the triples, but for quadruple you have to split the operating system into kernel and userland (usually the C library). While the architecture is most obvious; and operating systems differ slightly from one another (still being probably the most important data), the vendor value is usually just ignored. It is meant to actually be the vendor of the hardware system, rather than the vendor of the software, although presently it is mostly used by distributions to brand their toolchain ( i386-redhat-linux-gnu ) or their special systems ( i386-gentoo-freebsd7.0 ) and by vendors that provide their own specific toolchain ( i686-apple-darwin9 ). Most operating systems don't split their definitions further in kernel and userland because they only work as an \u201censemble\u201d: FreeBSD, (Open)Solaris, Darwin, \u2026 There are, though, a few operating systems that have a split between kernel and userland, being managed by different projects or even being replaceable independently. This is the case for instance of Linux, which can use (among others) the GNU C Library (GNU/Linux) or uClibc, which become respectively *-linux-gnu and *-linux-uclibc . Also, most operating systems using triples also have a single standardised version for both kernel and userland, and thus provide it as a suffix to the element ( *-freebsd7.0 , *-netbsd4.0 ). For a few operating systems, this value might differ from the one that is used as the \u201cproduct version\u201d used in public. For instance Solaris 10 uses as a definition *-solaris2.10 and Apple's Mac OS X 10.5 uses *-darwin9 . 2.2. When To Use System Definitions # To be extended 3. Adding Options # 4. Finding Libraries # 5. Custom Autoconf Tests # 6. Autoconf Building Blocks: Macros # 7. Caching Results #","title":"Introduction"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#autotools-mythbuster","text":"","title":"Autotools Mythbuster"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#chapter-1-configuring-the-build-autoconf","text":"Configuring the build consists of running a series of tests to identify the build environment and the presence of the required tools and libraries. It is a crucial step in allowing portability between different operating systems to detect this build environment system. In the autotools chain, this is done by the autoconf tool. The autoconf tool translates a configure.ac file, written in a mixture of m4 and shell scripting, into a configure POSIX shell script that executes the tests that determines what the build environment is.","title":"Chapter 1. Configuring The Build \u2014 autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#1-m4sh","text":"The language used to write the configure.ac is called M4sh , to make clear that it's based off both sh and the macro language M4 .","title":"1. M4sh"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#2-canonical-systems","text":"When using autoconf, there are three system definitions (or machine definitions ) that are used to identify the \u201cactors\u201d in the build process; each definition relates to a similarly-named variable which will be illustrated in detail later. These three definitions are: host ( CHOST ) The system that is going to run the software once it is built, which is the main actor. Once the software has been built, it will execute on this particular system. build ( CBUILD ) The system where the build process is being executed. For most uses this would be the same as the host system, but in case of cross-compilation the two obviously differ. target ( CTARGET ) The system against which the software being built will run on. This actor only exists, or rather has a meaning, when the software being built may interact specifically with a system that differs from the one it's being executed on (our host ). This is the case for compilers, debuggers, profilers and analyzers and other tools in general. To identify the current actors involved in the build process, autoconf provides three macros that take care of finding the so-called \u201ccanonical\u201d values (see Section 2.1, \u201cThe System Definition Tuples\u201d for their format): AC_CANONICAL_HOST , AC_CANONICAL_BUILD and AC_CANONICAL_TARGET . These three macros then provide to the configure script the sh variables with the name of the actor ( $host , $build and $target ), and three parameters with the same name to the configure script so that the user can override the default discovered values. The most basic autoconf based build systems won't need to know any of these values, at least directly. Some other tools, such as libtool, will require discovery of canonical systems by themselves. Since adding these macros unconditionally adds direct and indirect code to the configure script (and a dependency on the two support files config.sub and config.guess ); it is recommended not to call them unconditionally. It is actually quite easy to decide whether canonical system definitions are needed or not. We just have to look for the use of the related actor variable. For instance if the configure.ac script uses the $build variable, we would need to call AC_CANONICAL_BUILD to discover its value. If the system definition variables are used in a macro instead, we should use the AC_REQUIRE macro to ensure that they are executed before entering. Don't fear calling them in more than one place. See Section 6.2, \u201cOnce-Expansion\u201d for more details. One common mistake is to \u201cgo all the way\u201d and always use the AC_CANONICAL_TARGET macro, or its misnamed predecessor AC_CANONICAL_SYSTEM . This is particularly a problem; because most of the software will not have a target actor at all. This actor is only meaningful when the software that is being built manages data that is specific to a different system than the one it is being executed on (the host system). In practice, the only places where the target actor is meaningful are to the parts of a compile toolchain: assemblers, linkers, compilers, debuggers, profilers, analysers, \u2026 For the rest of the software, the presence of an extraneous --target option to configure is likely to just be confusing. Especially for software that processes the output of the script to identify some information about the package being built.","title":"2. Canonical Systems"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#21-the-system-definition-tuples","text":"The system definitions used by autoconf (but also by other packages like GCC and Binutils) are simple tuples in the form of strings. These are designed to provide, in a format easy to parse with \u201cglob masks\u201d; the major details that describe a computer system . The number of elements in these tuples is variable, for some uses that only deal with very low-level code, there can be just a single element, the system architecture ( i386 , x86_64 , powerpc , \u2026); others will have two, defining either the operating system or, most often for definition pairs, the executable format ( elf , coff , \u2026). These two formats though are usually, only related to components of the toolchain and not to autoconf directly. The tuples commonly used with autoconf are triples and quadruples, which define three components: architecture , vendor and operating system . These three components usually map directly into the triples, but for quadruple you have to split the operating system into kernel and userland (usually the C library). While the architecture is most obvious; and operating systems differ slightly from one another (still being probably the most important data), the vendor value is usually just ignored. It is meant to actually be the vendor of the hardware system, rather than the vendor of the software, although presently it is mostly used by distributions to brand their toolchain ( i386-redhat-linux-gnu ) or their special systems ( i386-gentoo-freebsd7.0 ) and by vendors that provide their own specific toolchain ( i686-apple-darwin9 ). Most operating systems don't split their definitions further in kernel and userland because they only work as an \u201censemble\u201d: FreeBSD, (Open)Solaris, Darwin, \u2026 There are, though, a few operating systems that have a split between kernel and userland, being managed by different projects or even being replaceable independently. This is the case for instance of Linux, which can use (among others) the GNU C Library (GNU/Linux) or uClibc, which become respectively *-linux-gnu and *-linux-uclibc . Also, most operating systems using triples also have a single standardised version for both kernel and userland, and thus provide it as a suffix to the element ( *-freebsd7.0 , *-netbsd4.0 ). For a few operating systems, this value might differ from the one that is used as the \u201cproduct version\u201d used in public. For instance Solaris 10 uses as a definition *-solaris2.10 and Apple's Mac OS X 10.5 uses *-darwin9 .","title":"2.1. The System Definition Tuples"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#22-when-to-use-system-definitions","text":"To be extended","title":"2.2. When To Use System Definitions"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#3-adding-options","text":"","title":"3. Adding Options"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#4-finding-libraries","text":"","title":"4. Finding Libraries"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#5-custom-autoconf-tests","text":"","title":"5. Custom Autoconf Tests"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#6-autoconf-building-blocks-macros","text":"","title":"6. Autoconf Building Blocks: Macros"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#7-caching-results","text":"","title":"7. Caching Results"},{"location":"Programming/01-02-Lib/organiztion-posix/","text":"the open group # \u7ad9\u70b9\u4e8c # http://pubs.opengroup.org/onlinepubs/9699919799/","title":"Introduction"},{"location":"Programming/01-02-Lib/organiztion-posix/#the-open-group","text":"","title":"the open group"},{"location":"Programming/01-02-Lib/organiztion-posix/#_1","text":"http://pubs.opengroup.org/onlinepubs/9699919799/","title":"\u7ad9\u70b9\u4e8c"},{"location":"Programming/01-03-Common/Capabilities/","text":"CAPABILITIES(7) # Linux capabilities 101 # Linux Capabilities: Why They Exist and How They Work # Taking Advantage of Linux Capabilities #","title":"Capabilities"},{"location":"Programming/01-03-Common/Capabilities/#capabilities7","text":"","title":"CAPABILITIES(7)"},{"location":"Programming/01-03-Common/Capabilities/#linux-capabilities-101","text":"","title":"Linux capabilities 101"},{"location":"Programming/01-03-Common/Capabilities/#linux-capabilities-why-they-exist-and-how-they-work","text":"","title":"Linux Capabilities: Why They Exist and How They Work"},{"location":"Programming/01-03-Common/Capabilities/#taking-advantage-of-linux-capabilities","text":"","title":"Taking Advantage of Linux Capabilities"},{"location":"Programming/01-03-Common/Path-resolution/","text":"PATH_RESOLUTION(7) #","title":"Path-resolution"},{"location":"Programming/01-03-Common/Path-resolution/#path_resolution7","text":"","title":"PATH_RESOLUTION(7)"},{"location":"Programming/01-03-Common/Unix-programming-time-and-space/","text":"\u65f6\u95f4\u4e0e\u7a7a\u95f4 # \u65f6\u95f4\u4e0e\u7a7a\u95f4\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u662f\u7ecf\u5e38\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\uff0c\u5728\u8fdb\u884c\u7cfb\u7edfprogram\u7684\u65f6\u5019\uff0c\u9700\u8981\u4ecetime\u548cspace\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u8003\u8651\u3002 space\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86race condition\u3002 time\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86\u51fd\u6570\u6267\u884c\u7684\u65f6\u95f4\uff0c\u7cfb\u7edf\u8c03\u7528\u963b\u585e\u7684\u65f6\u5019\u3002\u5e76\u4e14\u5f88\u591a\u7684\u7cfb\u7edf\u8c03\u7528\u90fd\u6b63\u5e38\u8bbe\u7f6emax blocked time\u3002 \u6bd4\u5982\u5728multiple thread\u73af\u5883\u4e2d\u8fdb\u884cprogram\u7684\u65f6\u5019\uff0c\u4ece\u7a7a\u95f4\u7684\u89d2\u5ea6\u8fdb\u884c\u8003\u8651\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u9632\u6b62thread\u4e4b\u95f4\u7684race condition\u3002\u5c31\u6d89\u53ca\u5230\u539f\u5b50\u64cd\u4f5c\u3002 \u539f\u5b50\u64cd\u4f5c\u7684\u8bdd\uff0c\u6709\u4e9bprogramming language\u63d0\u4f9b\u4e86\u539f\u5b50\u64cd\u4f5c\u5e93\uff0cOS\u4e5f\u63d0\u4f9b\u4e86\u539f\u5b50\u51fd\u6570\uff0c\u5982APUE\u7684chapter12.10\u4e2d\u6240\u63d0\u53ca\u7684 pread \uff0c pwrite \u7b49\u3002 time-of-check-to-time-of-use\u5c31\u5c5e\u4e8etime\u7684\u89d2\u5ea6\u4e86","title":"Time-and-space"},{"location":"Programming/01-03-Common/Unix-programming-time-and-space/#_1","text":"\u65f6\u95f4\u4e0e\u7a7a\u95f4\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u662f\u7ecf\u5e38\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\uff0c\u5728\u8fdb\u884c\u7cfb\u7edfprogram\u7684\u65f6\u5019\uff0c\u9700\u8981\u4ecetime\u548cspace\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u8003\u8651\u3002 space\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86race condition\u3002 time\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86\u51fd\u6570\u6267\u884c\u7684\u65f6\u95f4\uff0c\u7cfb\u7edf\u8c03\u7528\u963b\u585e\u7684\u65f6\u5019\u3002\u5e76\u4e14\u5f88\u591a\u7684\u7cfb\u7edf\u8c03\u7528\u90fd\u6b63\u5e38\u8bbe\u7f6emax blocked time\u3002 \u6bd4\u5982\u5728multiple thread\u73af\u5883\u4e2d\u8fdb\u884cprogram\u7684\u65f6\u5019\uff0c\u4ece\u7a7a\u95f4\u7684\u89d2\u5ea6\u8fdb\u884c\u8003\u8651\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u9632\u6b62thread\u4e4b\u95f4\u7684race condition\u3002\u5c31\u6d89\u53ca\u5230\u539f\u5b50\u64cd\u4f5c\u3002 \u539f\u5b50\u64cd\u4f5c\u7684\u8bdd\uff0c\u6709\u4e9bprogramming language\u63d0\u4f9b\u4e86\u539f\u5b50\u64cd\u4f5c\u5e93\uff0cOS\u4e5f\u63d0\u4f9b\u4e86\u539f\u5b50\u51fd\u6570\uff0c\u5982APUE\u7684chapter12.10\u4e2d\u6240\u63d0\u53ca\u7684 pread \uff0c pwrite \u7b49\u3002 time-of-check-to-time-of-use\u5c31\u5c5e\u4e8etime\u7684\u89d2\u5ea6\u4e86","title":"\u65f6\u95f4\u4e0e\u7a7a\u95f4"},{"location":"Programming/01-03-Common/Unix-system-call/","text":"\u524d\u8a00 \u4eceblocking\u7684\u89d2\u5ea6\u6765\u5bf9system call\u8fdb\u884c\u5206\u7c7b APUE 10.5 Interrupted System Calls man SIGNAL(7) Difference between slow system calls and fast system calls \u901a\u8fc7Nonblocking I/O\u6765\u8f6c\u6362slow system call \u524d\u8a00 APUE 14.2 Nonblocking I/O \u5e26\u6709max blocking time\u7684system call\u975e\u5e38\u91cd\u8981\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u603b\u7ed3\uff1a \u548c\u963b\u585e\u76f8\u5173\u95ee\u9898 \u975e\u963b\u585eI\u4e0e\u5f02\u6b65IO socket\u662f\u5982\u4f55\u5b9e\u73b0\u7684IO with timeout\uff1f \u524d\u8a00 # \u603b\u7ed3Unix system call\u7684\u76f8\u5173\u77e5\u8bc6 \u4eceblocking\u7684\u89d2\u5ea6\u6765\u5bf9system call\u8fdb\u884c\u5206\u7c7b # \u5728APUE\u768410.5 Interrupted System Calls\u4e2d\u4ecb\u7ecd\u4e86slow system call\uff0c\u8fd9\u63d0\u793a\u6709\u5fc5\u8981\u5bf9linux\u7684system call\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff1b APUE 10.5 Interrupted System Calls # A characteristic of earlier UNIX systems was that if a process caught a signal while the process was blocked in a \u2018\u2018 slow \u2019\u2019 system call, the system call was interrupted. The system call returned an error and errno was set to EINTR . This was done under the assumption that since a signal occurred and the process caught it, there is a good chance that something has happened that should wake up the blocked system call. To support this feature, the system calls are divided into two categories: the \u2018\u2018slow\u2019\u2019 system calls and all the others. The slow system calls are those that can block forever . Included in this category are \u2022 Reads that can block the caller forever if data isn\u2019t present with certain file types (pipes, terminal devices, and network devices) \u2022 Writes that can block the caller forever if the data can\u2019t be accepted immediately by these same file types \u2022 Opens on certain file types that block the caller until some condition occurs (such as a terminal device open waiting until an attached modem answers the phone) \u2022 The pause function (which by definition puts the calling process to sleep until a signal is caught) and the wait function \u2022 Certain ioctl operations \u2022 Some of the interprocess communication functions (Chapter 15) The notable exception to these slow system calls is anything related to disk I/O. Although a read or a write of a disk file can block the caller temporarily (while the disk driver queues the request and then the request is executed), unless a hardware error occurs, the I/O operation always returns and unblocks the caller quickly. SUMMARY : \u663e\u7136\uff0c\u4e0a\u8ff0\u5bf9system call\u7684\u5206\u7c7b\u65b9\u6cd5\u662f\u6839\u636e\u8fd9\u4e2asystem call\u662f\u5426\u53ef\u80fd\u4f1a\u5c06process block forever \u7684\uff0c\u77ed\u6682\u7684block\u662f\u4e0d\u7b97slow\u7684\uff0c\u8fd9\u4e2a\u77ed\u6682\u7684block\u5c31\u662fa read or a write of a disk file\u3002\u5e76\u4e14slow system call\u662f\u548csignal\u5bc6\u5207\u76f8\u5173\u7684\uff1b \u4e0a\u8ff0\u63cf\u8ff0\u7684\u662f\u5728Unix system\u4e2d\u7684\u60c5\u51b5\uff0c\u90a3\u4e48linux\u4e2d\u7684\u60c5\u51b5\u662f\u600e\u6837\u7684\u5462\uff1f\u53c2\u89c1 man SIGNAL(7) \uff0c\u5176\u4e2d\u5bf9linux\u4e2d\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u8bf4\u660e\uff1b man SIGNAL(7) # Difference between slow system calls and fast system calls # \u901a\u8fc7Nonblocking I/O\u6765\u8f6c\u6362slow system call # \u524d\u8a00 # \u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5176\u5b9e\u6240\u8c13\u7684slow system call\u662f\u8ddf\u5b83\u6240\u64cd\u4f5c\u7684device\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u800cUnix OS\u7684everything in Unix is file\u7684philosophy\uff0c\u5c06\u5f88\u591adevice\u90fd\u62bd\u8c61\u6210\u4e86file\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u8fd9\u4e9bfile\u7684file descriptor\u8fdb\u884c\u64cd\u4f5c\u6765\u5b9e\u73b0\u5bf9device\u7684\u64cd\u4f5c\uff0c\u56e0\u6b64\u5f88\u591a\u64cd\u4f5c\u90fd\u662f\u7c7b\u4f3c\u4e8eIO\uff1b\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f53\u6211\u4eec\u5bf9slow device\u6267\u884csystem call\u7684\u65f6\u5019\uff0c\u5c31\u975e\u5e38\u53ef\u80fd\u51fa\u73b0slow system call\u7684\u60c5\u51b5\uff1bUnix OS\u662f\u975e\u5e38\u7075\u6d3b\u7684\uff0c\u5b83\u662f\u6709\u63d0\u4f9bsystem call\u6765\u5141\u8bb8\u7528\u6237\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\u7684\uff1a\u8fd9\u5c31\u662fUnix\u4e2d\u7684nonblocking I/O\uff0c\u901a\u8fc7\u5728\u6307\u5b9a\u7684file descriptor\u4e0a\u8bbe\u7f6enonblocking\u6807\u5fd7\uff0c\u6765\u544a\u8bc9kernel\u4e0d\u8981block\u5bf9\u8be5file descriptor\u8fdb\u884c\u64cd\u4f5c\u7684thread\uff1b APUE 14.2 Nonblocking I/O # \u5728\u8fd9\u4e00\u8282\u5bf9\u8fd9\u4e2a\u4e3b\u9898\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u4ecb\u7ecd\uff1b \u5e26\u6709max blocking time\u7684system call\u975e\u5e38\u91cd\u8981\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u603b\u7ed3\uff1a # pthread_mutex_timedlock pthread_rwlock_timedrdlock pthread_cond_timedwait select poll epoll_wait \u548c\u963b\u585e\u76f8\u5173\u95ee\u9898 # \u5982\u4f55\u53d6\u6d88\u963b\u585e\u7684\u7cfb\u7edf\u8c03\u7528\uff08\u53ef\u4ee5\u4f7f\u7528\u4fe1\u53f7\uff09 sleep \u662f\u5426\u662f\u963b\u585e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u5b83\u548c\u963b\u585e\u6709\u4ec0\u4e48\u5f02\u540c\uff1f \u975e\u963b\u585eI\u4e0e\u5f02\u6b65IO # \u975e\u963b\u585eIO\u53c2\u89c1APUE 14.2 \u901a\u8fc7\u8bbe\u7f6e\u6807\u5fd7\u6765\u5b9e\u73b0\u975e\u963b\u585e \u5f02\u6b65IO\u53c2\u89c114.5 socket\u662f\u5982\u4f55\u5b9e\u73b0\u7684IO with timeout\uff1f #","title":"System-call"},{"location":"Programming/01-03-Common/Unix-system-call/#_1","text":"\u603b\u7ed3Unix system call\u7684\u76f8\u5173\u77e5\u8bc6","title":"\u524d\u8a00"},{"location":"Programming/01-03-Common/Unix-system-call/#blockingsystem-call","text":"\u5728APUE\u768410.5 Interrupted System Calls\u4e2d\u4ecb\u7ecd\u4e86slow system call\uff0c\u8fd9\u63d0\u793a\u6709\u5fc5\u8981\u5bf9linux\u7684system call\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff1b","title":"\u4eceblocking\u7684\u89d2\u5ea6\u6765\u5bf9system call\u8fdb\u884c\u5206\u7c7b"},{"location":"Programming/01-03-Common/Unix-system-call/#apue-105-interrupted-system-calls","text":"A characteristic of earlier UNIX systems was that if a process caught a signal while the process was blocked in a \u2018\u2018 slow \u2019\u2019 system call, the system call was interrupted. The system call returned an error and errno was set to EINTR . This was done under the assumption that since a signal occurred and the process caught it, there is a good chance that something has happened that should wake up the blocked system call. To support this feature, the system calls are divided into two categories: the \u2018\u2018slow\u2019\u2019 system calls and all the others. The slow system calls are those that can block forever . Included in this category are \u2022 Reads that can block the caller forever if data isn\u2019t present with certain file types (pipes, terminal devices, and network devices) \u2022 Writes that can block the caller forever if the data can\u2019t be accepted immediately by these same file types \u2022 Opens on certain file types that block the caller until some condition occurs (such as a terminal device open waiting until an attached modem answers the phone) \u2022 The pause function (which by definition puts the calling process to sleep until a signal is caught) and the wait function \u2022 Certain ioctl operations \u2022 Some of the interprocess communication functions (Chapter 15) The notable exception to these slow system calls is anything related to disk I/O. Although a read or a write of a disk file can block the caller temporarily (while the disk driver queues the request and then the request is executed), unless a hardware error occurs, the I/O operation always returns and unblocks the caller quickly. SUMMARY : \u663e\u7136\uff0c\u4e0a\u8ff0\u5bf9system call\u7684\u5206\u7c7b\u65b9\u6cd5\u662f\u6839\u636e\u8fd9\u4e2asystem call\u662f\u5426\u53ef\u80fd\u4f1a\u5c06process block forever \u7684\uff0c\u77ed\u6682\u7684block\u662f\u4e0d\u7b97slow\u7684\uff0c\u8fd9\u4e2a\u77ed\u6682\u7684block\u5c31\u662fa read or a write of a disk file\u3002\u5e76\u4e14slow system call\u662f\u548csignal\u5bc6\u5207\u76f8\u5173\u7684\uff1b \u4e0a\u8ff0\u63cf\u8ff0\u7684\u662f\u5728Unix system\u4e2d\u7684\u60c5\u51b5\uff0c\u90a3\u4e48linux\u4e2d\u7684\u60c5\u51b5\u662f\u600e\u6837\u7684\u5462\uff1f\u53c2\u89c1 man SIGNAL(7) \uff0c\u5176\u4e2d\u5bf9linux\u4e2d\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u8bf4\u660e\uff1b","title":"APUE 10.5 Interrupted System Calls"},{"location":"Programming/01-03-Common/Unix-system-call/#man-signal7","text":"","title":"man SIGNAL(7)"},{"location":"Programming/01-03-Common/Unix-system-call/#difference-between-slow-system-calls-and-fast-system-calls","text":"","title":"Difference between slow system calls and fast system calls"},{"location":"Programming/01-03-Common/Unix-system-call/#nonblocking-ioslow-system-call","text":"","title":"\u901a\u8fc7Nonblocking I/O\u6765\u8f6c\u6362slow system call"},{"location":"Programming/01-03-Common/Unix-system-call/#_2","text":"\u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5176\u5b9e\u6240\u8c13\u7684slow system call\u662f\u8ddf\u5b83\u6240\u64cd\u4f5c\u7684device\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u800cUnix OS\u7684everything in Unix is file\u7684philosophy\uff0c\u5c06\u5f88\u591adevice\u90fd\u62bd\u8c61\u6210\u4e86file\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u8fd9\u4e9bfile\u7684file descriptor\u8fdb\u884c\u64cd\u4f5c\u6765\u5b9e\u73b0\u5bf9device\u7684\u64cd\u4f5c\uff0c\u56e0\u6b64\u5f88\u591a\u64cd\u4f5c\u90fd\u662f\u7c7b\u4f3c\u4e8eIO\uff1b\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f53\u6211\u4eec\u5bf9slow device\u6267\u884csystem call\u7684\u65f6\u5019\uff0c\u5c31\u975e\u5e38\u53ef\u80fd\u51fa\u73b0slow system call\u7684\u60c5\u51b5\uff1bUnix OS\u662f\u975e\u5e38\u7075\u6d3b\u7684\uff0c\u5b83\u662f\u6709\u63d0\u4f9bsystem call\u6765\u5141\u8bb8\u7528\u6237\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\u7684\uff1a\u8fd9\u5c31\u662fUnix\u4e2d\u7684nonblocking I/O\uff0c\u901a\u8fc7\u5728\u6307\u5b9a\u7684file descriptor\u4e0a\u8bbe\u7f6enonblocking\u6807\u5fd7\uff0c\u6765\u544a\u8bc9kernel\u4e0d\u8981block\u5bf9\u8be5file descriptor\u8fdb\u884c\u64cd\u4f5c\u7684thread\uff1b","title":"\u524d\u8a00"},{"location":"Programming/01-03-Common/Unix-system-call/#apue-142-nonblocking-io","text":"\u5728\u8fd9\u4e00\u8282\u5bf9\u8fd9\u4e2a\u4e3b\u9898\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u4ecb\u7ecd\uff1b","title":"APUE 14.2 Nonblocking I/O"},{"location":"Programming/01-03-Common/Unix-system-call/#max-blocking-timesystem-call","text":"pthread_mutex_timedlock pthread_rwlock_timedrdlock pthread_cond_timedwait select poll epoll_wait","title":"\u5e26\u6709max blocking time\u7684system call\u975e\u5e38\u91cd\u8981\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u603b\u7ed3\uff1a"},{"location":"Programming/01-03-Common/Unix-system-call/#_3","text":"\u5982\u4f55\u53d6\u6d88\u963b\u585e\u7684\u7cfb\u7edf\u8c03\u7528\uff08\u53ef\u4ee5\u4f7f\u7528\u4fe1\u53f7\uff09 sleep \u662f\u5426\u662f\u963b\u585e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u5b83\u548c\u963b\u585e\u6709\u4ec0\u4e48\u5f02\u540c\uff1f","title":"\u548c\u963b\u585e\u76f8\u5173\u95ee\u9898"},{"location":"Programming/01-03-Common/Unix-system-call/#iio","text":"\u975e\u963b\u585eIO\u53c2\u89c1APUE 14.2 \u901a\u8fc7\u8bbe\u7f6e\u6807\u5fd7\u6765\u5b9e\u73b0\u975e\u963b\u585e \u5f02\u6b65IO\u53c2\u89c114.5","title":"\u975e\u963b\u585eI\u4e0e\u5f02\u6b65IO"},{"location":"Programming/01-03-Common/Unix-system-call/#socketio-with-timeout","text":"","title":"socket\u662f\u5982\u4f55\u5b9e\u73b0\u7684IO with timeout\uff1f"},{"location":"Programming/01-03-Common/safety/","text":"SIGNAL-SAFETY(7) # Reentrancy (computing) Async-cancel-safe functions # Thread-safe functions # Thread safety race condition #","title":"Safety"},{"location":"Programming/01-03-Common/safety/#signal-safety7","text":"Reentrancy (computing)","title":"SIGNAL-SAFETY(7)"},{"location":"Programming/01-03-Common/safety/#async-cancel-safe-functions","text":"","title":"Async-cancel-safe functions"},{"location":"Programming/01-03-Common/safety/#thread-safe-functions","text":"Thread safety","title":"Thread-safe functions"},{"location":"Programming/01-03-Common/safety/#race-condition","text":"","title":"race condition"},{"location":"Programming/01-philosophy/","text":"\u524d\u8a00 # \u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86 linux kernel \u6240\u91c7\u7528\u7684\u67b6\u6784\u662f monolithic kernel \uff0c\u4e5f\u5c31\u662flinux kernel\u638c\u7ba1\u7740\u8fd9\u4e2a\u7cfb\u7edf\uff0capplication\u63d0\u4f9b\u8c03\u7528system call\u6765\u83b7\u5f97\u7cfb\u7edf\u670d\u52a1\u3002\u6309\u7167linux\u7684\u7ba1\u7406\uff0csystem call\u6240\u8fd4\u56de\u7684\u53eb\u505adescriptor\uff0c\u5728Windows\u7cfb\u7edf\u4e2d\u4e00\u822c\u53eb\u505ahandler\u3002 20200128 # \u4f7f\u7528\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300b1.6. An Overview of Unix Kernels\u4e2d\u7684\u5185\u5bb9\u4f5c\u4e3a\u5165\u573a\u662f\u975e\u5e38\u597d\u7684\u3002 \u5173\u4e8eeverything is a file\uff0c\u9700\u8981\u6dfb\u52a0\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300bchapter 1.6.9. Device Drivers\u7684\u5185\u5bb9\u3002","title":"Introduction"},{"location":"Programming/01-philosophy/#_1","text":"\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86 linux kernel \u6240\u91c7\u7528\u7684\u67b6\u6784\u662f monolithic kernel \uff0c\u4e5f\u5c31\u662flinux kernel\u638c\u7ba1\u7740\u8fd9\u4e2a\u7cfb\u7edf\uff0capplication\u63d0\u4f9b\u8c03\u7528system call\u6765\u83b7\u5f97\u7cfb\u7edf\u670d\u52a1\u3002\u6309\u7167linux\u7684\u7ba1\u7406\uff0csystem call\u6240\u8fd4\u56de\u7684\u53eb\u505adescriptor\uff0c\u5728Windows\u7cfb\u7edf\u4e2d\u4e00\u822c\u53eb\u505ahandler\u3002","title":"\u524d\u8a00"},{"location":"Programming/01-philosophy/#20200128","text":"\u4f7f\u7528\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300b1.6. An Overview of Unix Kernels\u4e2d\u7684\u5185\u5bb9\u4f5c\u4e3a\u5165\u573a\u662f\u975e\u5e38\u597d\u7684\u3002 \u5173\u4e8eeverything is a file\uff0c\u9700\u8981\u6dfb\u52a0\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300bchapter 1.6.9. Device Drivers\u7684\u5185\u5bb9\u3002","title":"20200128"},{"location":"Programming/01-philosophy/File-descriptor/","text":"File descriptor Overview Operations on file descriptors Creating file descriptors Deriving file descriptors Operations on a single file descriptor Operations on multiple file descriptors Operations on the file descriptor table Operations that modify process state File locking Sockets Miscellaneous Upcoming operations File descriptors as capabilities File descriptor # In Unix and related computer operating systems, a file descriptor ( FD , less frequently fildes ) is an abstract indicator ( handle ) used to access a file or other input/output resource , such as a pipe or network socket . File descriptors form part of the POSIX application programming interface . A file descriptor is a non-negative integer , generally represented in the C programming language as the type int (negative values being reserved to indicate \"no value\" or an error condition). Each Unix process (except perhaps a daemon ) should expect to have three standard POSIX file descriptors, corresponding to the three standard streams : Integer value Name unistd.h symbolic constant[ 1] stdio.h file stream[ 2] 0 Standard input STDIN_FILENO stdin 1 Standard output STDOUT_FILENO stdout 2 Standard error STDERR_FILENO stderr Overview # In the traditional implementation of Unix, file descriptors index into a per-process file descriptor table maintained by the kernel, that in turn indexes into a system-wide table of files opened by all processes, called the file table . This table records the mode with which the file (or other resource) has been opened: for reading, writing, appending, and possibly other modes. It also indexes into a third table called the inode table that describes the actual underlying files.[ 3] To perform input or output, the process passes the file descriptor to the kernel through a system call , and the kernel will access the file on behalf of the process. The process does not have direct access to the file or inode tables . SUMMARY : \u5728APUE\u76843.10 File Sharing\u4e5f\u63cf\u8ff0\u4e86\u8fd9\u90e8\u5206\u5185\u5bb9\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1athe data structures used by the kernel for all I/O.\u5373\u6240\u6709\u7684IO\u90fd\u662f\u91c7\u7528\u7684\u7c7b\u4f3c\u4e8e\u4e0a\u8ff0\u7684\u7ed3\u6784\uff1b\u5e76\u4e14\u4e0a\u8ff0\u7ed3\u6784\u9700\u8981\u548c Process control block \u4e00\u8d77\u6765\u7406\u89e3\u624d\u80fd\u591f\u5f88\u597d\u7684\u5bf9Unix OS\u7684IO\u6709\u4e00\u4e2a\u6574\u4f53\u7684\u8ba4\u77e5\uff1b SUMMARY : \u9700\u8981\u6ce8\u610f\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63d0\u53ca file descriptor table \u548c file table \u65f6\uff0c\u524d\u9762\u5206\u522b\u52a0\u4e0a\u4e86\u4fee\u9970\u8bed\uff1a per-process \u548c system-wide \uff1b\u8fd9\u4e24\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u9700\u8981\u5c06\u5b83\u4eec\u548cthe data structures used by the kernel for all I/O\u4e00\u8d77\u6765\u8fdb\u884c\u7406\u89e3\uff1b\u56e0\u4e3a file descriptor table \u7684scope\u662fprocess\uff0c\u5373\u6bcf\u4e2aprocess\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u7684 file descriptor table \uff0c\u6240\u4ee5\u6bcf\u4e2aprocess\u7684file descriptor\u90fd\u662f\u4ece0\u5f00\u59cb\u589e\u957f\uff1b\u663e\u7136\u6bd4\u8f83\u4e24\u4e2aprocess\u7684file descriptor\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff08\u5904\u74060,1,2\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5df2\u7ecf\u88ab\u9ed8\u8ba4\u7ed1\u5b9a\u5230 STDIN_FILENO , STDOUT_FILENO , STDERR_FILENO \uff09\uff1b\u800cfile table\u7684scope\u662fsystem\uff0c\u5373\u6240\u6709\u7684process\u90fd\u5c06\u5171\u4eabfile table\uff1b SUMMARY : \u6bcf\u6b21\u8c03\u7528 open \u7cfb\u7edf\u8c03\u7528\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2afile table entry On Linux , the set of file descriptors open in a process can be accessed under the path /proc/PID/fd/ , where PID is the process identifier . In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b The FILE data structure in the C standard I/O library usually includes a low level file descriptor for the object in question on Unix-like systems. The overall data structure provides additional abstraction and is instead known as a file handle. File descriptors for a single process, file table and inode table. Note that multiple file descriptors can refer to the same file table entry (e.g., as a result of the dup system call[ 3] :104 and that multiple file table entries can in turn refer to the same inode (if it has been opened multiple times; the table is still simplified because it represents inodes by file names, even though an inode can have multiple names ). File descriptor 3 does not refer to anything in the file table, signifying that it has been closed. SUMMARY : \u4e0a\u8ff0\u7684\u4e09\u5c42\u5bf9\u5e94\u5173\u7cfb\u5b58\u5728\u7740\u591a\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\uff0c\u518d\u52a0\u4e0aOS\u63d0\u4f9b\u7684fork\u673a\u5236\uff08\u5b50\u8fdb\u7a0b\u7ee7\u627f\u7236\u8fdb\u7a0b\u7684file descriptor\u548cfile tableentry\uff09\uff0c\u5404\u79cdIO\u64cd\u4f5c\uff08\u6bd4\u5982dup\uff0cread\uff0cwrite\uff09\u7b49\u7b49\u90fd\u5bfc\u81f4\u4e86\u95ee\u9898\u7684\u590d\u6742\u6027\uff1b \u6bd4\u5982\u5b58\u5728\u7740\u8fd9\u4e9b\u53ef\u80fd\u7684\u60c5\u51b5\uff1a dup \uff0c\u540c\u4e00\u8fdb\u7a0b\u4e2d\uff0c\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry fork \u540e\uff0c\u7236\u8fdb\u7a0b\uff0c\u5b50\u8fdb\u7a0b\u7684\u540c\u4e00\u4e2afile descriptor\u5171\u4eab\u540c\u4e00\u4e2afile table entry\uff08\u56e0\u4e3afile descriptor table\u662f\u6bcf\u4e2a\u8fdb\u7a0b\u79c1\u6709\u7684\uff0c\u6240\u4ee5\u8fd9\u79cd\u60c5\u51b5\u5176\u5b9e\u7c7b\u4f3c\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5\uff0c\u5373\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry\uff09 \u4e0a\u9762\u63cf\u8ff0\u4e86file descriptor\u548cfile table entry\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e0b\u9762\u63cf\u8ff0file table entry\u548ciNode\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a \u662f\u6709\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u4e0d\u540c\u7684file table entry\u6307\u5411\u4e86\u540c\u4e00\u4e2aiNode\u7684\uff1b \u663e\u7136OS\u7684\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u5c31\u5bfc\u81f4\u5f53\u4e00\u4e2a\u6587\u4ef6\u88ab\u591a\u4e2a\u4e0d\u540c\u7684process\u8fdb\u884cshare\u7684\u65f6\u5019\uff0c\u800c\u6bcf\u4e2aprocess\u90fd\u53ef\u4ee5\u6267\u884c\u4e00\u7cfb\u5217\u7684IO\u64cd\u4f5c\uff0c\u8fd9\u5c31\u5bfc\u81f4\u4e86\u53ef\u80fd\u5b58\u5728\u7684\u6570\u636e\u51b2\u7a81\u95ee\u9898\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u6309\u7167OS\u7684\u8fd9\u603b\u7ed3\u6784\u8bbe\u8ba1\uff0c\u4ee5\u53caOS\u63d0\u4f9b\u7684\u5404\u79cd\u64cd\u4f5c\uff0c\u662f\u53ef\u4ee5\u603b\u7ed3\u51fa\u53ef\u80fd\u7684\u6240\u6709\u60c5\u5f62\u7684\uff1b Operations on file descriptors # The following lists typical operations on file descriptors on modern Unix-like systems. Most of these functions are declared in the <unistd.h> header, but some are in the <fcntl.h> header instead. Creating file descriptors # open () creat() [ 4] socket() accept() socketpair () pipe() opendir() open_by_handle_at() (Linux) signalfd() (Linux) eventfd() (Linux) timerfd_create() (Linux) memfd_create() (Linux) userfaultfd() (Linux) Deriving file descriptors # dirfd() fileno() Operations on a single file descriptor # read (), write () readv(), writev() pread(), pwrite() recv(), send() recvmsg(), sendmsg() (including allowing sending FDs) sendfile() lseek() fstat() fchmod() fchown() fdopen() ftruncate() fsync() fdatasync() fstatvfs() dprintf() vmsplice() (Linux) Operations on multiple file descriptors # select() , pselect() poll() epoll() (for Linux) kqueue() (for BSD-based systems). sendfile() splice() , tee() (for Linux) Operations on the file descriptor table # The fcntl() function is used to perform various operations on a file descriptor, depending on the command argument passed to it. There are commands to get and set attributes associated with a file descriptor, including F_GETFD , F_SETFD , F_GETFL and F_SETFL . close() closefrom() (BSD and Solaris only; deletes all file descriptors greater than or equal to specified number) dup() (duplicates an existing file descriptor guaranteeing to be the lowest number available file descriptor) dup2() (the new file descriptor will have the value passed as an argument) fcntl ( F_DUPFD ) Operations that modify process state # fchdir() (sets the process's current working directory based on a directory file descriptor) mmap() (maps ranges of a file into the process's address space) File locking # flock() fcntl() ( F_GETLK , F_SETLK ) and F_SETLKW lockf() Sockets # connect() bind() listen() accept() (creates a new file descriptor for an incoming connection) getsockname() getpeername() getsockopt() setsockopt() shutdown() (shuts down one or both halves of a full duplex connection) Miscellaneous # ioctl() (a large collection of miscellaneous operations on a single file descriptor, often associated with a device) Upcoming operations # A series of new operations on file descriptors has been added to many modern Unix-like systems, as well as numerous C libraries, to be standardized in a future version of POSIX .[ 5] The at suffix signifies that the function takes an additional first argument supplying a file descriptor from which relative paths are resolved, the forms lacking the at suffix thus becoming equivalent to passing a file descriptor corresponding to the current working directory . The purpose of these new operations is to defend against a certain class of TOCTTOU attacks. openat() faccessat() fchmodat() fchownat() fstatat() futimesat() linkat() mkdirat() mknodat() readlinkat() renameat() symlinkat() unlinkat() mkfifoat() fdopendir() File descriptors as capabilities # Unix file descriptors behave in many ways as capabilities . They can be passed between processes across Unix domain sockets using the sendmsg() system call. Note, however, that what is actually passed is a reference to an \"open file description\" that has mutable state (the file offset, and the file status and access flags). This complicates the secure use of file descriptors as capabilities, since when programs share access to the same open file description, they can interfere with each other's use of it by changing its offset or whether it is blocking or non-blocking, for example.[ 6] [ 7] In operating systems that are specifically designed as capability systems, there is very rarely any mutable state associated with a capability itself. A Unix process' file descriptor table is an example of a C-list .","title":"File-descriptor"},{"location":"Programming/01-philosophy/File-descriptor/#file-descriptor","text":"In Unix and related computer operating systems, a file descriptor ( FD , less frequently fildes ) is an abstract indicator ( handle ) used to access a file or other input/output resource , such as a pipe or network socket . File descriptors form part of the POSIX application programming interface . A file descriptor is a non-negative integer , generally represented in the C programming language as the type int (negative values being reserved to indicate \"no value\" or an error condition). Each Unix process (except perhaps a daemon ) should expect to have three standard POSIX file descriptors, corresponding to the three standard streams : Integer value Name unistd.h symbolic constant[ 1] stdio.h file stream[ 2] 0 Standard input STDIN_FILENO stdin 1 Standard output STDOUT_FILENO stdout 2 Standard error STDERR_FILENO stderr","title":"File descriptor"},{"location":"Programming/01-philosophy/File-descriptor/#overview","text":"In the traditional implementation of Unix, file descriptors index into a per-process file descriptor table maintained by the kernel, that in turn indexes into a system-wide table of files opened by all processes, called the file table . This table records the mode with which the file (or other resource) has been opened: for reading, writing, appending, and possibly other modes. It also indexes into a third table called the inode table that describes the actual underlying files.[ 3] To perform input or output, the process passes the file descriptor to the kernel through a system call , and the kernel will access the file on behalf of the process. The process does not have direct access to the file or inode tables . SUMMARY : \u5728APUE\u76843.10 File Sharing\u4e5f\u63cf\u8ff0\u4e86\u8fd9\u90e8\u5206\u5185\u5bb9\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1athe data structures used by the kernel for all I/O.\u5373\u6240\u6709\u7684IO\u90fd\u662f\u91c7\u7528\u7684\u7c7b\u4f3c\u4e8e\u4e0a\u8ff0\u7684\u7ed3\u6784\uff1b\u5e76\u4e14\u4e0a\u8ff0\u7ed3\u6784\u9700\u8981\u548c Process control block \u4e00\u8d77\u6765\u7406\u89e3\u624d\u80fd\u591f\u5f88\u597d\u7684\u5bf9Unix OS\u7684IO\u6709\u4e00\u4e2a\u6574\u4f53\u7684\u8ba4\u77e5\uff1b SUMMARY : \u9700\u8981\u6ce8\u610f\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63d0\u53ca file descriptor table \u548c file table \u65f6\uff0c\u524d\u9762\u5206\u522b\u52a0\u4e0a\u4e86\u4fee\u9970\u8bed\uff1a per-process \u548c system-wide \uff1b\u8fd9\u4e24\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u9700\u8981\u5c06\u5b83\u4eec\u548cthe data structures used by the kernel for all I/O\u4e00\u8d77\u6765\u8fdb\u884c\u7406\u89e3\uff1b\u56e0\u4e3a file descriptor table \u7684scope\u662fprocess\uff0c\u5373\u6bcf\u4e2aprocess\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u7684 file descriptor table \uff0c\u6240\u4ee5\u6bcf\u4e2aprocess\u7684file descriptor\u90fd\u662f\u4ece0\u5f00\u59cb\u589e\u957f\uff1b\u663e\u7136\u6bd4\u8f83\u4e24\u4e2aprocess\u7684file descriptor\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff08\u5904\u74060,1,2\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5df2\u7ecf\u88ab\u9ed8\u8ba4\u7ed1\u5b9a\u5230 STDIN_FILENO , STDOUT_FILENO , STDERR_FILENO \uff09\uff1b\u800cfile table\u7684scope\u662fsystem\uff0c\u5373\u6240\u6709\u7684process\u90fd\u5c06\u5171\u4eabfile table\uff1b SUMMARY : \u6bcf\u6b21\u8c03\u7528 open \u7cfb\u7edf\u8c03\u7528\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2afile table entry On Linux , the set of file descriptors open in a process can be accessed under the path /proc/PID/fd/ , where PID is the process identifier . In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b The FILE data structure in the C standard I/O library usually includes a low level file descriptor for the object in question on Unix-like systems. The overall data structure provides additional abstraction and is instead known as a file handle. File descriptors for a single process, file table and inode table. Note that multiple file descriptors can refer to the same file table entry (e.g., as a result of the dup system call[ 3] :104 and that multiple file table entries can in turn refer to the same inode (if it has been opened multiple times; the table is still simplified because it represents inodes by file names, even though an inode can have multiple names ). File descriptor 3 does not refer to anything in the file table, signifying that it has been closed. SUMMARY : \u4e0a\u8ff0\u7684\u4e09\u5c42\u5bf9\u5e94\u5173\u7cfb\u5b58\u5728\u7740\u591a\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\uff0c\u518d\u52a0\u4e0aOS\u63d0\u4f9b\u7684fork\u673a\u5236\uff08\u5b50\u8fdb\u7a0b\u7ee7\u627f\u7236\u8fdb\u7a0b\u7684file descriptor\u548cfile tableentry\uff09\uff0c\u5404\u79cdIO\u64cd\u4f5c\uff08\u6bd4\u5982dup\uff0cread\uff0cwrite\uff09\u7b49\u7b49\u90fd\u5bfc\u81f4\u4e86\u95ee\u9898\u7684\u590d\u6742\u6027\uff1b \u6bd4\u5982\u5b58\u5728\u7740\u8fd9\u4e9b\u53ef\u80fd\u7684\u60c5\u51b5\uff1a dup \uff0c\u540c\u4e00\u8fdb\u7a0b\u4e2d\uff0c\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry fork \u540e\uff0c\u7236\u8fdb\u7a0b\uff0c\u5b50\u8fdb\u7a0b\u7684\u540c\u4e00\u4e2afile descriptor\u5171\u4eab\u540c\u4e00\u4e2afile table entry\uff08\u56e0\u4e3afile descriptor table\u662f\u6bcf\u4e2a\u8fdb\u7a0b\u79c1\u6709\u7684\uff0c\u6240\u4ee5\u8fd9\u79cd\u60c5\u51b5\u5176\u5b9e\u7c7b\u4f3c\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5\uff0c\u5373\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry\uff09 \u4e0a\u9762\u63cf\u8ff0\u4e86file descriptor\u548cfile table entry\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e0b\u9762\u63cf\u8ff0file table entry\u548ciNode\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a \u662f\u6709\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u4e0d\u540c\u7684file table entry\u6307\u5411\u4e86\u540c\u4e00\u4e2aiNode\u7684\uff1b \u663e\u7136OS\u7684\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u5c31\u5bfc\u81f4\u5f53\u4e00\u4e2a\u6587\u4ef6\u88ab\u591a\u4e2a\u4e0d\u540c\u7684process\u8fdb\u884cshare\u7684\u65f6\u5019\uff0c\u800c\u6bcf\u4e2aprocess\u90fd\u53ef\u4ee5\u6267\u884c\u4e00\u7cfb\u5217\u7684IO\u64cd\u4f5c\uff0c\u8fd9\u5c31\u5bfc\u81f4\u4e86\u53ef\u80fd\u5b58\u5728\u7684\u6570\u636e\u51b2\u7a81\u95ee\u9898\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u6309\u7167OS\u7684\u8fd9\u603b\u7ed3\u6784\u8bbe\u8ba1\uff0c\u4ee5\u53caOS\u63d0\u4f9b\u7684\u5404\u79cd\u64cd\u4f5c\uff0c\u662f\u53ef\u4ee5\u603b\u7ed3\u51fa\u53ef\u80fd\u7684\u6240\u6709\u60c5\u5f62\u7684\uff1b","title":"Overview"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-file-descriptors","text":"The following lists typical operations on file descriptors on modern Unix-like systems. Most of these functions are declared in the <unistd.h> header, but some are in the <fcntl.h> header instead.","title":"Operations on file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#creating-file-descriptors","text":"open () creat() [ 4] socket() accept() socketpair () pipe() opendir() open_by_handle_at() (Linux) signalfd() (Linux) eventfd() (Linux) timerfd_create() (Linux) memfd_create() (Linux) userfaultfd() (Linux)","title":"Creating file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#deriving-file-descriptors","text":"dirfd() fileno()","title":"Deriving file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-a-single-file-descriptor","text":"read (), write () readv(), writev() pread(), pwrite() recv(), send() recvmsg(), sendmsg() (including allowing sending FDs) sendfile() lseek() fstat() fchmod() fchown() fdopen() ftruncate() fsync() fdatasync() fstatvfs() dprintf() vmsplice() (Linux)","title":"Operations on a single file descriptor"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-multiple-file-descriptors","text":"select() , pselect() poll() epoll() (for Linux) kqueue() (for BSD-based systems). sendfile() splice() , tee() (for Linux)","title":"Operations on multiple file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-the-file-descriptor-table","text":"The fcntl() function is used to perform various operations on a file descriptor, depending on the command argument passed to it. There are commands to get and set attributes associated with a file descriptor, including F_GETFD , F_SETFD , F_GETFL and F_SETFL . close() closefrom() (BSD and Solaris only; deletes all file descriptors greater than or equal to specified number) dup() (duplicates an existing file descriptor guaranteeing to be the lowest number available file descriptor) dup2() (the new file descriptor will have the value passed as an argument) fcntl ( F_DUPFD )","title":"Operations on the file descriptor table"},{"location":"Programming/01-philosophy/File-descriptor/#operations-that-modify-process-state","text":"fchdir() (sets the process's current working directory based on a directory file descriptor) mmap() (maps ranges of a file into the process's address space)","title":"Operations that modify process state"},{"location":"Programming/01-philosophy/File-descriptor/#file-locking","text":"flock() fcntl() ( F_GETLK , F_SETLK ) and F_SETLKW lockf()","title":"File locking"},{"location":"Programming/01-philosophy/File-descriptor/#sockets","text":"connect() bind() listen() accept() (creates a new file descriptor for an incoming connection) getsockname() getpeername() getsockopt() setsockopt() shutdown() (shuts down one or both halves of a full duplex connection)","title":"Sockets"},{"location":"Programming/01-philosophy/File-descriptor/#miscellaneous","text":"ioctl() (a large collection of miscellaneous operations on a single file descriptor, often associated with a device)","title":"Miscellaneous"},{"location":"Programming/01-philosophy/File-descriptor/#upcoming-operations","text":"A series of new operations on file descriptors has been added to many modern Unix-like systems, as well as numerous C libraries, to be standardized in a future version of POSIX .[ 5] The at suffix signifies that the function takes an additional first argument supplying a file descriptor from which relative paths are resolved, the forms lacking the at suffix thus becoming equivalent to passing a file descriptor corresponding to the current working directory . The purpose of these new operations is to defend against a certain class of TOCTTOU attacks. openat() faccessat() fchmodat() fchownat() fstatat() futimesat() linkat() mkdirat() mknodat() readlinkat() renameat() symlinkat() unlinkat() mkfifoat() fdopendir()","title":"Upcoming operations"},{"location":"Programming/01-philosophy/File-descriptor/#file-descriptors-as-capabilities","text":"Unix file descriptors behave in many ways as capabilities . They can be passed between processes across Unix domain sockets using the sendmsg() system call. Note, however, that what is actually passed is a reference to an \"open file description\" that has mutable state (the file offset, and the file status and access flags). This complicates the secure use of file descriptors as capabilities, since when programs share access to the same open file description, they can interfere with each other's use of it by changing its offset or whether it is blocking or non-blocking, for example.[ 6] [ 7] In operating systems that are specifically designed as capability systems, there is very rarely any mutable state associated with a capability itself. A Unix process' file descriptor table is an example of a C-list .","title":"File descriptors as capabilities"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/","text":"Everything is a file Why is \u201cEverything is a file\u201d unique to the Unix operating systems? A Event loop Device file Beej's Guide to Network Programming APUE chapter 16 Network IPC: Sockets 20190523 why everything in Unix is a file everything in Unix is file \u548c file API \u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u5f85everything in Unix is file Everything is a file # \"Everything is a file\" describes one of the defining(\u6700\u5178\u578b\u7684) features of Unix , and its derivatives \u2014 that a wide range of input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some inter-process and network communications are simple streams of bytes exposed through the filesystem name space .[ 1] SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u5bf9 \"Everything is a file\" \u542b\u4e49\u7684\u89e3\u91ca\uff1a\u5373\u5c06\u8fd9\u4e9bresource\u90fd\u770b\u505a\u662ffile\uff08 streams of bytes \uff09 SUMMARY : \u4e0a\u8ff0 hard-drives\uff0cmodems\uff0ckeyboards\uff0c\u7b49\u90fd\u662fdevice\uff0c\u663e\u7136\u5728Unix\u4e2d\uff0c\u5b83\u4eec\u90fd\u88ab\u770b\u505a\u6210\u4e86file\uff08 streams of bytes \uff09 The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources. There are a number of file types . When a file is opened, a file descriptor is created. The file path becoming the addressing system and the file descriptor being the byte stream I/O interface. But file descriptors are also created for things like anonymous pipes and network sockets via different methods. So it is more accurate to say \"Everything is a file descriptor\" .[ 2] [ 3] Additionally, a range of pseudo and virtual filesystems exists which exposes information about processes and other system information in a hierarchical file-like structure. These are mounted into the single file hierarchy . An example of this purely virtual filesystem is under /proc that exposes many system properties as files. All of these \"files\" have standard Unix file attributes such as an owner and access permissions , and can be queried by the same classic Unix tools and filters . However, this is not universally considered a fast or portable approach. Some operating systems do not even mount /proc by default due to security or speed concerns.[ 4] It is, though, used heavily by both the widely installed BusyBox [ 5] on embedded systems and by procps, which is used on most Linux systems. In both cases it is used in implementations of process-related POSIX shell commands. It is similarly used on Android systems in the operating system's Toolbox program.[ 6] Unix's successor Plan 9 took this concept into distributed computing with the 9P protocol. Why is \u201cEverything is a file\u201d unique to the Unix operating systems? # A # So, why is this unique to Unix? Typical operating systems, prior to Unix, treated files one way and treated each peripheral device(\u5916\u8bbe) according to the characteristics of that device. That is, if the output of a program was written to a file on disk, that was the only place the output could go; you could not send it to the printer or the tape drive. Each program had to be aware of each device used for input and output, and have command options to deal with alternate I/O devices. Unix treats all devices as files , but with special attributes. To simplify programs, standard input and standard output are the default input and output devices of a program(\u8fd9\u53e5\u8bdd\u89e3\u91ca\u4e86 standard input \uff0c standard output \u7684\u539f\u56e0 ). So program output normally intended for the console screen could go anywhere, to a disk file or a printer or a serial port. This is called I/O redirection . Does other operating systems such as Windows and Macs not operate on files? Of course all modern OSes support various filesystems and can \"operate on files\", but the distinction is how are devices handled? Don't know about Mac, but Windows does offer some I/O redirection. And, compared to what other operating systems is it unique? Not really any more. Linux has the same feature. Of course, if an OS adopts I/O redirection, then it tends to use other Unix features and ends up Unix-like in the end. Event loop # \u5728\u8fd9\u7bc7\u6587\u7ae0\u7684 File_interface \u7ae0\u8282\u5bf9every thing is a file\u8fdb\u884c\u9610\u91ca\uff1b Device file # \u5c06device\u62bd\u8c61\u4e3afile\uff0c\u8fd9\u5c31\u662feverything is a file\u6700\u597d\u7684\u4f53\u73b0\uff1b Beej's Guide to Network Programming # \u5728\u8fd9\u672c\u4e66\u7684\u7b2c\u4e8c\u7ae0 2. What is a socket? \u4e2d\u5bf9everything is a file\u8fdb\u884c\u4e86\u9610\u8ff0\uff1b APUE chapter 16 Network IPC: Sockets # 20190523 # \u6628\u5929\u5728\u9605\u8bfbAPUE\u7684\u7684chapter 16 Network IPC: Sockets\u65f6\uff0c\u6240\u60f3\uff1a everything in Unix is a file\uff0c\u6240\u4ee5\u548c\u6211\u5e94\u8be5\u91c7\u7528\u770b\u5f85\u666e\u901a\u6587\u4ef6\u7684\u65b9\u5f0f\u6765\u770b\u5f85Unix\u7684socket\u3002socket\u548cfile\u4e00\u6837\uff0c\u90fd\u662f\u901a\u8fc7 file descriptor \u6765\u8fdb\u884c\u8bbf\u95ee\u3002POSIX\u4e2d\u63d0\u4f9b\u7684\u64cd\u4f5csocket\u7684\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u90fd\u662f fd \uff0c\u8868\u793a\u8fd9\u4e2asocket\u7684file descriptor\uff0c\u8fd9\u79cd\u505a\u6cd5\u548cfile\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\u3002 socket() \u51fd\u6570\u5c31\u597d\u6bd4 create() \u51fd\u6570\u3002\u5176\u5b9eAPUE\u7684\u4f5c\u8005\u572816.2\u4e2d\u5c31\u5bf9\u6bd4\u4e86Unix\u7684\u9488\u5bf9file\u7684API\u548c\u9488\u5bf9socket\u7684API\u3002 \u5982\u679c\u4ece\u9762\u5411\u5bf9\u8c61\u7684\u89d2\u5ea6\u6765\u6784\u9020POSIX\u7684\u6587\u4ef6api\u548csocket api\u7684\u8bdd\uff0c\u63a5\u53d7file descriptor\u7684api\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u6210\u5458\u51fd\u6570\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u6709\u4e00\u4e2afile descriptor\u3002 \u5728 Beej's Guide to Network Programming \u76842. What is a socket?\u7ae0\u8282\u4e5f\u662f\u4ecefile descriptor\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0socket\u7684\uff1b why everything in Unix is a file # Unix\u662f\u5178\u578b\u7684 Monolithic kernel \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u5c06\u5f88\u591a\u4e1c\u897f\u5c01\u88c5\u597d\u800c\u53ea\u63d0\u4f9b\u4e00\u4e2adescriptor\u6765\u4f9b\u7528\u6237\u4f7f\u7528\uff0c\u8fd9\u4e2adescriptor\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\u5c31\u662ffile descriptor\u3002\u663e\u7136\uff0ceverything in Unix is a file\u662f\u4e00\u79cd\u7b80\u5316\u7684\u62bd\u8c61\uff0c\u5b83\u8ba9\u7528\u6237\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u3002 \u5f53\u7136\uff0c\u4ece\u5185\u6838\u7684\u5b9e\u73b0\u4e0a\u662f\u5426\u771f\u7684\u662f\u5982\u6b64\u6211\u76ee\u524d\u8fd8\u4e0d\u5f97\u800c\u77e5\uff0c\u4f46\u662f\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002 everything in Unix is file \u548c file API # \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0ceverything in Unix is file\u662f\u4e00\u4e2a\u4e2aphilosophy\uff0c\u5b83\u662f\u6982\u5ff5\u4e0a\u7684\uff0c\u5b83\u66f4\u591a\u7684\u662f\u6307\uff1a\u5c06\u5b83\u770b\u505a\u662f\u4e00\u4e2afile\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884cIO\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u662f\u6307\u6211\u4eec\u53ef\u4ee5\u5bf9everything in Unix\u90fd\u4f7f\u7528Unix file\u7684API\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0cAPUE\u768416.2 Socket Descriptors\u8fdb\u884c\u4e86\u4e00\u4e9b\u63cf\u8ff0\uff1b \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 pipe(7) - Linux man page \u7684I/O on pipes and FIFOs\u7ae0\u8282\u4e2d\u63d0\u53ca\uff1a It is not possible to apply lseek(2) to a pipe. \u663e\u7136\uff0c\u6211\u4eec\u53ef\u4ee5 \u8ba4\u4e3a \uff08\u4ece\u903b\u8f91\u4e0a\uff09pipe\u662f\u4e00\u4e2afile\uff0c\u4f46\u662f\u5b83\u5b9e\u9645\u4e0a\u5e76\u4e0d\u662ffile\uff0c\u6240\u4ee5\uff0c\u5e76\u4e0d\u80fd\u591f\u5bf9\u5176\u4f7f\u7528lseek\u7cfb\u7edf\u8c03\u7528\u3002 \u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u5f85everything in Unix is file # \u5f15\u7528\u81ea File descriptor : In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b \u5bf9\u8fd9\u4e2a\u89c2\u70b9\u7684\u9a8c\u8bc1\u5305\u62ec\uff1a EPOLL instance","title":"Unix-philosophy-everything-is-a-file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#everything-is-a-file","text":"\"Everything is a file\" describes one of the defining(\u6700\u5178\u578b\u7684) features of Unix , and its derivatives \u2014 that a wide range of input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some inter-process and network communications are simple streams of bytes exposed through the filesystem name space .[ 1] SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u5bf9 \"Everything is a file\" \u542b\u4e49\u7684\u89e3\u91ca\uff1a\u5373\u5c06\u8fd9\u4e9bresource\u90fd\u770b\u505a\u662ffile\uff08 streams of bytes \uff09 SUMMARY : \u4e0a\u8ff0 hard-drives\uff0cmodems\uff0ckeyboards\uff0c\u7b49\u90fd\u662fdevice\uff0c\u663e\u7136\u5728Unix\u4e2d\uff0c\u5b83\u4eec\u90fd\u88ab\u770b\u505a\u6210\u4e86file\uff08 streams of bytes \uff09 The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources. There are a number of file types . When a file is opened, a file descriptor is created. The file path becoming the addressing system and the file descriptor being the byte stream I/O interface. But file descriptors are also created for things like anonymous pipes and network sockets via different methods. So it is more accurate to say \"Everything is a file descriptor\" .[ 2] [ 3] Additionally, a range of pseudo and virtual filesystems exists which exposes information about processes and other system information in a hierarchical file-like structure. These are mounted into the single file hierarchy . An example of this purely virtual filesystem is under /proc that exposes many system properties as files. All of these \"files\" have standard Unix file attributes such as an owner and access permissions , and can be queried by the same classic Unix tools and filters . However, this is not universally considered a fast or portable approach. Some operating systems do not even mount /proc by default due to security or speed concerns.[ 4] It is, though, used heavily by both the widely installed BusyBox [ 5] on embedded systems and by procps, which is used on most Linux systems. In both cases it is used in implementations of process-related POSIX shell commands. It is similarly used on Android systems in the operating system's Toolbox program.[ 6] Unix's successor Plan 9 took this concept into distributed computing with the 9P protocol.","title":"Everything is a file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#why-is-everything-is-a-file-unique-to-the-unix-operating-systems","text":"","title":"Why is \u201cEverything is a file\u201d unique to the Unix operating systems?"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#a","text":"So, why is this unique to Unix? Typical operating systems, prior to Unix, treated files one way and treated each peripheral device(\u5916\u8bbe) according to the characteristics of that device. That is, if the output of a program was written to a file on disk, that was the only place the output could go; you could not send it to the printer or the tape drive. Each program had to be aware of each device used for input and output, and have command options to deal with alternate I/O devices. Unix treats all devices as files , but with special attributes. To simplify programs, standard input and standard output are the default input and output devices of a program(\u8fd9\u53e5\u8bdd\u89e3\u91ca\u4e86 standard input \uff0c standard output \u7684\u539f\u56e0 ). So program output normally intended for the console screen could go anywhere, to a disk file or a printer or a serial port. This is called I/O redirection . Does other operating systems such as Windows and Macs not operate on files? Of course all modern OSes support various filesystems and can \"operate on files\", but the distinction is how are devices handled? Don't know about Mac, but Windows does offer some I/O redirection. And, compared to what other operating systems is it unique? Not really any more. Linux has the same feature. Of course, if an OS adopts I/O redirection, then it tends to use other Unix features and ends up Unix-like in the end.","title":"A"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#event-loop","text":"\u5728\u8fd9\u7bc7\u6587\u7ae0\u7684 File_interface \u7ae0\u8282\u5bf9every thing is a file\u8fdb\u884c\u9610\u91ca\uff1b","title":"Event loop"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#device-file","text":"\u5c06device\u62bd\u8c61\u4e3afile\uff0c\u8fd9\u5c31\u662feverything is a file\u6700\u597d\u7684\u4f53\u73b0\uff1b","title":"Device file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#beejs-guide-to-network-programming","text":"\u5728\u8fd9\u672c\u4e66\u7684\u7b2c\u4e8c\u7ae0 2. What is a socket? \u4e2d\u5bf9everything is a file\u8fdb\u884c\u4e86\u9610\u8ff0\uff1b","title":"Beej's Guide to Network Programming"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#apue-chapter-16-network-ipc-sockets","text":"","title":"APUE chapter 16 Network IPC: Sockets"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#20190523","text":"\u6628\u5929\u5728\u9605\u8bfbAPUE\u7684\u7684chapter 16 Network IPC: Sockets\u65f6\uff0c\u6240\u60f3\uff1a everything in Unix is a file\uff0c\u6240\u4ee5\u548c\u6211\u5e94\u8be5\u91c7\u7528\u770b\u5f85\u666e\u901a\u6587\u4ef6\u7684\u65b9\u5f0f\u6765\u770b\u5f85Unix\u7684socket\u3002socket\u548cfile\u4e00\u6837\uff0c\u90fd\u662f\u901a\u8fc7 file descriptor \u6765\u8fdb\u884c\u8bbf\u95ee\u3002POSIX\u4e2d\u63d0\u4f9b\u7684\u64cd\u4f5csocket\u7684\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u90fd\u662f fd \uff0c\u8868\u793a\u8fd9\u4e2asocket\u7684file descriptor\uff0c\u8fd9\u79cd\u505a\u6cd5\u548cfile\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\u3002 socket() \u51fd\u6570\u5c31\u597d\u6bd4 create() \u51fd\u6570\u3002\u5176\u5b9eAPUE\u7684\u4f5c\u8005\u572816.2\u4e2d\u5c31\u5bf9\u6bd4\u4e86Unix\u7684\u9488\u5bf9file\u7684API\u548c\u9488\u5bf9socket\u7684API\u3002 \u5982\u679c\u4ece\u9762\u5411\u5bf9\u8c61\u7684\u89d2\u5ea6\u6765\u6784\u9020POSIX\u7684\u6587\u4ef6api\u548csocket api\u7684\u8bdd\uff0c\u63a5\u53d7file descriptor\u7684api\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u6210\u5458\u51fd\u6570\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u6709\u4e00\u4e2afile descriptor\u3002 \u5728 Beej's Guide to Network Programming \u76842. What is a socket?\u7ae0\u8282\u4e5f\u662f\u4ecefile descriptor\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0socket\u7684\uff1b","title":"20190523"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#why-everything-in-unix-is-a-file","text":"Unix\u662f\u5178\u578b\u7684 Monolithic kernel \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u5c06\u5f88\u591a\u4e1c\u897f\u5c01\u88c5\u597d\u800c\u53ea\u63d0\u4f9b\u4e00\u4e2adescriptor\u6765\u4f9b\u7528\u6237\u4f7f\u7528\uff0c\u8fd9\u4e2adescriptor\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\u5c31\u662ffile descriptor\u3002\u663e\u7136\uff0ceverything in Unix is a file\u662f\u4e00\u79cd\u7b80\u5316\u7684\u62bd\u8c61\uff0c\u5b83\u8ba9\u7528\u6237\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u3002 \u5f53\u7136\uff0c\u4ece\u5185\u6838\u7684\u5b9e\u73b0\u4e0a\u662f\u5426\u771f\u7684\u662f\u5982\u6b64\u6211\u76ee\u524d\u8fd8\u4e0d\u5f97\u800c\u77e5\uff0c\u4f46\u662f\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002","title":"why everything in Unix is a file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#everything-in-unix-is-file-file-api","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0ceverything in Unix is file\u662f\u4e00\u4e2a\u4e2aphilosophy\uff0c\u5b83\u662f\u6982\u5ff5\u4e0a\u7684\uff0c\u5b83\u66f4\u591a\u7684\u662f\u6307\uff1a\u5c06\u5b83\u770b\u505a\u662f\u4e00\u4e2afile\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884cIO\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u662f\u6307\u6211\u4eec\u53ef\u4ee5\u5bf9everything in Unix\u90fd\u4f7f\u7528Unix file\u7684API\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0cAPUE\u768416.2 Socket Descriptors\u8fdb\u884c\u4e86\u4e00\u4e9b\u63cf\u8ff0\uff1b \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 pipe(7) - Linux man page \u7684I/O on pipes and FIFOs\u7ae0\u8282\u4e2d\u63d0\u53ca\uff1a It is not possible to apply lseek(2) to a pipe. \u663e\u7136\uff0c\u6211\u4eec\u53ef\u4ee5 \u8ba4\u4e3a \uff08\u4ece\u903b\u8f91\u4e0a\uff09pipe\u662f\u4e00\u4e2afile\uff0c\u4f46\u662f\u5b83\u5b9e\u9645\u4e0a\u5e76\u4e0d\u662ffile\uff0c\u6240\u4ee5\uff0c\u5e76\u4e0d\u80fd\u591f\u5bf9\u5176\u4f7f\u7528lseek\u7cfb\u7edf\u8c03\u7528\u3002","title":"everything in Unix is file \u548c file API"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#kerneleverything-in-unix-is-file","text":"\u5f15\u7528\u81ea File descriptor : In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b \u5bf9\u8fd9\u4e2a\u89c2\u70b9\u7684\u9a8c\u8bc1\u5305\u62ec\uff1a EPOLL instance","title":"\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u5f85everything in Unix is file"},{"location":"Programming/01-philosophy/Unix-philosophy/","text":"Unix philosophy Origin Unix philosophy # The Unix philosophy , originated by Ken Thompson , is a set of cultural norms and philosophical approaches to minimalist , modular software development . It is based on the experience of leading developers of the Unix operating system . Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a \" software tools \" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the \"Unix philosophy.\" The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design . SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u5f3a\u8c03\u4e86Unix philosophy\u7684\u6838\u5fc3\u6240\u5728\uff0c\u5373composability\u3002 Origin # The UNIX philosophy is documented by Doug McIlroy [ 1] in the Bell System Technical Journal from 1978:[ 2] Make each program do one thing well. To do a new job, build a fresh rather than complicate old programs by adding new \"features\". Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently\uff08\u4e25\u683c\u7684\uff09 columnar or binary input formats. Don't insist on interactive input. Design and build software, even\uff08\u751a\u81f3\u662f\uff09 operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them. \u8bbe\u8ba1\u548c\u6784\u5efa\u8f6f\u4ef6\uff0c\u751a\u81f3\u662f\u64cd\u4f5c\u7cfb\u7edf\uff0c\u8981\u53ca\u65e9\u5c1d\u8bd5\uff0c\u6700\u597d\u5728\u51e0\u5468\u5185\u5b8c\u6210\u3002 \u4e0d\u8981\u72b9\u8c6b\u6254\u6389\u7b28\u62d9\u7684\u90e8\u5206\u5e76\u91cd\u5efa\u5b83\u4eec\u3002 Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them. \u4f7f\u7528\u5de5\u5177\u4f18\u5148\u4e8e\u4e0d\u719f\u7ec3\u7684\u5e2e\u52a9\u6765\u51cf\u8f7b\u7f16\u7a0b\u4efb\u52a1\uff0c\u5373\u4f7f\u4f60\u4e0d\u5f97\u4e0d\u7ed5\u9053\u53bb\u6784\u5efa\u5de5\u5177\u5e76\u671f\u671b\u5728\u4f60\u4f7f\u7528\u5b83\u4eec\u4e4b\u540e\u629b\u51fa\u4e00\u4e9b\u5de5\u5177\u3002 It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):[ 1] Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. In their award-winning Unix paper of 1974, Ritchie and Thompson quote the following design considerations:[ 3] Make it easy to write, test, and run programs. Interactive use instead of batch processing . Economy and elegance of design due to size constraints (\"salvation through suffering\"). Self-supporting system: all Unix software is maintained under Unix. The whole philosophy of UNIX seems to stay out of assembler . \u2014\u2009 Michael Sean Mahoney [ 4]","title":"Unix-philosophy"},{"location":"Programming/01-philosophy/Unix-philosophy/#unix-philosophy","text":"The Unix philosophy , originated by Ken Thompson , is a set of cultural norms and philosophical approaches to minimalist , modular software development . It is based on the experience of leading developers of the Unix operating system . Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a \" software tools \" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the \"Unix philosophy.\" The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design . SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u5f3a\u8c03\u4e86Unix philosophy\u7684\u6838\u5fc3\u6240\u5728\uff0c\u5373composability\u3002","title":"Unix philosophy"},{"location":"Programming/01-philosophy/Unix-philosophy/#origin","text":"The UNIX philosophy is documented by Doug McIlroy [ 1] in the Bell System Technical Journal from 1978:[ 2] Make each program do one thing well. To do a new job, build a fresh rather than complicate old programs by adding new \"features\". Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently\uff08\u4e25\u683c\u7684\uff09 columnar or binary input formats. Don't insist on interactive input. Design and build software, even\uff08\u751a\u81f3\u662f\uff09 operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them. \u8bbe\u8ba1\u548c\u6784\u5efa\u8f6f\u4ef6\uff0c\u751a\u81f3\u662f\u64cd\u4f5c\u7cfb\u7edf\uff0c\u8981\u53ca\u65e9\u5c1d\u8bd5\uff0c\u6700\u597d\u5728\u51e0\u5468\u5185\u5b8c\u6210\u3002 \u4e0d\u8981\u72b9\u8c6b\u6254\u6389\u7b28\u62d9\u7684\u90e8\u5206\u5e76\u91cd\u5efa\u5b83\u4eec\u3002 Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them. \u4f7f\u7528\u5de5\u5177\u4f18\u5148\u4e8e\u4e0d\u719f\u7ec3\u7684\u5e2e\u52a9\u6765\u51cf\u8f7b\u7f16\u7a0b\u4efb\u52a1\uff0c\u5373\u4f7f\u4f60\u4e0d\u5f97\u4e0d\u7ed5\u9053\u53bb\u6784\u5efa\u5de5\u5177\u5e76\u671f\u671b\u5728\u4f60\u4f7f\u7528\u5b83\u4eec\u4e4b\u540e\u629b\u51fa\u4e00\u4e9b\u5de5\u5177\u3002 It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):[ 1] Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. In their award-winning Unix paper of 1974, Ritchie and Thompson quote the following design considerations:[ 3] Make it easy to write, test, and run programs. Interactive use instead of batch processing . Economy and elegance of design due to size constraints (\"salvation through suffering\"). Self-supporting system: all Unix software is maintained under Unix. The whole philosophy of UNIX seems to stay out of assembler . \u2014\u2009 Michael Sean Mahoney [ 4]","title":"Origin"},{"location":"Programming/02-Process/","text":"\u524d\u8a00 # \u4e0eprocess\u76f8\u5173\u7684\u4e3b\u9898\u3002","title":"Introduction"},{"location":"Programming/02-Process/#_1","text":"\u4e0eprocess\u76f8\u5173\u7684\u4e3b\u9898\u3002","title":"\u524d\u8a00"},{"location":"Programming/02-Process/IPC/POSIX-message-queues/","text":"MQ_OVERVIEW(7) #","title":"POSIX-message-queues"},{"location":"Programming/02-Process/IPC/POSIX-message-queues/#mq_overview7","text":"","title":"MQ_OVERVIEW(7)"},{"location":"Programming/02-Process/IPC/POSIX-semaphores/","text":"SEM_OVERVIEW(7) #","title":"POSIX-semaphores"},{"location":"Programming/02-Process/IPC/POSIX-semaphores/#sem_overview7","text":"","title":"SEM_OVERVIEW(7)"},{"location":"Programming/02-Process/IPC/System-V-IPC/System-V-IPC/","text":"SVIPC(7) #","title":"System-V-IPC"},{"location":"Programming/02-Process/IPC/System-V-IPC/System-V-IPC/#svipc7","text":"","title":"SVIPC(7)"},{"location":"Programming/02-Process/Thread/pthreads/","text":"PTHREADS(7) # NAME # pthreads - POSIX threads DESCRIPTION # POSIX.1 specifies a set of interfaces (functions, header files) for threaded programming commonly known as POSIX threads, or Pthreads. A single process can contain multiple threads, all of which are executing the same program. These threads share the same global memory (data and heap segments), but each thread has its own stack (automatic variables). POSIX.1 also requires that threads share a range of other attributes (i.e., these attributes are process-wide rather than per-thread): process ID parent process ID process group ID and session ID controlling terminal user and group IDs open file descriptors record locks (see fcntl(2) ) signal dispositions file mode creation mask ( umask(2) ) current directory ( chdir(2) ) and root directory ( chroot(2) ) interval timers ( setitimer(2) ) and POSIX timers ( timer_create(2) ) nice value ( setpriority(2) ) resource limits ( setrlimit(2) ) measurements of the consumption of CPU time ( times(2) ) and resources ( getrusage(2) ) SUMMARY : \u5982\u4f55\u7edf\u8ba1the consumption of CPU of a thread\uff1f\u53c2\u89c1 PTHREAD_GETCPUCLOCKID(3) As well as the stack, POSIX.1 specifies that various other attributes are distinct for each thread, including: thread ID (the pthread_t data type) signal mask ( pthread_sigmask(3) ) the errno variable alternate signal stack ( sigaltstack(2) ) real-time scheduling policy and priority ( sched(7) ) The following Linux-specific features are also per-thread: capabilities (see capabilities(7) ) CPU affinity ( sched_setaffinity(2) ) Pthreads function return values # Most pthreads functions return 0 on success, and an error number on failure. Note that the pthreads functions do not set errno . For each of the pthreads functions that can return an error, POSIX.1-2001 specifies that the function can never fail with the error EINTR . SUMMARY : linux system call\u662f\u4f1a\u8bbe\u7f6e errno \u7684\u3002 Thread IDs # Each of the threads in a process has a unique thread identifier (stored in the type pthread_t). This identifier is returned to the caller of pthread_create(3) , and a thread can obtain its own thread identifier using pthread_self(3) . Thread IDs are guaranteed to be unique only within a process. (In all pthreads functions that accept a thread ID as an argument, that ID by definition refers to a thread in the same process as the caller.) The system may reuse a thread ID after a terminated thread has been joined, or a detached thread has terminated. POSIX says: \"If an application attempts to use a thread ID whose lifetime has ended, the behavior is undefined.\" Thread-safe functions # Async-cancel-safe functions # An async-cancel-safe function is one that can be safely called in an application where asynchronous cancelability is enabled (see pthread_setcancelstate(3) ). Only the following functions are required to be async-cancel-safe by POSIX.1-2001 and POSIX.1-2008: pthread_cancel() pthread_setcancelstate() pthread_setcanceltype() Cancellation points # POSIX.1 specifies that certain functions must, and certain other functions may, be cancellation points . If a thread is cancelable, its cancelability type is deferred , and a cancellation request is pending for the thread, then the thread is canceled when it calls a function that is a cancellation point . The following functions are required to be cancellation points by POSIX.1-2001 and/or POSIX.1-2008: accept() aio_suspend() clock_nanosleep() close() connect() creat() fcntl() F_SETLKW fdatasync() fsync() getmsg() getpmsg() lockf() F_LOCK mq_receive() mq_send() mq_timedreceive() mq_timedsend() msgrcv() msgsnd() msync() nanosleep() open() openat() [Added in POSIX.1-2008] pause() poll() pread() pselect() pthread_cond_timedwait() pthread_cond_wait() pthread_join() pthread_testcancel() putmsg() putpmsg() pwrite() read() readv() recv() recvfrom() recvmsg() select() sem_timedwait() sem_wait() send() sendmsg() sendto() sigpause() [POSIX.1-2001 only (moves to \"may\" list in POSIX.1-2008)] sigsuspend() sigtimedwait() sigwait() sigwaitinfo() sleep() system() tcdrain() usleep() [POSIX.1-2001 only (function removed in POSIX.1-2008)] wait() waitid() waitpid() write() writev() The following functions may be cancellation points according to POSIX.1-2001 and/or POSIX.1-2008: access() asctime() asctime_r() catclose() catgets() catopen() chmod() [Added in POSIX.1-2008] chown() [Added in POSIX.1-2008] closedir() closelog() ctermid() ctime() ctime_r() dbm_close() dbm_delete() dbm_fetch() dbm_nextkey() dbm_open() dbm_store() dlclose() dlopen() dprintf() [Added in POSIX.1-2008] endgrent() endhostent() endnetent() endprotoent() endpwent() endservent() endutxent() faccessat() [Added in POSIX.1-2008] fchmod() [Added in POSIX.1-2008] fchmodat() [Added in POSIX.1-2008] fchown() [Added in POSIX.1-2008] fchownat() [Added in POSIX.1-2008] fclose() fcntl() (for any value of cmd argument) fflush() fgetc() fgetpos() fgets() fgetwc() fgetws() fmtmsg() fopen() fpathconf() fprintf() fputc() fputs() fputwc() fputws() fread() freopen() fscanf() fseek() fseeko() fsetpos() fstat() fstatat() [Added in POSIX.1-2008] ftell() ftello() ftw() futimens() [Added in POSIX.1-2008] fwprintf() fwrite() fwscanf() getaddrinfo() getc() getc_unlocked() getchar() getchar_unlocked() getcwd() getdate() getdelim() [Added in POSIX.1-2008] getgrent() getgrgid() getgrgid_r() getgrnam() getgrnam_r() gethostbyaddr() [SUSv3 only (function removed in POSIX.1-2008)] gethostbyname() [SUSv3 only (function removed in POSIX.1-2008)] gethostent() gethostid() gethostname() getline() [Added in POSIX.1-2008] getlogin() getlogin_r() getnameinfo() getnetbyaddr() getnetbyname() getnetent() getopt() (if opterr is nonzero) getprotobyname() getprotobynumber() getprotoent() getpwent() getpwnam() getpwnam_r() getpwuid() getpwuid_r() gets() getservbyname() getservbyport() getservent() getutxent() getutxid() getutxline() getwc() getwchar() getwd() [SUSv3 only (function removed in POSIX.1-2008)] glob() iconv_close() iconv_open() ioctl() link() linkat() [Added in POSIX.1-2008] lio_listio() [Added in POSIX.1-2008] localtime() localtime_r() lockf() [Added in POSIX.1-2008] lseek() lstat() mkdir() [Added in POSIX.1-2008] mkdirat() [Added in POSIX.1-2008] mkdtemp() [Added in POSIX.1-2008] mkfifo() [Added in POSIX.1-2008] mkfifoat() [Added in POSIX.1-2008] mknod() [Added in POSIX.1-2008] mknodat() [Added in POSIX.1-2008] mkstemp() mktime() nftw() opendir() openlog() pathconf() pclose() perror() popen() posix_fadvise() posix_fallocate() posix_madvise() posix_openpt() posix_spawn() posix_spawnp() posix_trace_clear() posix_trace_close() posix_trace_create() posix_trace_create_withlog() posix_trace_eventtypelist_getnext_id() posix_trace_eventtypelist_rewind() posix_trace_flush() posix_trace_get_attr() posix_trace_get_filter() posix_trace_get_status() posix_trace_getnext_event() posix_trace_open() posix_trace_rewind() posix_trace_set_filter() posix_trace_shutdown() posix_trace_timedgetnext_event() posix_typed_mem_open() printf() psiginfo() [Added in POSIX.1-2008] psignal() [Added in POSIX.1-2008] pthread_rwlock_rdlock() pthread_rwlock_timedrdlock() pthread_rwlock_timedwrlock() pthread_rwlock_wrlock() putc() putc_unlocked() putchar() putchar_unlocked() puts() pututxline() putwc() putwchar() readdir() readdir_r() readlink() [Added in POSIX.1-2008] readlinkat() [Added in POSIX.1-2008] remove() rename() renameat() [Added in POSIX.1-2008] rewind() rewinddir() scandir() [Added in POSIX.1-2008] scanf() seekdir() semop() setgrent() sethostent() setnetent() setprotoent() setpwent() setservent() setutxent() sigpause() [Added in POSIX.1-2008] stat() strerror() strerror_r() strftime() symlink() symlinkat() [Added in POSIX.1-2008] sync() syslog() tmpfile() tmpnam() ttyname() ttyname_r() tzset() ungetc() ungetwc() unlink() unlinkat() [Added in POSIX.1-2008] utime() [Added in POSIX.1-2008] utimensat() [Added in POSIX.1-2008] utimes() [Added in POSIX.1-2008] vdprintf() [Added in POSIX.1-2008] vfprintf() vfwprintf() vprintf() vwprintf() wcsftime() wordexp() wprintf() wscanf() An implementation may also mark other functions not specified in the standard as cancellation points . In particular, an implementation is likely to mark any nonstandard function that may block as a cancellation point . (This includes most functions that can touch files.) Linux implementations of POSIX threads # Over time, two threading implementations have been provided by the GNU C library on Linux: LinuxThreads This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. NPTL (Native POSIX Threads Library) This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity . Both threading implementations employ the Linux clone(2) system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex(2) system call.","title":"pthreads"},{"location":"Programming/02-Process/Thread/pthreads/#pthreads7","text":"","title":"PTHREADS(7)"},{"location":"Programming/02-Process/Thread/pthreads/#name","text":"pthreads - POSIX threads","title":"NAME"},{"location":"Programming/02-Process/Thread/pthreads/#description","text":"POSIX.1 specifies a set of interfaces (functions, header files) for threaded programming commonly known as POSIX threads, or Pthreads. A single process can contain multiple threads, all of which are executing the same program. These threads share the same global memory (data and heap segments), but each thread has its own stack (automatic variables). POSIX.1 also requires that threads share a range of other attributes (i.e., these attributes are process-wide rather than per-thread): process ID parent process ID process group ID and session ID controlling terminal user and group IDs open file descriptors record locks (see fcntl(2) ) signal dispositions file mode creation mask ( umask(2) ) current directory ( chdir(2) ) and root directory ( chroot(2) ) interval timers ( setitimer(2) ) and POSIX timers ( timer_create(2) ) nice value ( setpriority(2) ) resource limits ( setrlimit(2) ) measurements of the consumption of CPU time ( times(2) ) and resources ( getrusage(2) ) SUMMARY : \u5982\u4f55\u7edf\u8ba1the consumption of CPU of a thread\uff1f\u53c2\u89c1 PTHREAD_GETCPUCLOCKID(3) As well as the stack, POSIX.1 specifies that various other attributes are distinct for each thread, including: thread ID (the pthread_t data type) signal mask ( pthread_sigmask(3) ) the errno variable alternate signal stack ( sigaltstack(2) ) real-time scheduling policy and priority ( sched(7) ) The following Linux-specific features are also per-thread: capabilities (see capabilities(7) ) CPU affinity ( sched_setaffinity(2) )","title":"DESCRIPTION"},{"location":"Programming/02-Process/Thread/pthreads/#pthreads-function-return-values","text":"Most pthreads functions return 0 on success, and an error number on failure. Note that the pthreads functions do not set errno . For each of the pthreads functions that can return an error, POSIX.1-2001 specifies that the function can never fail with the error EINTR . SUMMARY : linux system call\u662f\u4f1a\u8bbe\u7f6e errno \u7684\u3002","title":"Pthreads function return values"},{"location":"Programming/02-Process/Thread/pthreads/#thread-ids","text":"Each of the threads in a process has a unique thread identifier (stored in the type pthread_t). This identifier is returned to the caller of pthread_create(3) , and a thread can obtain its own thread identifier using pthread_self(3) . Thread IDs are guaranteed to be unique only within a process. (In all pthreads functions that accept a thread ID as an argument, that ID by definition refers to a thread in the same process as the caller.) The system may reuse a thread ID after a terminated thread has been joined, or a detached thread has terminated. POSIX says: \"If an application attempts to use a thread ID whose lifetime has ended, the behavior is undefined.\"","title":"Thread IDs"},{"location":"Programming/02-Process/Thread/pthreads/#thread-safe-functions","text":"","title":"Thread-safe functions"},{"location":"Programming/02-Process/Thread/pthreads/#async-cancel-safe-functions","text":"An async-cancel-safe function is one that can be safely called in an application where asynchronous cancelability is enabled (see pthread_setcancelstate(3) ). Only the following functions are required to be async-cancel-safe by POSIX.1-2001 and POSIX.1-2008: pthread_cancel() pthread_setcancelstate() pthread_setcanceltype()","title":"Async-cancel-safe functions"},{"location":"Programming/02-Process/Thread/pthreads/#cancellation-points","text":"POSIX.1 specifies that certain functions must, and certain other functions may, be cancellation points . If a thread is cancelable, its cancelability type is deferred , and a cancellation request is pending for the thread, then the thread is canceled when it calls a function that is a cancellation point . The following functions are required to be cancellation points by POSIX.1-2001 and/or POSIX.1-2008: accept() aio_suspend() clock_nanosleep() close() connect() creat() fcntl() F_SETLKW fdatasync() fsync() getmsg() getpmsg() lockf() F_LOCK mq_receive() mq_send() mq_timedreceive() mq_timedsend() msgrcv() msgsnd() msync() nanosleep() open() openat() [Added in POSIX.1-2008] pause() poll() pread() pselect() pthread_cond_timedwait() pthread_cond_wait() pthread_join() pthread_testcancel() putmsg() putpmsg() pwrite() read() readv() recv() recvfrom() recvmsg() select() sem_timedwait() sem_wait() send() sendmsg() sendto() sigpause() [POSIX.1-2001 only (moves to \"may\" list in POSIX.1-2008)] sigsuspend() sigtimedwait() sigwait() sigwaitinfo() sleep() system() tcdrain() usleep() [POSIX.1-2001 only (function removed in POSIX.1-2008)] wait() waitid() waitpid() write() writev() The following functions may be cancellation points according to POSIX.1-2001 and/or POSIX.1-2008: access() asctime() asctime_r() catclose() catgets() catopen() chmod() [Added in POSIX.1-2008] chown() [Added in POSIX.1-2008] closedir() closelog() ctermid() ctime() ctime_r() dbm_close() dbm_delete() dbm_fetch() dbm_nextkey() dbm_open() dbm_store() dlclose() dlopen() dprintf() [Added in POSIX.1-2008] endgrent() endhostent() endnetent() endprotoent() endpwent() endservent() endutxent() faccessat() [Added in POSIX.1-2008] fchmod() [Added in POSIX.1-2008] fchmodat() [Added in POSIX.1-2008] fchown() [Added in POSIX.1-2008] fchownat() [Added in POSIX.1-2008] fclose() fcntl() (for any value of cmd argument) fflush() fgetc() fgetpos() fgets() fgetwc() fgetws() fmtmsg() fopen() fpathconf() fprintf() fputc() fputs() fputwc() fputws() fread() freopen() fscanf() fseek() fseeko() fsetpos() fstat() fstatat() [Added in POSIX.1-2008] ftell() ftello() ftw() futimens() [Added in POSIX.1-2008] fwprintf() fwrite() fwscanf() getaddrinfo() getc() getc_unlocked() getchar() getchar_unlocked() getcwd() getdate() getdelim() [Added in POSIX.1-2008] getgrent() getgrgid() getgrgid_r() getgrnam() getgrnam_r() gethostbyaddr() [SUSv3 only (function removed in POSIX.1-2008)] gethostbyname() [SUSv3 only (function removed in POSIX.1-2008)] gethostent() gethostid() gethostname() getline() [Added in POSIX.1-2008] getlogin() getlogin_r() getnameinfo() getnetbyaddr() getnetbyname() getnetent() getopt() (if opterr is nonzero) getprotobyname() getprotobynumber() getprotoent() getpwent() getpwnam() getpwnam_r() getpwuid() getpwuid_r() gets() getservbyname() getservbyport() getservent() getutxent() getutxid() getutxline() getwc() getwchar() getwd() [SUSv3 only (function removed in POSIX.1-2008)] glob() iconv_close() iconv_open() ioctl() link() linkat() [Added in POSIX.1-2008] lio_listio() [Added in POSIX.1-2008] localtime() localtime_r() lockf() [Added in POSIX.1-2008] lseek() lstat() mkdir() [Added in POSIX.1-2008] mkdirat() [Added in POSIX.1-2008] mkdtemp() [Added in POSIX.1-2008] mkfifo() [Added in POSIX.1-2008] mkfifoat() [Added in POSIX.1-2008] mknod() [Added in POSIX.1-2008] mknodat() [Added in POSIX.1-2008] mkstemp() mktime() nftw() opendir() openlog() pathconf() pclose() perror() popen() posix_fadvise() posix_fallocate() posix_madvise() posix_openpt() posix_spawn() posix_spawnp() posix_trace_clear() posix_trace_close() posix_trace_create() posix_trace_create_withlog() posix_trace_eventtypelist_getnext_id() posix_trace_eventtypelist_rewind() posix_trace_flush() posix_trace_get_attr() posix_trace_get_filter() posix_trace_get_status() posix_trace_getnext_event() posix_trace_open() posix_trace_rewind() posix_trace_set_filter() posix_trace_shutdown() posix_trace_timedgetnext_event() posix_typed_mem_open() printf() psiginfo() [Added in POSIX.1-2008] psignal() [Added in POSIX.1-2008] pthread_rwlock_rdlock() pthread_rwlock_timedrdlock() pthread_rwlock_timedwrlock() pthread_rwlock_wrlock() putc() putc_unlocked() putchar() putchar_unlocked() puts() pututxline() putwc() putwchar() readdir() readdir_r() readlink() [Added in POSIX.1-2008] readlinkat() [Added in POSIX.1-2008] remove() rename() renameat() [Added in POSIX.1-2008] rewind() rewinddir() scandir() [Added in POSIX.1-2008] scanf() seekdir() semop() setgrent() sethostent() setnetent() setprotoent() setpwent() setservent() setutxent() sigpause() [Added in POSIX.1-2008] stat() strerror() strerror_r() strftime() symlink() symlinkat() [Added in POSIX.1-2008] sync() syslog() tmpfile() tmpnam() ttyname() ttyname_r() tzset() ungetc() ungetwc() unlink() unlinkat() [Added in POSIX.1-2008] utime() [Added in POSIX.1-2008] utimensat() [Added in POSIX.1-2008] utimes() [Added in POSIX.1-2008] vdprintf() [Added in POSIX.1-2008] vfprintf() vfwprintf() vprintf() vwprintf() wcsftime() wordexp() wprintf() wscanf() An implementation may also mark other functions not specified in the standard as cancellation points . In particular, an implementation is likely to mark any nonstandard function that may block as a cancellation point . (This includes most functions that can touch files.)","title":"Cancellation points"},{"location":"Programming/02-Process/Thread/pthreads/#linux-implementations-of-posix-threads","text":"Over time, two threading implementations have been provided by the GNU C library on Linux: LinuxThreads This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. NPTL (Native POSIX Threads Library) This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity . Both threading implementations employ the Linux clone(2) system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex(2) system call.","title":"Linux implementations of POSIX threads"},{"location":"Programming/03-IO/","text":"","title":"Introduction"},{"location":"Programming/03-Time/","text":"","title":"Introduction"},{"location":"Programming/04-Signal/","text":"Signal # \u5185\u5bb9\uff1a APUE # Chapter 10 Signals # signal\u662f\u4ec0\u4e48\uff1f 10.2 Signal Concepts\uff1aSignals are software interrupts. \u4fe1\u53f7\u7684\u6765\u6e90\uff1f 10.2 Signal Concepts \u8fd9\u4e00\u8282\u5bf9\u4fe1\u53f7\u6765\u6e90\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002\u53ef\u4ee5\u770b\u5230signal\u7684\u6765\u6e90\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\u3002\u5728\u9605\u8bfb Understanding.The.Linux.kernel.3rd.Edition \u7684Chapter 4. Interrupts and Exceptions\u65f6\uff0c\u6211\u601d\u8003\u4e86\u5982\u4e0b\u95ee\u9898\uff1aUnix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u663e\u7136\u4e0d\u662f\u7684\uff0c\u6bd4\u5982 SIGINT \u5c31\u4e0d\u662f\u6e90\u81ea\u4e8eexception\u3002\u5728\u672c\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u6240\u4ee5\u6211\u4eec\u5e94\u8be5\u6e05\u695a\uff0cexception\u662f\u4fe1\u53f7\u7684\u4f17\u591a\u6765\u6e90\u4e2d\u7684\u4e00\u79cd\u3002 Understanding.The.Linux.kernel.3rd.Edition # Chapter 4. Interrupts and Exceptions # Chapter 11. Signals # SIGNAL(7) #","title":"Introduction"},{"location":"Programming/04-Signal/#signal","text":"\u5185\u5bb9\uff1a","title":"Signal"},{"location":"Programming/04-Signal/#apue","text":"","title":"APUE"},{"location":"Programming/04-Signal/#chapter-10-signals","text":"signal\u662f\u4ec0\u4e48\uff1f 10.2 Signal Concepts\uff1aSignals are software interrupts. \u4fe1\u53f7\u7684\u6765\u6e90\uff1f 10.2 Signal Concepts \u8fd9\u4e00\u8282\u5bf9\u4fe1\u53f7\u6765\u6e90\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002\u53ef\u4ee5\u770b\u5230signal\u7684\u6765\u6e90\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\u3002\u5728\u9605\u8bfb Understanding.The.Linux.kernel.3rd.Edition \u7684Chapter 4. Interrupts and Exceptions\u65f6\uff0c\u6211\u601d\u8003\u4e86\u5982\u4e0b\u95ee\u9898\uff1aUnix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u663e\u7136\u4e0d\u662f\u7684\uff0c\u6bd4\u5982 SIGINT \u5c31\u4e0d\u662f\u6e90\u81ea\u4e8eexception\u3002\u5728\u672c\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u6240\u4ee5\u6211\u4eec\u5e94\u8be5\u6e05\u695a\uff0cexception\u662f\u4fe1\u53f7\u7684\u4f17\u591a\u6765\u6e90\u4e2d\u7684\u4e00\u79cd\u3002","title":"Chapter 10 Signals"},{"location":"Programming/04-Signal/#understandingthelinuxkernel3rdedition","text":"","title":"Understanding.The.Linux.kernel.3rd.Edition"},{"location":"Programming/04-Signal/#chapter-4-interrupts-and-exceptions","text":"","title":"Chapter 4. Interrupts and Exceptions"},{"location":"Programming/04-Signal/#chapter-11-signals","text":"","title":"Chapter 11. Signals"},{"location":"Programming/04-Signal/#signal7","text":"","title":"SIGNAL(7)"},{"location":"Programming/05-Linux-Virtualization/","text":"\u524d\u8a00 # \u672c\u7ae0\u4e3b\u8981\u4ecb\u7ecd virtualization \u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8linux OS-level virtualization \u3002 \u7136\u540e\u91cd\u70b9\u4ecb\u7ecdlinux kernel\u662f\u5982\u4f55\u5b9e\u73b0 OS-level virtualization \u7684\u3002","title":"Introduction"},{"location":"Programming/05-Linux-Virtualization/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u4ecb\u7ecd virtualization \u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8linux OS-level virtualization \u3002 \u7136\u540e\u91cd\u70b9\u4ecb\u7ecdlinux kernel\u662f\u5982\u4f55\u5b9e\u73b0 OS-level virtualization \u7684\u3002","title":"\u524d\u8a00"},{"location":"Programming/05-Linux-Virtualization/Linux-containers/","text":"Linux containers # LXC #","title":"Linux-containers"},{"location":"Programming/05-Linux-Virtualization/Linux-containers/#linux-containers","text":"","title":"Linux containers"},{"location":"Programming/05-Linux-Virtualization/Linux-containers/#lxc","text":"","title":"LXC"},{"location":"Programming/05-Linux-Virtualization/Virtualization/","text":"Virtualization # NOTE: \u4e0b\u9762\u4ecb\u7ecd\u5404\u4e2a\u5c42\u7ea7\u7684\u7684virtualization Hardware virtualization # Main article: Hardware virtualization See also: Mobile virtualization Desktop virtualization # Main article: Desktop virtualization Containerization # Main article: Operating-system-level virtualization NOTE: \u8fd9\u662f\u672c\u7ae0\u4e3b\u8981\u5173\u6ce8\u7684 OS-level virtualization # OS-level virtualization refers to an operating system paradigm in which the kernel allows the existence of multiple isolated user space instances. Such instances, called containers ( Solaris , Docker ), Zones ( Solaris ), virtual private servers ( OpenVZ ), partitions , virtual environments (VEs), virtual kernel ( DragonFly BSD ), or jails ( FreeBSD jail or chroot jail ),[ 1] may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares , CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container. NOTE: \u5e38\u5e38\u542c\u5230\u7684 Docker \uff0c container \u6240\u4f7f\u7528\u7684\u5c31\u662f OS-level virtualization On Unix-like operating systems, this feature can be seen as an advanced implementation of the standard chroot mechanism, which changes the apparent root folder for the current running process and its children. In addition to isolation mechanisms, the kernel often provides resource-management features to limit the impact of one container's activities on other containers. NOTE: linux kernel\u7279\u6027 Linux namespaces \u7528\u4e8e\u652f\u6301isolation\uff1b linux kernel\u7279\u6027 Linux control groups \u7528\u4e8e\u652f\u6301 resource-management \uff1b The term \"container,\" while most popularly referring to OS-level virtualization systems, is sometimes ambiguously used to refer to fuller virtual machine environments operating in varying degrees of concert with the host OS, e.g. Microsoft's \" Hyper-V Containers.\" List of Linux containers #","title":"Virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#virtualization","text":"NOTE: \u4e0b\u9762\u4ecb\u7ecd\u5404\u4e2a\u5c42\u7ea7\u7684\u7684virtualization","title":"Virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#hardware-virtualization","text":"Main article: Hardware virtualization See also: Mobile virtualization","title":"Hardware virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#desktop-virtualization","text":"Main article: Desktop virtualization","title":"Desktop virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#containerization","text":"Main article: Operating-system-level virtualization NOTE: \u8fd9\u662f\u672c\u7ae0\u4e3b\u8981\u5173\u6ce8\u7684","title":"Containerization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#os-level-virtualization","text":"OS-level virtualization refers to an operating system paradigm in which the kernel allows the existence of multiple isolated user space instances. Such instances, called containers ( Solaris , Docker ), Zones ( Solaris ), virtual private servers ( OpenVZ ), partitions , virtual environments (VEs), virtual kernel ( DragonFly BSD ), or jails ( FreeBSD jail or chroot jail ),[ 1] may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares , CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container. NOTE: \u5e38\u5e38\u542c\u5230\u7684 Docker \uff0c container \u6240\u4f7f\u7528\u7684\u5c31\u662f OS-level virtualization On Unix-like operating systems, this feature can be seen as an advanced implementation of the standard chroot mechanism, which changes the apparent root folder for the current running process and its children. In addition to isolation mechanisms, the kernel often provides resource-management features to limit the impact of one container's activities on other containers. NOTE: linux kernel\u7279\u6027 Linux namespaces \u7528\u4e8e\u652f\u6301isolation\uff1b linux kernel\u7279\u6027 Linux control groups \u7528\u4e8e\u652f\u6301 resource-management \uff1b The term \"container,\" while most popularly referring to OS-level virtualization systems, is sometimes ambiguously used to refer to fuller virtual machine environments operating in varying degrees of concert with the host OS, e.g. Microsoft's \" Hyper-V Containers.\"","title":"OS-level virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#list-of-linux-containers","text":"","title":"List of Linux containers"},{"location":"Programming/05-Linux-Virtualization/Linux-control-groups/Control-groups/","text":"NAMESPACES(7) # cgroups #","title":"Introduction"},{"location":"Programming/05-Linux-Virtualization/Linux-control-groups/Control-groups/#namespaces7","text":"","title":"NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-control-groups/Control-groups/#cgroups","text":"","title":"cgroups"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/","text":"\u524d\u8a00 # \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7c7b\u6bd4\u7684\u65b9\u6cd5\u6765\u7406\u89e3linux namespace\uff0c\u5373\u4ece\u5176\u4ed6\u4f7f\u7528\u4e86namespace\u7684\u9886\u57df\u6765\u7c7b\u6bd4\u7406\u89e3linux namespace\uff0c\u6bd4\u5982 c++ \u4e2d\u7684 namespace \u3002\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684 namespace \u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u597d\u3002\u663e\u7136\uff0c\u65e0\u8bba\u5728\u54ea\u4e2a\u5c42\u7ea7\uff08programming language\u3001operating system\uff09\uff0c\u4f7f\u7528namespace\u7684\u76ee\u7684\u662f\uff1a separation \u4ee5Hierarchy\u7684\u7ed3\u6784\u6765\u7ec4\u7ec7\u6570\u636e \u5728\u7406\u89e3\u4e86\u4f7f\u7528namespace\u7684\u76ee\u7684\u540e\uff0c\u63a8\u8350\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u8f93\u5165\u6d45\u51fa\uff0c\u9605\u8bfb\u5b8c\u6210\u540e\uff0c\u57fa\u672c\u4e0a\u80fd\u591f\u77e5\u9053linux namespace\u6240\u89e3\u51b3\u7684\u5b9e\u9645\u95ee\u9898\u548c\u5b83\u7684\u4ef7\u503c\u4e86\u3002 \u638c\u63e1\u4e86\u8fd9\u4e9b\u540e\uff0c\u518d\u53bb\u9605\u8bfbman\u4e2d\u5bf9\u5b83\u7684\u89e3\u91ca\u5c31\u4f1a\u975e\u5e38\u5bb9\u6613\u4e86\u3002","title":"Introduction"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/#_1","text":"\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7c7b\u6bd4\u7684\u65b9\u6cd5\u6765\u7406\u89e3linux namespace\uff0c\u5373\u4ece\u5176\u4ed6\u4f7f\u7528\u4e86namespace\u7684\u9886\u57df\u6765\u7c7b\u6bd4\u7406\u89e3linux namespace\uff0c\u6bd4\u5982 c++ \u4e2d\u7684 namespace \u3002\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684 namespace \u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u597d\u3002\u663e\u7136\uff0c\u65e0\u8bba\u5728\u54ea\u4e2a\u5c42\u7ea7\uff08programming language\u3001operating system\uff09\uff0c\u4f7f\u7528namespace\u7684\u76ee\u7684\u662f\uff1a separation \u4ee5Hierarchy\u7684\u7ed3\u6784\u6765\u7ec4\u7ec7\u6570\u636e \u5728\u7406\u89e3\u4e86\u4f7f\u7528namespace\u7684\u76ee\u7684\u540e\uff0c\u63a8\u8350\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u8f93\u5165\u6d45\u51fa\uff0c\u9605\u8bfb\u5b8c\u6210\u540e\uff0c\u57fa\u672c\u4e0a\u80fd\u591f\u77e5\u9053linux namespace\u6240\u89e3\u51b3\u7684\u5b9e\u9645\u95ee\u9898\u548c\u5b83\u7684\u4ef7\u503c\u4e86\u3002 \u638c\u63e1\u4e86\u8fd9\u4e9b\u540e\uff0c\u518d\u53bb\u9605\u8bfbman\u4e2d\u5bf9\u5b83\u7684\u89e3\u91ca\u5c31\u4f1a\u975e\u5e38\u5bb9\u6613\u4e86\u3002","title":"\u524d\u8a00"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/","text":"Linux namespaces # Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces # With the advent of tools like Docker , Linux Containers , and others, it has become super easy to isolate Linux processes into their own little system environments. This makes it possible to run a whole range of applications on a single real Linux machine and ensure no two of them can interfere with each other, without having to resort to using virtual machines . These tools have been a huge boon to PaaS providers. But what exactly happens under the hood? These tools rely on a number of features and components of the Linux kernel. Some of these features were introduced fairly recently, while others still require you to patch the kernel itself. But one of the key components, using Linux namespaces , has been a feature of Linux since version 2.6.24 was released in 2008. Anyone familiar with chroot already has a basic idea of what Linux namespaces can do and how to use namespace generally. Just as chroot allows processes to see any arbitrary directory as the root of the system (independent of the rest of the processes), Linux namespaces allow other aspects of the operating system to be independently modified as well. This includes the process tree , networking interfaces, mount points , inter-process communication resources and more. Why Use Namespaces for Process Isolation? # In a single-user computer, a single system environment may be fine. But on a server, where you want to run multiple services, it is essential to security and stability that the services are as isolated from each other as possible. Imagine a server running multiple services, one of which gets compromised by an intruder. In such a case, the intruder may be able to exploit that service and work his way to the other services, and may even be able compromise the entire server. Namespace isolation can provide a secure environment to eliminate this risk. For example, using namespacing, it is possible to safely execute arbitrary or unknown programs on your server. Recently, there has been a growing number of programming contest and \u201chackathon\u201d platforms, such as HackerRank , TopCoder , Codeforces , and many more. A lot of them utilize automated pipelines to run and validate programs that are submitted by the contestants. It is often impossible to know in advance the true nature of contestants\u2019 programs, and some may even contain malicious elements. By running these programs namespaced in complete isolation from the rest of the system, the software can be tested and validated without putting the rest of the machine at risk. Similarly, online continuous integration services, such as Drone.io , automatically fetch your code repository and execute the test scripts on their own servers. Again, namespace isolation is what makes it possible to provide these services safely. Namespacing tools like Docker also allow better control over processes\u2019 use of system resources, making such tools extremely popular for use by PaaS providers. Services like Heroku and Google App Engine use such tools to isolate and run multiple web server applications on the same real hardware. These tools allow them to run each application (which may have been deployed by any of a number of different users) without worrying about one of them using too many system resources, or interfering and/or conflicting with other deployed services on the same machine. With such process isolation, it is even possible to have entirely different stacks of dependency softwares (and versions) for each isolated environment! If you\u2019ve used tools like Docker, you already know that these tools are capable of isolating processes in small \u201ccontainers\u201d. Running processes in Docker containers is like running them in virtual machines, only these containers are significantly lighter than virtual machines. Process Namespace # Historically, the Linux kernel has maintained a single process tree . The tree contains a reference to every process currently running in a parent-child hierarchy . A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. With the introduction of Linux namespaces , it became possible to have multiple \u201cnested\u201d process trees. Each process tree can have an entirely isolated set of processes. This can ensure that processes belonging to one process tree cannot inspect or kill - in fact cannot even know of the existence of - processes in other sibling or parent process trees. Every time a computer with Linux boots up, it starts with just one process, with process identifier (PID) 1. This process is the root of the process tree , and it initiates the rest of the system by performing the appropriate maintenance work and starting the correct daemons/services. All the other processes start below this process in the tree. The PID namespace allows one to spin off a new tree, with its own PID 1 process . The process that does this remains in the parent namespace , in the original tree, but makes the child the root of its own process tree . With PID namespace isolation , processes in the child namespace have no way of knowing of the parent process\u2019s existence. However, processes in the parent namespace have a complete view of processes in the child namespace , as if they were any other process in the parent namespace . It is possible to create a nested set of child namespaces : one process starts a child process in a new PID namespace, and that child process spawns yet another process in a new PID namespace, and so on. With the introduction of PID namespaces , a single process can now have multiple PIDs associated with it, one for each namespace it falls under. In the Linux source code, we can see that a struct named pid , which used to keep track of just a single PID, now tracks multiple PIDs through the use of a struct named upid : struct upid { int nr; // the PID value struct pid_namespace *ns; // namespace where this PID is relevant // ... }; struct pid { // ... int level; // number of upids struct upid numbers[0]; // array of upids }; To create a new PID namespace , one must call the clone() system call with a special flag CLONE_NEWPID . (C provides a wrapper to expose this system call, and so do many other popular languages.) Whereas the other namespaces discussed below can also be created using the unshare() system call, a PID namespace can only be created at the time a new process is spawned using clone() . Once clone() is called with this flag, the new process immediately starts in a new PID namespace, under a new process tree. This can be demonstrated with a simple C program: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"PID: %ld\\n\", (long)getpid()); return 0; } int main() { pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); printf(\"clone() = %ld\\n\", (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } Compile and run this program with root privileges and you will notice an output that resembles this: clone() = 5304 PID: 1 The PID, as printed from within the child_fn , will be 1 . Try replacing the static int child_fn() function with the following, to print the parent PID from the isolated process\u2019s perspective: static int child_fn() { printf(\"Parent PID: %ld\\n\", (long)getppid()); return 0; } Running the program this time yields the following output: clone() = 11449 Parent PID: 0 Notice how the parent PID from the isolated process\u2019s perspective is 0, indicating no parent. Try running the same program again, but this time, remove the CLONE_NEWPID flag from within the clone() function call: pid_t child_pid = clone(child_fn, child_stack+1048576, SIGCHLD, NULL); This time, you will notice that the parent PID is no longer 0: clone() = 11561 Parent PID: 11560 However, this is just the first step in our tutorial. These processes still have unrestricted access to other common or shared resources. For example, the networking interface: if the child process created above were to listen on port 80, it would prevent every other process on the system from being able to listen on it. Linux Network Namespace # This is where a network namespace becomes useful. A network namespace allows each of these processes to see an entirely different set of networking interfaces . Even the loopback interface is different for each network namespace. Isolating a process into its own network namespace involves introducing another flag to the clone() function call: CLONE_NEWNET ; #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } Output: Original `net` Namespace: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:24:8c:a1:ac:e7 brd ff:ff:ff:ff:ff:ff New `net` Namespace: 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 What\u2019s going on here? The physical ethernet device enp4s0 belongs to the global network namespace , as indicated by the \u201c ip \u201d tool run from this namespace. However, the physical interface is not available in the new network namespace . Moreover, the loopback device is active in the original network namespace , but is \u201cdown\u201d in the child network namespace . In order to provide a usable network interface in the child namespace , it is necessary to set up additional \u201cvirtual\u201d network interfaces which span multiple namespaces. Once that is done, it is then possible to create Ethernet bridges , and even route packets between the namespaces. Finally, to make the whole thing work, a \u201crouting process\u201d must be running in the global network namespace to receive traffic from the physical interface, and route it through the appropriate virtual interfaces to to the correct child network namespaces . Maybe you can see why tools like Docker, which do all this heavy lifting for you, are so popular! To do this by hand, you can create a pair of virtual Ethernet connections between a parent and a child namespace by running a single command from the parent namespace : ip link add name veth0 type veth peer name veth1 netns <pid> Here, <pid> should be replaced by the process ID of the process in the child namespace as observed by the parent. Running this command establishes a pipe-like connection between these two namespaces. The parent namespace retains the veth0 device, and passes the veth1 device to the child namespace. Anything that enters one of the ends, comes out through the other end, just as you would expect from a real Ethernet connection between two real nodes. Accordingly, both sides of this virtual Ethernet connection must be assigned IP addresses. Mount Namespace # Linux also maintains a data structure for all the mountpoints of the system. It includes information like what disk partitions are mounted, where they are mounted, whether they are readonly, et cetera. With Linux namespaces, one can have this data structure cloned, so that processes under different namespaces can change the mountpoints without affecting each other. Creating separate mount namespace has an effect similar to doing a chroot() . chroot() is good, but it does not provide complete isolation, and its effects are restricted to the root mountpoint only. Creating a separate mount namespace allows each of these isolated processes to have a completely different view of the entire system\u2019s mountpoint structure from the original one. This allows you to have a different root for each isolated process, as well as other mountpoints that are specific to those processes. Used with care per this tutorial, you can avoid exposing any information about the underlying system. The clone() flag required to achieve this is CLONE_NEWNS : clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | CLONE_NEWNS | SIGCHLD, NULL) Initially, the child process sees the exact same mountpoints as its parent process would. However, being under a new mount namespace , the child process can mount or unmount whatever endpoints it wants to, and the change will affect neither its parent\u2019s namespace, nor any other mount namespace in the entire system. For example, if the parent process has a particular disk partition mounted at root, the isolated process will see the exact same disk partition mounted at the root in the beginning. But the benefit of isolating the mount namespace is apparent when the isolated process tries to change the root partition to something else, as the change will only affect the isolated mount namespace. Interestingly, this actually makes it a bad idea to spawn the target child process directly with the CLONE_NEWNS flag. A better approach is to start a special \u201cinit\u201d process with the CLONE_NEWNS flag, have that \u201cinit\u201d process change the \u201c/\u201d, \u201c/proc\u201d, \u201c/dev\u201d or other mountpoints as desired, and then start the target process. This is discussed in a little more detail near the end of this namespace tutorial. Other Namespaces # There are other namespaces that these processes can be isolated into, namely user, IPC, and UTS. The user namespace allows a process to have root privileges within the namespace , without giving it that access to processes outside of the namespace. Isolating a process by the IPC namespace gives it its own interprocess communication resources, for example, System V IPC and POSIX messages. The UTS namespace isolates two specific identifiers of the system: nodename and domainname . A quick example to show how UTS namespace is isolated is shown below: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/utsname.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static void print_nodename() { struct utsname utsname; uname(&utsname); printf(\"%s\\n\", utsname.nodename); } static int child_fn() { printf(\"New UTS namespace nodename: \"); print_nodename(); printf(\"Changing nodename inside new UTS namespace\\n\"); sethostname(\"GLaDOS\", 6); printf(\"New UTS namespace nodename: \"); print_nodename(); return 0; } int main() { printf(\"Original UTS namespace nodename: \"); print_nodename(); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWUTS | SIGCHLD, NULL); sleep(1); printf(\"Original UTS namespace nodename: \"); print_nodename(); waitpid(child_pid, NULL, 0); return 0; } This program yields the following output: Original UTS namespace nodename: XT New UTS namespace nodename: XT Changing nodename inside new UTS namespace New UTS namespace nodename: GLaDOS Original UTS namespace nodename: XT Here, child_fn() prints the nodename , changes it to something else, and prints it again. Naturally, the change happens only inside the new UTS namespace. More information on what all of the namespaces provide and isolate can be found in the tutorial here Cross-Namespace Communication # Often it is necessary to establish some sort of communication between the parent and the child namespace. This might be for doing configuration work within an isolated environment, or it can simply be to retain the ability to peek into the condition of that environment from outside. One way of doing that is to keep an SSH daemon running within that environment. You can have a separate SSH daemon inside each network namespace. However, having multiple SSH daemons running uses a lot of valuable resources like memory. This is where having a special \u201cinit\u201d process proves to be a good idea again. The \u201cinit\u201d process can establish a communication channel between the parent namespace and the child namespace. This channel can be based on UNIX sockets or can even use TCP. To create a UNIX socket that spans two different mount namespaces, you need to first create the child process, then create the UNIX socket, and then isolate the child into a separate mount namespace. But how can we create the process first, and isolate it later? Linux provides unshare() . This special system call allows a process to isolate itself from the original namespace, instead of having the parent isolate the child in the first place. For example, the following code has the exact same effect as the code previously mentioned in the network namespace section: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { // calling unshare() from inside the init process lets you create a new namespace after a new process has been spawned unshare(CLONE_NEWNET); printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } And since the \u201cinit\u201d process is something you have devised, you can make it do all the necessary work first, and then isolate itself from the rest of the system before executing the target child. Conclusion # This tutorial is just an overview of how to use namespaces in Linux. It should give you a basic idea of how a Linux developer might start to implement system isolation, an integral part of the architecture of tools like Docker or Linux Containers. In most cases, it would be best to simply use one of these existing tools, which are already well-known and tested. But in some cases, it might make sense to have your very own, customized process isolation mechanism, and in that case, this namespace tutorial will help you out tremendously. There is a lot more going on under the hood than I\u2019ve covered in this article, and there are more ways you might want to limit your target processes for added safety and isolation. But, hopefully, this can serve as a useful starting point for someone who is interested in knowing more about how namespace isolation with Linux really works. Tags Sandboxing SystemIsolation Linux Namespaces","title":"Linux-Namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#linux-namespaces","text":"","title":"Linux namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#separation-anxiety-a-tutorial-for-isolating-your-system-with-linux-namespaces","text":"With the advent of tools like Docker , Linux Containers , and others, it has become super easy to isolate Linux processes into their own little system environments. This makes it possible to run a whole range of applications on a single real Linux machine and ensure no two of them can interfere with each other, without having to resort to using virtual machines . These tools have been a huge boon to PaaS providers. But what exactly happens under the hood? These tools rely on a number of features and components of the Linux kernel. Some of these features were introduced fairly recently, while others still require you to patch the kernel itself. But one of the key components, using Linux namespaces , has been a feature of Linux since version 2.6.24 was released in 2008. Anyone familiar with chroot already has a basic idea of what Linux namespaces can do and how to use namespace generally. Just as chroot allows processes to see any arbitrary directory as the root of the system (independent of the rest of the processes), Linux namespaces allow other aspects of the operating system to be independently modified as well. This includes the process tree , networking interfaces, mount points , inter-process communication resources and more.","title":"Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#why-use-namespaces-for-process-isolation","text":"In a single-user computer, a single system environment may be fine. But on a server, where you want to run multiple services, it is essential to security and stability that the services are as isolated from each other as possible. Imagine a server running multiple services, one of which gets compromised by an intruder. In such a case, the intruder may be able to exploit that service and work his way to the other services, and may even be able compromise the entire server. Namespace isolation can provide a secure environment to eliminate this risk. For example, using namespacing, it is possible to safely execute arbitrary or unknown programs on your server. Recently, there has been a growing number of programming contest and \u201chackathon\u201d platforms, such as HackerRank , TopCoder , Codeforces , and many more. A lot of them utilize automated pipelines to run and validate programs that are submitted by the contestants. It is often impossible to know in advance the true nature of contestants\u2019 programs, and some may even contain malicious elements. By running these programs namespaced in complete isolation from the rest of the system, the software can be tested and validated without putting the rest of the machine at risk. Similarly, online continuous integration services, such as Drone.io , automatically fetch your code repository and execute the test scripts on their own servers. Again, namespace isolation is what makes it possible to provide these services safely. Namespacing tools like Docker also allow better control over processes\u2019 use of system resources, making such tools extremely popular for use by PaaS providers. Services like Heroku and Google App Engine use such tools to isolate and run multiple web server applications on the same real hardware. These tools allow them to run each application (which may have been deployed by any of a number of different users) without worrying about one of them using too many system resources, or interfering and/or conflicting with other deployed services on the same machine. With such process isolation, it is even possible to have entirely different stacks of dependency softwares (and versions) for each isolated environment! If you\u2019ve used tools like Docker, you already know that these tools are capable of isolating processes in small \u201ccontainers\u201d. Running processes in Docker containers is like running them in virtual machines, only these containers are significantly lighter than virtual machines.","title":"Why Use Namespaces for Process Isolation?"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#process-namespace","text":"Historically, the Linux kernel has maintained a single process tree . The tree contains a reference to every process currently running in a parent-child hierarchy . A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. With the introduction of Linux namespaces , it became possible to have multiple \u201cnested\u201d process trees. Each process tree can have an entirely isolated set of processes. This can ensure that processes belonging to one process tree cannot inspect or kill - in fact cannot even know of the existence of - processes in other sibling or parent process trees. Every time a computer with Linux boots up, it starts with just one process, with process identifier (PID) 1. This process is the root of the process tree , and it initiates the rest of the system by performing the appropriate maintenance work and starting the correct daemons/services. All the other processes start below this process in the tree. The PID namespace allows one to spin off a new tree, with its own PID 1 process . The process that does this remains in the parent namespace , in the original tree, but makes the child the root of its own process tree . With PID namespace isolation , processes in the child namespace have no way of knowing of the parent process\u2019s existence. However, processes in the parent namespace have a complete view of processes in the child namespace , as if they were any other process in the parent namespace . It is possible to create a nested set of child namespaces : one process starts a child process in a new PID namespace, and that child process spawns yet another process in a new PID namespace, and so on. With the introduction of PID namespaces , a single process can now have multiple PIDs associated with it, one for each namespace it falls under. In the Linux source code, we can see that a struct named pid , which used to keep track of just a single PID, now tracks multiple PIDs through the use of a struct named upid : struct upid { int nr; // the PID value struct pid_namespace *ns; // namespace where this PID is relevant // ... }; struct pid { // ... int level; // number of upids struct upid numbers[0]; // array of upids }; To create a new PID namespace , one must call the clone() system call with a special flag CLONE_NEWPID . (C provides a wrapper to expose this system call, and so do many other popular languages.) Whereas the other namespaces discussed below can also be created using the unshare() system call, a PID namespace can only be created at the time a new process is spawned using clone() . Once clone() is called with this flag, the new process immediately starts in a new PID namespace, under a new process tree. This can be demonstrated with a simple C program: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"PID: %ld\\n\", (long)getpid()); return 0; } int main() { pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); printf(\"clone() = %ld\\n\", (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } Compile and run this program with root privileges and you will notice an output that resembles this: clone() = 5304 PID: 1 The PID, as printed from within the child_fn , will be 1 . Try replacing the static int child_fn() function with the following, to print the parent PID from the isolated process\u2019s perspective: static int child_fn() { printf(\"Parent PID: %ld\\n\", (long)getppid()); return 0; } Running the program this time yields the following output: clone() = 11449 Parent PID: 0 Notice how the parent PID from the isolated process\u2019s perspective is 0, indicating no parent. Try running the same program again, but this time, remove the CLONE_NEWPID flag from within the clone() function call: pid_t child_pid = clone(child_fn, child_stack+1048576, SIGCHLD, NULL); This time, you will notice that the parent PID is no longer 0: clone() = 11561 Parent PID: 11560 However, this is just the first step in our tutorial. These processes still have unrestricted access to other common or shared resources. For example, the networking interface: if the child process created above were to listen on port 80, it would prevent every other process on the system from being able to listen on it.","title":"Process Namespace"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#linux-network-namespace","text":"This is where a network namespace becomes useful. A network namespace allows each of these processes to see an entirely different set of networking interfaces . Even the loopback interface is different for each network namespace. Isolating a process into its own network namespace involves introducing another flag to the clone() function call: CLONE_NEWNET ; #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } Output: Original `net` Namespace: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:24:8c:a1:ac:e7 brd ff:ff:ff:ff:ff:ff New `net` Namespace: 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 What\u2019s going on here? The physical ethernet device enp4s0 belongs to the global network namespace , as indicated by the \u201c ip \u201d tool run from this namespace. However, the physical interface is not available in the new network namespace . Moreover, the loopback device is active in the original network namespace , but is \u201cdown\u201d in the child network namespace . In order to provide a usable network interface in the child namespace , it is necessary to set up additional \u201cvirtual\u201d network interfaces which span multiple namespaces. Once that is done, it is then possible to create Ethernet bridges , and even route packets between the namespaces. Finally, to make the whole thing work, a \u201crouting process\u201d must be running in the global network namespace to receive traffic from the physical interface, and route it through the appropriate virtual interfaces to to the correct child network namespaces . Maybe you can see why tools like Docker, which do all this heavy lifting for you, are so popular! To do this by hand, you can create a pair of virtual Ethernet connections between a parent and a child namespace by running a single command from the parent namespace : ip link add name veth0 type veth peer name veth1 netns <pid> Here, <pid> should be replaced by the process ID of the process in the child namespace as observed by the parent. Running this command establishes a pipe-like connection between these two namespaces. The parent namespace retains the veth0 device, and passes the veth1 device to the child namespace. Anything that enters one of the ends, comes out through the other end, just as you would expect from a real Ethernet connection between two real nodes. Accordingly, both sides of this virtual Ethernet connection must be assigned IP addresses.","title":"Linux Network Namespace"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#mount-namespace","text":"Linux also maintains a data structure for all the mountpoints of the system. It includes information like what disk partitions are mounted, where they are mounted, whether they are readonly, et cetera. With Linux namespaces, one can have this data structure cloned, so that processes under different namespaces can change the mountpoints without affecting each other. Creating separate mount namespace has an effect similar to doing a chroot() . chroot() is good, but it does not provide complete isolation, and its effects are restricted to the root mountpoint only. Creating a separate mount namespace allows each of these isolated processes to have a completely different view of the entire system\u2019s mountpoint structure from the original one. This allows you to have a different root for each isolated process, as well as other mountpoints that are specific to those processes. Used with care per this tutorial, you can avoid exposing any information about the underlying system. The clone() flag required to achieve this is CLONE_NEWNS : clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | CLONE_NEWNS | SIGCHLD, NULL) Initially, the child process sees the exact same mountpoints as its parent process would. However, being under a new mount namespace , the child process can mount or unmount whatever endpoints it wants to, and the change will affect neither its parent\u2019s namespace, nor any other mount namespace in the entire system. For example, if the parent process has a particular disk partition mounted at root, the isolated process will see the exact same disk partition mounted at the root in the beginning. But the benefit of isolating the mount namespace is apparent when the isolated process tries to change the root partition to something else, as the change will only affect the isolated mount namespace. Interestingly, this actually makes it a bad idea to spawn the target child process directly with the CLONE_NEWNS flag. A better approach is to start a special \u201cinit\u201d process with the CLONE_NEWNS flag, have that \u201cinit\u201d process change the \u201c/\u201d, \u201c/proc\u201d, \u201c/dev\u201d or other mountpoints as desired, and then start the target process. This is discussed in a little more detail near the end of this namespace tutorial.","title":"Mount Namespace"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#other-namespaces","text":"There are other namespaces that these processes can be isolated into, namely user, IPC, and UTS. The user namespace allows a process to have root privileges within the namespace , without giving it that access to processes outside of the namespace. Isolating a process by the IPC namespace gives it its own interprocess communication resources, for example, System V IPC and POSIX messages. The UTS namespace isolates two specific identifiers of the system: nodename and domainname . A quick example to show how UTS namespace is isolated is shown below: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/utsname.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static void print_nodename() { struct utsname utsname; uname(&utsname); printf(\"%s\\n\", utsname.nodename); } static int child_fn() { printf(\"New UTS namespace nodename: \"); print_nodename(); printf(\"Changing nodename inside new UTS namespace\\n\"); sethostname(\"GLaDOS\", 6); printf(\"New UTS namespace nodename: \"); print_nodename(); return 0; } int main() { printf(\"Original UTS namespace nodename: \"); print_nodename(); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWUTS | SIGCHLD, NULL); sleep(1); printf(\"Original UTS namespace nodename: \"); print_nodename(); waitpid(child_pid, NULL, 0); return 0; } This program yields the following output: Original UTS namespace nodename: XT New UTS namespace nodename: XT Changing nodename inside new UTS namespace New UTS namespace nodename: GLaDOS Original UTS namespace nodename: XT Here, child_fn() prints the nodename , changes it to something else, and prints it again. Naturally, the change happens only inside the new UTS namespace. More information on what all of the namespaces provide and isolate can be found in the tutorial here","title":"Other Namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#cross-namespace-communication","text":"Often it is necessary to establish some sort of communication between the parent and the child namespace. This might be for doing configuration work within an isolated environment, or it can simply be to retain the ability to peek into the condition of that environment from outside. One way of doing that is to keep an SSH daemon running within that environment. You can have a separate SSH daemon inside each network namespace. However, having multiple SSH daemons running uses a lot of valuable resources like memory. This is where having a special \u201cinit\u201d process proves to be a good idea again. The \u201cinit\u201d process can establish a communication channel between the parent namespace and the child namespace. This channel can be based on UNIX sockets or can even use TCP. To create a UNIX socket that spans two different mount namespaces, you need to first create the child process, then create the UNIX socket, and then isolate the child into a separate mount namespace. But how can we create the process first, and isolate it later? Linux provides unshare() . This special system call allows a process to isolate itself from the original namespace, instead of having the parent isolate the child in the first place. For example, the following code has the exact same effect as the code previously mentioned in the network namespace section: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { // calling unshare() from inside the init process lets you create a new namespace after a new process has been spawned unshare(CLONE_NEWNET); printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } And since the \u201cinit\u201d process is something you have devised, you can make it do all the necessary work first, and then isolate itself from the rest of the system before executing the target child.","title":"Cross-Namespace Communication"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#conclusion","text":"This tutorial is just an overview of how to use namespaces in Linux. It should give you a basic idea of how a Linux developer might start to implement system isolation, an integral part of the architecture of tools like Docker or Linux Containers. In most cases, it would be best to simply use one of these existing tools, which are already well-known and tested. But in some cases, it might make sense to have your very own, customized process isolation mechanism, and in that case, this namespace tutorial will help you out tremendously. There is a lot more going on under the hood than I\u2019ve covered in this article, and there are more ways you might want to limit your target processes for added safety and isolation. But, hopefully, this can serve as a useful starting point for someone who is interested in knowing more about how namespace isolation with Linux really works. Tags Sandboxing SystemIsolation Linux Namespaces","title":"Conclusion"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/","text":"NAMESPACES(7) CGROUP_NAMESPACES(7) IPC_NAMESPACES(7) NETWORK_NAMESPACES(7) MOUNT_NAMESPACES(7) PID_NAMESPACES(7) USER_NAMESPACES(7) UTS_NAMESPACES(7) NAMESPACES(7) # CGROUP_NAMESPACES(7) # IPC_NAMESPACES(7) # NETWORK_NAMESPACES(7) # MOUNT_NAMESPACES(7) # PID_NAMESPACES(7) # USER_NAMESPACES(7) # UTS_NAMESPACES(7) #","title":"man-7-namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#namespaces7","text":"","title":"NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#cgroup_namespaces7","text":"","title":"CGROUP_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#ipc_namespaces7","text":"","title":"IPC_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#network_namespaces7","text":"","title":"NETWORK_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#mount_namespaces7","text":"","title":"MOUNT_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#pid_namespaces7","text":"","title":"PID_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#user_namespaces7","text":"","title":"USER_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#uts_namespaces7","text":"","title":"UTS_NAMESPACES(7)"},{"location":"Programming/06-Util/","text":"","title":"Introduction"},{"location":"Programming/06-Util/Log/Linux-log-files/","text":"LinuxLogFiles #","title":"Linux-log-files"},{"location":"Programming/06-Util/Log/Linux-log-files/#linuxlogfiles","text":"","title":"LinuxLogFiles"},{"location":"Programming/06-Util/Log/Syslog/Syslog/","text":"syslog # Syslog Tutorial: How It Works, Examples, Best Practices, and More 10.5 The UNIX System Log (syslog) Facility","title":"Syslog"},{"location":"Programming/06-Util/Log/Syslog/Syslog/#syslog","text":"Syslog Tutorial: How It Works, Examples, Best Practices, and More 10.5 The UNIX System Log (syslog) Facility","title":"syslog"},{"location":"Programming/07-Linux-Kernel-module/Loadable-kernel-module/","text":"Loadable kernel module # The Linux Kernel Module Programming Guide #","title":"Loadable-kernel-module"},{"location":"Programming/07-Linux-Kernel-module/Loadable-kernel-module/#loadable-kernel-module","text":"","title":"Loadable kernel module"},{"location":"Programming/07-Linux-Kernel-module/Loadable-kernel-module/#the-linux-kernel-module-programming-guide","text":"","title":"The Linux Kernel Module Programming Guide"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/","text":"How do I read the output of dmesg to determine how much memory a process is using when oom-killer is invoked? How to translate kernel's trap divide error rsp:2b6d2ea40450 to a source location? A How do you read a segfault kernel log message A When the report points to a program, not a shared library If it's a shared library What the error means A A How do I read the output of dmesg to determine how much memory a process is using when oom-killer is invoked? # How to translate kernel's trap divide error rsp:2b6d2ea40450 to a source location? # Customer reported an error in one of our programs caused by division by zero. We have only this VLM line: kernel: myprog[16122] trap divide error rip:79dd99 rsp:2b6d2ea40450 error:0 I do not believe there is core file for that. I searched through the Internet to find how I can tell the line of the program that caused this division by zero, but so far I am failing. I understand that 16122 is pid of the program, so that will not help me. I suspect that rsp:2b6d2ea40450 has something to do with the address of the line that caused the error ( 0x2b6d2ea40450 ) but is that true? If it is then how can I translate it to a physical approximate location in the source assuming I can load debug version of myprog into gdb, and then request to show the context around this address... Any, any help will be greatly appreciated! A # ip is the instruction pointer , rsp is the stack pointer . The stack pointer is not too useful unless you have a core image or a running process. You can use either addr2line or the disassemble command in gdb to see the line that got the error, based on the ip . $ cat divtest.c main() { int a, b; a = 1; b = a/0; } $ ./divtest Floating point exception (core dumped) $ dmesg|tail -1 [ 6827.463256] traps: divtest[3255] trap divide error ip:400504 sp:7fff54e81330 error:0 in divtest[400000+1000] $ addr2line -e divtest 400504 ./divtest.c:5 $ gdb divtest (gdb) disass /m 0x400504 Dump of assembler code for function main: 2 { 0x00000000004004f0 : push %rbp 0x00000000004004f1 : mov %rsp,%rbp 3 int a, b; 4 5 a = 1; b = a/0; 0x00000000004004f4 : movl $0x1,-0x4(%rbp) 0x00000000004004fb : mov -0x4(%rbp),%eax 0x00000000004004fe : mov $0x0,%ecx 0x0000000000400503 : cltd 0x0000000000400504 : idiv %ecx 0x0000000000400506 : mov %eax,-0x8(%rbp) 6 } 0x0000000000400509 : pop %rbp 0x000000000040050a : retq End of assembler dump. How do you read a segfault kernel log message # This can be a very simple question, I'm am attempting to debug an application which generates the following segfault error in the kern.log kernel: myapp[15514]: segfault at 794ef0 ip 080513b sp 794ef0 error 6 in myapp[8048000+24000] Here are my questions: Is there any documentation as to what are the diff error numbers on segfault, in this instance it is error 6, but i've seen error 4, 5 What is the meaning of the information at bf794ef0 ip 0805130b sp bf794ef0 and myapp[8048000+24000] ? So far i was able to compile with symbols, and when i do a x 0x8048000+24000 it returns a symbol, is that the correct way of doing it? My assumptions thus far are the following: sp = stack pointer? ip = instruction pointer at = ???? myapp[8048000+24000] = address of symbol? A # When the report points to a program, not a shared library # Run addr2line -e myapp 080513b (and repeat for the other instruction pointer values given) to see where the error is happening. Better, get a debug-instrumented build , and reproduce the problem under a debugger such as gdb. If it's a shared library # In the libfoo.so[NNNNNN+YYYY] part, the NNNNNN is where the library was loaded. Subtract this from the instruction pointer ( ip ) and you'll get the offset into the .so of the offending instruction. Then you can use objdump -DCgl libfoo.so and search for the instruction at that offset. You should easily be able to figure out which function it is from the asm labels. If the .so doesn't have optimizations you can also try using addr2line -e libfoo.so <offset> . What the error means # Here's the breakdown of the fields: address - the location in memory the code is trying to access (it's likely that 10 and 11 are offsets from a pointer we expect to be set to a valid value but which is instead pointing to 0 ) ip - instruction pointer, ie. where the code which is trying to do this lives sp - stack pointer error - Architecture-specific flags; see arch/*/mm/fault.c for your platform. COMMENTS : you gotta be wrong about error \u2013 Dima Tisnek Sep 12 '12 at 12:25 much better now \u2013 Dima Tisnek Sep 30 '12 at 9:07 Event for a shared lib, the \"[8048000+24000]\" part should give a hint where the crashing segment of the lib was mapped in memory. \"readelf --segments mylib.so\" lists these segments, and then you can calculate the EIP offset into the crashing segment and feed that to addr2line (or view it in \"objdump -dgS\"). \u2013 oliver Jun 13 '13 at 17:18 I believe 0x8048000 is (probably) the address where the text segment was mapped, so you will want to pass -j .text to the objdump command. (At least, that is what I needed when diagnosing one of these just now.) \u2013 Nemo Jun 5 '14 at 16:30 @Charles Duffy If I ever see you I will hug like I never hugged a living soul. \u2013 Baroudi Safwen Jan 11 '18 at 18:47 A # Based on my limited knowledge, your assumptions are correct. sp = stack pointer ip = instruction pointer myapp[8048000+24000] = address If I were debugging the problem I would modify the code to produce a core dump or log a stack backtrace on the crash. You might also run the program under (or attach) GDB. The error code is just the architectural error code for page faults and seems to be architecture specific. They are often documented in arch/*/mm/fault.c in the kernel source. My copy of Linux/arch/i386/mm/fault.c has the following definition for error_code: bit 0 == 0 means no page found, 1 means protection fault bit 1 == 0 means read, 1 means write bit 2 == 0 means kernel, 1 means user-mode My copy of Linux/arch/x86_64/mm/fault.c adds the following: bit 3 == 1 means fault was an instruction fetch Beat me to it :) \u2013 David Titarenco Feb 1 '10 at 19:34 The issue i have is that: 1) The application is segfaulting in a production environment, where symbols are stripped, all i have is just the logs 2) I'm trying to find that memory location in the development env, so at least i can see where it is crashing. \u2013 Sullenx Feb 1 '10 at 19:42 1 If you have the pre-stripped binary, try running it through nm or objdump. \u2013 jschmier Feb 1 '10 at 19:52 nm is pretty helpful, at least I have an idea where the crash happened. One last thing, what is an error 6? ... is there any table out there? \u2013 Sullenx Feb 1 '10 at 20:07 I updated my answer to include the error code. \u2013 jschmier Feb 1 '10 at 20:47 3 segfault at 794ef0 ... sp 794ef0 - stack is obviously corrupted. \u2013 Nikolai Fetissov Feb 1 '10 at 20:54 Thank you, this is very helpful \u2013 Sullenx Feb 1 '10 at 20:56 A # If it's a shared library You're hosed, unfortunately; it's not possible to know where the libraries were placed in memory by the dynamic linker after-the-fact . Well, there is still a possibility to retrieve the information, not from the binary, but from the object. But you need the base address of the object. And this information still is within the coredump, in the link_map structure. So first you want to import the struct link_map into GDB. So lets compile a program with it with debug symbol and add it to the GDB. link.c #include <link.h> toto(){struct link_map * s = 0x400;} get_baseaddr_from_coredump.sh #!/bin/bash BINARY=$(which myapplication) IsBinPIE () { readelf -h $1|grep 'Type' |grep \"EXEC\">/dev/null || return 0 return 1 } Hex2Decimal () { export number=\"`echo \"$1\" | sed -e 's:^0[xX]::' | tr '[a-f]' '[A-F]'`\" export number=`echo \"ibase=16; $number\" | bc` } GetBinaryLength () { if [ $# != 1 ]; then echo \"Error, no argument provided\" fi IsBinPIE $1 || (echo \"ET_EXEC file, need a base_address\"; exit 0) export totalsize=0 # Get PT_LOAD's size segment out of Program Header Table (ELF format) export sizes=\"$(readelf -l $1 |grep LOAD |awk '{print $6}'|tr '\\n' ' ')\" for size in $sizes do Hex2Decimal \"$size\"; export totalsize=$(expr $number + $totalsize); export totalsize=$(expr $number + $totalsize) done return $totalsize } if [ $# = 1 ]; then echo \"Using binary $1\" IsBinPIE $1 && (echo \"NOT ET_EXEC, need a base_address...\"; exit 0) BINARY=$1 fi gcc -g3 -fPIC -shared link.c -o link.so GOTADDR=$(readelf -S $BINARY|grep -E '\\.got.plt[ \\t]'|awk '{print $4}') echo \"First do the following command :\" echo file $BINARY echo add-symbol-file ./link.so 0x0 read echo \"Now copy/paste the following into your gdb session with attached coredump\" cat <<EOF set \\$linkmapaddr = *(0x$GOTADDR + 4) set \\$mylinkmap = (struct link_map *) \\$linkmapaddr while (\\$mylinkmap != 0) if (\\$mylinkmap->l_addr) printf \"add-symbol-file .%s %#.08x\\n\", \\$mylinkmap->l_name, \\$mylinkmap->l_addr end set \\$mylinkmap = \\$mylinkmap->l_next end it will print you the whole link_map content, within a set of GDB command. It itself it might seems unnesseray but with the base_addr of the shared object we are about, you might get some more information out of an address by debuging directly the involved shared object in another GDB instance. Keep the first gdb to have an idee of the symbol. NOTE : the script is rather incomplete i suspect you may add to the second parameter of add-symbol-file printed the sum with this value : readelf -S $SO_PATH|grep -E '\\.text[ \\t]'|awk '{print $5}' where $SO_PATH is the first argument of the add-symbol-file Hope it helps","title":"Dmesg-format"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#how-do-i-read-the-output-of-dmesg-to-determine-how-much-memory-a-process-is-using-when-oom-killer-is-invoked","text":"","title":"How do I read the output of dmesg to determine how much memory a process is using when oom-killer is invoked?"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#how-to-translate-kernels-trap-divide-error-rsp2b6d2ea40450-to-a-source-location","text":"Customer reported an error in one of our programs caused by division by zero. We have only this VLM line: kernel: myprog[16122] trap divide error rip:79dd99 rsp:2b6d2ea40450 error:0 I do not believe there is core file for that. I searched through the Internet to find how I can tell the line of the program that caused this division by zero, but so far I am failing. I understand that 16122 is pid of the program, so that will not help me. I suspect that rsp:2b6d2ea40450 has something to do with the address of the line that caused the error ( 0x2b6d2ea40450 ) but is that true? If it is then how can I translate it to a physical approximate location in the source assuming I can load debug version of myprog into gdb, and then request to show the context around this address... Any, any help will be greatly appreciated!","title":"How to translate kernel's trap divide error rsp:2b6d2ea40450 to a source location?"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a","text":"ip is the instruction pointer , rsp is the stack pointer . The stack pointer is not too useful unless you have a core image or a running process. You can use either addr2line or the disassemble command in gdb to see the line that got the error, based on the ip . $ cat divtest.c main() { int a, b; a = 1; b = a/0; } $ ./divtest Floating point exception (core dumped) $ dmesg|tail -1 [ 6827.463256] traps: divtest[3255] trap divide error ip:400504 sp:7fff54e81330 error:0 in divtest[400000+1000] $ addr2line -e divtest 400504 ./divtest.c:5 $ gdb divtest (gdb) disass /m 0x400504 Dump of assembler code for function main: 2 { 0x00000000004004f0 : push %rbp 0x00000000004004f1 : mov %rsp,%rbp 3 int a, b; 4 5 a = 1; b = a/0; 0x00000000004004f4 : movl $0x1,-0x4(%rbp) 0x00000000004004fb : mov -0x4(%rbp),%eax 0x00000000004004fe : mov $0x0,%ecx 0x0000000000400503 : cltd 0x0000000000400504 : idiv %ecx 0x0000000000400506 : mov %eax,-0x8(%rbp) 6 } 0x0000000000400509 : pop %rbp 0x000000000040050a : retq End of assembler dump.","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#how-do-you-read-a-segfault-kernel-log-message","text":"This can be a very simple question, I'm am attempting to debug an application which generates the following segfault error in the kern.log kernel: myapp[15514]: segfault at 794ef0 ip 080513b sp 794ef0 error 6 in myapp[8048000+24000] Here are my questions: Is there any documentation as to what are the diff error numbers on segfault, in this instance it is error 6, but i've seen error 4, 5 What is the meaning of the information at bf794ef0 ip 0805130b sp bf794ef0 and myapp[8048000+24000] ? So far i was able to compile with symbols, and when i do a x 0x8048000+24000 it returns a symbol, is that the correct way of doing it? My assumptions thus far are the following: sp = stack pointer? ip = instruction pointer at = ???? myapp[8048000+24000] = address of symbol?","title":"How do you read a segfault kernel log message"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a_1","text":"","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#when-the-report-points-to-a-program-not-a-shared-library","text":"Run addr2line -e myapp 080513b (and repeat for the other instruction pointer values given) to see where the error is happening. Better, get a debug-instrumented build , and reproduce the problem under a debugger such as gdb.","title":"When the report points to a program, not a shared library"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#if-its-a-shared-library","text":"In the libfoo.so[NNNNNN+YYYY] part, the NNNNNN is where the library was loaded. Subtract this from the instruction pointer ( ip ) and you'll get the offset into the .so of the offending instruction. Then you can use objdump -DCgl libfoo.so and search for the instruction at that offset. You should easily be able to figure out which function it is from the asm labels. If the .so doesn't have optimizations you can also try using addr2line -e libfoo.so <offset> .","title":"If it's a shared library"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#what-the-error-means","text":"Here's the breakdown of the fields: address - the location in memory the code is trying to access (it's likely that 10 and 11 are offsets from a pointer we expect to be set to a valid value but which is instead pointing to 0 ) ip - instruction pointer, ie. where the code which is trying to do this lives sp - stack pointer error - Architecture-specific flags; see arch/*/mm/fault.c for your platform. COMMENTS : you gotta be wrong about error \u2013 Dima Tisnek Sep 12 '12 at 12:25 much better now \u2013 Dima Tisnek Sep 30 '12 at 9:07 Event for a shared lib, the \"[8048000+24000]\" part should give a hint where the crashing segment of the lib was mapped in memory. \"readelf --segments mylib.so\" lists these segments, and then you can calculate the EIP offset into the crashing segment and feed that to addr2line (or view it in \"objdump -dgS\"). \u2013 oliver Jun 13 '13 at 17:18 I believe 0x8048000 is (probably) the address where the text segment was mapped, so you will want to pass -j .text to the objdump command. (At least, that is what I needed when diagnosing one of these just now.) \u2013 Nemo Jun 5 '14 at 16:30 @Charles Duffy If I ever see you I will hug like I never hugged a living soul. \u2013 Baroudi Safwen Jan 11 '18 at 18:47","title":"What the error means"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a_2","text":"Based on my limited knowledge, your assumptions are correct. sp = stack pointer ip = instruction pointer myapp[8048000+24000] = address If I were debugging the problem I would modify the code to produce a core dump or log a stack backtrace on the crash. You might also run the program under (or attach) GDB. The error code is just the architectural error code for page faults and seems to be architecture specific. They are often documented in arch/*/mm/fault.c in the kernel source. My copy of Linux/arch/i386/mm/fault.c has the following definition for error_code: bit 0 == 0 means no page found, 1 means protection fault bit 1 == 0 means read, 1 means write bit 2 == 0 means kernel, 1 means user-mode My copy of Linux/arch/x86_64/mm/fault.c adds the following: bit 3 == 1 means fault was an instruction fetch Beat me to it :) \u2013 David Titarenco Feb 1 '10 at 19:34 The issue i have is that: 1) The application is segfaulting in a production environment, where symbols are stripped, all i have is just the logs 2) I'm trying to find that memory location in the development env, so at least i can see where it is crashing. \u2013 Sullenx Feb 1 '10 at 19:42 1 If you have the pre-stripped binary, try running it through nm or objdump. \u2013 jschmier Feb 1 '10 at 19:52 nm is pretty helpful, at least I have an idea where the crash happened. One last thing, what is an error 6? ... is there any table out there? \u2013 Sullenx Feb 1 '10 at 20:07 I updated my answer to include the error code. \u2013 jschmier Feb 1 '10 at 20:47 3 segfault at 794ef0 ... sp 794ef0 - stack is obviously corrupted. \u2013 Nikolai Fetissov Feb 1 '10 at 20:54 Thank you, this is very helpful \u2013 Sullenx Feb 1 '10 at 20:56","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a_3","text":"If it's a shared library You're hosed, unfortunately; it's not possible to know where the libraries were placed in memory by the dynamic linker after-the-fact . Well, there is still a possibility to retrieve the information, not from the binary, but from the object. But you need the base address of the object. And this information still is within the coredump, in the link_map structure. So first you want to import the struct link_map into GDB. So lets compile a program with it with debug symbol and add it to the GDB. link.c #include <link.h> toto(){struct link_map * s = 0x400;} get_baseaddr_from_coredump.sh #!/bin/bash BINARY=$(which myapplication) IsBinPIE () { readelf -h $1|grep 'Type' |grep \"EXEC\">/dev/null || return 0 return 1 } Hex2Decimal () { export number=\"`echo \"$1\" | sed -e 's:^0[xX]::' | tr '[a-f]' '[A-F]'`\" export number=`echo \"ibase=16; $number\" | bc` } GetBinaryLength () { if [ $# != 1 ]; then echo \"Error, no argument provided\" fi IsBinPIE $1 || (echo \"ET_EXEC file, need a base_address\"; exit 0) export totalsize=0 # Get PT_LOAD's size segment out of Program Header Table (ELF format) export sizes=\"$(readelf -l $1 |grep LOAD |awk '{print $6}'|tr '\\n' ' ')\" for size in $sizes do Hex2Decimal \"$size\"; export totalsize=$(expr $number + $totalsize); export totalsize=$(expr $number + $totalsize) done return $totalsize } if [ $# = 1 ]; then echo \"Using binary $1\" IsBinPIE $1 && (echo \"NOT ET_EXEC, need a base_address...\"; exit 0) BINARY=$1 fi gcc -g3 -fPIC -shared link.c -o link.so GOTADDR=$(readelf -S $BINARY|grep -E '\\.got.plt[ \\t]'|awk '{print $4}') echo \"First do the following command :\" echo file $BINARY echo add-symbol-file ./link.so 0x0 read echo \"Now copy/paste the following into your gdb session with attached coredump\" cat <<EOF set \\$linkmapaddr = *(0x$GOTADDR + 4) set \\$mylinkmap = (struct link_map *) \\$linkmapaddr while (\\$mylinkmap != 0) if (\\$mylinkmap->l_addr) printf \"add-symbol-file .%s %#.08x\\n\", \\$mylinkmap->l_name, \\$mylinkmap->l_addr end set \\$mylinkmap = \\$mylinkmap->l_next end it will print you the whole link_map content, within a set of GDB command. It itself it might seems unnesseray but with the base_addr of the shared object we are about, you might get some more information out of an address by debuging directly the involved shared object in another GDB instance. Keep the first gdb to have an idee of the symbol. NOTE : the script is rather incomplete i suspect you may add to the second parameter of add-symbol-file printed the sum with this value : readelf -S $SO_PATH|grep -E '\\.text[ \\t]'|awk '{print $5}' where $SO_PATH is the first argument of the add-symbol-file Hope it helps","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/","text":"What the Linux kernel's messages about segfaulting programs mean on 64-bit x86 # For quite a while the Linux kernel has had an option to log a kernel message about every faulting user program , and it probably defaults to on in your Linux distribution. I've seen these messages fly by for years, but for reasons beyond the scope of this entry I've recently wanted to understand what they mean in some moderate amount of detail. I'll start with a straightforward and typical example, one that I see every time I build and test Go (as this is a test case that is supposed to crash): testp[19288]: segfault at 0 ip 0000000000401271 sp 00007fff2ce4d210 error 4 in testp[400000+98000] The meaning of this is: ' testp[19288] ' is the faulting program and its PID ' segfault at 0 ' tells us the memory address (in hex) that caused the segfault when the program tried to access it. Here the address is 0, so we have a null dereference of some sort. ' ip 0000000000401271 ' is the value of the instruction pointer at the time of the fault. This should be the instruction that attempted to do the invalid memory access. In 64-bit x86, this will be register %rip (useful for inspecting things in GDB and elsewhere). ' sp 00007fff2ce4d210 ' is the value of the stack pointer. In 64-bit x86, this will be %rsp . ' error 4 ' is the page fault error code bits from traps.h in hex, as usual, and will almost always be at least 4 (which means 'user-mode access'). A value of 4 means it was a read of an unmapped area , such as address 0, while a value of 6 (4+2) means it was a write of an unmapped area . ' in testp[400000+98000] ' tells us the specific virtual memory area that the instruction pointer is in, specifying which file it is (here it's the executable), the starting address that VMA is mapped at ( 0x400000 ), and the size of the mapping ( 0x98000 ). With a faulting address of 0 and an error code of 4, we know this particular segfault is a read of a null pointer . Here's two more error messages: bash[12235]: segfault at 1054808 ip 000000000041d989 sp 00007ffec1f1cbd8 error 6 in bash[400000+f4000] 'Error 6' means a write to an unmapped user address , here 0x1054808 . bash[11909]: segfault at 0 ip 00007f83c03db746 sp 00007ffccbeda010 error 4 in libc-2.23.so[7f83c0350000+1c0000] Error 4 and address 0 is a null pointer read but this time it's in some libc function, not in bash's own code, since it's reported as 'in libc-2.23.so [...]'. Since I looked at the core dump, I can tell you that this was in strlen() . On 64-bit x86 Linux, you'll get a somewhat different message if the problem is actually with the instruction being executed, not the address it's referencing. For example: bash[2848] trap invalid opcode ip:48db90 sp:7ffddc8879e8 error:0 in bash[400000+f4000] There are a number of such trap types set up in traps.c . Two notable additional ones are 'divide error', which you get if you do an integer division by zero, and 'general protection', which you can get for certain extremely wild pointers (one case I know of is when your 64-bit x86 address is not in 'canonical form' ). Although these fields are formatted slightly differently, most of them mean the same thing as in segfaults. The exception is ' error:0 ', which is not a page fault error code. I don't understand the relevant kernel code enough to know what it means, but if I'm reading between the lines correctly in entry_64.txt , then it's either 0 (the usual case) or an error code from the CPU. Here is one possible list of exceptions that get error codes. Sometimes these messages can be a little bit unusual and surprising. Here is a silly sample program and the error it produces when run. The code: ``` include # int main(int argc, char argv) { int ( p)(); p = 0x0; return printf(\"%d\\n\", ( p)()); } ``` If compiled (without optimization is best) and run, this generates the kernel message: a.out[3714]: segfault at 0 ip (null) sp 00007ffe872aa418 error 14 in a.out[400000+1000] The ' (null) ' bit turns out to be expected; it's what the general kernel printf() function generates when asked to print something as a pointer and it's null (as seen here ). In our case the instruction pointer is 0 (null) because we've made a subroutine call through a null pointer and thus we're trying to execute code at address 0. I don't know why the 'in ...' portion says that we're in the executable (although in this case the call actually was there). The error code of 14 is in hex, which means that as bits it's 010100. This is a user mode read of an unmapped area (our usual '4' case), but it's an instruction fetch, not a normal data read or write. Any error 14s are a sign of some form of mangled function call or a return to a mangled address because the stack has been mashed. (These bits turn out to come straight from the CPU's page fault IDT .) For 64-bit x86 Linux kernels (and possibly for 32-bit x86 ones as well), the code you want to look at is show_signal_msg in fault.c , which prints the general 'segfault at ..' message, do_trap and do_general_protection in traps.c , which print the 'trap ...' messages, and print_vma_addr in memory.c , which prints the 'in ...' portion for all of these messages. Sidebar: The various error code bits as numbers # +1 protection fault in a mapped area (eg writing to a read-only mapping) +2 write (instead of a read) +4 user mode access (instead of kernel mode access) +8 use of reserved bits in the page table entry detected (the kernel will panic if this happens) +16 (+0x10) fault was an instruction fetch, not data read or write +32 (+0x20) 'protection keys block access' (don't ask me) Hex 0x14 is 0x10 + 4; (hex) 6 is 4 + 2. Error code 7 (0x7) is 4 + 2 + 1, a user-mode write to a read-only mapping, and is what you get if you attempt to write to a string constant in C: char *ex = \"example\"; int main(int argc, char **argv) { *ex = 'E'; } Compile and run this and you will get: a.out[8832]: segfault at 400540 ip 0000000000400499 sp 00007ffce6831490 error 7 in a.out[400000+1000] It appears that the program code always gets loaded at 0x400000 for ordinary programs, although I believe that shared libraries can have their location randomized. PS: Per a comment in the kernel source, all accesses to addresses above the end of user space will be labeled as 'protection fault in a mapped area' whether or not there are actual page table entries there. The kernel does this so you can't work out where its memory pages are by looking at the error code. (I believe that user space normally ends around 0x07fffffffffff, per mm.txt , although see the comments about TASK_SIZE_MAX in processor.h and also page_64_types.h .)","title":"Dmesg-segfaulting"},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/#what-the-linux-kernels-messages-about-segfaulting-programs-mean-on-64-bit-x86","text":"For quite a while the Linux kernel has had an option to log a kernel message about every faulting user program , and it probably defaults to on in your Linux distribution. I've seen these messages fly by for years, but for reasons beyond the scope of this entry I've recently wanted to understand what they mean in some moderate amount of detail. I'll start with a straightforward and typical example, one that I see every time I build and test Go (as this is a test case that is supposed to crash): testp[19288]: segfault at 0 ip 0000000000401271 sp 00007fff2ce4d210 error 4 in testp[400000+98000] The meaning of this is: ' testp[19288] ' is the faulting program and its PID ' segfault at 0 ' tells us the memory address (in hex) that caused the segfault when the program tried to access it. Here the address is 0, so we have a null dereference of some sort. ' ip 0000000000401271 ' is the value of the instruction pointer at the time of the fault. This should be the instruction that attempted to do the invalid memory access. In 64-bit x86, this will be register %rip (useful for inspecting things in GDB and elsewhere). ' sp 00007fff2ce4d210 ' is the value of the stack pointer. In 64-bit x86, this will be %rsp . ' error 4 ' is the page fault error code bits from traps.h in hex, as usual, and will almost always be at least 4 (which means 'user-mode access'). A value of 4 means it was a read of an unmapped area , such as address 0, while a value of 6 (4+2) means it was a write of an unmapped area . ' in testp[400000+98000] ' tells us the specific virtual memory area that the instruction pointer is in, specifying which file it is (here it's the executable), the starting address that VMA is mapped at ( 0x400000 ), and the size of the mapping ( 0x98000 ). With a faulting address of 0 and an error code of 4, we know this particular segfault is a read of a null pointer . Here's two more error messages: bash[12235]: segfault at 1054808 ip 000000000041d989 sp 00007ffec1f1cbd8 error 6 in bash[400000+f4000] 'Error 6' means a write to an unmapped user address , here 0x1054808 . bash[11909]: segfault at 0 ip 00007f83c03db746 sp 00007ffccbeda010 error 4 in libc-2.23.so[7f83c0350000+1c0000] Error 4 and address 0 is a null pointer read but this time it's in some libc function, not in bash's own code, since it's reported as 'in libc-2.23.so [...]'. Since I looked at the core dump, I can tell you that this was in strlen() . On 64-bit x86 Linux, you'll get a somewhat different message if the problem is actually with the instruction being executed, not the address it's referencing. For example: bash[2848] trap invalid opcode ip:48db90 sp:7ffddc8879e8 error:0 in bash[400000+f4000] There are a number of such trap types set up in traps.c . Two notable additional ones are 'divide error', which you get if you do an integer division by zero, and 'general protection', which you can get for certain extremely wild pointers (one case I know of is when your 64-bit x86 address is not in 'canonical form' ). Although these fields are formatted slightly differently, most of them mean the same thing as in segfaults. The exception is ' error:0 ', which is not a page fault error code. I don't understand the relevant kernel code enough to know what it means, but if I'm reading between the lines correctly in entry_64.txt , then it's either 0 (the usual case) or an error code from the CPU. Here is one possible list of exceptions that get error codes. Sometimes these messages can be a little bit unusual and surprising. Here is a silly sample program and the error it produces when run. The code: ```","title":"What the Linux kernel's messages about segfaulting programs mean on 64-bit x86"},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/#include","text":"int main(int argc, char argv) { int ( p)(); p = 0x0; return printf(\"%d\\n\", ( p)()); } ``` If compiled (without optimization is best) and run, this generates the kernel message: a.out[3714]: segfault at 0 ip (null) sp 00007ffe872aa418 error 14 in a.out[400000+1000] The ' (null) ' bit turns out to be expected; it's what the general kernel printf() function generates when asked to print something as a pointer and it's null (as seen here ). In our case the instruction pointer is 0 (null) because we've made a subroutine call through a null pointer and thus we're trying to execute code at address 0. I don't know why the 'in ...' portion says that we're in the executable (although in this case the call actually was there). The error code of 14 is in hex, which means that as bits it's 010100. This is a user mode read of an unmapped area (our usual '4' case), but it's an instruction fetch, not a normal data read or write. Any error 14s are a sign of some form of mangled function call or a return to a mangled address because the stack has been mashed. (These bits turn out to come straight from the CPU's page fault IDT .) For 64-bit x86 Linux kernels (and possibly for 32-bit x86 ones as well), the code you want to look at is show_signal_msg in fault.c , which prints the general 'segfault at ..' message, do_trap and do_general_protection in traps.c , which print the 'trap ...' messages, and print_vma_addr in memory.c , which prints the 'in ...' portion for all of these messages.","title":"include "},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/#sidebar-the-various-error-code-bits-as-numbers","text":"+1 protection fault in a mapped area (eg writing to a read-only mapping) +2 write (instead of a read) +4 user mode access (instead of kernel mode access) +8 use of reserved bits in the page table entry detected (the kernel will panic if this happens) +16 (+0x10) fault was an instruction fetch, not data read or write +32 (+0x20) 'protection keys block access' (don't ask me) Hex 0x14 is 0x10 + 4; (hex) 6 is 4 + 2. Error code 7 (0x7) is 4 + 2 + 1, a user-mode write to a read-only mapping, and is what you get if you attempt to write to a string constant in C: char *ex = \"example\"; int main(int argc, char **argv) { *ex = 'E'; } Compile and run this and you will get: a.out[8832]: segfault at 400540 ip 0000000000400499 sp 00007ffce6831490 error 7 in a.out[400000+1000] It appears that the program code always gets loaded at 0x400000 for ordinary programs, although I believe that shared libraries can have their location randomized. PS: Per a comment in the kernel source, all accesses to addresses above the end of user space will be labeled as 'protection fault in a mapped area' whether or not there are actual page table entries there. The kernel does this so you can't work out where its memory pages are by looking at the error code. (I believe that user space normally ends around 0x07fffffffffff, per mm.txt , although see the comments about TASK_SIZE_MAX in processor.h and also page_64_types.h .)","title":"Sidebar: The various error code bits as numbers"},{"location":"Programming/08-Shell/Dmesg/Dmesg/","text":"How to find out why process was killed on server QuotePredict [8\u6708 1 09:20] traps: hsserver[194024] trap divide error ip:7f9090ab8885 sp:7f8e67ffe8c8 error:0 [ +0.000007] traps: hsserver[194014] trap divide error ip:7f9090ab8885 sp:7f8e83ffe8c8 error:0 [8\u6708 2 09:19] traps: hsserver[106631] trap divide error ip:7f94437d4885 sp:7f925e7fb8c8 error:0 in libfsc_quote_predict.so[7f9443771000+8a000]","title":"Dmesg"},{"location":"Programming/08-Shell/Linux-Kernel-module/","text":"MODINFO(8) # LSMOD(8) # MODPROBE(8) #","title":"Introduction"},{"location":"Programming/08-Shell/Linux-Kernel-module/#modinfo8","text":"","title":"MODINFO(8)"},{"location":"Programming/08-Shell/Linux-Kernel-module/#lsmod8","text":"","title":"LSMOD(8)"},{"location":"Programming/08-Shell/Linux-Kernel-module/#modprobe8","text":"","title":"MODPROBE(8)"}]}