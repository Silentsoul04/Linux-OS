{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u9879\u76ee # \u4ece\u591a\u4e2a\u65b9\u9762\u5bf9Linux operating system\u8fdb\u884c\u5206\u6790\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4e0b\u51e0\u4e2a\u65b9\u9762: Architecture # \u63cf\u8ff0Computing system\u7684architecture\u3002 Kernel # \u5305\u542b kernel \u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u4e3b\u8981\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 linux kernel \u7684\u5b9e\u73b0\uff0c\u4e3b\u8981\u53c2\u8003\u4e66\u7c4d\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b Programming # \u5305\u542bLinux operating system\u4e2d\u8fdb\u884cprogramming\u65f6\u9700\u8981\u638c\u63e1\u7684\u6240\u6709\u77e5\u8bc6\uff0c\u5305\u62ec\uff1a system call interface philosophy \u4e3b\u8981\u53c2\u8003\uff1a Advanced Programming in the UNIX\u00ae Environment, Third Edition The Linux Programming Interface man7.org Linux man pages die.net Linux man pages Shell # Linux operating system\u4e2d\u7684\u5e38\u89c1\u547d\u4ee4 \u4e3b\u8981\u53c2\u8003\uff1a Linux Documentation man7.org Linux man pages die.net Linux man pages","title":"Home"},{"location":"#_1","text":"\u4ece\u591a\u4e2a\u65b9\u9762\u5bf9Linux operating system\u8fdb\u884c\u5206\u6790\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4e0b\u51e0\u4e2a\u65b9\u9762:","title":"\u5173\u4e8e\u672c\u9879\u76ee"},{"location":"#architecture","text":"\u63cf\u8ff0Computing system\u7684architecture\u3002","title":"Architecture"},{"location":"#kernel","text":"\u5305\u542b kernel \u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u4e3b\u8981\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 linux kernel \u7684\u5b9e\u73b0\uff0c\u4e3b\u8981\u53c2\u8003\u4e66\u7c4d\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b","title":"Kernel"},{"location":"#programming","text":"\u5305\u542bLinux operating system\u4e2d\u8fdb\u884cprogramming\u65f6\u9700\u8981\u638c\u63e1\u7684\u6240\u6709\u77e5\u8bc6\uff0c\u5305\u62ec\uff1a system call interface philosophy \u4e3b\u8981\u53c2\u8003\uff1a Advanced Programming in the UNIX\u00ae Environment, Third Edition The Linux Programming Interface man7.org Linux man pages die.net Linux man pages","title":"Programming"},{"location":"#shell","text":"Linux operating system\u4e2d\u7684\u5e38\u89c1\u547d\u4ee4 \u4e3b\u8981\u53c2\u8003\uff1a Linux Documentation man7.org Linux man pages die.net Linux man pages","title":"Shell"},{"location":"Operating-system/","text":"Operating system\u6982\u8ff0 # Operating system\uff08\u7b80\u79f0OS\uff09\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u6d89\u53ca\u7684\u5185\u5bb9\u975e\u5e38\u591a\u3002\u4f5c\u4e3a\u4e00\u540dsoftware engineer\uff0c\u638c\u63e1operating system\u7684\u57fa\u7840\u77e5\u8bc6\u662f\u5fc5\u987b\u7684\u3002\u672c\u8282\u662f\u5bf9OS\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Operating system \u3002 An operating system ( OS ) is system software that manages computer hardware and software resources and provides common services for computer programs . Types of operating systems # NOTE: \u4e0b\u9762\u7f57\u5217\u4e86\u591a\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u6bcf\u79cd\u5206\u7c7b\u65b9\u6cd5\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cd\u7279\u6027\u3002\u663e\u7136\u6709\u5f88\u591a\u7684operating system\u53ef\u4ee5\u517c\u5177\u591a\u79cd\u7279\u6027\uff0c\u6bd4\u5982linux\uff0c\u5b83\u5177\u5907multi-tasking\u3001 multi-user\u7b49\u7279\u6027 Single-tasking and multi-tasking # single-tasking operating system multi-tasking operating system Single- and multi-user # single-user operating system multi-user operating system Distributed # distributed operating system Templated # templating Embedded # embedded operating systems Real-time # real-time operating system Examples # Unix and Unix-like operating systems # Main article: Unix Unix Unix-like System V BSD Linux . Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants. BSD and its descendants # Main article: Berkeley Software Distribution FreeBSD NetBSD OpenBSD macOS # Main article: macOS Linux # Main articles: Linux and Linux kernel The Linux kernel is used in some popular distributions, such as Red Hat , Debian , Ubuntu , Linux Mint and Google 's Android , Chrome OS , and Chromium OS . Microsoft Windows # Main article: Microsoft Windows Components # The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component. Kernel # A kernel connects the application software to the hardware of a computer. Main article: Kernel (computing) Program execution # Main article: Process (computing) Interrupts # Main article: Interrupt Modes # Main articles: User mode and Supervisor mode Memory management # Main article: Memory management Virtual memory # Main article: Virtual memory Further information: Page fault Multitasking # Main articles: Computer multitasking and Process management (computing) Further information: Context switch , Preemptive multitasking , and Cooperative multitasking Disk access and file systems # Main article: Virtual file system Device drivers # Main article: Device driver Networking # Main article: Computer network Security # Main article: Computer security User interface # Main article: Operating system user interface Diversity of operating systems and portability # Market share # Further information: Usage share of operating systems \u5173\u4e8eOS\u7684\u66f4\u52a0\u6df1\u523b\u7684\u63cf\u8ff0 # \u4e0a\u8ff0\u5185\u5bb9\u662f\u975e\u5e38\u6982\u62ec\u7684\uff0c\u4e0b\u9762\u662f\u5173\u4e8eOS\u7684\u66f4\u52a0\u6df1\u523b\u7684\u63cf\u8ff0\uff1a Understanding.The.Linux.kernel.3rd.Edition \u76841.4. Basic Operating System Concepts","title":"Operating-system-\u6982\u8ff0"},{"location":"Operating-system/#operating-system","text":"Operating system\uff08\u7b80\u79f0OS\uff09\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u4e3b\u9898\uff0c\u6d89\u53ca\u7684\u5185\u5bb9\u975e\u5e38\u591a\u3002\u4f5c\u4e3a\u4e00\u540dsoftware engineer\uff0c\u638c\u63e1operating system\u7684\u57fa\u7840\u77e5\u8bc6\u662f\u5fc5\u987b\u7684\u3002\u672c\u8282\u662f\u5bf9OS\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Operating system \u3002 An operating system ( OS ) is system software that manages computer hardware and software resources and provides common services for computer programs .","title":"Operating system\u6982\u8ff0"},{"location":"Operating-system/#types-of-operating-systems","text":"NOTE: \u4e0b\u9762\u7f57\u5217\u4e86\u591a\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u6bcf\u79cd\u5206\u7c7b\u65b9\u6cd5\u5176\u5b9e\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cd\u7279\u6027\u3002\u663e\u7136\u6709\u5f88\u591a\u7684operating system\u53ef\u4ee5\u517c\u5177\u591a\u79cd\u7279\u6027\uff0c\u6bd4\u5982linux\uff0c\u5b83\u5177\u5907multi-tasking\u3001 multi-user\u7b49\u7279\u6027","title":"Types of operating systems"},{"location":"Operating-system/#single-tasking-and-multi-tasking","text":"single-tasking operating system multi-tasking operating system","title":"Single-tasking and multi-tasking"},{"location":"Operating-system/#single-and-multi-user","text":"single-user operating system multi-user operating system","title":"Single- and multi-user"},{"location":"Operating-system/#distributed","text":"distributed operating system","title":"Distributed"},{"location":"Operating-system/#templated","text":"templating","title":"Templated"},{"location":"Operating-system/#embedded","text":"embedded operating systems","title":"Embedded"},{"location":"Operating-system/#real-time","text":"real-time operating system","title":"Real-time"},{"location":"Operating-system/#examples","text":"","title":"Examples"},{"location":"Operating-system/#unix-and-unix-like-operating-systems","text":"Main article: Unix Unix Unix-like System V BSD Linux . Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.","title":"Unix and Unix-like operating systems"},{"location":"Operating-system/#bsd-and-its-descendants","text":"Main article: Berkeley Software Distribution FreeBSD NetBSD OpenBSD","title":"BSD and its descendants"},{"location":"Operating-system/#macos","text":"Main article: macOS","title":"macOS"},{"location":"Operating-system/#linux","text":"Main articles: Linux and Linux kernel The Linux kernel is used in some popular distributions, such as Red Hat , Debian , Ubuntu , Linux Mint and Google 's Android , Chrome OS , and Chromium OS .","title":"Linux"},{"location":"Operating-system/#microsoft-windows","text":"Main article: Microsoft Windows","title":"Microsoft Windows"},{"location":"Operating-system/#components","text":"The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.","title":"Components"},{"location":"Operating-system/#kernel","text":"A kernel connects the application software to the hardware of a computer. Main article: Kernel (computing)","title":"Kernel"},{"location":"Operating-system/#program-execution","text":"Main article: Process (computing)","title":"Program execution"},{"location":"Operating-system/#interrupts","text":"Main article: Interrupt","title":"Interrupts"},{"location":"Operating-system/#modes","text":"Main articles: User mode and Supervisor mode","title":"Modes"},{"location":"Operating-system/#memory-management","text":"Main article: Memory management","title":"Memory management"},{"location":"Operating-system/#virtual-memory","text":"Main article: Virtual memory Further information: Page fault","title":"Virtual memory"},{"location":"Operating-system/#multitasking","text":"Main articles: Computer multitasking and Process management (computing) Further information: Context switch , Preemptive multitasking , and Cooperative multitasking","title":"Multitasking"},{"location":"Operating-system/#disk-access-and-file-systems","text":"Main article: Virtual file system","title":"Disk access and file systems"},{"location":"Operating-system/#device-drivers","text":"Main article: Device driver","title":"Device drivers"},{"location":"Operating-system/#networking","text":"Main article: Computer network","title":"Networking"},{"location":"Operating-system/#security","text":"Main article: Computer security","title":"Security"},{"location":"Operating-system/#user-interface","text":"Main article: Operating system user interface","title":"User interface"},{"location":"Operating-system/#diversity-of-operating-systems-and-portability","text":"","title":"Diversity of operating systems and portability"},{"location":"Operating-system/#market-share","text":"Further information: Usage share of operating systems","title":"Market share"},{"location":"Operating-system/#os","text":"\u4e0a\u8ff0\u5185\u5bb9\u662f\u975e\u5e38\u6982\u62ec\u7684\uff0c\u4e0b\u9762\u662f\u5173\u4e8eOS\u7684\u66f4\u52a0\u6df1\u523b\u7684\u63cf\u8ff0\uff1a Understanding.The.Linux.kernel.3rd.Edition \u76841.4. Basic Operating System Concepts","title":"\u5173\u4e8eOS\u7684\u66f4\u52a0\u6df1\u523b\u7684\u63cf\u8ff0"},{"location":"TODO/","text":"20190817 # \u4eca\u5929\u5728\u9605\u8bfb\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b\u76846.2.1.2. The jiffies variable\u7ae0\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u5bf9\u539f\u5b50\u6027\u7684\u601d\u8003\uff1a You might wonder why jiffies has not been directly declared as a 64-bit unsigned long long integer on the 80 x 86 architecture. The answer is that accesses to 64-bit variables in 32-bit architectures cannot be done atomically . Therefore, every read operation on the whole 64 bits requires some synchronization technique to ensure that the counter is not updated while the two 32-bit half-counters are read; as a consequence, every 64-bit read operation is significantly slower than a 32-bit read operation. \u572832\u4f4d\u7684\u673a\u5668\u4e2d\uff0c\u4e00\u6b21\u80fd\u591f\u8bfb\u53d6\u7684\u6570\u636e\u7684\u957f\u5ea6\u4e3a32\u4f4d\uff0c\u6240\u4ee5\u8bfb\u53d6\u8d85\u8fc732\u7684\u6570\u636e\u5c31\u9700\u8981\u591a\u6761\u6307\u4ee4\uff0c\u663e\u7136\uff0c\u8fd9\u5c31\u4e0d\u662f\u539f\u5b50\u6027\u7684\u4e86\uff1b \u8054\u60f3\u5230\u4eca\u5929\u5728\u9605\u8bfb\u300a\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\u300b\u76841.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807\uff0c\u6211\u6240\u505a\u7684\u7b14\u8bb0\u5982\u4e0b\uff1a \u6307\u5904\u7406\u673a \u8fd0\u7b97\u5668 \u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684 \u4f4d\u6570 \uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b SUMMARY : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right SUMMARY : \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f \u73b0\u5728\u8054\u7cfb\u5230\u539f\u5b50\u6027\uff0c\u663e\u7136\uff0c\u4e0a\u8ff0\u8fd9\u6bb5\u8bdd\u4e2d\u7684 \u4e00\u6b21 \u7684\u542b\u4e49\u662f\u975e\u5e38\u6df1\u523b\u7684\uff1a\u5b83\u8574\u542b\u7740\u539f\u5b50\u6027\u7684\u4fdd\u8bc1\uff1b \u5176\u5b9e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u5728\u4e4b\u524d\u5c31\u5df2\u7ecf\u9047\u5230\u8fc7\u7684\uff0c\u8bb0\u5f97\u5f53\u65f6\u9605\u8bfb\u7684\u6587\u7ae0\u662f\uff1a Atomic vs. Non-Atomic Operations \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6211\u5df2\u7ecf\u6536\u5f55\u4e86\uff1b \u7efc\u4e0a\u6240\u8ff0\uff0c\u5176\u5b9e\u6211\u7684\u95ee\u9898\u53ef\u4ee5\u5f52\u7eb3\u4e3a\uff1awhy read 64 bit data in 32 bit is not atomic Google\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a How to Customize Serialization in Java Using the Externalizable Interface Are 64 bit operations atomic for a 32 bit app on 64 bit Windows 20190829 # epoll_wait \u3001 epoll_pwait \u548c\u4fe1\u53f7\u4e4b\u95f4\u7684\u5173\u7cfb # http://man7.org/linux/man-pages/man2/epoll_wait.2.html epoll and nonblocking # \u4f7f\u7528epoll\u7684\u65f6\u5019\uff0c\u662f\u5426\u4e00\u5b9a\u8981\u4f7f\u7528nonblocking IO\uff1f 20190905 # passing file descriptor between process # https://docs.python.org/3.5/library/multiprocessing.html \u5728python\u7684multiprocessing\u6587\u6863\u4e2d\u770b\u5230\u4e86\u5b83\u63d0\u51fa\u7684\u8fd9\u4e2a\u95ee\u9898\uff1a forkserver When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited. Available on Unix platforms which support passing file descriptors over Unix pipes. http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch17lev1sec4.html https://openforums.wordpress.com/2016/08/07/open-file-descriptor-passing-over-unix-domain-sockets/ semaphore tracker process # https://docs.python.org/3.5/library/multiprocessing.html Synchronization between processes # https://stackoverflow.com/questions/248911/how-do-i-synchronize-two-processes https://en.wikipedia.org/wiki/Semaphore_%28programming%29 http://sce2.umkc.edu/csee/cotterr/cs431_sp13/CS431_Linux_Process_Sync_12_bw.ppt 20190906 # https://stackoverflow.com/questions/11129212/tcp-can-two-different-sockets-share-a-port https://lwn.net/Articles/542629/ https://stackoverflow.com/questions/1694144/can-two-applications-listen-to-the-same-port/25033226 20190909 # TCP backlog # https://stackoverflow.com/questions/36594400/what-is-backlog-in-tcp-connections http://www.linuxjournal.com/files/linuxjournal.com/linuxjournal/articles/023/2333/2333s2.html https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html 20190913 # atomic & race condition & lock & consistent model \u4e13\u9898 # \u4eca\u5929\u5728\u9605\u8bfbredis source code\u7684\u65f6\u5019\uff0c\u603b\u7ed3\u4e86redis\u4e2dsocket file descriptor\u90fd\u662fnon blocking\u7684\uff0c\u7136\u540e\u6211\u5c31\u67e5\u9605APUE\u4e2d\u5173\u4e8e\u6587\u4ef6\u63cf\u8ff0\u7b26\u6807\u5fd7\uff0c\u6587\u4ef6\u6807\u5fd7\u7684\u5185\u5bb9\uff1b\u53c8\u91cd\u65b0\u770b\u4e86\u4e00\u904dAPUE 3.3\u8282\u4e2d\u5173\u4e8eTOCTTOU\u7684\u63cf\u8ff0\uff0c\u4ee5\u53caAPUE 3.11 \u539f\u5b50\u64cd\u4f5c\u7684\u63cf\u8ff0\uff0c\u6211\u624d\u610f\u8bc6\u5230\u539f\u7406linux\u7684system call\u4e5f\u662f\u80fd\u591f\u4fdd\u8bc1atomic\u7684\uff08\u4ece\u5e95\u5c42\u5b9e\u73b0\u6765\u770b\uff0c\u662f\u56e0\u4e3abig kernel lock \uff09\uff1b\u539f\u6765\u6211\u4ec5\u4ec5\u8ba4\u77e5\u5230\u4e00\u4e2ainstruction\u662fatomic\uff0c\u5176\u5b9e\u8fd9\u79cd\u8ba4\u77e5\u662f\u6bd4\u8f83\u5c40\u9650\u7684\uff1b\u8054\u60f3\u5230\u6211\u4e4b\u524d\u603b\u7ed3\u8fc7\u5173\u4e8eatomic\u7684\u5185\u5bb9\uff0c\u5982\u4e0b\uff1a everything\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u627e\u6240\u6709atomic\u76f8\u5173\u7684\u5185\u5bb9\uff1a nowholeword:c:\\users\\dengkai17334\\appdata\\local\\ynote\\data\\ content:atomic \u73b0\u5728\u662f\u6709\u5fc5\u8981\u6574\u7406\u4e00\u756a\u4e86\u3002 linux system call atomic # network # data integrity in network # \u5728 2.1. Two Types of Internet Sockets \u4e2d\u63d0\u53caTCP\u80fd\u591f\u4fdd\u8bc1 data integrity \uff0c\u53ef\u89c1 data integrity \u5e76\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u4e00\u65b9\u9762\uff1b What's the difference between 127.0.0.1 and 0.0.0.0? # What's the difference between 127.0.0.1 and 0.0.0.0? interface address # \u5728\u5f88\u591a\u5730\u65b9\u90fd\u89c1\u5230\u4e86\u8fd9\u4e2a\u8bcd\uff1a - getaddrinfo(3) - Linux man page # \u5b83\u5230\u5e95\u662f\u4ec0\u4e48\u542b\u4e49 stream of bytes and bitstream # Bitstream \u5df2\u7ecf\u9605\u8bfb Maximum segment lifetime # https://en.wikipedia.org/wiki/Maximum_segment_lifetime 20190909 # epoll\u4e2d\u7684file descripor\u662f\u5426\u4e00\u5b9a\u8981\u8bbe\u7f6e\u4e3anon blocking SO_RCVLOWAT and SO_SNDLOWAT \u7684\u503c\u90fd\u662f1\uff0c\u90a3\u4e48\u5982\u4f55\u786e\u5b9amessage boundary\uff1f shell # getconf echo \"2^12\" | bc How to check if port is in use on Linux or Unix https://unix.stackexchange.com/a/185767 how to get all threads of a process process # https://www.cyberciti.biz/faq/show-all-running-processes-in-linux/ https://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes How to print out a variable in makefile # https://stackoverflow.com/questions/16467718/how-to-print-out-a-variable-in-makefile Process control block # https://www.tldp.org/LDP/lki/lki-2.html \u4e0a\u9762\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u8bb2\u8ff0\u7684\u662flinux kernel\u7684\u5b9e\u73b0 How to Find Out Which Windows Process is Using a File # user the software:Process Explorer How find out which process is using a file in Linux? # fuser file_name what will happen if a process exceed its resource limits # \u8bb0\u5f97\u5728redis in action\u8fd9\u672c\u4e66\u4e2d\u6709\u63d0\u53ca\u8fc7\u7c7b\u4f3c\u7684\u95ee\u9898\uff1b C POSIX library # https://en.wikipedia.org/wiki/C_POSIX_library http://pubs.opengroup.org/onlinepubs/9699919799/idx/head.html C standard library # https://en.wikipedia.org/wiki/C_standard_library process id # APUE 4.4 Set-User-ID and Set-Group-ID https://en.wikipedia.org/wiki/User_identifier Time of check to time of use # https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use User identifier # https://en.wikipedia.org/wiki/User_identifier Secure Programming HOWTO # https://dwheeler.com/secure-programs/Secure-Programs-HOWTO/index.html thread-local # thread-local\u548creentry\u4e4b\u95f4\u7684\u5173\u7cfb hole in file # hole\u5bf9\u6587\u4ef6\u5927\u5c0f\u7684\u5f71\u54cd\uff1b\u4e0a\u5468\u5728\u67e5\u770bhttps://liftoff.github.io/pyminifier/\u6587\u6863\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709minifying\u529f\u80fd\uff0c\u662f\u548chole in file\u6709\u5173\u7684\uff1b \u540c\u65f6binary mode\u6765\u4fdd\u5b58\u6587\u4ef6\u4e5f\u80fd\u591f\u964d\u4f4e\u6587\u4ef6\u5927\u5c0f\uff1b linux memory # memory usage memory available virtual process space # https://www.tutorialspoint.com/where-are-static-variables-stored-in-c-cplusplus https://cs61.seas.harvard.edu/wiki/2016/Kernel2X \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d https://www.cs.utexas.edu/~lorenzo/corsi/cs372/06F/hw/3sol.html https://cs.stackexchange.com/questions/56825/how-to-calculate-virtual-address-space-from-page-size-virtual-address-size-and process environment # 7.12\u4e0d\u540c\u8bed\u8a00\u7684environment\u662f\u5426\u4e0d\u540c file IO\uff1apython VS linux # Python\u4e0eLinux\u7c7b\u4f3c\uff0c\u4e0e\u6587\u4ef6\u76f8\u5173\u7684\u64cd\u4f5c\u90fd\u662f\u4eceopen\u51fd\u6570\u5f00\u59cb\u7684 Fragmentation (computing) # https://en.wikipedia.org/wiki/Fragmentation_(computing) page table size # https://www.cs.cornell.edu/courses/cs4410/2015su/lectures/lec14-pagetables.html http://www.cs.cornell.edu/courses/cs4410/2016su/slides/lecture11.pdf http://www.cs.cornell.edu/courses/cs4410/2016su/schedule.html Data segment # https://en.wikipedia.org/wiki/Data_segment ENOENT # Why does ENOENT mean \u201cNo such file or directory\u201d? EACCES # cron # https://en.wikipedia.org/wiki/Cron https://www.adminschoice.com/crontab-quick-reference \u5728shell\u4e2d\u5220\u9664\u6389\u8fdb\u7a0b\u4f7f\u7528\u7684\u6587\u4ef6 # \u4eca\u5929\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u53d1\u73b0\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u95ee\u9898\uff1a\u8fdb\u7a0b\u8fd0\u884c\u4e2d\uff0c\u8fd9\u4e2a\u8fdb\u7a0b\u4f1a\u4e0d\u65ad\u5730\u5411\u5176\u65e5\u5fd7\u6587\u4ef6\u4e2d\u5199\u5165\u65e5\u5fd7\uff1b\u7136\u540e\u6211\u5728shell\u4e2d\u5c06\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\u7ed9\u5220\u9664\u4e86\uff0c\u53d1\u73b0\u8fdb\u7a0b\u5e76\u6ca1\u6709\u53d1\u73b0\u5b83\u7684\u65e5\u5fd7\u6587\u4ef6\u88ab\u5220\u4e86\uff0c\u4e5f\u6ca1\u6709\u51fa\u73b0\u521b\u5efa\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\uff0c\u7a0b\u5e8f\u4e5f\u6ca1\u6709\u505c\u6b62\u4e0b\u6765\uff1b \u78c1\u76d8\u7a7a\u95f4\u6ee1\u540e\uff0c\u4e5f\u4f1a\u5bfc\u81f4process\u65e0\u6cd5\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff1b\u4f46\u662f\u5f53\u91ca\u653e\u4e00\u90e8\u5206\u7a7a\u95f4\u540e\uff0c\u53d1\u73b0process\u4ecd\u7136\u4e0d\u4f1a\u5199\u5165\uff0c\u5c31\u50cf\u662f\u653e\u5f03\u4e86\u4e00\u6837\uff1b Segmentation fault # hiredis\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u4eca\u5929\u5728\u591a\u7ebf\u7a0b\u73af\u5883\u4e0b\u6d4b\u8bd5\u51fa\u5b83\u4f1a\u5bfc\u81f4process core dump\uff0cdump\u7684\u539f\u56e0\u662f Program terminated with signal 11, Segmentation fault. https://kb.iu.edu/d/aqsj https://en.wikipedia.org/wiki/Segmentation_fault thread unsafe and core dump # how to test # \u5982\u679c\u662f\u652f\u6301\u7f51\u7edc\uff0c\u9700\u8981\u6d4b\u8bd5\u591a\u4e2aclient\u8fde\u63a5\uff1b \u9700\u8981\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5 \u9700\u8981\u8fdb\u884c\u5e76\u53d1\u6d4b\u8bd5 process and its thread # Is there a way to see details of all the threads that a process has in Linux? google:how to get all threads of a process https://www.unix.com/aix/154772-how-list-all-threads-running-process.html http://ask.xmodulo.com/view-threads-process-linux.html how OS know process use a illegal memory location # https://en.wikipedia.org/wiki/Memory_protection https://stackoverflow.com/questions/41172563/how-os-catches-illegal-memory-references-at-paging-scheme https://unix.stackexchange.com/questions/511963/how-linux-finds-out-about-illegal-memory-access-error https://www.kernel.org/ https://www.kernel.org/doc/gorman/html/understand/index.html https://www.kernel.org/doc/gorman/ https://www.kernel.org/doc/ dev file # https://unix.stackexchange.com/questions/93531/what-is-stored-in-dev-pts-files-and-can-we-open-them Unix memory usage # https://utcc.utoronto.ca/~cks/space/blog/linux/LinuxMemoryStats https://utcc.utoronto.ca/~cks/space/blog/unix/UnderstandingRSS https://stackoverflow.com/questions/131303/how-to-measure-actual-memory-usage-of-an-application-or-process https://unix.stackexchange.com/questions/554/how-to-monitor-cpu-memory-usage-of-a-single-process How can I kill a process by name instead of PID? # https://stackoverflow.com/questions/160924/how-can-i-kill-a-process-by-name-instead-of-pid http://osxdaily.com/2017/01/12/kill-process-by-name-command-line/ APUE\u7684\u300aUnix-interruption-and-atom\u300b\u8fd8\u6ca1\u6709wanch # \u5176\u4e2d\u4e3b\u8981\u8ba8\u8bba\u4e86Atomicity Consistency models # \u5728parallel computing\u4e2d\u7684Consistency models\u8fd8\u6ca1\u6709\u5b8c\u6210 https://en.wikipedia.org/wiki/Category:Consistency_models epoll # https://en.wikipedia.org/wiki/Epoll youdao Unix-abort # init of linux # init\u8fdb\u7a0b # ch8.2\u4e2d\u6709\u5bf9init\u8fdb\u7a0b\u7684\u4e00\u4e2a\u4ecb\u7ecd ch9.2\u4ecb\u7ecd\u5230\uff0cinit\u4f1a\u8bfb\u53d6\u6587\u4ef6 /etc/ttys how to know what init system linux use https://unix.stackexchange.com/questions/18209/detect-init-system-using-the-shell https://fedoramagazine.org/what-is-an-init-system/ https://en.wikipedia.org/wiki/Init https://en.wikipedia.org/wiki/Systemd Convert between Unix and Windows text files # https://kb.iu.edu/d/acux https://stackoverflow.com/questions/16239551/eol-conversion-in-notepad Memory management algorithms # https://en.wikipedia.org/wiki/Category:Memory_management_algorithms Slab allocation interrupt in Unix # interrupt vector table http://www.cis.upenn.edu/~lee/03cse380/lectures/ln2-process-v4.pdf \u5728 Context switch \u4e2d\u4e5f\u5bf9interrupt\u8fdb\u884c\u4e86\u4ecb\u7ecd signal # signal\u4e5f\u662f\u4e00\u79cdinterrupt\uff0c\u6545\u5c06\u5b83\u653e\u5728interrupt\u4e4b\u4e0b Google unix signals and threads # https://stackoverflow.com/questions/2575106/posix-threads-and-signals https://en.wikipedia.org/wiki/Signal_(IPC) Scheduling (computing) # https://en.wikipedia.org/wiki/Scheduling_(computing) thread join and detach # wait # https://linux.die.net/man/2/waitpid","title":"20190817"},{"location":"TODO/#20190817","text":"\u4eca\u5929\u5728\u9605\u8bfb\u300a Understanding.The.Linux.kernel.3rd.Edition \u300b\u76846.2.1.2. The jiffies variable\u7ae0\u8282\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u5bf9\u539f\u5b50\u6027\u7684\u601d\u8003\uff1a You might wonder why jiffies has not been directly declared as a 64-bit unsigned long long integer on the 80 x 86 architecture. The answer is that accesses to 64-bit variables in 32-bit architectures cannot be done atomically . Therefore, every read operation on the whole 64 bits requires some synchronization technique to ensure that the counter is not updated while the two 32-bit half-counters are read; as a consequence, every 64-bit read operation is significantly slower than a 32-bit read operation. \u572832\u4f4d\u7684\u673a\u5668\u4e2d\uff0c\u4e00\u6b21\u80fd\u591f\u8bfb\u53d6\u7684\u6570\u636e\u7684\u957f\u5ea6\u4e3a32\u4f4d\uff0c\u6240\u4ee5\u8bfb\u53d6\u8d85\u8fc732\u7684\u6570\u636e\u5c31\u9700\u8981\u591a\u6761\u6307\u4ee4\uff0c\u663e\u7136\uff0c\u8fd9\u5c31\u4e0d\u662f\u539f\u5b50\u6027\u7684\u4e86\uff1b \u8054\u60f3\u5230\u4eca\u5929\u5728\u9605\u8bfb\u300a\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\u300b\u76841.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807\uff0c\u6211\u6240\u505a\u7684\u7b14\u8bb0\u5982\u4e0b\uff1a \u6307\u5904\u7406\u673a \u8fd0\u7b97\u5668 \u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684 \u4f4d\u6570 \uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b SUMMARY : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right SUMMARY : \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f \u73b0\u5728\u8054\u7cfb\u5230\u539f\u5b50\u6027\uff0c\u663e\u7136\uff0c\u4e0a\u8ff0\u8fd9\u6bb5\u8bdd\u4e2d\u7684 \u4e00\u6b21 \u7684\u542b\u4e49\u662f\u975e\u5e38\u6df1\u523b\u7684\uff1a\u5b83\u8574\u542b\u7740\u539f\u5b50\u6027\u7684\u4fdd\u8bc1\uff1b \u5176\u5b9e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u5728\u4e4b\u524d\u5c31\u5df2\u7ecf\u9047\u5230\u8fc7\u7684\uff0c\u8bb0\u5f97\u5f53\u65f6\u9605\u8bfb\u7684\u6587\u7ae0\u662f\uff1a Atomic vs. Non-Atomic Operations \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6211\u5df2\u7ecf\u6536\u5f55\u4e86\uff1b \u7efc\u4e0a\u6240\u8ff0\uff0c\u5176\u5b9e\u6211\u7684\u95ee\u9898\u53ef\u4ee5\u5f52\u7eb3\u4e3a\uff1awhy read 64 bit data in 32 bit is not atomic Google\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a How to Customize Serialization in Java Using the Externalizable Interface Are 64 bit operations atomic for a 32 bit app on 64 bit Windows","title":"20190817"},{"location":"TODO/#20190829","text":"","title":"20190829"},{"location":"TODO/#epoll_wait-epoll_pwait","text":"http://man7.org/linux/man-pages/man2/epoll_wait.2.html","title":"epoll_wait \u3001 epoll_pwait\u548c\u4fe1\u53f7\u4e4b\u95f4\u7684\u5173\u7cfb"},{"location":"TODO/#epoll-and-nonblocking","text":"\u4f7f\u7528epoll\u7684\u65f6\u5019\uff0c\u662f\u5426\u4e00\u5b9a\u8981\u4f7f\u7528nonblocking IO\uff1f","title":"epoll and nonblocking"},{"location":"TODO/#20190905","text":"","title":"20190905"},{"location":"TODO/#passing-file-descriptor-between-process","text":"https://docs.python.org/3.5/library/multiprocessing.html \u5728python\u7684multiprocessing\u6587\u6863\u4e2d\u770b\u5230\u4e86\u5b83\u63d0\u51fa\u7684\u8fd9\u4e2a\u95ee\u9898\uff1a forkserver When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited. Available on Unix platforms which support passing file descriptors over Unix pipes. http://poincare.matf.bg.ac.rs/~ivana/courses/ps/sistemi_knjige/pomocno/apue/APUE/0201433079/ch17lev1sec4.html https://openforums.wordpress.com/2016/08/07/open-file-descriptor-passing-over-unix-domain-sockets/","title":"passing file descriptor between process"},{"location":"TODO/#semaphore-tracker-process","text":"https://docs.python.org/3.5/library/multiprocessing.html","title":"semaphore tracker process"},{"location":"TODO/#synchronization-between-processes","text":"https://stackoverflow.com/questions/248911/how-do-i-synchronize-two-processes https://en.wikipedia.org/wiki/Semaphore_%28programming%29 http://sce2.umkc.edu/csee/cotterr/cs431_sp13/CS431_Linux_Process_Sync_12_bw.ppt","title":"Synchronization between processes"},{"location":"TODO/#20190906","text":"https://stackoverflow.com/questions/11129212/tcp-can-two-different-sockets-share-a-port https://lwn.net/Articles/542629/ https://stackoverflow.com/questions/1694144/can-two-applications-listen-to-the-same-port/25033226","title":"20190906"},{"location":"TODO/#20190909","text":"","title":"20190909"},{"location":"TODO/#tcp-backlog","text":"https://stackoverflow.com/questions/36594400/what-is-backlog-in-tcp-connections http://www.linuxjournal.com/files/linuxjournal.com/linuxjournal/articles/023/2333/2333s2.html https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html","title":"TCP backlog"},{"location":"TODO/#20190913","text":"","title":"20190913"},{"location":"TODO/#atomic-race-condition-lock-consistent-model","text":"\u4eca\u5929\u5728\u9605\u8bfbredis source code\u7684\u65f6\u5019\uff0c\u603b\u7ed3\u4e86redis\u4e2dsocket file descriptor\u90fd\u662fnon blocking\u7684\uff0c\u7136\u540e\u6211\u5c31\u67e5\u9605APUE\u4e2d\u5173\u4e8e\u6587\u4ef6\u63cf\u8ff0\u7b26\u6807\u5fd7\uff0c\u6587\u4ef6\u6807\u5fd7\u7684\u5185\u5bb9\uff1b\u53c8\u91cd\u65b0\u770b\u4e86\u4e00\u904dAPUE 3.3\u8282\u4e2d\u5173\u4e8eTOCTTOU\u7684\u63cf\u8ff0\uff0c\u4ee5\u53caAPUE 3.11 \u539f\u5b50\u64cd\u4f5c\u7684\u63cf\u8ff0\uff0c\u6211\u624d\u610f\u8bc6\u5230\u539f\u7406linux\u7684system call\u4e5f\u662f\u80fd\u591f\u4fdd\u8bc1atomic\u7684\uff08\u4ece\u5e95\u5c42\u5b9e\u73b0\u6765\u770b\uff0c\u662f\u56e0\u4e3abig kernel lock \uff09\uff1b\u539f\u6765\u6211\u4ec5\u4ec5\u8ba4\u77e5\u5230\u4e00\u4e2ainstruction\u662fatomic\uff0c\u5176\u5b9e\u8fd9\u79cd\u8ba4\u77e5\u662f\u6bd4\u8f83\u5c40\u9650\u7684\uff1b\u8054\u60f3\u5230\u6211\u4e4b\u524d\u603b\u7ed3\u8fc7\u5173\u4e8eatomic\u7684\u5185\u5bb9\uff0c\u5982\u4e0b\uff1a everything\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u627e\u6240\u6709atomic\u76f8\u5173\u7684\u5185\u5bb9\uff1a nowholeword:c:\\users\\dengkai17334\\appdata\\local\\ynote\\data\\ content:atomic \u73b0\u5728\u662f\u6709\u5fc5\u8981\u6574\u7406\u4e00\u756a\u4e86\u3002","title":"atomic &amp; race condition &amp; lock &amp; consistent model \u4e13\u9898"},{"location":"TODO/#linux-system-call-atomic","text":"","title":"linux system call atomic"},{"location":"TODO/#network","text":"","title":"network"},{"location":"TODO/#data-integrity-in-network","text":"\u5728 2.1. Two Types of Internet Sockets \u4e2d\u63d0\u53caTCP\u80fd\u591f\u4fdd\u8bc1 data integrity \uff0c\u53ef\u89c1 data integrity \u5e76\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8e\u4e00\u65b9\u9762\uff1b","title":"data integrity in network"},{"location":"TODO/#whats-the-difference-between-127001-and-0000","text":"What's the difference between 127.0.0.1 and 0.0.0.0?","title":"What's the difference between 127.0.0.1 and 0.0.0.0?"},{"location":"TODO/#interface-address","text":"\u5728\u5f88\u591a\u5730\u65b9\u90fd\u89c1\u5230\u4e86\u8fd9\u4e2a\u8bcd\uff1a","title":"interface address"},{"location":"TODO/#-getaddrinfo3-linux-man-page","text":"\u5b83\u5230\u5e95\u662f\u4ec0\u4e48\u542b\u4e49","title":"- getaddrinfo(3) - Linux man page"},{"location":"TODO/#stream-of-bytes-and-bitstream","text":"Bitstream \u5df2\u7ecf\u9605\u8bfb","title":"stream of bytes and bitstream"},{"location":"TODO/#maximum-segment-lifetime","text":"https://en.wikipedia.org/wiki/Maximum_segment_lifetime","title":"Maximum segment lifetime"},{"location":"TODO/#20190909_1","text":"epoll\u4e2d\u7684file descripor\u662f\u5426\u4e00\u5b9a\u8981\u8bbe\u7f6e\u4e3anon blocking SO_RCVLOWAT and SO_SNDLOWAT \u7684\u503c\u90fd\u662f1\uff0c\u90a3\u4e48\u5982\u4f55\u786e\u5b9amessage boundary\uff1f","title":"20190909"},{"location":"TODO/#shell","text":"getconf echo \"2^12\" | bc How to check if port is in use on Linux or Unix https://unix.stackexchange.com/a/185767 how to get all threads of a process","title":"shell"},{"location":"TODO/#process","text":"https://www.cyberciti.biz/faq/show-all-running-processes-in-linux/ https://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes","title":"process"},{"location":"TODO/#how-to-print-out-a-variable-in-makefile","text":"https://stackoverflow.com/questions/16467718/how-to-print-out-a-variable-in-makefile","title":"How to print out a variable in makefile"},{"location":"TODO/#process-control-block","text":"https://www.tldp.org/LDP/lki/lki-2.html \u4e0a\u9762\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u8bb2\u8ff0\u7684\u662flinux kernel\u7684\u5b9e\u73b0","title":"Process control block"},{"location":"TODO/#how-to-find-out-which-windows-process-is-using-a-file","text":"user the software:Process Explorer","title":"How to Find Out Which Windows Process is Using a File"},{"location":"TODO/#how-find-out-which-process-is-using-a-file-in-linux","text":"fuser file_name","title":"How find out which process is using a file in Linux?"},{"location":"TODO/#what-will-happen-if-a-process-exceed-its-resource-limits","text":"\u8bb0\u5f97\u5728redis in action\u8fd9\u672c\u4e66\u4e2d\u6709\u63d0\u53ca\u8fc7\u7c7b\u4f3c\u7684\u95ee\u9898\uff1b","title":"what will happen if a process exceed its resource limits"},{"location":"TODO/#c-posix-library","text":"https://en.wikipedia.org/wiki/C_POSIX_library http://pubs.opengroup.org/onlinepubs/9699919799/idx/head.html","title":"C POSIX library"},{"location":"TODO/#c-standard-library","text":"https://en.wikipedia.org/wiki/C_standard_library","title":"C standard library"},{"location":"TODO/#process-id","text":"APUE 4.4 Set-User-ID and Set-Group-ID https://en.wikipedia.org/wiki/User_identifier","title":"process id"},{"location":"TODO/#time-of-check-to-time-of-use","text":"https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use","title":"Time of check to time of use"},{"location":"TODO/#user-identifier","text":"https://en.wikipedia.org/wiki/User_identifier","title":"User identifier"},{"location":"TODO/#secure-programming-howto","text":"https://dwheeler.com/secure-programs/Secure-Programs-HOWTO/index.html","title":"Secure Programming HOWTO"},{"location":"TODO/#thread-local","text":"thread-local\u548creentry\u4e4b\u95f4\u7684\u5173\u7cfb","title":"thread-local"},{"location":"TODO/#hole-in-file","text":"hole\u5bf9\u6587\u4ef6\u5927\u5c0f\u7684\u5f71\u54cd\uff1b\u4e0a\u5468\u5728\u67e5\u770bhttps://liftoff.github.io/pyminifier/\u6587\u6863\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709minifying\u529f\u80fd\uff0c\u662f\u548chole in file\u6709\u5173\u7684\uff1b \u540c\u65f6binary mode\u6765\u4fdd\u5b58\u6587\u4ef6\u4e5f\u80fd\u591f\u964d\u4f4e\u6587\u4ef6\u5927\u5c0f\uff1b","title":"hole in file"},{"location":"TODO/#linux-memory","text":"memory usage memory available","title":"linux memory"},{"location":"TODO/#virtual-process-space","text":"https://www.tutorialspoint.com/where-are-static-variables-stored-in-c-cplusplus https://cs61.seas.harvard.edu/wiki/2016/Kernel2X \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d https://www.cs.utexas.edu/~lorenzo/corsi/cs372/06F/hw/3sol.html https://cs.stackexchange.com/questions/56825/how-to-calculate-virtual-address-space-from-page-size-virtual-address-size-and","title":"virtual process space"},{"location":"TODO/#process-environment","text":"7.12\u4e0d\u540c\u8bed\u8a00\u7684environment\u662f\u5426\u4e0d\u540c","title":"process environment"},{"location":"TODO/#file-iopython-vs-linux","text":"Python\u4e0eLinux\u7c7b\u4f3c\uff0c\u4e0e\u6587\u4ef6\u76f8\u5173\u7684\u64cd\u4f5c\u90fd\u662f\u4eceopen\u51fd\u6570\u5f00\u59cb\u7684","title":"file IO\uff1apython VS linux"},{"location":"TODO/#fragmentation-computing","text":"https://en.wikipedia.org/wiki/Fragmentation_(computing)","title":"Fragmentation (computing)"},{"location":"TODO/#page-table-size","text":"https://www.cs.cornell.edu/courses/cs4410/2015su/lectures/lec14-pagetables.html http://www.cs.cornell.edu/courses/cs4410/2016su/slides/lecture11.pdf http://www.cs.cornell.edu/courses/cs4410/2016su/schedule.html","title":"page table size"},{"location":"TODO/#data-segment","text":"https://en.wikipedia.org/wiki/Data_segment","title":"Data segment"},{"location":"TODO/#enoent","text":"Why does ENOENT mean \u201cNo such file or directory\u201d?","title":"ENOENT"},{"location":"TODO/#eacces","text":"","title":"EACCES"},{"location":"TODO/#cron","text":"https://en.wikipedia.org/wiki/Cron https://www.adminschoice.com/crontab-quick-reference","title":"cron"},{"location":"TODO/#shell_1","text":"\u4eca\u5929\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u53d1\u73b0\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u95ee\u9898\uff1a\u8fdb\u7a0b\u8fd0\u884c\u4e2d\uff0c\u8fd9\u4e2a\u8fdb\u7a0b\u4f1a\u4e0d\u65ad\u5730\u5411\u5176\u65e5\u5fd7\u6587\u4ef6\u4e2d\u5199\u5165\u65e5\u5fd7\uff1b\u7136\u540e\u6211\u5728shell\u4e2d\u5c06\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\u7ed9\u5220\u9664\u4e86\uff0c\u53d1\u73b0\u8fdb\u7a0b\u5e76\u6ca1\u6709\u53d1\u73b0\u5b83\u7684\u65e5\u5fd7\u6587\u4ef6\u88ab\u5220\u4e86\uff0c\u4e5f\u6ca1\u6709\u51fa\u73b0\u521b\u5efa\u8fd9\u4e2a\u65e5\u5fd7\u6587\u4ef6\uff0c\u7a0b\u5e8f\u4e5f\u6ca1\u6709\u505c\u6b62\u4e0b\u6765\uff1b \u78c1\u76d8\u7a7a\u95f4\u6ee1\u540e\uff0c\u4e5f\u4f1a\u5bfc\u81f4process\u65e0\u6cd5\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff1b\u4f46\u662f\u5f53\u91ca\u653e\u4e00\u90e8\u5206\u7a7a\u95f4\u540e\uff0c\u53d1\u73b0process\u4ecd\u7136\u4e0d\u4f1a\u5199\u5165\uff0c\u5c31\u50cf\u662f\u653e\u5f03\u4e86\u4e00\u6837\uff1b","title":"\u5728shell\u4e2d\u5220\u9664\u6389\u8fdb\u7a0b\u4f7f\u7528\u7684\u6587\u4ef6"},{"location":"TODO/#segmentation-fault","text":"hiredis\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u4eca\u5929\u5728\u591a\u7ebf\u7a0b\u73af\u5883\u4e0b\u6d4b\u8bd5\u51fa\u5b83\u4f1a\u5bfc\u81f4process core dump\uff0cdump\u7684\u539f\u56e0\u662f Program terminated with signal 11, Segmentation fault. https://kb.iu.edu/d/aqsj https://en.wikipedia.org/wiki/Segmentation_fault","title":"Segmentation fault"},{"location":"TODO/#thread-unsafe-and-core-dump","text":"","title":"thread unsafe and core dump"},{"location":"TODO/#how-to-test","text":"\u5982\u679c\u662f\u652f\u6301\u7f51\u7edc\uff0c\u9700\u8981\u6d4b\u8bd5\u591a\u4e2aclient\u8fde\u63a5\uff1b \u9700\u8981\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5 \u9700\u8981\u8fdb\u884c\u5e76\u53d1\u6d4b\u8bd5","title":"how to test"},{"location":"TODO/#process-and-its-thread","text":"Is there a way to see details of all the threads that a process has in Linux? google:how to get all threads of a process https://www.unix.com/aix/154772-how-list-all-threads-running-process.html http://ask.xmodulo.com/view-threads-process-linux.html","title":"process and its thread"},{"location":"TODO/#how-os-know-process-use-a-illegal-memory-location","text":"https://en.wikipedia.org/wiki/Memory_protection https://stackoverflow.com/questions/41172563/how-os-catches-illegal-memory-references-at-paging-scheme https://unix.stackexchange.com/questions/511963/how-linux-finds-out-about-illegal-memory-access-error https://www.kernel.org/ https://www.kernel.org/doc/gorman/html/understand/index.html https://www.kernel.org/doc/gorman/ https://www.kernel.org/doc/","title":"how OS know process use a illegal memory location"},{"location":"TODO/#dev-file","text":"https://unix.stackexchange.com/questions/93531/what-is-stored-in-dev-pts-files-and-can-we-open-them","title":"dev file"},{"location":"TODO/#unix-memory-usage","text":"https://utcc.utoronto.ca/~cks/space/blog/linux/LinuxMemoryStats https://utcc.utoronto.ca/~cks/space/blog/unix/UnderstandingRSS https://stackoverflow.com/questions/131303/how-to-measure-actual-memory-usage-of-an-application-or-process https://unix.stackexchange.com/questions/554/how-to-monitor-cpu-memory-usage-of-a-single-process","title":"Unix memory usage"},{"location":"TODO/#how-can-i-kill-a-process-by-name-instead-of-pid","text":"https://stackoverflow.com/questions/160924/how-can-i-kill-a-process-by-name-instead-of-pid http://osxdaily.com/2017/01/12/kill-process-by-name-command-line/","title":"How can I kill a process by name instead of PID?"},{"location":"TODO/#apueunix-interruption-and-atomwanch","text":"\u5176\u4e2d\u4e3b\u8981\u8ba8\u8bba\u4e86Atomicity","title":"APUE\u7684\u300aUnix-interruption-and-atom\u300b\u8fd8\u6ca1\u6709wanch"},{"location":"TODO/#consistency-models","text":"\u5728parallel computing\u4e2d\u7684Consistency models\u8fd8\u6ca1\u6709\u5b8c\u6210 https://en.wikipedia.org/wiki/Category:Consistency_models","title":"Consistency models"},{"location":"TODO/#epoll","text":"https://en.wikipedia.org/wiki/Epoll","title":"epoll"},{"location":"TODO/#youdao-unix-abort","text":"","title":"youdao Unix-abort"},{"location":"TODO/#init-of-linux","text":"","title":"init of linux"},{"location":"TODO/#init","text":"ch8.2\u4e2d\u6709\u5bf9init\u8fdb\u7a0b\u7684\u4e00\u4e2a\u4ecb\u7ecd ch9.2\u4ecb\u7ecd\u5230\uff0cinit\u4f1a\u8bfb\u53d6\u6587\u4ef6 /etc/ttys how to know what init system linux use https://unix.stackexchange.com/questions/18209/detect-init-system-using-the-shell https://fedoramagazine.org/what-is-an-init-system/ https://en.wikipedia.org/wiki/Init https://en.wikipedia.org/wiki/Systemd","title":"init\u8fdb\u7a0b"},{"location":"TODO/#convert-between-unix-and-windows-text-files","text":"https://kb.iu.edu/d/acux https://stackoverflow.com/questions/16239551/eol-conversion-in-notepad","title":"Convert between Unix and Windows text files"},{"location":"TODO/#memory-management-algorithms","text":"https://en.wikipedia.org/wiki/Category:Memory_management_algorithms Slab allocation","title":"Memory management algorithms"},{"location":"TODO/#interrupt-in-unix","text":"interrupt vector table http://www.cis.upenn.edu/~lee/03cse380/lectures/ln2-process-v4.pdf \u5728 Context switch \u4e2d\u4e5f\u5bf9interrupt\u8fdb\u884c\u4e86\u4ecb\u7ecd","title":"interrupt in Unix"},{"location":"TODO/#signal","text":"signal\u4e5f\u662f\u4e00\u79cdinterrupt\uff0c\u6545\u5c06\u5b83\u653e\u5728interrupt\u4e4b\u4e0b","title":"signal"},{"location":"TODO/#google-unix-signals-and-threads","text":"https://stackoverflow.com/questions/2575106/posix-threads-and-signals https://en.wikipedia.org/wiki/Signal_(IPC)","title":"Google unix signals and threads"},{"location":"TODO/#scheduling-computing","text":"https://en.wikipedia.org/wiki/Scheduling_(computing)","title":"Scheduling (computing)"},{"location":"TODO/#thread-join-and-detach","text":"","title":"thread join and detach"},{"location":"TODO/#wait","text":"https://linux.die.net/man/2/waitpid","title":"wait"},{"location":"Unix-&-Unix-like-&-Linux/","text":"Unix & Unix like & Linux # \u672c\u8282\u533a\u5206\u8fd9\u51e0\u4e2a\u6982\u5ff5\uff0c\u5e76\u8bb0\u5f55\u4e00\u4e9b\u6709\u7528\u8d44\u6e90\uff1a Unix # wikipedia Unix Unix-like # wikipedia Unix-like Linux # \u5982\u4e0b\u662f\u5173\u4e8elinux OS\u7684\u4e00\u4e9b\u8d44\u6e90\uff1a wikipedia Linux NOTE: linux\u662fUnix-like wikipedia Linux kernel The Linux Kernel Archives The Linux Kernel documentation The Linux man-pages project LWN.net","title":"Unix-&-Unix-like-&-Linux"},{"location":"Unix-&-Unix-like-&-Linux/#unix-unix-like-linux","text":"\u672c\u8282\u533a\u5206\u8fd9\u51e0\u4e2a\u6982\u5ff5\uff0c\u5e76\u8bb0\u5f55\u4e00\u4e9b\u6709\u7528\u8d44\u6e90\uff1a","title":"Unix &amp; Unix like &amp; Linux"},{"location":"Unix-&-Unix-like-&-Linux/#unix","text":"wikipedia Unix","title":"Unix"},{"location":"Unix-&-Unix-like-&-Linux/#unix-like","text":"wikipedia Unix-like","title":"Unix-like"},{"location":"Unix-&-Unix-like-&-Linux/#linux","text":"\u5982\u4e0b\u662f\u5173\u4e8elinux OS\u7684\u4e00\u4e9b\u8d44\u6e90\uff1a wikipedia Linux NOTE: linux\u662fUnix-like wikipedia Linux kernel The Linux Kernel Archives The Linux Kernel documentation The Linux man-pages project LWN.net","title":"Linux"},{"location":"Unix-standardization-and-implementation/","text":"UNIX standardization and implementations # \u672c\u8282\u68b3\u7406 Unix operating system \u548c Unix-like operating system \u4e4b\u95f4\u7684\u6807\u51c6\u3001\u6f14\u8fdb\u3001\u5173\u7cfb\u7b49\u3002\u5bf9\u4e8e\u8fd9\u4e9b\u6807\u51c6\u6709\u5fc5\u8981\u4e86\u89e3\u4e00\u4e0b\uff0c\u56e0\u4e3a\u5728\u5f88\u591a\u4e66\u7c4d\uff0c\u6587\u7ae0\u4e2d\u90fd\u4f1a\u89c1\u5230\u8fd9\u4e9b\u6807\u51c6\u3002\u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81ea\u4e8e Advanced Programming in the UNIX\u00ae Environment, Third Edition \u7684chapter 2 UNIX Standardization and Implementations Introduction # Much work has gone into standardizing the UNIX programming environment and the C programming language. In this chapter we first look at the various standardization efforts that have been under way over the past two and a half decades. We then discuss the effects of these UNIX programming standards on the operating system implementations that are described in this book. An important part of all the standardization efforts is the specification of various limits that each implementation must define, so we look at these limits and the various ways to determine their values. UNIX Standardization # ISO C # NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86ISO C standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eISO C standard\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia C (programming language) The Standard wikipedia ANSI C This standard defines not only the syntax and semantics of the programming language but also a standard library . The ISO C library can be divided into 24 areas, based on the headers defined by the standard (see Figure 2.1). The POSIX.1 standard includes these headers, as well as others. As Figure 2.1 shows, all of these headers are supported by the four implementations (FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris 10) that are described later in this chapter. NOTE: POSIX.1 standard \u662fISO C standard\u7684\u8d85\u96c6\u3002 The ISO C headers depend on which version of the C compiler is used with the operating system. FreeBSD 8.0 ships with version 4.2.1 of gcc, Solaris 10 ships with version 3.4.3 of gcc (in addition to its own C compiler in Sun Studio), Ubuntu 12.04 (Linux 3.2.0) ships with version 4.6.3 of gcc, and Mac OS X 10.6.8 ships with both versions 4.0.1 and 4.2.1 of gcc. Headers defined by the ISO C standard # C standard library IEEE POSIX # NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86IEEE POSIX standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eIEEE POSIX\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia POSIX POSIX official site POSIX is a family of standards initially developed by the IEEE (Institute of Electrical and Electronics Engineers). POSIX stands for Portable Operating System Interface. It originally referred only to the IEEE Standard 1003.1-1988 \u2014 the operating system interface \u2014 but was later extended to include many of the standards and draft standards with the 1003 designation, including the shell and utilities (1003.2). Because the 1003.1 standard specifies an interface and not an implementation , no distinction is made between system calls and library functions . All the routines in the standard are called functions . POSIX Threads # POSIX Threads The Open Group Base Specifications Issue 7, IEEE Std 1003.1 C POSIX library header files # C POSIX library C POSIX library header files Official List of headers in the POSIX library on opengroup.org The Single UNIX Specification # The Single UNIX Specification, a superset of the POSIX.1 standard, specifies additional interfaces that extend the functionality provided by the POSIX.1 specification. POSIX.1 is equivalent to the Base Specifications portion of the Single UNIX Specification. NOTE: wikipedia The Open Group wikipedia Single UNIX Specification The X/Open System Interfaces (XSI) option in POSIX.1 describes optional interfaces and defines which optional portions of POSIX.1 must be supported for an implementation to be deemed XSI conforming . These include file synchronization, thread stack address and size attributes, thread process-shared synchronization, and the _XOPEN_UNIX symbolic constant (marked \u2018\u2018SUS mandatory\u2019\u2019 in Figure 2.5). Only XSI-conforming implementations can be called UNIX systems . NOTE: X/Open The Single UNIX Specification is a publication of The Open Group, which was formed in 1996 as a merger of X/Open and the Open Software Foundation (OSF), both industry consortia. X/Open used to publish the X/Open Portability Guide, which adopted specific standards and filled in the gaps where functionality was missing. The goal of these guides was to improve application portability beyond what was possible by merely conforming to published standards. NOTE: \u4e0b\u9762\u89e3\u91ca\u4e00\u4e9b\u7b80\u79f0\u7684\u542b\u4e49 SUSv3\uff1a the third version of the Single UNIX Specification SUSv4\uff1a the forth version of the Single UNIX Specification UNIX System Implementations # The previous section described ISO C, IEEE POSIX, and the Single UNIX Specification \u2014 three standards originally created by independent organizations. Standards, however, are interface specifications. How do these standards relate to the real world? These standards are taken by vendors and turned into actual implementations. In this book, we are interested in both these standards and their implementation. UNIX System V Release 4 # UNIX System V Release 4 (SVR4) was a product of AT&T\u2019s UNIX System Laboratories (USL, formerly AT&T\u2019s UNIX Software Operation). SVR4 merged functionality from AT&T UNIX System V Release 3.2 (SVR3.2), the SunOS operating system from Sun Microsystems, the 4.3BSD release from the University of California, and the Xenix system from Microsoft into one coherent operating system. (Xenix was originally developed from Version 7, with many features later taken from System V.) The SVR4 source code was released in late 1989, with the first end-user copies becoming available during 1990. SVR4 conformed to both the POSIX 1003.1 standard and the X/Open Portability Guide, Issue 3 (XPG3). 4.4BSD # FreeBSD # Linux # Mac OS X # Solaris # Relationship of Standards and Implementations # The standards that we\u2019ve mentioned define a subset of any actual system. The focus of this book is on four real systems: FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris Although only Mac OS X and Solaris can call themselves UNIX systems, all four provide a similar programming environment. Because all four are POSIX compliant to varying degrees, we will also concentrate on the features required by the POSIX.1 standard, noting any differences between POSIX and the actual implementations of these four systems. Those features and routines that are specific to only a particular implementation are clearly marked. We\u2019ll also note any features that are required on UNIX systems but are optional on other POSIX-conforming systems. Be aware that the implementations provide backward compatibility for features in earlier releases, such as SVR3.2 and 4.3BSD. For example, Solaris supports both the POSIX.1 specification for nonblocking I/O (O_NONBLOCK) and the traditional System V method ( O_NDELAY ). In this text, we\u2019ll use only the POSIX.1 feature, although we\u2019ll mention the nonstandard feature that it replaces. Similarly, both SVR3.2 and 4.3BSD provided reliable signals in a way that differs from the POSIX.1 standard. In Chapter 10 we describe only the POSIX.1 signal mechanism. man STANDARDS(7) # NOTE: \u8fd9\u662flinux\u6587\u6863\u4e2d\u5bf9standard\u7684\u4ecb\u7ecd\u3002","title":"Unix-standardization-and-implementation"},{"location":"Unix-standardization-and-implementation/#unix-standardization-and-implementations","text":"\u672c\u8282\u68b3\u7406 Unix operating system \u548c Unix-like operating system \u4e4b\u95f4\u7684\u6807\u51c6\u3001\u6f14\u8fdb\u3001\u5173\u7cfb\u7b49\u3002\u5bf9\u4e8e\u8fd9\u4e9b\u6807\u51c6\u6709\u5fc5\u8981\u4e86\u89e3\u4e00\u4e0b\uff0c\u56e0\u4e3a\u5728\u5f88\u591a\u4e66\u7c4d\uff0c\u6587\u7ae0\u4e2d\u90fd\u4f1a\u89c1\u5230\u8fd9\u4e9b\u6807\u51c6\u3002\u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81ea\u4e8e Advanced Programming in the UNIX\u00ae Environment, Third Edition \u7684chapter 2 UNIX Standardization and Implementations","title":"UNIX standardization and implementations"},{"location":"Unix-standardization-and-implementation/#introduction","text":"Much work has gone into standardizing the UNIX programming environment and the C programming language. In this chapter we first look at the various standardization efforts that have been under way over the past two and a half decades. We then discuss the effects of these UNIX programming standards on the operating system implementations that are described in this book. An important part of all the standardization efforts is the specification of various limits that each implementation must define, so we look at these limits and the various ways to determine their values.","title":"Introduction"},{"location":"Unix-standardization-and-implementation/#unix-standardization","text":"","title":"UNIX Standardization"},{"location":"Unix-standardization-and-implementation/#iso-c","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86ISO C standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eISO C standard\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia C (programming language) The Standard wikipedia ANSI C This standard defines not only the syntax and semantics of the programming language but also a standard library . The ISO C library can be divided into 24 areas, based on the headers defined by the standard (see Figure 2.1). The POSIX.1 standard includes these headers, as well as others. As Figure 2.1 shows, all of these headers are supported by the four implementations (FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris 10) that are described later in this chapter. NOTE: POSIX.1 standard \u662fISO C standard\u7684\u8d85\u96c6\u3002 The ISO C headers depend on which version of the C compiler is used with the operating system. FreeBSD 8.0 ships with version 4.2.1 of gcc, Solaris 10 ships with version 3.4.3 of gcc (in addition to its own C compiler in Sun Studio), Ubuntu 12.04 (Linux 3.2.0) ships with version 4.6.3 of gcc, and Mac OS X 10.6.8 ships with both versions 4.0.1 and 4.2.1 of gcc.","title":"ISO C"},{"location":"Unix-standardization-and-implementation/#headers-defined-by-the-iso-c-standard","text":"C standard library","title":"Headers defined by the ISO C standard"},{"location":"Unix-standardization-and-implementation/#ieee-posix","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u4e86IEEE POSIX standard\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u5386\u53f2\u53ef\u4ee5pass\u6389\u3002\u4e0b\u9762\u5173\u4e8eIEEE POSIX\u7684\u4e00\u4e9b\u7f51\u7ad9\u94fe\u63a5\uff1a wikipedia POSIX POSIX official site POSIX is a family of standards initially developed by the IEEE (Institute of Electrical and Electronics Engineers). POSIX stands for Portable Operating System Interface. It originally referred only to the IEEE Standard 1003.1-1988 \u2014 the operating system interface \u2014 but was later extended to include many of the standards and draft standards with the 1003 designation, including the shell and utilities (1003.2). Because the 1003.1 standard specifies an interface and not an implementation , no distinction is made between system calls and library functions . All the routines in the standard are called functions .","title":"IEEE POSIX"},{"location":"Unix-standardization-and-implementation/#posix-threads","text":"POSIX Threads The Open Group Base Specifications Issue 7, IEEE Std 1003.1","title":"POSIX Threads"},{"location":"Unix-standardization-and-implementation/#c-posix-library-header-files","text":"C POSIX library C POSIX library header files Official List of headers in the POSIX library on opengroup.org","title":"C POSIX library header files"},{"location":"Unix-standardization-and-implementation/#the-single-unix-specification","text":"The Single UNIX Specification, a superset of the POSIX.1 standard, specifies additional interfaces that extend the functionality provided by the POSIX.1 specification. POSIX.1 is equivalent to the Base Specifications portion of the Single UNIX Specification. NOTE: wikipedia The Open Group wikipedia Single UNIX Specification The X/Open System Interfaces (XSI) option in POSIX.1 describes optional interfaces and defines which optional portions of POSIX.1 must be supported for an implementation to be deemed XSI conforming . These include file synchronization, thread stack address and size attributes, thread process-shared synchronization, and the _XOPEN_UNIX symbolic constant (marked \u2018\u2018SUS mandatory\u2019\u2019 in Figure 2.5). Only XSI-conforming implementations can be called UNIX systems . NOTE: X/Open The Single UNIX Specification is a publication of The Open Group, which was formed in 1996 as a merger of X/Open and the Open Software Foundation (OSF), both industry consortia. X/Open used to publish the X/Open Portability Guide, which adopted specific standards and filled in the gaps where functionality was missing. The goal of these guides was to improve application portability beyond what was possible by merely conforming to published standards. NOTE: \u4e0b\u9762\u89e3\u91ca\u4e00\u4e9b\u7b80\u79f0\u7684\u542b\u4e49 SUSv3\uff1a the third version of the Single UNIX Specification SUSv4\uff1a the forth version of the Single UNIX Specification","title":"The Single UNIX Specification"},{"location":"Unix-standardization-and-implementation/#unix-system-implementations","text":"The previous section described ISO C, IEEE POSIX, and the Single UNIX Specification \u2014 three standards originally created by independent organizations. Standards, however, are interface specifications. How do these standards relate to the real world? These standards are taken by vendors and turned into actual implementations. In this book, we are interested in both these standards and their implementation.","title":"UNIX System Implementations"},{"location":"Unix-standardization-and-implementation/#unix-system-v-release-4","text":"UNIX System V Release 4 (SVR4) was a product of AT&T\u2019s UNIX System Laboratories (USL, formerly AT&T\u2019s UNIX Software Operation). SVR4 merged functionality from AT&T UNIX System V Release 3.2 (SVR3.2), the SunOS operating system from Sun Microsystems, the 4.3BSD release from the University of California, and the Xenix system from Microsoft into one coherent operating system. (Xenix was originally developed from Version 7, with many features later taken from System V.) The SVR4 source code was released in late 1989, with the first end-user copies becoming available during 1990. SVR4 conformed to both the POSIX 1003.1 standard and the X/Open Portability Guide, Issue 3 (XPG3).","title":"UNIX System V Release 4"},{"location":"Unix-standardization-and-implementation/#44bsd","text":"","title":"4.4BSD"},{"location":"Unix-standardization-and-implementation/#freebsd","text":"","title":"FreeBSD"},{"location":"Unix-standardization-and-implementation/#linux","text":"","title":"Linux"},{"location":"Unix-standardization-and-implementation/#mac-os-x","text":"","title":"Mac OS X"},{"location":"Unix-standardization-and-implementation/#solaris","text":"","title":"Solaris"},{"location":"Unix-standardization-and-implementation/#relationship-of-standards-and-implementations","text":"The standards that we\u2019ve mentioned define a subset of any actual system. The focus of this book is on four real systems: FreeBSD 8.0, Linux 3.2.0, Mac OS X 10.6.8, and Solaris Although only Mac OS X and Solaris can call themselves UNIX systems, all four provide a similar programming environment. Because all four are POSIX compliant to varying degrees, we will also concentrate on the features required by the POSIX.1 standard, noting any differences between POSIX and the actual implementations of these four systems. Those features and routines that are specific to only a particular implementation are clearly marked. We\u2019ll also note any features that are required on UNIX systems but are optional on other POSIX-conforming systems. Be aware that the implementations provide backward compatibility for features in earlier releases, such as SVR3.2 and 4.3BSD. For example, Solaris supports both the POSIX.1 specification for nonblocking I/O (O_NONBLOCK) and the traditional System V method ( O_NDELAY ). In this text, we\u2019ll use only the POSIX.1 feature, although we\u2019ll mention the nonstandard feature that it replaces. Similarly, both SVR3.2 and 4.3BSD provided reliable signals in a way that differs from the POSIX.1 standard. In Chapter 10 we describe only the POSIX.1 signal mechanism.","title":"Relationship of Standards and Implementations"},{"location":"Unix-standardization-and-implementation/#man-standards7","text":"NOTE: \u8fd9\u662flinux\u6587\u6863\u4e2d\u5bf9standard\u7684\u4ecb\u7ecd\u3002","title":"man STANDARDS(7)"},{"location":"Architecture/Architecture-of-computing-system/","text":"Architecture of computing system # \u672c\u7ae0\u6240\u8981\u63a2\u8ba8\u7684\u662farchitecture of computing system \uff0c\u800c\u4e0d\u662f\u5bfb\u5e38\u6240\u8bf4\u7684 computer architecture \uff0c\u4e00\u822c computer architecture \u6240\u6307\u7684\u662f\u8bf8\u5982 Von Neumann architecture \u3001 Harvard architecture \u7b49\u63cf\u8ff0computer\u786c\u4ef6\u7684\u67b6\u6784\u3002\u672c\u8282\u6240\u8981\u63cf\u8ff0\u7684\u5185\u5bb9\u662f\u4ece\u4e00\u4e2a\u66f4\u52a0\u9ad8\u7684\u89d2\u5ea6\u6765\u770b\u5f85 computing system \uff0c\u5305\u62ec\u6700\u5e95\u5c42\u7684hardware\uff0coperating system\uff0capplication software\u3002 \u6b63\u5982 Computer hardware \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a The progression from levels of \"hardness\" to \"softness\" in computer systems parallels a progression of layers of abstraction in computing. A combination of hardware and software forms a usable computing system. \u73b0\u4ee3 computing system \u7684\u6574\u4f53\u67b6\u6784\u7684\u53d1\u5c55\u662f\u53d7\u5230\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u7684 layers of abstraction \u601d\u60f3\uff08\u5206\u5c42\u601d\u60f3\uff09\u7684\u5f71\u54cd\u7684\uff0c\u4e00\u4e2a computing system \u53ef\u4ee5\u8ba4\u4e3a\u7531\u4e24\u5c42\u6784\u6210\uff1a software hardware Instruction set \u662fsoftware\u548chardware\u4e4b\u95f4\u7684\u63a5\u53e3\u3002 \u73b0\u4ee3 computing system \u7684\u8fd0\u884c\u662f\u79bb\u4e0d\u5f00operating system\u7684\uff0coperating system\u6240\u5c5e\u7684\u662fsoftware\u8fd9\u4e00\u5c42\uff0c\u4e0b\u9762\u5bf9operating system\u6765\u8fdb\u884c\u66f4\u52a0\u7cbe\u7ec6\u7684\u5206\u5c42\u3002 Architecture of operating system # \u5728 Operating system \u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5178\u578b\u7684operating system\u7684architecture\u5982\u4e0b\uff1a Operating systems \u4e0a\u56fe\u4e2dUser\u8868\u793a\u7684\u662f\u7528\u6237\uff0c\u4e0d\u5c5e\u4e8eOS\u4e2d\uff0c\u540e\u9762\u7684\u8ba8\u8bba\u4f1a\u5c06\u5176\u5ffd\u89c6\u3002 \u5728 Advanced Programming in the UNIX\u00ae Environment, Third Edition \u76841.2 UNIX Architecture\u8282\u4e2d\u7ed9\u51fa\u4e86Architecture of the UNIX operating system\uff0c\u5982\u4e0b\uff1a \u4e0a\u8ff0\u4e24\u4e2a\u67b6\u6784\u56fe\u90fd\u5927\u4f53\u5c55\u793aOS\u7684architecture\uff0c\u4e24\u8005\u90fd\u5404\u6709\u5229\u5f0a\uff0c\u56fe\u4e00\u5305\u542b\u4e86hardware\uff0c\u4f46\u662f\u5ffd\u89c6\u4e86\u5404\u5c42\u4e4b\u95f4\u7684interface\u3002\u56fe\u4e8c\u5219\u6b63\u597d\u76f8\u53cd\u3002\u6240\u4ee5\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u5219\u6b63\u597d\uff0c\u540e\u9762\u4f1a\u6309\u7167\u56fe\u4e8c\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5c06interface\u4e5f\u770b\u505a\u662f\u4e00\u5c42\u3002\u5145\u5f53interface\u7684layer\u4f5c\u4e3a\u5b83\u7684\u4e0a\u4e0b\u4e24\u5c42\u4e4b\u95f4\u7684interface\u3002 upper layer layer role software application system calls &library routines interface kernel Instruction set interface hardware hardware The Process/Kernel Model # \u5728 Understanding.The.Linux.kernel.3rd.Edition \u76841.6.1. The Process/Kernel Model\u4e2d\uff0c\u6240\u63cf\u8ff0Process/Kernel Model\u4e0e\u4e0a\u9762\u6240\u63cf\u8ff0\u7684architecture\u7c7b\u4f3c\uff0c\u4f46\u662f\u66f4\u52a0\u7cbe\u7b80\u3002 \u603b\u7ed3 # \u4e0a\u9762\u6211\u4f7f\u7528\u4e86 \u5c42\u6b21\u5316\u7684\u7ed3\u6784 \u6765\u63cf\u8ff0 computing system \u7684\u67b6\u6784\uff0c\u81f3\u6b64\uff0c\u5df2\u7ecf\u5efa\u7acb\u4e86operating system\u7684\u6574\u4f53architecture\uff08model\uff09\u3002","title":"Architecture-of-computing-system"},{"location":"Architecture/Architecture-of-computing-system/#architecture-of-computing-system","text":"\u672c\u7ae0\u6240\u8981\u63a2\u8ba8\u7684\u662farchitecture of computing system \uff0c\u800c\u4e0d\u662f\u5bfb\u5e38\u6240\u8bf4\u7684 computer architecture \uff0c\u4e00\u822c computer architecture \u6240\u6307\u7684\u662f\u8bf8\u5982 Von Neumann architecture \u3001 Harvard architecture \u7b49\u63cf\u8ff0computer\u786c\u4ef6\u7684\u67b6\u6784\u3002\u672c\u8282\u6240\u8981\u63cf\u8ff0\u7684\u5185\u5bb9\u662f\u4ece\u4e00\u4e2a\u66f4\u52a0\u9ad8\u7684\u89d2\u5ea6\u6765\u770b\u5f85 computing system \uff0c\u5305\u62ec\u6700\u5e95\u5c42\u7684hardware\uff0coperating system\uff0capplication software\u3002 \u6b63\u5982 Computer hardware \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a The progression from levels of \"hardness\" to \"softness\" in computer systems parallels a progression of layers of abstraction in computing. A combination of hardware and software forms a usable computing system. \u73b0\u4ee3 computing system \u7684\u6574\u4f53\u67b6\u6784\u7684\u53d1\u5c55\u662f\u53d7\u5230\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u7684 layers of abstraction \u601d\u60f3\uff08\u5206\u5c42\u601d\u60f3\uff09\u7684\u5f71\u54cd\u7684\uff0c\u4e00\u4e2a computing system \u53ef\u4ee5\u8ba4\u4e3a\u7531\u4e24\u5c42\u6784\u6210\uff1a software hardware Instruction set \u662fsoftware\u548chardware\u4e4b\u95f4\u7684\u63a5\u53e3\u3002 \u73b0\u4ee3 computing system \u7684\u8fd0\u884c\u662f\u79bb\u4e0d\u5f00operating system\u7684\uff0coperating system\u6240\u5c5e\u7684\u662fsoftware\u8fd9\u4e00\u5c42\uff0c\u4e0b\u9762\u5bf9operating system\u6765\u8fdb\u884c\u66f4\u52a0\u7cbe\u7ec6\u7684\u5206\u5c42\u3002","title":"Architecture of computing system"},{"location":"Architecture/Architecture-of-computing-system/#architecture-of-operating-system","text":"\u5728 Operating system \u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5178\u578b\u7684operating system\u7684architecture\u5982\u4e0b\uff1a Operating systems \u4e0a\u56fe\u4e2dUser\u8868\u793a\u7684\u662f\u7528\u6237\uff0c\u4e0d\u5c5e\u4e8eOS\u4e2d\uff0c\u540e\u9762\u7684\u8ba8\u8bba\u4f1a\u5c06\u5176\u5ffd\u89c6\u3002 \u5728 Advanced Programming in the UNIX\u00ae Environment, Third Edition \u76841.2 UNIX Architecture\u8282\u4e2d\u7ed9\u51fa\u4e86Architecture of the UNIX operating system\uff0c\u5982\u4e0b\uff1a \u4e0a\u8ff0\u4e24\u4e2a\u67b6\u6784\u56fe\u90fd\u5927\u4f53\u5c55\u793aOS\u7684architecture\uff0c\u4e24\u8005\u90fd\u5404\u6709\u5229\u5f0a\uff0c\u56fe\u4e00\u5305\u542b\u4e86hardware\uff0c\u4f46\u662f\u5ffd\u89c6\u4e86\u5404\u5c42\u4e4b\u95f4\u7684interface\u3002\u56fe\u4e8c\u5219\u6b63\u597d\u76f8\u53cd\u3002\u6240\u4ee5\u5c06\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u5219\u6b63\u597d\uff0c\u540e\u9762\u4f1a\u6309\u7167\u56fe\u4e8c\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5c06interface\u4e5f\u770b\u505a\u662f\u4e00\u5c42\u3002\u5145\u5f53interface\u7684layer\u4f5c\u4e3a\u5b83\u7684\u4e0a\u4e0b\u4e24\u5c42\u4e4b\u95f4\u7684interface\u3002 upper layer layer role software application system calls &library routines interface kernel Instruction set interface hardware hardware","title":"Architecture of operating system"},{"location":"Architecture/Architecture-of-computing-system/#the-processkernel-model","text":"\u5728 Understanding.The.Linux.kernel.3rd.Edition \u76841.6.1. The Process/Kernel Model\u4e2d\uff0c\u6240\u63cf\u8ff0Process/Kernel Model\u4e0e\u4e0a\u9762\u6240\u63cf\u8ff0\u7684architecture\u7c7b\u4f3c\uff0c\u4f46\u662f\u66f4\u52a0\u7cbe\u7b80\u3002","title":"The Process/Kernel Model"},{"location":"Architecture/Architecture-of-computing-system/#_1","text":"\u4e0a\u9762\u6211\u4f7f\u7528\u4e86 \u5c42\u6b21\u5316\u7684\u7ed3\u6784 \u6765\u63cf\u8ff0 computing system \u7684\u67b6\u6784\uff0c\u81f3\u6b64\uff0c\u5df2\u7ecf\u5efa\u7acb\u4e86operating system\u7684\u6574\u4f53architecture\uff08model\uff09\u3002","title":"\u603b\u7ed3"},{"location":"Kernel/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u4e3b\u8981\u8ba8\u8bbaOS kernel\u7684\u57fa\u7840\u77e5\u8bc6\u548c linux kernel \u7684\u5b9e\u73b0\u3002","title":"Introduction"},{"location":"Kernel/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u8ba8\u8bbaOS kernel\u7684\u57fa\u7840\u77e5\u8bc6\u548c linux kernel \u7684\u5b9e\u73b0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Kernel/Kernel(operating-system)/","text":"Kernel (operating system)\u6982\u8ff0 # \u672c\u7ae0\u5f00\u59cb\u5957\u5229OS\u7684kernel\uff0c\u672c\u8282\u662f\u5bf9Kernel\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Kernel (operating system) \u3002\u5173\u4e8elinux kernel\uff0c\u53c2\u89c1 The kernel is a computer program that is the core of a computer's operating system , with complete control over everything in the system. On most systems, it is one of the first programs loaded on start-up (after the bootloader ). It handles the rest of start-up as well as input/output requests from software , translating them into data-processing instructions for the central processing unit . It handles memory and peripherals (\u5916\u8bbe) like keyboards, monitors, printers, and speakers. The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application programs or other, less critical parts of the operating system. The kernel performs its tasks, such as running processes, managing hardware devices such as the hard disk , and handling interrupts, in this protected kernel space . In contrast, everything a user does is in user space : writing text in a text editor, running programs in a GUI , etc. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness, as well as preventing malfunctioning application programs from crashing the entire operating system. NOTE:\u9694\u79bb\u5e26\u6765\u5b89\u5168 The kernel's interface is a low-level abstraction layer . When a process makes requests of the kernel, it is called a system call . Kernel designs differ in how they manage these system calls and resources . A monolithic kernel runs all the operating system instructions in the same address space for speed. A microkernel runs most processes in user space, for modularity . A kernel connects the application software to the hardware of a computer. Function of Kernel # \u5173\u4e8e\u5185\u6838\u7684\u529f\u80fd\uff0c\u5728\u672c\u7bc7\u6587\u7ae0\u4e2d\u6ca1\u6709\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u7c7b\u4ecb\u7ecd\uff0c\u6240\u4ee5\u6b64\u5904\u8fdb\u884c\u7701\u7565\uff1b\u5728 Operating system \u7684 Kernel \u7ae0\u8282\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u63a8\u8350\u53bb\u9605\u8bfb\u3002 Kernel design decisions # \u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5728\u8bbe\u8ba1\u4e00\u4e2akernel\u7684\u65f6\u5019\u9700\u8981\u8003\u8651\u54ea\u4e9b\u95ee\u9898\uff0c\u8bfb\u8005\u5e94\u8be5\u5bf9\u8fd9\u4e9b\u95ee\u9898\u6709\u4e9b\u4e86\u89e3\uff0c\u8fd9\u4e9b\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u4e86kernel\u7684\u5b9e\u73b0\u3002 Kernel-wide design approaches # \u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5b9e\u73b0kernel\uff08\u672c\u8d28\u4e0akernel\u662f\u4e00\u4e2asoftware\uff09\u65f6\uff0c\u91c7\u53d6\u600e\u6837\u7684\u8f6f\u4ef6\u67b6\u6784\u3002\u76ee\u524d\u4e3b\u6d41\u7684\u7684\u67b6\u6784\u6709\u4e24\u79cd\u3002\u539f\u6587\u4e2d\u8fd8\u5bf9\u8fd9\u4e24\u79cd\u8f6f\u4ef6\u67b6\u6784\u80cc\u540e\u7684philosophy\u5373 separation of mechanism and policy \u8fdb\u884c\u4e86\u5206\u6790\uff0c\u6211\u8bfb\u5b8c\u4ecd\u7136\u4e00\u5934\u96fe\u6c34\u3002 While monolithic kernels execute all of their code in the same address space ( kernel space ), microkernels try to run most of their services in user space, aiming to improve maintainability and modularity of the codebase. Most kernels do not fit exactly into one of these categories, but are rather found in between these two designs. These are called hybrid kernels . More exotic designs such as nanokernels and exokernels are available, but are seldom used for production systems. The Xen hypervisor, for example, is an exokernel. Monolithic kernels # Main article: Monolithic kernel Diagram of a monolithic kernel In a monolithic kernel, all OS services run along with the main kernel thread, thus also residing in the same memory area. This approach provides rich and powerful hardware access. Some developers, such as UNIX developer Ken Thompson , maintain that it is \"easier to implement a monolithic kernel\" than microkernels. The main disadvantages of monolithic kernels are the dependencies between system components \u2013 a bug in a device driver might crash the entire system \u2013 and the fact that large kernels can become very difficult to maintain. Microkernels # Main article: Microkernel Microkernel (also abbreviated \u03bcK or uK) is the term describing an approach to operating system design by which the functionality of the system is moved out of the traditional \"kernel\", into a set of \"servers\" that communicate through a \"minimal\" kernel, leaving as little as possible in \"system space\" and as much as possible in \"user space\". A microkernel that is designed for a specific platform or device is only ever going to have what it needs to operate. The microkernel approach consists of defining a simple abstraction over the hardware, with a set of primitives or system calls to implement minimal OS services such as memory management , multitasking , and inter-process communication . Other services, including those normally provided by the kernel, such as networking , are implemented in user-space programs, referred to as servers . Microkernels are easier to maintain than monolithic kernels, but the large number of system calls and context switches might slow down the system because they typically generate more overhead than plain function calls.","title":"Kernel"},{"location":"Kernel/Kernel(operating-system)/#kernel-operating-system","text":"\u672c\u7ae0\u5f00\u59cb\u5957\u5229OS\u7684kernel\uff0c\u672c\u8282\u662f\u5bf9Kernel\u7684\u6982\u8ff0\uff0c\u53c2\u8003\u7684\u662f\u7ef4\u57fa\u767e\u79d1 Kernel (operating system) \u3002\u5173\u4e8elinux kernel\uff0c\u53c2\u89c1 The kernel is a computer program that is the core of a computer's operating system , with complete control over everything in the system. On most systems, it is one of the first programs loaded on start-up (after the bootloader ). It handles the rest of start-up as well as input/output requests from software , translating them into data-processing instructions for the central processing unit . It handles memory and peripherals (\u5916\u8bbe) like keyboards, monitors, printers, and speakers. The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application programs or other, less critical parts of the operating system. The kernel performs its tasks, such as running processes, managing hardware devices such as the hard disk , and handling interrupts, in this protected kernel space . In contrast, everything a user does is in user space : writing text in a text editor, running programs in a GUI , etc. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness, as well as preventing malfunctioning application programs from crashing the entire operating system. NOTE:\u9694\u79bb\u5e26\u6765\u5b89\u5168 The kernel's interface is a low-level abstraction layer . When a process makes requests of the kernel, it is called a system call . Kernel designs differ in how they manage these system calls and resources . A monolithic kernel runs all the operating system instructions in the same address space for speed. A microkernel runs most processes in user space, for modularity . A kernel connects the application software to the hardware of a computer.","title":"Kernel (operating system)\u6982\u8ff0"},{"location":"Kernel/Kernel(operating-system)/#function-of-kernel","text":"\u5173\u4e8e\u5185\u6838\u7684\u529f\u80fd\uff0c\u5728\u672c\u7bc7\u6587\u7ae0\u4e2d\u6ca1\u6709\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u7c7b\u4ecb\u7ecd\uff0c\u6240\u4ee5\u6b64\u5904\u8fdb\u884c\u7701\u7565\uff1b\u5728 Operating system \u7684 Kernel \u7ae0\u8282\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u63a8\u8350\u53bb\u9605\u8bfb\u3002","title":"Function of Kernel"},{"location":"Kernel/Kernel(operating-system)/#kernel-design-decisions","text":"\u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5728\u8bbe\u8ba1\u4e00\u4e2akernel\u7684\u65f6\u5019\u9700\u8981\u8003\u8651\u54ea\u4e9b\u95ee\u9898\uff0c\u8bfb\u8005\u5e94\u8be5\u5bf9\u8fd9\u4e9b\u95ee\u9898\u6709\u4e9b\u4e86\u89e3\uff0c\u8fd9\u4e9b\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u4e86kernel\u7684\u5b9e\u73b0\u3002","title":"Kernel design decisions"},{"location":"Kernel/Kernel(operating-system)/#kernel-wide-design-approaches","text":"\u539f\u6587\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662f\u5b9e\u73b0kernel\uff08\u672c\u8d28\u4e0akernel\u662f\u4e00\u4e2asoftware\uff09\u65f6\uff0c\u91c7\u53d6\u600e\u6837\u7684\u8f6f\u4ef6\u67b6\u6784\u3002\u76ee\u524d\u4e3b\u6d41\u7684\u7684\u67b6\u6784\u6709\u4e24\u79cd\u3002\u539f\u6587\u4e2d\u8fd8\u5bf9\u8fd9\u4e24\u79cd\u8f6f\u4ef6\u67b6\u6784\u80cc\u540e\u7684philosophy\u5373 separation of mechanism and policy \u8fdb\u884c\u4e86\u5206\u6790\uff0c\u6211\u8bfb\u5b8c\u4ecd\u7136\u4e00\u5934\u96fe\u6c34\u3002 While monolithic kernels execute all of their code in the same address space ( kernel space ), microkernels try to run most of their services in user space, aiming to improve maintainability and modularity of the codebase. Most kernels do not fit exactly into one of these categories, but are rather found in between these two designs. These are called hybrid kernels . More exotic designs such as nanokernels and exokernels are available, but are seldom used for production systems. The Xen hypervisor, for example, is an exokernel.","title":"Kernel-wide design approaches"},{"location":"Kernel/Kernel(operating-system)/#monolithic-kernels","text":"Main article: Monolithic kernel Diagram of a monolithic kernel In a monolithic kernel, all OS services run along with the main kernel thread, thus also residing in the same memory area. This approach provides rich and powerful hardware access. Some developers, such as UNIX developer Ken Thompson , maintain that it is \"easier to implement a monolithic kernel\" than microkernels. The main disadvantages of monolithic kernels are the dependencies between system components \u2013 a bug in a device driver might crash the entire system \u2013 and the fact that large kernels can become very difficult to maintain.","title":"Monolithic kernels"},{"location":"Kernel/Kernel(operating-system)/#microkernels","text":"Main article: Microkernel Microkernel (also abbreviated \u03bcK or uK) is the term describing an approach to operating system design by which the functionality of the system is moved out of the traditional \"kernel\", into a set of \"servers\" that communicate through a \"minimal\" kernel, leaving as little as possible in \"system space\" and as much as possible in \"user space\". A microkernel that is designed for a specific platform or device is only ever going to have what it needs to operate. The microkernel approach consists of defining a simple abstraction over the hardware, with a set of primitives or system calls to implement minimal OS services such as memory management , multitasking , and inter-process communication . Other services, including those normally provided by the kernel, such as networking , are implemented in user-space programs, referred to as servers . Microkernels are easier to maintain than monolithic kernels, but the large number of system calls and context switches might slow down the system because they typically generate more overhead than plain function calls.","title":"Microkernels"},{"location":"Kernel/TODO/","text":"20190126linux\u5185\u6838\u662f\u5982\u4f55\u6765\u7ec4\u7ec7process\u7684 # \u4eca\u5929\u5728\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \u8fd9\u7bc7\u6587\u7ae0\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u4e00\u6bb5\u8bdd\uff1a Historically, the Linux kernel has maintained a single process tree. The tree contains a reference to every process currently running in a parent-child hierarchy. A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. \u8fd9\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u7684\u601d\u8003\uff1a\u5728linux\u7684\u5185\u6838\u4e2d\uff0c\u662f\u4f7f\u7528tree\u6765\u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\u7684\u5417\uff1f \u521a\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff0c\u56e0\u4e3a ptree \uff0c\u663e\u7136\u6309\u7167parent-child\u5173\u7cfb\u662f\u53ef\u4ee5\u6309\u7167tree\u7ed3\u6784\u6765\u7ec4\u7ec7process\u7684\uff0c\u4f46\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\u8981\u8fdc\u6bd4\u8fd9\u590d\u6742\uff0c\u56e0\u4e3a\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u6709\uff1a schedule\uff0c\u5373\u8c03\u5ea6\u95ee\u9898\uff0c\u5185\u6838\u9700\u8981\u8fdb\u884c\u9ad8\u6548\u5730\u8c03\u5ea6\uff0c\u6240\u4ee5\u5bf9process\u7684\u7ec4\u7ec7\u5c31\u975e\u5e38\u91cd\u8981 \u4e0b\u9762\u662f\u4e00\u4e9b\u53c2\u8003\u5185\u5bb9\uff1a The Linux Kernel/Processing Traverse the Process Tree Process management Scheduling (computing) Completely Fair Scheduler The Linux process tree","title":"20190126linux\u5185\u6838\u662f\u5982\u4f55\u6765\u7ec4\u7ec7process\u7684"},{"location":"Kernel/TODO/#20190126linuxprocess","text":"\u4eca\u5929\u5728\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \u8fd9\u7bc7\u6587\u7ae0\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u4e00\u6bb5\u8bdd\uff1a Historically, the Linux kernel has maintained a single process tree. The tree contains a reference to every process currently running in a parent-child hierarchy. A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. \u8fd9\u6bb5\u8bdd\u5f15\u8d77\u4e86\u6211\u7684\u601d\u8003\uff1a\u5728linux\u7684\u5185\u6838\u4e2d\uff0c\u662f\u4f7f\u7528tree\u6765\u6309\u7167parent-child\u5173\u7cfb\u6765\u7ec4\u7ec7process\u7684\u5417\uff1f \u521a\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff0c\u56e0\u4e3a ptree \uff0c\u663e\u7136\u6309\u7167parent-child\u5173\u7cfb\u662f\u53ef\u4ee5\u6309\u7167tree\u7ed3\u6784\u6765\u7ec4\u7ec7process\u7684\uff0c\u4f46\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\u8981\u8fdc\u6bd4\u8fd9\u590d\u6742\uff0c\u56e0\u4e3a\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\u6709\uff1a schedule\uff0c\u5373\u8c03\u5ea6\u95ee\u9898\uff0c\u5185\u6838\u9700\u8981\u8fdb\u884c\u9ad8\u6548\u5730\u8c03\u5ea6\uff0c\u6240\u4ee5\u5bf9process\u7684\u7ec4\u7ec7\u5c31\u975e\u5e38\u91cd\u8981 \u4e0b\u9762\u662f\u4e00\u4e9b\u53c2\u8003\u5185\u5bb9\uff1a The Linux Kernel/Processing Traverse the Process Tree Process management Scheduling (computing) Completely Fair Scheduler The Linux process tree","title":"20190126linux\u5185\u6838\u662f\u5982\u4f55\u6765\u7ec4\u7ec7process\u7684"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/","text":"\u5173\u4e8e\u672c\u4e66 # Understanding.The.Linux.kernel.3rd.Edition Understanding the Linux Kernel","title":"Introduction"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/#_1","text":"Understanding.The.Linux.kernel.3rd.Edition Understanding the Linux Kernel","title":"\u5173\u4e8e\u672c\u4e66"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/","text":"1.1. Linux Versus Other Unix-Like Kernels # The various Unix-like systems on the market, some of which have a long history and show signs of archaic\uff08\u53e4\u8001\u7684\uff0c\u9648\u65e7\u7684\uff09 practices, differ in many important respects. All commercial variants were derived from either SVR4 or 4.4BSD, and all tend to agree on some common standards like IEEE's Portable Operating Systems based on Unix (POSIX) and X/Open's Common Applications Environment (CAE). The current standards specify only an application programming interface (API)that is, a well-defined environment in which user programs should run. Therefore, the standards do not impose any restriction on internal design choices of a compliant kernel . [*] [*] As a matter of fact, several non-Unix operating systems, such as Windows NT and its descendents, are POSIX-compliant. NOTE: \u5173\u4e8eUnix-like system\u7684standard\uff0c\u6f14\u8fdb\u5386\u7a0b\uff0c\u53c2\u89c1\u300a Unix-standardization-and-implementation.md \u300b To define a common user interface, Unix-like kernels often share fundamental design ideas and features . In this respect, Linux is comparable with the other Unix-like operating systems. Reading this book and studying the Linux kernel, therefore, may help you understand the other Unix variants, too. The 2.6 version of the Linux kernel aims to be compliant with the IEEE POSIX standard. This, of course, means that most existing Unix programs can be compiled and executed on a Linux system with very little effort or even without the need for patches to the source code. Moreover, Linux includes all the features of a modern Unix operating system, such as virtual memory , a virtual filesystem , lightweight processes , Unix signals , SVR4 interprocess communications , support for Symmetric Multiprocessor (SMP) systems , and so on. When Linus Torvalds wrote the first kernel, he referred to some classical books on Unix internals, like Maurice Bach's The Design of the Unix Operating System (Prentice Hall, 1986). Actually, Linux still has some bias toward the Unix baseline described in Bach's book (i.e., SVR2). However, Linux doesn't stick to any particular variant. Instead, it tries to adopt the best features and design choices of several different Unix kernels. The following list describes how Linux competes against some well-known commercial Unix kernels: Monolithic kernel # It is a large, complex do-it-yourself program, composed of several logically different components. In this, it is quite conventional; most commercial Unix variants are monolithic. (Notable exceptions are the Apple Mac OS X and the GNU Hurd operating systems, both derived from the Carnegie-Mellon's Mach, which follow a microkernel approach.) NOTE: See also Monolithic kernel Compiled and statically linked traditional Unix kernels # Most modern kernels can dynamically load and unload some portions of the kernel code (typically, device drivers ), which are usually called modules . Linux's support for modules is very good, because it is able to automatically load and unload modules on demand. Among the main commercial Unix variants, only the SVR4.2 and Solaris kernels have a similar feature. NOTE: \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbamodule\u7684\u7ae0\u8282\uff1a 1.4.4. Kernel Architecture \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbadevice driver\u7684\u7ae0\u8282\uff1a 1.6.9. Device Drivers See also Loadable kernel module Kernel threading # Some Unix kernels, such as Solaris and SVR4.2/MP, are organized as a set of kernel threads .A kernel thread is an execution context that can be independently scheduled; it may be associated with a user program, or it may run only some kernel functions. Context switches between kernel threads are usually much less expensive than context switches between ordinary processes, because the former usually operate on a common address space. Linux uses kernel threads in a very limited way to execute a few kernel functions periodically; however, they do not represent the basic execution context abstraction. (That's the topic of the next item.) NOTE: \u672c\u4e66\u8ba8\u8bbakernel thread\u7684\u7ae0\u8282\uff1a 3.4.2. Kernel Threads Multithreaded application support # Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes, which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads , Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. NOTE: See also Light-weight process \u9605\u8bfb\u4ee5\u4e0b clone( ) system call\u7684\u6587\u6863\u6709\u52a9\u4e8e\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u7684\u542b\u4e49\u3002 NOTE: LWP VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy Preemptive kernel # When compiled with the \"Preemptible Kernel\" option, Linux 2.6 can arbitrarily interleave execution flows while they are in privileged mode . Besides Linux 2.6, a few other conventional, general-purpose Unix systems, such as Solaris and Mach 3.0 , are fully preemptive kernels . SVR4.2/MP introduces some fixed preemption points as a method to get limited preemption capability. NOTE: See also Preemption (computing) Kernel preemption Multiprocessor support # Several Unix kernel variants take advantage of multiprocessor systems. Linux 2.6 supports symmetric multiprocessing (SMP ) for different memory models, including NUMA: the system can use multiple processors and each processor can handle any task there is no discrimination among them. Although a few parts of the kernel code are still serialized by means of a single \" big kernel lock ,\" it is fair to say that Linux 2.6 makes a near optimal use of SMP. NOTE: Giant-lock Filesystem # Linux's standard filesystems come in many flavors. You can use the plain old Ext2 filesystem if you don't have specific needs. You might switch to Ext3 if you want to avoid lengthy filesystem checks after a system crash. If you'll have to deal with many small files, the ReiserFS filesystem is likely to be the best choice. Besides Ext3 and ReiserFS, several other journaling filesystems can be used in Linux; they include IBM AIX's Journaling File System (JFS ) and Silicon Graphics IRIX 's XFS filesystem. Thanks to a powerful object-oriented Virtual File System technology (inspired by Solaris and SVR4), porting a foreign filesystem to Linux is generally easier than porting to other kernels. STREAMS # Linux has no analog to the STREAMS I/O subsystem introduced in SVR4, although it is included now in most Unix kernels and has become the preferred interface for writing device drivers, terminal drivers, and network protocols.","title":"1.1-Linux-Versus-Other-Unix-Like-Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#11-linux-versus-other-unix-like-kernels","text":"The various Unix-like systems on the market, some of which have a long history and show signs of archaic\uff08\u53e4\u8001\u7684\uff0c\u9648\u65e7\u7684\uff09 practices, differ in many important respects. All commercial variants were derived from either SVR4 or 4.4BSD, and all tend to agree on some common standards like IEEE's Portable Operating Systems based on Unix (POSIX) and X/Open's Common Applications Environment (CAE). The current standards specify only an application programming interface (API)that is, a well-defined environment in which user programs should run. Therefore, the standards do not impose any restriction on internal design choices of a compliant kernel . [*] [*] As a matter of fact, several non-Unix operating systems, such as Windows NT and its descendents, are POSIX-compliant. NOTE: \u5173\u4e8eUnix-like system\u7684standard\uff0c\u6f14\u8fdb\u5386\u7a0b\uff0c\u53c2\u89c1\u300a Unix-standardization-and-implementation.md \u300b To define a common user interface, Unix-like kernels often share fundamental design ideas and features . In this respect, Linux is comparable with the other Unix-like operating systems. Reading this book and studying the Linux kernel, therefore, may help you understand the other Unix variants, too. The 2.6 version of the Linux kernel aims to be compliant with the IEEE POSIX standard. This, of course, means that most existing Unix programs can be compiled and executed on a Linux system with very little effort or even without the need for patches to the source code. Moreover, Linux includes all the features of a modern Unix operating system, such as virtual memory , a virtual filesystem , lightweight processes , Unix signals , SVR4 interprocess communications , support for Symmetric Multiprocessor (SMP) systems , and so on. When Linus Torvalds wrote the first kernel, he referred to some classical books on Unix internals, like Maurice Bach's The Design of the Unix Operating System (Prentice Hall, 1986). Actually, Linux still has some bias toward the Unix baseline described in Bach's book (i.e., SVR2). However, Linux doesn't stick to any particular variant. Instead, it tries to adopt the best features and design choices of several different Unix kernels. The following list describes how Linux competes against some well-known commercial Unix kernels:","title":"1.1. Linux Versus Other Unix-Like Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#monolithic-kernel","text":"It is a large, complex do-it-yourself program, composed of several logically different components. In this, it is quite conventional; most commercial Unix variants are monolithic. (Notable exceptions are the Apple Mac OS X and the GNU Hurd operating systems, both derived from the Carnegie-Mellon's Mach, which follow a microkernel approach.) NOTE: See also Monolithic kernel","title":"Monolithic kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#compiled-and-statically-linked-traditional-unix-kernels","text":"Most modern kernels can dynamically load and unload some portions of the kernel code (typically, device drivers ), which are usually called modules . Linux's support for modules is very good, because it is able to automatically load and unload modules on demand. Among the main commercial Unix variants, only the SVR4.2 and Solaris kernels have a similar feature. NOTE: \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbamodule\u7684\u7ae0\u8282\uff1a 1.4.4. Kernel Architecture \u672c\u4e66\u4e2d\uff0c\u8ba8\u8bbadevice driver\u7684\u7ae0\u8282\uff1a 1.6.9. Device Drivers See also Loadable kernel module","title":"Compiled and statically linked traditional Unix kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#kernel-threading","text":"Some Unix kernels, such as Solaris and SVR4.2/MP, are organized as a set of kernel threads .A kernel thread is an execution context that can be independently scheduled; it may be associated with a user program, or it may run only some kernel functions. Context switches between kernel threads are usually much less expensive than context switches between ordinary processes, because the former usually operate on a common address space. Linux uses kernel threads in a very limited way to execute a few kernel functions periodically; however, they do not represent the basic execution context abstraction. (That's the topic of the next item.) NOTE: \u672c\u4e66\u8ba8\u8bbakernel thread\u7684\u7ae0\u8282\uff1a 3.4.2. Kernel Threads","title":"Kernel threading"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#multithreaded-application-support","text":"Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes, which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads , Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. NOTE: See also Light-weight process \u9605\u8bfb\u4ee5\u4e0b clone( ) system call\u7684\u6587\u6863\u6709\u52a9\u4e8e\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u7684\u542b\u4e49\u3002 NOTE: LWP VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy","title":"Multithreaded application support"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#preemptive-kernel","text":"When compiled with the \"Preemptible Kernel\" option, Linux 2.6 can arbitrarily interleave execution flows while they are in privileged mode . Besides Linux 2.6, a few other conventional, general-purpose Unix systems, such as Solaris and Mach 3.0 , are fully preemptive kernels . SVR4.2/MP introduces some fixed preemption points as a method to get limited preemption capability. NOTE: See also Preemption (computing) Kernel preemption","title":"Preemptive kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#multiprocessor-support","text":"Several Unix kernel variants take advantage of multiprocessor systems. Linux 2.6 supports symmetric multiprocessing (SMP ) for different memory models, including NUMA: the system can use multiple processors and each processor can handle any task there is no discrimination among them. Although a few parts of the kernel code are still serialized by means of a single \" big kernel lock ,\" it is fair to say that Linux 2.6 makes a near optimal use of SMP. NOTE: Giant-lock","title":"Multiprocessor support"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#filesystem","text":"Linux's standard filesystems come in many flavors. You can use the plain old Ext2 filesystem if you don't have specific needs. You might switch to Ext3 if you want to avoid lengthy filesystem checks after a system crash. If you'll have to deal with many small files, the ReiserFS filesystem is likely to be the best choice. Besides Ext3 and ReiserFS, several other journaling filesystems can be used in Linux; they include IBM AIX's Journaling File System (JFS ) and Silicon Graphics IRIX 's XFS filesystem. Thanks to a powerful object-oriented Virtual File System technology (inspired by Solaris and SVR4), porting a foreign filesystem to Linux is generally easier than porting to other kernels.","title":"Filesystem"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.1-Linux-Versus-Other-Unix-Like-Kernels/#streams","text":"Linux has no analog to the STREAMS I/O subsystem introduced in SVR4, although it is included now in most Unix kernels and has become the preferred interface for writing device drivers, terminal drivers, and network protocols.","title":"STREAMS"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.2-Hardware-Dependency/","text":"1.2. Hardware Dependency # Linux tries to maintain a neat distinction between hardware-dependent and hardware-independent source code. To that end, both the arch and the include directories include 23 subdirectories that correspond to the different types of hardware platforms supported. The standard names of the platforms are: arm, arm26 ARM processor-based computers such as PDAs and embedded devices i386 IBM-compatible personal computers based on 80x86 microprocessors ia64 Workstations based on the Intel 64-bit Itanium microprocessor mips Workstations based on MIPS microprocessors, such as those marketed by Silicon Graphics x86_64 Workstations based on the AMD's 64-bit microprocessorssuch Athlon and Opteron and Intel's ia32e/EM64T 64-bit microprocessors","title":"1.2-Hardware-Dependency"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.2-Hardware-Dependency/#12-hardware-dependency","text":"Linux tries to maintain a neat distinction between hardware-dependent and hardware-independent source code. To that end, both the arch and the include directories include 23 subdirectories that correspond to the different types of hardware platforms supported. The standard names of the platforms are: arm, arm26 ARM processor-based computers such as PDAs and embedded devices i386 IBM-compatible personal computers based on 80x86 microprocessors ia64 Workstations based on the Intel 64-bit Itanium microprocessor mips Workstations based on MIPS microprocessors, such as those marketed by Silicon Graphics x86_64 Workstations based on the AMD's 64-bit microprocessorssuch Athlon and Opteron and Intel's ia32e/EM64T 64-bit microprocessors","title":"1.2. Hardware Dependency"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/","text":"1.4. Basic Operating System Concepts # Each computer system includes a basic set of programs called the operating system . The most important program in the set is called the kernel . It is loaded into RAM when the system boots and contains many critical procedures that are needed for the system to operate. The other programs are less crucial utilities; they can provide a wide variety of interactive experiences for the user as well as doing all the jobs the user bought the computer for but the essential shape and capabilities of the system are determined by the kernel . The kernel provides key facilities to everything else on the system and determines many of the characteristics of higher software. Hence, we often use the term \"operating system\" as a synonym for \"kernel.\" NOTE: See also Operating system Kernel (operating system) The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). Some operating systems allow all user programs to directly play with the hardware components (a typical example is MS-DOS ). In contrast, a Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. When a program wants to use a hardware resource, it must issue a request to the operating system. The kernel evaluates the request and, if it chooses to grant the resource, interacts with the proper hardware components on behalf of the user program. To enforce this mechanism, modern operating systems rely on the availability of specific hardware features that forbid user programs to directly interact with low-level hardware components or to access arbitrary memory locations. In particular, the hardware introduces at least two different execution modes for the CPU: a nonprivileged mode for user programs and a privileged mode for the kernel. Unix calls these User Mode and Kernel Mode , respectively. NOTE: See also User space CPU modes Protection ring In the rest of this chapter, we introduce the basic concepts that have motivated the design of Unix over the past two decades, as well as Linux and other operating systems. While the concepts are probably familiar to you as a Linux user, these sections try to delve into them a bit more deeply than usual to explain the requirements they place on an operating system kernel. These broad considerations refer to virtually all Unix-like systems. The other chapters of this book will hopefully help you understand the Linux kernel internals. 1.4.1. Multiuser Systems # A multiuser system is a computer that is able to concurrently and independently execute several applications belonging to two or more users . Concurrently means that applications can be active at the same time and contend for the various resources such as CPU, memory, hard disks, and so on. Independently means that each application can perform its task with no concern for what the applications of the other users are doing. Switching from one application to another, of course, slows down each of them and affects the response time seen by the users. Many of the complexities of modern operating system kernels, which we will examine in this book, are present to minimize the delays enforced on each program and to provide the user with responses that are as fast as possible. Multiuser operating systems must include several features: An authentication mechanism for verifying the user's identity A protection mechanism against buggy user programs that could block other applications running in the system A protection mechanism against malicious user programs that could interfere with or spy on the activity of other users An accounting mechanism that limits the amount of resource units assigned to each user To ensure safe protection mechanisms , operating systems must use the hardware protection associated with the CPU privileged mode . Otherwise, a user program would be able to directly access the system circuitry and overcome the imposed bounds. Unix is a multiuser system that enforces the hardware protection of system resources . 1.4.2. Users and Groups # In a multiuser system , each user has a private space on the machine; typically, he owns some quota of the disk space to store files, receives private mail messages, and so on. The operating system must ensure that the private portion of a user space is visible only to its owner. In particular, it must ensure that no user can exploit a system application for the purpose of violating the private space of another user. All users are identified by a unique number called the User ID , or UID . Usually only a restricted number of persons are allowed to make use of a computer system. When one of these users starts a working session, the system asks for a login name and a password. If the user does not input a valid pair, the system denies access. Because the password is assumed to be secret, the user's privacy is ensured. To selectively share material with other users, each user is a member of one or more user groups , which are identified by a unique number called a user group ID . Each file is associated with exactly one group. For example, access can be set so the user owning the file has read and write privileges, the group has read-only privileges, and other users on the system are denied access to the file. Any Unix-like operating system has a special user called root or superuser . The system administrator must log in as root to handle user accounts, perform maintenance tasks such as system backups and program upgrades, and so on. The root user can do almost everything, because the operating system does not apply the usual protection mechanisms to her. In particular, the root user can access every file on the system and can manipulate every running user program. 1.4.3. Processes # NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process All operating systems use one fundamental abstraction: the process . A process can be defined either as \"an instance of a program in execution\" or as the \"execution context\" of a running program. In traditional operating systems, a process executes a single sequence of instructions in an address space ; the address space is the set of memory addresses that the process is allowed to reference. Modern operating systems allow processes with multiple execution flows that is, multiple sequences of instructions executed in the same address space . NOTE: \u6bcf\u4e2aexecution flow\u5bf9\u5e94\u7684\u662f\u4e00\u4e2athread Multiuser systems must enforce an execution environment in which several processes can be active concurrently and contend for system resources, mainly the CPU. Systems that allow concurrent active processes are said to be multiprogramming or multiprocessing . [*] It is important to distinguish programs from processes; several processes can execute the same program concurrently, while the same process can execute several programs sequentially. [*] Some multiprocessing operating systems are not multiuser; an example is Microsoft Windows 98. On uniprocessor systems, just one process can hold the CPU, and hence just one execution flow can progress at a time. In general, the number of CPUs is always restricted, and therefore only a few processes can progress at once. An operating system component called the scheduler chooses the process that can progress. Some operating systems allow only nonpreemptable processes, which means that the scheduler is invoked only when a process voluntarily relinquishes the CPU. But processes of a multiuser system must be preemptable; the operating system tracks how long each process holds the CPU and periodically activates the scheduler. NOTE: See also Single-tasking and multi-tasking Preemption (computing) Unix is a multiprocessing operating system with preemptable processes . Unix-like operating systems adopt a process/kernel model . Each process has the illusion that it's the only process on the machine, and it has exclusive access to the operating system services. Whenever a process makes a system call (i.e., a request to the kernel, see Chapter 10), the hardware changes the privilege mode from User Mode to Kernel Mode , and the process starts the execution of a kernel procedure with a strictly limited purpose. In this way, the operating system acts within the execution context of the process in order to satisfy its request. Whenever the request is fully satisfied, the kernel procedure forces the hardware to return to User Mode and the process continues its execution from the instruction following the system call. NOTE: process/kernel model \u4f1a\u57281.6.1. The Process/Kernel Model\u8282\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002 1.4.4. Kernel Architecture # As stated before, most Unix kernels are monolithic: each kernel layer is integrated into the whole kernel program and runs in Kernel Mode on behalf of the current process . In contrast, microkernel operating systems demand a very small set of functions from the kernel, generally including a few synchronization primitives , a simple scheduler , and an interprocess communication mechanism . Several system processes that run on top of the microkernel implement other operating system-layer functions , like memory allocators , device drivers , and system call handlers . Although academic research on operating systems is oriented toward microkernels , such operating systems are generally slower than monolithic ones, because the explicit message passing between the different layers of the operating system has a cost. However, microkernel operating systems might have some theoretical advantages over monolithic ones. Microkernels force the system programmers to adopt a modularized approach, because each operating system layer is a relatively independent program that must interact with the other layers through well-defined and clean software interfaces. Moreover, an existing microkernel operating system can be easily ported to other architectures fairly easily, because all hardware-dependent components are generally encapsulated in the microkernel code. Finally, microkernel operating systems tend to make better use of random access memory (RAM) than monolithic ones, because system processes that aren't implementing needed functionalities might be swapped out or destroyed. To achieve many of the theoretical advantages of microkernels without introducing performance penalties, the Linux kernel offers modules . A module is an object file whose code can be linked to (and unlinked from) the kernel at runtime. The object code usually consists of a set of functions that implements a filesystem, a device driver, or other features at the kernel's upper layer. The module, unlike the external layers of microkernel operating systems, does not run as a specific process. Instead, it is executed in Kernel Mode on behalf of the current process, like any other statically linked kernel function. The main advantages of using modules include: modularized approach Because any module can be linked and unlinked at runtime, system programmers must introduce well-defined software interfaces to access the data structures handled by modules. This makes it easy to develop new modules. Platform independence Even if it may rely on some specific hardware features, a module doesn't depend on a fixed hardware platform. For example, a disk driver module that relies on the SCSI standard works as well on an IBM-compatible PC as it does on Hewlett-Packard's Alpha. Frugal main memory usage A module can be linked to the running kernel when its functionality is required and unlinked when it is no longer useful; this is quite useful for small embedded systems. No performance penalty Once linked in, the object code of a module is equivalent to the object code of the statically linked kernel. Therefore, no explicit message passing is required when the functions of the module are invoked. [*] [*] A small performance penalty occurs when the module is linked and unlinked. However, this penalty can be compared to the penalty caused by the creation and deletion of system processes in microkernel operating systems.","title":"1.4-Basic-Operating-System-Concepts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#14-basic-operating-system-concepts","text":"Each computer system includes a basic set of programs called the operating system . The most important program in the set is called the kernel . It is loaded into RAM when the system boots and contains many critical procedures that are needed for the system to operate. The other programs are less crucial utilities; they can provide a wide variety of interactive experiences for the user as well as doing all the jobs the user bought the computer for but the essential shape and capabilities of the system are determined by the kernel . The kernel provides key facilities to everything else on the system and determines many of the characteristics of higher software. Hence, we often use the term \"operating system\" as a synonym for \"kernel.\" NOTE: See also Operating system Kernel (operating system) The operating system must fulfill two main objectives: Interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. Provide an execution environment to the applications that run on the computer system (the so-called user programs). Some operating systems allow all user programs to directly play with the hardware components (a typical example is MS-DOS ). In contrast, a Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. When a program wants to use a hardware resource, it must issue a request to the operating system. The kernel evaluates the request and, if it chooses to grant the resource, interacts with the proper hardware components on behalf of the user program. To enforce this mechanism, modern operating systems rely on the availability of specific hardware features that forbid user programs to directly interact with low-level hardware components or to access arbitrary memory locations. In particular, the hardware introduces at least two different execution modes for the CPU: a nonprivileged mode for user programs and a privileged mode for the kernel. Unix calls these User Mode and Kernel Mode , respectively. NOTE: See also User space CPU modes Protection ring In the rest of this chapter, we introduce the basic concepts that have motivated the design of Unix over the past two decades, as well as Linux and other operating systems. While the concepts are probably familiar to you as a Linux user, these sections try to delve into them a bit more deeply than usual to explain the requirements they place on an operating system kernel. These broad considerations refer to virtually all Unix-like systems. The other chapters of this book will hopefully help you understand the Linux kernel internals.","title":"1.4. Basic Operating System Concepts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#141-multiuser-systems","text":"A multiuser system is a computer that is able to concurrently and independently execute several applications belonging to two or more users . Concurrently means that applications can be active at the same time and contend for the various resources such as CPU, memory, hard disks, and so on. Independently means that each application can perform its task with no concern for what the applications of the other users are doing. Switching from one application to another, of course, slows down each of them and affects the response time seen by the users. Many of the complexities of modern operating system kernels, which we will examine in this book, are present to minimize the delays enforced on each program and to provide the user with responses that are as fast as possible. Multiuser operating systems must include several features: An authentication mechanism for verifying the user's identity A protection mechanism against buggy user programs that could block other applications running in the system A protection mechanism against malicious user programs that could interfere with or spy on the activity of other users An accounting mechanism that limits the amount of resource units assigned to each user To ensure safe protection mechanisms , operating systems must use the hardware protection associated with the CPU privileged mode . Otherwise, a user program would be able to directly access the system circuitry and overcome the imposed bounds. Unix is a multiuser system that enforces the hardware protection of system resources .","title":"1.4.1. Multiuser Systems"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#142-users-and-groups","text":"In a multiuser system , each user has a private space on the machine; typically, he owns some quota of the disk space to store files, receives private mail messages, and so on. The operating system must ensure that the private portion of a user space is visible only to its owner. In particular, it must ensure that no user can exploit a system application for the purpose of violating the private space of another user. All users are identified by a unique number called the User ID , or UID . Usually only a restricted number of persons are allowed to make use of a computer system. When one of these users starts a working session, the system asks for a login name and a password. If the user does not input a valid pair, the system denies access. Because the password is assumed to be secret, the user's privacy is ensured. To selectively share material with other users, each user is a member of one or more user groups , which are identified by a unique number called a user group ID . Each file is associated with exactly one group. For example, access can be set so the user owning the file has read and write privileges, the group has read-only privileges, and other users on the system are denied access to the file. Any Unix-like operating system has a special user called root or superuser . The system administrator must log in as root to handle user accounts, perform maintenance tasks such as system backups and program upgrades, and so on. The root user can do almost everything, because the operating system does not apply the usual protection mechanisms to her. In particular, the root user can access every file on the system and can manipulate every running user program.","title":"1.4.2. Users and Groups"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#143-processes","text":"NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process All operating systems use one fundamental abstraction: the process . A process can be defined either as \"an instance of a program in execution\" or as the \"execution context\" of a running program. In traditional operating systems, a process executes a single sequence of instructions in an address space ; the address space is the set of memory addresses that the process is allowed to reference. Modern operating systems allow processes with multiple execution flows that is, multiple sequences of instructions executed in the same address space . NOTE: \u6bcf\u4e2aexecution flow\u5bf9\u5e94\u7684\u662f\u4e00\u4e2athread Multiuser systems must enforce an execution environment in which several processes can be active concurrently and contend for system resources, mainly the CPU. Systems that allow concurrent active processes are said to be multiprogramming or multiprocessing . [*] It is important to distinguish programs from processes; several processes can execute the same program concurrently, while the same process can execute several programs sequentially. [*] Some multiprocessing operating systems are not multiuser; an example is Microsoft Windows 98. On uniprocessor systems, just one process can hold the CPU, and hence just one execution flow can progress at a time. In general, the number of CPUs is always restricted, and therefore only a few processes can progress at once. An operating system component called the scheduler chooses the process that can progress. Some operating systems allow only nonpreemptable processes, which means that the scheduler is invoked only when a process voluntarily relinquishes the CPU. But processes of a multiuser system must be preemptable; the operating system tracks how long each process holds the CPU and periodically activates the scheduler. NOTE: See also Single-tasking and multi-tasking Preemption (computing) Unix is a multiprocessing operating system with preemptable processes . Unix-like operating systems adopt a process/kernel model . Each process has the illusion that it's the only process on the machine, and it has exclusive access to the operating system services. Whenever a process makes a system call (i.e., a request to the kernel, see Chapter 10), the hardware changes the privilege mode from User Mode to Kernel Mode , and the process starts the execution of a kernel procedure with a strictly limited purpose. In this way, the operating system acts within the execution context of the process in order to satisfy its request. Whenever the request is fully satisfied, the kernel procedure forces the hardware to return to User Mode and the process continues its execution from the instruction following the system call. NOTE: process/kernel model \u4f1a\u57281.6.1. The Process/Kernel Model\u8282\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"1.4.3. Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.4-Basic-Operating-System-Concepts/#144-kernel-architecture","text":"As stated before, most Unix kernels are monolithic: each kernel layer is integrated into the whole kernel program and runs in Kernel Mode on behalf of the current process . In contrast, microkernel operating systems demand a very small set of functions from the kernel, generally including a few synchronization primitives , a simple scheduler , and an interprocess communication mechanism . Several system processes that run on top of the microkernel implement other operating system-layer functions , like memory allocators , device drivers , and system call handlers . Although academic research on operating systems is oriented toward microkernels , such operating systems are generally slower than monolithic ones, because the explicit message passing between the different layers of the operating system has a cost. However, microkernel operating systems might have some theoretical advantages over monolithic ones. Microkernels force the system programmers to adopt a modularized approach, because each operating system layer is a relatively independent program that must interact with the other layers through well-defined and clean software interfaces. Moreover, an existing microkernel operating system can be easily ported to other architectures fairly easily, because all hardware-dependent components are generally encapsulated in the microkernel code. Finally, microkernel operating systems tend to make better use of random access memory (RAM) than monolithic ones, because system processes that aren't implementing needed functionalities might be swapped out or destroyed. To achieve many of the theoretical advantages of microkernels without introducing performance penalties, the Linux kernel offers modules . A module is an object file whose code can be linked to (and unlinked from) the kernel at runtime. The object code usually consists of a set of functions that implements a filesystem, a device driver, or other features at the kernel's upper layer. The module, unlike the external layers of microkernel operating systems, does not run as a specific process. Instead, it is executed in Kernel Mode on behalf of the current process, like any other statically linked kernel function. The main advantages of using modules include: modularized approach Because any module can be linked and unlinked at runtime, system programmers must introduce well-defined software interfaces to access the data structures handled by modules. This makes it easy to develop new modules. Platform independence Even if it may rely on some specific hardware features, a module doesn't depend on a fixed hardware platform. For example, a disk driver module that relies on the SCSI standard works as well on an IBM-compatible PC as it does on Hewlett-Packard's Alpha. Frugal main memory usage A module can be linked to the running kernel when its functionality is required and unlinked when it is no longer useful; this is quite useful for small embedded systems. No performance penalty Once linked in, the object code of a module is equivalent to the object code of the statically linked kernel. Therefore, no explicit message passing is required when the functions of the module are invoked. [*] [*] A small performance penalty occurs when the module is linked and unlinked. However, this penalty can be compared to the penalty caused by the creation and deletion of system processes in microkernel operating systems.","title":"1.4.4. Kernel Architecture"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6-An-Overview-of-Unix-Kernels/","text":"1.6. An Overview of Unix Kernels 1.6. An Overview of Unix Kernels # Unix kernels provide an execution environment in which applications may run. Therefore, the kernel must implement a set of services and corresponding interfaces . Applications use those interfaces and do not usually interact directly with hardware resources .","title":"1.6-An-Overview-of-Unix-Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6-An-Overview-of-Unix-Kernels/#16-an-overview-of-unix-kernels","text":"Unix kernels provide an execution environment in which applications may run. Therefore, the kernel must implement a set of services and corresponding interfaces . Applications use those interfaces and do not usually interact directly with hardware resources .","title":"1.6. An Overview of Unix Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.1-The-Process-Kernel-Model/","text":"1.6.1. The Process/Kernel Model # When a program is executed in User Mode , it cannot directly access the kernel data structures or the kernel programs . When an application executes in Kernel Mode , however, these restrictions no longer apply. Each CPU model provides special instructions to switch from User Mode to Kernel Mode and vice versa. A program usually executes in User Mode and switches to Kernel Mode only when requesting a service provided by the kernel. When the kernel has satisfied the program's request, it puts the program back in User Mode . NOTE: See also User space CPU modes Protection ring Processes are dynamic entities that usually have a limited life span within the system. The task of creating, eliminating, and synchronizing the existing processes is delegated to a group of routines in the kernel. The kernel itself is not a process but a process manager . The process/kernel model assumes that processes that require a kernel service use specific programming constructs called system calls . Each system call sets up the group of parameters that identifies the process request and then executes the hardware-dependent CPU instruction to switch from User Mode to Kernel Mode . NOTE: See also System call Besides user processes, Unix systems include a few privileged processes called kernel threads with the following characteristics: They run in Kernel Mode in the kernel address space. They do not interact with users, and thus do not require terminal devices. They are usually created during system startup and remain alive until the system is shut down. NOTE : What is a Kernel thread? Understanding Kernel Threads On a uniprocessor system, only one process is running at a time, and it may run either in User or in Kernel Mode. If it runs in Kernel Mode , the processor is executing some kernel routine . Figure 1-2 illustrates examples of transitions between User and Kernel Mode. Process 1 in User Mode issues a system call , after which the process switches to Kernel Mode , and the system call is serviced. Process 1 then resumes execution in User Mode until a timer interrupt occurs, and the scheduler is activated in Kernel Mode . A process switch takes place, and Process 2 starts its execution in User Mode until a hardware device raises an interrupt. As a consequence of the interrupt, Process 2 switches to Kernel Mode and services the interrupt. Unix kernels do much more than handle system calls ; in fact, kernel routines can be activated in several ways: A process invokes a system call . The CPU executing the process signals an exception , which is an unusual condition such as an invalid instruction. The kernel handles the exception on behalf of the process that caused it. A peripheral device issues an interrupt signal to the CPU to notify it of an event such as a request for attention, a status change, or the completion of an I/O operation. Each interrupt signal is dealt by a kernel program called an interrupt handler . Because peripheral devices operate asynchronously with respect to the CPU, interrupts occur at unpredictable times. A kernel thread is executed. Because it runs in Kernel Mode, the corresponding program must be considered part of the kernel.","title":"1.6.1-The-Process-Kernel-Model"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.1-The-Process-Kernel-Model/#161-the-processkernel-model","text":"When a program is executed in User Mode , it cannot directly access the kernel data structures or the kernel programs . When an application executes in Kernel Mode , however, these restrictions no longer apply. Each CPU model provides special instructions to switch from User Mode to Kernel Mode and vice versa. A program usually executes in User Mode and switches to Kernel Mode only when requesting a service provided by the kernel. When the kernel has satisfied the program's request, it puts the program back in User Mode . NOTE: See also User space CPU modes Protection ring Processes are dynamic entities that usually have a limited life span within the system. The task of creating, eliminating, and synchronizing the existing processes is delegated to a group of routines in the kernel. The kernel itself is not a process but a process manager . The process/kernel model assumes that processes that require a kernel service use specific programming constructs called system calls . Each system call sets up the group of parameters that identifies the process request and then executes the hardware-dependent CPU instruction to switch from User Mode to Kernel Mode . NOTE: See also System call Besides user processes, Unix systems include a few privileged processes called kernel threads with the following characteristics: They run in Kernel Mode in the kernel address space. They do not interact with users, and thus do not require terminal devices. They are usually created during system startup and remain alive until the system is shut down. NOTE : What is a Kernel thread? Understanding Kernel Threads On a uniprocessor system, only one process is running at a time, and it may run either in User or in Kernel Mode. If it runs in Kernel Mode , the processor is executing some kernel routine . Figure 1-2 illustrates examples of transitions between User and Kernel Mode. Process 1 in User Mode issues a system call , after which the process switches to Kernel Mode , and the system call is serviced. Process 1 then resumes execution in User Mode until a timer interrupt occurs, and the scheduler is activated in Kernel Mode . A process switch takes place, and Process 2 starts its execution in User Mode until a hardware device raises an interrupt. As a consequence of the interrupt, Process 2 switches to Kernel Mode and services the interrupt. Unix kernels do much more than handle system calls ; in fact, kernel routines can be activated in several ways: A process invokes a system call . The CPU executing the process signals an exception , which is an unusual condition such as an invalid instruction. The kernel handles the exception on behalf of the process that caused it. A peripheral device issues an interrupt signal to the CPU to notify it of an event such as a request for attention, a status change, or the completion of an I/O operation. Each interrupt signal is dealt by a kernel program called an interrupt handler . Because peripheral devices operate asynchronously with respect to the CPU, interrupts occur at unpredictable times. A kernel thread is executed. Because it runs in Kernel Mode, the corresponding program must be considered part of the kernel.","title":"1.6.1. The Process/Kernel Model"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.2-Process-Implementation/","text":"1.6.2. Process Implementation # To let the kernel manage processes, each process is represented by a process descriptor that includes information about the current state of the process. NOTE: process descriptor \u57283.2. Process Descriptor\u4e2d\u8fdb\u884c\u4e13\u95e8\u4ecb\u7ecd \u672c\u8282\u4e2d\u7684process\u6240\u6307\u4e3alightweight process\uff0c\u800c\u4e0d\u662f\u6807\u51c6\u7684 Process (computing) When the kernel stops the execution of a process, it saves the current contents of several processor registers in the process descriptor. These include: The program counter (PC) and stack pointer (SP) registers The general purpose registers The floating point registers The processor control registers (Processor Status Word) containing information about the CPU state The memory management registers used to keep track of the RAM accessed by the process NOTE: See also Program counter Stack register Processor register When the kernel decides to resume executing a process, it uses the proper process descriptor fields to load the CPU registers. Because the stored value of the program counter points to the instruction following the last instruction executed, the process resumes execution at the point where it was stopped. When a process is not executing on the CPU, it is waiting for some event. Unix kernels distinguish many wait states , which are usually implemented by queues of process descriptors ; each (possibly empty) queue corresponds to the set of processes waiting for a specific event. NOTE: \u53c2\u89c13.2.4. How Processes Are Organized","title":"1.6.2-Process-Implementation"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.2-Process-Implementation/#162-process-implementation","text":"To let the kernel manage processes, each process is represented by a process descriptor that includes information about the current state of the process. NOTE: process descriptor \u57283.2. Process Descriptor\u4e2d\u8fdb\u884c\u4e13\u95e8\u4ecb\u7ecd \u672c\u8282\u4e2d\u7684process\u6240\u6307\u4e3alightweight process\uff0c\u800c\u4e0d\u662f\u6807\u51c6\u7684 Process (computing) When the kernel stops the execution of a process, it saves the current contents of several processor registers in the process descriptor. These include: The program counter (PC) and stack pointer (SP) registers The general purpose registers The floating point registers The processor control registers (Processor Status Word) containing information about the CPU state The memory management registers used to keep track of the RAM accessed by the process NOTE: See also Program counter Stack register Processor register When the kernel decides to resume executing a process, it uses the proper process descriptor fields to load the CPU registers. Because the stored value of the program counter points to the instruction following the last instruction executed, the process resumes execution at the point where it was stopped. When a process is not executing on the CPU, it is waiting for some event. Unix kernels distinguish many wait states , which are usually implemented by queues of process descriptors ; each (possibly empty) queue corresponds to the set of processes waiting for a specific event. NOTE: \u53c2\u89c13.2.4. How Processes Are Organized","title":"1.6.2. Process Implementation"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.3-Reentrant-Kernels/","text":"1.6.3. Reentrant Kernels # NOTE: \u4ece\u4e00\u4e2a\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u672c\u8282\u7684\u5185\u5bb9\uff0c\u5c06\u66f4\u52a0\u5bb9\u6613\u638c\u63e1\u4f5c\u8005\u6240\u8981\u4f20\u8fbe\u7684\u601d\u60f3\u3002\u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2a\u8bf7\u6c42\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2a\u8bf7\u6c42\uff1b\u5f53\u8be5\u8bf7\u6c42\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u6302\u8d77\u5f53\u524d\u7684\u8bf7\u6c42\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684handler\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662f\u672c\u8282\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728\u672c\u8282\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization \u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\uff0c\u672c\u6bb5\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u8fd9\u4e2a\u6982\u5ff5\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684\u6d3b\u52a8\uff0c\u4e3b\u8981\u5305\u62ec\u5982\u4e0b\u4e24\u79cd\u60c5\u51b5\uff1a system call interrupt and exception \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff0cresume\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u8bf4\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u5173\u4e8e\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u7684\u4e00\u4e9b\u6267\u884c\u7ec6\u8282\uff0c\u6bd4\u5982kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662fkernel control path runs on behalf of process\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff1a Kernel Control Path Definition All Unix kernels are reentrant . This means that several processes\uff08\u6307\u7684\u662flightweight process\uff09 may be executing in Kernel Mode at the same time. Of course, on uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes. An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution. One way to provide reentrancy is to write functions so that they modify only local variables and do not alter global data structures . Such functions are called reentrant functions . But a reentrant kernel is not limited only to such reentrant functions (although that is how some real-time kernels are implemented). Instead, the kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process can execute a nonreentrant function at a time. If a hardware interrupt occurs, a reentrant kernel is able to suspend the current running process even if that process is in Kernel Mode . This capability is very important, because it improves the throughput of the device controllers that issue interrupts. Once a device has issued an interrupt, it waits until the CPU acknowledges it. If the kernel is able to answer quickly, the device controller will be able to perform other tasks while the CPU handles the interrupt. Now let's look at kernel reentrancy and its impact on the organization of the kernel. A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call , an exception , or an interrupt . In the simplest case, the CPU executes a kernel control path sequentially from the first instruction to the last. When one of the following events occurs, however, the CPU interleaves the kernel control paths : A process executing in User Mode invokes a system call , and the corresponding kernel control path verifies that the request cannot be satisfied immediately; it then invokes the scheduler to select a new process to run. As a result, a process switch occurs. The first kernel control path is left unfinished, and the CPU resumes the execution of some other kernel control path . In this case, the two control paths are executed on behalf of two different processes. The CPU detects an exception for example, access to a page not present in RAM while running a kernel control path . The first control path is suspended, and the CPU starts the execution of a suitable procedure. In our example, this type of procedure can allocate a new page for the process and read its contents from disk. When the procedure terminates, the first control path can be resumed. In this case, the two control paths are executed on behalf of the same process. A hardware interrupt occurs while the CPU is running a kernel control path with the interrupts enabled. The first kernel control path is left unfinished, and the CPU starts processing another kernel control path to handle the interrupt. The first kernel control path resumes when the interrupt handler terminates. In this case, the two kernel control paths run in the execution context of the same process, and the total system CPU time is accounted to it. However, the interrupt handler doesn't necessarily operate on behalf of the process. An interrupt occurs while the CPU is running with kernel preemption enabled, and a higher priority process is runnable. In this case, the first kernel control path is left unfinished, and the CPU resumes executing another kernel control path on behalf of the higher priority process. This occurs only if the kernel has been compiled with kernel preemption support. Figure 1-3 illustrates a few examples of noninterleaved and interleaved kernel control paths. Three different CPU states are considered: Running a process in User Mode ( User ) Running an exception or a system call handler ( Excp ) Running an interrupt handler ( Intr )","title":"1.6.3-Reentrant-Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.3-Reentrant-Kernels/#163-reentrant-kernels","text":"NOTE: \u4ece\u4e00\u4e2a\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u672c\u8282\u7684\u5185\u5bb9\uff0c\u5c06\u66f4\u52a0\u5bb9\u6613\u638c\u63e1\u4f5c\u8005\u6240\u8981\u4f20\u8fbe\u7684\u601d\u60f3\u3002\u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2a\u8bf7\u6c42\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2a\u8bf7\u6c42\uff1b\u5f53\u8be5\u8bf7\u6c42\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u6302\u8d77\u5f53\u524d\u7684\u8bf7\u6c42\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684handler\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662f\u672c\u8282\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728\u672c\u8282\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization \u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\uff0c\u672c\u6bb5\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u8fd9\u4e2a\u6982\u5ff5\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684\u6d3b\u52a8\uff0c\u4e3b\u8981\u5305\u62ec\u5982\u4e0b\u4e24\u79cd\u60c5\u51b5\uff1a system call interrupt and exception \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff0cresume\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u8bf4\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u5173\u4e8e\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u7684\u4e00\u4e9b\u6267\u884c\u7ec6\u8282\uff0c\u6bd4\u5982kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662fkernel control path runs on behalf of process\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff1a Kernel Control Path Definition All Unix kernels are reentrant . This means that several processes\uff08\u6307\u7684\u662flightweight process\uff09 may be executing in Kernel Mode at the same time. Of course, on uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/O operation. For instance, after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes. An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution. One way to provide reentrancy is to write functions so that they modify only local variables and do not alter global data structures . Such functions are called reentrant functions . But a reentrant kernel is not limited only to such reentrant functions (although that is how some real-time kernels are implemented). Instead, the kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process can execute a nonreentrant function at a time. If a hardware interrupt occurs, a reentrant kernel is able to suspend the current running process even if that process is in Kernel Mode . This capability is very important, because it improves the throughput of the device controllers that issue interrupts. Once a device has issued an interrupt, it waits until the CPU acknowledges it. If the kernel is able to answer quickly, the device controller will be able to perform other tasks while the CPU handles the interrupt. Now let's look at kernel reentrancy and its impact on the organization of the kernel. A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call , an exception , or an interrupt . In the simplest case, the CPU executes a kernel control path sequentially from the first instruction to the last. When one of the following events occurs, however, the CPU interleaves the kernel control paths : A process executing in User Mode invokes a system call , and the corresponding kernel control path verifies that the request cannot be satisfied immediately; it then invokes the scheduler to select a new process to run. As a result, a process switch occurs. The first kernel control path is left unfinished, and the CPU resumes the execution of some other kernel control path . In this case, the two control paths are executed on behalf of two different processes. The CPU detects an exception for example, access to a page not present in RAM while running a kernel control path . The first control path is suspended, and the CPU starts the execution of a suitable procedure. In our example, this type of procedure can allocate a new page for the process and read its contents from disk. When the procedure terminates, the first control path can be resumed. In this case, the two control paths are executed on behalf of the same process. A hardware interrupt occurs while the CPU is running a kernel control path with the interrupts enabled. The first kernel control path is left unfinished, and the CPU starts processing another kernel control path to handle the interrupt. The first kernel control path resumes when the interrupt handler terminates. In this case, the two kernel control paths run in the execution context of the same process, and the total system CPU time is accounted to it. However, the interrupt handler doesn't necessarily operate on behalf of the process. An interrupt occurs while the CPU is running with kernel preemption enabled, and a higher priority process is runnable. In this case, the first kernel control path is left unfinished, and the CPU resumes executing another kernel control path on behalf of the higher priority process. This occurs only if the kernel has been compiled with kernel preemption support. Figure 1-3 illustrates a few examples of noninterleaved and interleaved kernel control paths. Three different CPU states are considered: Running a process in User Mode ( User ) Running an exception or a system call handler ( Excp ) Running an interrupt handler ( Intr )","title":"1.6.3. Reentrant Kernels"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.4-Process-Address-Space/","text":"1.6.4. Process Address Space # Each process runs in its private address space. A process running in User Mode refers to private stack, data, and code areas. When running in Kernel Mode, the process addresses the kernel data and code areas and uses another private stack. NOTE: \u4e0d\u540c\u7684mode\uff0c\u4f7f\u7528\u4e0d\u540c\u7684address space Because the kernel is reentrant, several kernel control paths each related to a different process may be executed in turn. In this case, each kernel control path refers to its own private kernel stack . NOTE : \u5173\u4e8ekernel stack\u53c2\u89c13.2.2.1. Process descriptors handling While it appears to each process that it has access to a private address space , there are times when part of the address space is shared among processes. In some cases, this sharing is explicitly requested by processes; in others, it is done automatically by the kernel to reduce memory usage. If the same program, say an editor, is needed simultaneously by several users, the program is loaded into memory only once, and its instructions can be shared by all of the users who need it. Its data, of course, must not be shared, because each user will have separate data. This kind of shared address space is done automatically by the kernel to save memory. Processes also can share parts of their address space as a kind of interprocess communication , using the \"shared memory\" technique introduced in System V and supported by Linux. Finally, Linux supports the mmap( ) system call, which allows part of a file or the information stored on a block device to be mapped into a part of a process address space. Memory mapping can provide an alternative to normal reads and writes for transferring data. If the same file is shared by several processes, its memory mapping is included in the address space of each of the processes that share it.","title":"1.6.4-Process-Address-Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.4-Process-Address-Space/#164-process-address-space","text":"Each process runs in its private address space. A process running in User Mode refers to private stack, data, and code areas. When running in Kernel Mode, the process addresses the kernel data and code areas and uses another private stack. NOTE: \u4e0d\u540c\u7684mode\uff0c\u4f7f\u7528\u4e0d\u540c\u7684address space Because the kernel is reentrant, several kernel control paths each related to a different process may be executed in turn. In this case, each kernel control path refers to its own private kernel stack . NOTE : \u5173\u4e8ekernel stack\u53c2\u89c13.2.2.1. Process descriptors handling While it appears to each process that it has access to a private address space , there are times when part of the address space is shared among processes. In some cases, this sharing is explicitly requested by processes; in others, it is done automatically by the kernel to reduce memory usage. If the same program, say an editor, is needed simultaneously by several users, the program is loaded into memory only once, and its instructions can be shared by all of the users who need it. Its data, of course, must not be shared, because each user will have separate data. This kind of shared address space is done automatically by the kernel to save memory. Processes also can share parts of their address space as a kind of interprocess communication , using the \"shared memory\" technique introduced in System V and supported by Linux. Finally, Linux supports the mmap( ) system call, which allows part of a file or the information stored on a block device to be mapped into a part of a process address space. Memory mapping can provide an alternative to normal reads and writes for transferring data. If the same file is shared by several processes, its memory mapping is included in the address space of each of the processes that share it.","title":"1.6.4. Process Address Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/","text":"1.6.5. Synchronization and Critical Regions # NOTE: \u867d\u7136\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662fkernel\u7684synchronization\uff0c\u4f46\u662f\u5176\u4e2d\u6240\u63cf\u8ff0\u7684\u65b9\u6cd5\u3001\u601d\u8def\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u3002 Implementing a reentrant kernel requires the use of synchronization . If a kernel control path is suspended while acting on a kernel data structure, no other kernel control path should be allowed to act on the same data structure unless it has been reset to a consistent state . Otherwise, the interaction of the two control paths could corrupt the stored information. For example, suppose a global variable V contains the number of available items of some system resource. The first kernel control path, A , reads the variable and determines that there is just one available item. At this point, another kernel control path, B , is activated and reads the same variable, which still contains the value 1 . Thus, B decreases V and starts using the resource item. Then A resumes the execution; because it has already read the value of V , it assumes that it can decrease V and take the resource item, which B already uses. As a final result, V contains -1, and two kernel control paths use the same resource item with potentially disastrous effects. When the outcome of a computation depends on how two or more processes are scheduled, the code is incorrect. We say that there is a race condition . In general, safe access to a global variable is ensured by using atomic operations . In the previous example, data corruption is not possible if the two control paths read and decrease V with a single, noninterruptible operation . However, kernels contain many data structures that cannot be accessed with a single operation. For example, it usually isn't possible to remove an element from a linked list with a single operation, because the kernel needs to access at least two pointers at once. Any section of code that should be finished by each process that begins it before another process can enter it is called a critical region . [*] [*] Synchronization problems have been fully described in other works; we refer the interested reader to books on the Unix operating systems (see the Bibliography). These problems occur not only among kernel control paths but also among processes sharing common data. Several synchronization techniques have been adopted. The following section concentrates on how to synchronize kernel control paths . 1.6.5.1. Kernel preemption disabling # To provide a drastically simple solution to synchronization problems, some traditional Unix kernels are nonpreemptive: when a process executes in Kernel Mode, it cannot be arbitrarily suspended and substituted with another process. Therefore, on a uniprocessor system, all kernel data structures that are not updated by interrupts or exception handlers are safe for the kernel to access. Of course, a process in Kernel Mode can voluntarily relinquish the CPU, but in this case, it must ensure that all data structures are left in a consistent state. Moreover, when it resumes its execution, it must recheck the value of any previously accessed data structures that could be changed. A synchronization mechanism applicable to preemptive kernels consists of disabling kernel preemption before entering a critical region and reenabling it right after leaving the region. Nonpreemptability is not enough for multiprocessor systems, because two kernel control paths running on different CPUs can concurrently access the same data structure. 1.6.5.2. Interrupt disabling # Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts before entering a critical region and reenabling them right after leaving it. This mechanism, while simple, is far from optimal. If the critical region is large, interrupts can remain disabled for a relatively long time, potentially causing all hardware activities to freeze. 1.6.5.3. Semaphores # A widely used mechanism, effective in both uniprocessor and multiprocessor systems, relies on the use of semaphores . A semaphore is simply a counter associated with a data structure; it is checked by all kernel threads before they try to access the data structure. Each semaphore may be viewed as an object composed of: An integer variable A list of waiting processes Two atomic methods: down( ) and up( ) The down( ) method decreases the value of the semaphore. If the new value is less than 0, the method adds the running process to the semaphore list and then blocks (i.e., invokes the scheduler ). The up( ) method increases the value of the semaphore and, if its new value is greater than or equal to 0, reactivates one or more processes in the semaphore list. Each data structure to be protected has its own semaphore, which is initialized to 1. When a kernel control path wishes to access the data structure, it executes the down( ) method on the proper semaphore. If the value of the new semaphore isn't negative, access to the data structure is granted. Otherwise, the process that is executing the kernel control path is added to the semaphore list and blocked. When another process executes the up( ) method on that semaphore, one of the processes in the semaphore list is allowed to proceed. 1.6.5.4. Spin locks # In multiprocessor systems, semaphores are not always the best solution to the synchronization problems. Some kernel data structures should be protected from being concurrently accessed by kernel control paths that run on different CPUs. In this case, if the time required to update the data structure is short, a semaphore could be very inefficient. To check a semaphore, the kernel must insert a process in the semaphore list and then suspend it. Because both operations are relatively expensive, in the time it takes to complete them, the other kernel control path could have already released the semaphore. In these cases, multiprocessor operating systems use spin locks . A spin lock is very similar to a semaphore, but it has no process list ; when a process finds the lock closed by another process, it \"spins\" around repeatedly, executing a tight instruction loop until the lock becomes open. Of course, spin locks are useless in a uniprocessor environment. When a kernel control path tries to access a locked data structure, it starts an endless loop. Therefore, the kernel control path that is updating the protected data structure would not have a chance to continue the execution and release the spin lock. The final result would be that the system hangs. 1.6.5.5. Avoiding deadlocks # Processes or kernel control paths that synchronize with other control paths may easily enter a deadlock state. The simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains access to b , but p1 then waits for b and p2 waits for a . Other more complex cyclic waits among groups of processes also may occur. Of course, a deadlock condition causes a complete freeze of the affected processes or kernel control paths. As far as kernel design is concerned, deadlocks become an issue when the number of kernel locks used is high. In this case, it may be quite difficult to ensure that no deadlock state will ever be reached for all possible ways to interleave kernel control paths. Several operating systems, including Linux, avoid this problem by requesting locks in a predefined order.","title":"1.6.5-Synchronization-and-Critical-Regions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#165-synchronization-and-critical-regions","text":"NOTE: \u867d\u7136\u672c\u8282\u6240\u63cf\u8ff0\u7684\u662fkernel\u7684synchronization\uff0c\u4f46\u662f\u5176\u4e2d\u6240\u63cf\u8ff0\u7684\u65b9\u6cd5\u3001\u601d\u8def\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u3002 Implementing a reentrant kernel requires the use of synchronization . If a kernel control path is suspended while acting on a kernel data structure, no other kernel control path should be allowed to act on the same data structure unless it has been reset to a consistent state . Otherwise, the interaction of the two control paths could corrupt the stored information. For example, suppose a global variable V contains the number of available items of some system resource. The first kernel control path, A , reads the variable and determines that there is just one available item. At this point, another kernel control path, B , is activated and reads the same variable, which still contains the value 1 . Thus, B decreases V and starts using the resource item. Then A resumes the execution; because it has already read the value of V , it assumes that it can decrease V and take the resource item, which B already uses. As a final result, V contains -1, and two kernel control paths use the same resource item with potentially disastrous effects. When the outcome of a computation depends on how two or more processes are scheduled, the code is incorrect. We say that there is a race condition . In general, safe access to a global variable is ensured by using atomic operations . In the previous example, data corruption is not possible if the two control paths read and decrease V with a single, noninterruptible operation . However, kernels contain many data structures that cannot be accessed with a single operation. For example, it usually isn't possible to remove an element from a linked list with a single operation, because the kernel needs to access at least two pointers at once. Any section of code that should be finished by each process that begins it before another process can enter it is called a critical region . [*] [*] Synchronization problems have been fully described in other works; we refer the interested reader to books on the Unix operating systems (see the Bibliography). These problems occur not only among kernel control paths but also among processes sharing common data. Several synchronization techniques have been adopted. The following section concentrates on how to synchronize kernel control paths .","title":"1.6.5. Synchronization and Critical Regions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1651-kernel-preemption-disabling","text":"To provide a drastically simple solution to synchronization problems, some traditional Unix kernels are nonpreemptive: when a process executes in Kernel Mode, it cannot be arbitrarily suspended and substituted with another process. Therefore, on a uniprocessor system, all kernel data structures that are not updated by interrupts or exception handlers are safe for the kernel to access. Of course, a process in Kernel Mode can voluntarily relinquish the CPU, but in this case, it must ensure that all data structures are left in a consistent state. Moreover, when it resumes its execution, it must recheck the value of any previously accessed data structures that could be changed. A synchronization mechanism applicable to preemptive kernels consists of disabling kernel preemption before entering a critical region and reenabling it right after leaving the region. Nonpreemptability is not enough for multiprocessor systems, because two kernel control paths running on different CPUs can concurrently access the same data structure.","title":"1.6.5.1. Kernel preemption disabling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1652-interrupt-disabling","text":"Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts before entering a critical region and reenabling them right after leaving it. This mechanism, while simple, is far from optimal. If the critical region is large, interrupts can remain disabled for a relatively long time, potentially causing all hardware activities to freeze.","title":"1.6.5.2. Interrupt disabling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1653-semaphores","text":"A widely used mechanism, effective in both uniprocessor and multiprocessor systems, relies on the use of semaphores . A semaphore is simply a counter associated with a data structure; it is checked by all kernel threads before they try to access the data structure. Each semaphore may be viewed as an object composed of: An integer variable A list of waiting processes Two atomic methods: down( ) and up( ) The down( ) method decreases the value of the semaphore. If the new value is less than 0, the method adds the running process to the semaphore list and then blocks (i.e., invokes the scheduler ). The up( ) method increases the value of the semaphore and, if its new value is greater than or equal to 0, reactivates one or more processes in the semaphore list. Each data structure to be protected has its own semaphore, which is initialized to 1. When a kernel control path wishes to access the data structure, it executes the down( ) method on the proper semaphore. If the value of the new semaphore isn't negative, access to the data structure is granted. Otherwise, the process that is executing the kernel control path is added to the semaphore list and blocked. When another process executes the up( ) method on that semaphore, one of the processes in the semaphore list is allowed to proceed.","title":"1.6.5.3. Semaphores"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1654-spin-locks","text":"In multiprocessor systems, semaphores are not always the best solution to the synchronization problems. Some kernel data structures should be protected from being concurrently accessed by kernel control paths that run on different CPUs. In this case, if the time required to update the data structure is short, a semaphore could be very inefficient. To check a semaphore, the kernel must insert a process in the semaphore list and then suspend it. Because both operations are relatively expensive, in the time it takes to complete them, the other kernel control path could have already released the semaphore. In these cases, multiprocessor operating systems use spin locks . A spin lock is very similar to a semaphore, but it has no process list ; when a process finds the lock closed by another process, it \"spins\" around repeatedly, executing a tight instruction loop until the lock becomes open. Of course, spin locks are useless in a uniprocessor environment. When a kernel control path tries to access a locked data structure, it starts an endless loop. Therefore, the kernel control path that is updating the protected data structure would not have a chance to continue the execution and release the spin lock. The final result would be that the system hangs.","title":"1.6.5.4. Spin locks"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.5-Synchronization-and-Critical-Regions/#1655-avoiding-deadlocks","text":"Processes or kernel control paths that synchronize with other control paths may easily enter a deadlock state. The simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains access to b , but p1 then waits for b and p2 waits for a . Other more complex cyclic waits among groups of processes also may occur. Of course, a deadlock condition causes a complete freeze of the affected processes or kernel control paths. As far as kernel design is concerned, deadlocks become an issue when the number of kernel locks used is high. In this case, it may be quite difficult to ensure that no deadlock state will ever be reached for all possible ways to interleave kernel control paths. Several operating systems, including Linux, avoid this problem by requesting locks in a predefined order.","title":"1.6.5.5. Avoiding deadlocks"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.6-Signals-and-Interprocess-Communication/","text":"1.6.6. Signals and Interprocess Communication # Unix signals provide a mechanism for notifying processes of system events. Each event has its own signal number, which is usually referred to by a symbolic constant such as SIGTERM . There are two kinds of system events: Asynchronous notifications For instance, a user can send the interrupt signal SIGINT to a foreground process by pressing the interrupt keycode (usually Ctrl-C) at the terminal. Synchronous notifications For instance, the kernel sends the signal SIGSEGV to a process when it accesses a memory location at an invalid address.","title":"1.6.6-Signals-and-Interprocess-Communication"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.6-Signals-and-Interprocess-Communication/#166-signals-and-interprocess-communication","text":"Unix signals provide a mechanism for notifying processes of system events. Each event has its own signal number, which is usually referred to by a symbolic constant such as SIGTERM . There are two kinds of system events: Asynchronous notifications For instance, a user can send the interrupt signal SIGINT to a foreground process by pressing the interrupt keycode (usually Ctrl-C) at the terminal. Synchronous notifications For instance, the kernel sends the signal SIGSEGV to a process when it accesses a memory location at an invalid address.","title":"1.6.6. Signals and Interprocess Communication"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/","text":"1.6.7. Process Management # Unix makes a neat distinction between the process and the program it is executing. To that end, the fork( ) and _exit( ) system calls are used respectively to create a new process and to terminate it, while an exec( ) -like system call is invoked to load a new program. After such a system call is executed, the process resumes execution with a brand new address space containing the loaded program. The process that invokes a fork( ) is the parent , while the new process is its child . Parents and children can find one another because the data structure describing each process includes a pointer to its immediate parent and pointers to all its immediate children. A naive implementation of the fork( ) would require both the parent's data and the parent's code to be duplicated and the copies assigned to the child. This would be quite time consuming. Current kernels that can rely on hardware paging units follow the Copy-On-Write approach, which defers page duplication until the last moment (i.e., until the parent or the child is required to write into a page). We shall describe how Linux implements this technique in the section \"Copy On Write\" in Chapter 9. The _exit( ) system call terminates a process. The kernel handles this system call by releasing the resources owned by the process and sending the parent process a SIGCHLD signal, which is ignored by default. 1.6.7.1. Zombie processes # How can a parent process inquire about termination of its children? The wait4( ) system call allows a process to wait until one of its children terminates; it returns the process ID (PID) of the terminated child. When executing this system call, the kernel checks whether a child has already terminated. A special zombie process state is introduced to represent terminated processes: a process remains in that state until its parent process executes a wait4( ) system call on it. The system call handler extracts data about resource usage from the process descriptor fields; the process descriptor may be released once the data is collected. If no child process has already terminated when the wait4( ) system call is executed, the kernel usually puts the process in a wait state until a child terminates. Many kernels also implement a waitpid( ) system call, which allows a process to wait for a specific child process. Other variants of wait4( ) system calls are also quite common. It's good practice for the kernel to keep around information on a child process until the parent issues its wait4( ) call, but suppose the parent process terminates without issuing that call? The information takes up valuable memory slots that could be used to serve living processes. For example, many shells allow the user to start a command in the background and then log out. The process that is running the command shell terminates, but its children continue their execution. The solution lies in a special system process called init , which is created during system initialization. When a process terminates, the kernel changes the appropriate process descriptor pointers of all the existing children of the terminated process to make them become children of init . This process monitors the execution of all its children and routinely issues wait4( ) system calls, whose side effect is to get rid of all orphaned zombies. 1.6.7.2. Process groups and login sessions # Modern Unix operating systems introduce the notion of process groups to represent a \"job\" abstraction. For example, in order to execute the command line: $ ls | sort | more a shell that supports process groups , such as bash , creates a new group for the three processes corresponding to ls , sort , and more . In this way, the shell acts on the three processes as if they were a single entity (the job , to be precise). Each process descriptor includes a field containing the process group ID . Each group of processes may have a group leader , which is the process whose PID coincides with the process group ID . A newly created process is initially inserted into the process group of its parent. Modern Unix kernels also introduce login sessions . Informally, a login session contains all processes that are descendants of the process that has started a working session on a specific terminal usually, the first command shell process created for the user. All processes in a process group must be in the same login session . A login session may have several process groups active simultaneously; one of these process groups is always in the foreground, which means that it has access to the terminal. The other active process groups are in the background. When a background process tries to access the terminal, it receives a SIGTTIN or SIGTTOUT signal. In many command shells, the internal commands bg and fg can be used to put a process group in either the background or the foreground.","title":"1.6.7-Process-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/#167-process-management","text":"Unix makes a neat distinction between the process and the program it is executing. To that end, the fork( ) and _exit( ) system calls are used respectively to create a new process and to terminate it, while an exec( ) -like system call is invoked to load a new program. After such a system call is executed, the process resumes execution with a brand new address space containing the loaded program. The process that invokes a fork( ) is the parent , while the new process is its child . Parents and children can find one another because the data structure describing each process includes a pointer to its immediate parent and pointers to all its immediate children. A naive implementation of the fork( ) would require both the parent's data and the parent's code to be duplicated and the copies assigned to the child. This would be quite time consuming. Current kernels that can rely on hardware paging units follow the Copy-On-Write approach, which defers page duplication until the last moment (i.e., until the parent or the child is required to write into a page). We shall describe how Linux implements this technique in the section \"Copy On Write\" in Chapter 9. The _exit( ) system call terminates a process. The kernel handles this system call by releasing the resources owned by the process and sending the parent process a SIGCHLD signal, which is ignored by default.","title":"1.6.7. Process Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/#1671-zombie-processes","text":"How can a parent process inquire about termination of its children? The wait4( ) system call allows a process to wait until one of its children terminates; it returns the process ID (PID) of the terminated child. When executing this system call, the kernel checks whether a child has already terminated. A special zombie process state is introduced to represent terminated processes: a process remains in that state until its parent process executes a wait4( ) system call on it. The system call handler extracts data about resource usage from the process descriptor fields; the process descriptor may be released once the data is collected. If no child process has already terminated when the wait4( ) system call is executed, the kernel usually puts the process in a wait state until a child terminates. Many kernels also implement a waitpid( ) system call, which allows a process to wait for a specific child process. Other variants of wait4( ) system calls are also quite common. It's good practice for the kernel to keep around information on a child process until the parent issues its wait4( ) call, but suppose the parent process terminates without issuing that call? The information takes up valuable memory slots that could be used to serve living processes. For example, many shells allow the user to start a command in the background and then log out. The process that is running the command shell terminates, but its children continue their execution. The solution lies in a special system process called init , which is created during system initialization. When a process terminates, the kernel changes the appropriate process descriptor pointers of all the existing children of the terminated process to make them become children of init . This process monitors the execution of all its children and routinely issues wait4( ) system calls, whose side effect is to get rid of all orphaned zombies.","title":"1.6.7.1. Zombie processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.7-Process-Management/#1672-process-groups-and-login-sessions","text":"Modern Unix operating systems introduce the notion of process groups to represent a \"job\" abstraction. For example, in order to execute the command line: $ ls | sort | more a shell that supports process groups , such as bash , creates a new group for the three processes corresponding to ls , sort , and more . In this way, the shell acts on the three processes as if they were a single entity (the job , to be precise). Each process descriptor includes a field containing the process group ID . Each group of processes may have a group leader , which is the process whose PID coincides with the process group ID . A newly created process is initially inserted into the process group of its parent. Modern Unix kernels also introduce login sessions . Informally, a login session contains all processes that are descendants of the process that has started a working session on a specific terminal usually, the first command shell process created for the user. All processes in a process group must be in the same login session . A login session may have several process groups active simultaneously; one of these process groups is always in the foreground, which means that it has access to the terminal. The other active process groups are in the background. When a background process tries to access the terminal, it receives a SIGTTIN or SIGTTOUT signal. In many command shells, the internal commands bg and fg can be used to put a process group in either the background or the foreground.","title":"1.6.7.2. Process groups and login sessions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/","text":"1.6.8. Memory Management # Memory management is by far the most complex activity in a Unix kernel. More than a third of this book is dedicated just to describing how Linux handles memory management. This section illustrates some of the main issues related to memory management. 1.6.8.1. Virtual memory # All recent Unix systems provide a useful abstraction called virtual memory . Virtual memory acts as a logical layer between the application memory requests and the hardware Memory Management Unit (MMU). Virtual memory has many purposes and advantages: Several processes can be executed concurrently. It is possible to run applications whose memory needs are larger than the available physical memory. Processes can execute a program whose code is only partially loaded in memory. Each process is allowed to access a subset of the available physical memory. Processes can share a single memory image of a library or program. Programs can be relocatable that is, they can be placed anywhere in physical memory. The main ingredient of a virtual memory subsystem is the notion of virtual address space . The set of memory references that a process can use is different from physical memory addresses. When a process uses a virtual address , [*] the kernel and the MMU cooperate to find the actual physical location of the requested memory item. [*] These addresses have different nomenclatures, depending on the computer architecture. As we'll see in Chapter 2, Intel manuals refer to them as \"logical addresses.\" Today's CPUs include hardware circuits that automatically translate the virtual addresses into physical ones. To that end, the available RAM is partitioned into page frames typically 4 or 8 KB in length and a set of Page Tables is introduced to specify how virtual addresses correspond to physical addresses . These circuits make memory allocation simpler, because a request for a block of contiguous virtual addresses can be satisfied by allocating a group of page frames having noncontiguous physical addresses. NOTE: \u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u5bf9\u5982\u4e0b\u7ae0\u8282\u7684\u5185\u5bb9\u7684\u6982\u62ec\uff1a chapter 2.1. Memory Addresses chapter 2.4. Paging in Hardware chapter 2.5. Paging in Linux 1.6.8.2. Random access memory usage # All Unix operating systems clearly distinguish between two portions of the random access memory (RAM). A few megabytes are dedicated to storing the kernel image (i.e., the kernel code and the kernel static data structures). The remaining portion of RAM is usually handled by the virtual memory system and is used in three possible ways: To satisfy kernel requests for buffers, descriptors, and other dynamic kernel data structures To satisfy process requests for generic memory areas and for memory mapping of files To get better performance from disks and other buffered devices by means of caches NOTE: \u5173\u4e8eRAM\u7684usage\uff0c\u5728Chapter 8. Memory Management\u6709\u7c7b\u4f3c\u4e0a\u9762\u8fd9\u6bb5\u7684\u63cf\u8ff0\u3002 Each request type is valuable. On the other hand, because the available RAM is limited, some balancing among request types must be done, particularly when little available memory is left. Moreover, when some critical threshold of available memory is reached and a page-frame-reclaiming algorithm is invoked to free additional memory, which are the page frames most suitable for reclaiming? As we will see in Chapter 17, there is no simple answer to this question and very little support from theory. The only available solution lies in developing carefully tuned empirical algorithms. One major problem that must be solved by the virtual memory system is memory fragmentation . Ideally, a memory request should fail only when the number of free page frames is too small. However, the kernel is often forced to use physically contiguous memory areas. Hence the memory request could fail even if there is enough memory available, but it is not available as one contiguous chunk. 1.6.8.3. Kernel Memory Allocator # The Kernel Memory Allocator (KMA) is a subsystem that tries to satisfy the requests for memory areas from all parts of the system. Some of these requests come from other kernel subsystems needing memory for kernel use, and some requests come via system calls from user programs to increase their processes' address spaces. A good KMA should have the following features: It must be fast. Actually, this is the most crucial attribute, because it is invoked by all kernel subsystems (including the interrupt handlers). It should minimize the amount of wasted memory. It should try to reduce the memory fragmentation problem. It should be able to cooperate with the other memory management subsystems to borrow and release page frames from them. Several proposed KMAs, which are based on a variety of different algorithmic techniques, include: Resource map allocator Power-of-two free lists McKusick-Karels allocator Buddy system Mach's Zone allocator Dynix allocator Solaris 's Slab allocator As we will see in Chapter 8, Linux's KMA uses a Slab allocator on top of a buddy system. NOTE: Memory Allocation Guide 1.6.8.4. Process virtual address space handling # The address space of a process contains all the virtual memory addresses that the process is allowed to reference. The kernel usually stores a process virtual address space as a list of memory area descriptors . For example, when a process starts the execution of some program via an exec( ) -like system call, the kernel assigns to the process a virtual address space that comprises memory areas for: The executable code of the program The initialized data of the program The uninitialized data of the program The initial program stack (i.e., the User Mode stack) The executable code and data of needed shared libraries The heap (the memory dynamically requested by the program) All recent Unix operating systems adopt a memory allocation strategy called demand paging . With demand paging, a process can start program execution with none of its pages in physical memory. As it accesses a nonpresent page, the MMU generates an exception; the exception handler finds the affected memory region, allocates a free page, and initializes it with the appropriate data. In a similar fashion, when the process dynamically requires memory by using malloc( ) , or the brk( ) system call (which is invoked internally by malloc( ) ), the kernel just updates the size of the heap memory region of the process. A page frame is assigned to the process only when it generates an exception by trying to refer its virtual memory addresses. Virtual address spaces also allow other efficient strategies, such as the Copy On Write strategy mentioned earlier. For example, when a new process is created, the kernel just assigns the parent's page frames to the child address space, but marks them read-only. An exception is raised as soon the parent or the child tries to modify the contents of a page. The exception handler assigns a new page frame to the affected process and initializes it with the contents of the original page. 1.6.8.5. Caching # A good part of the available physical memory is used as cache for hard disks and other block devices. This is because hard drives are very slow: a disk access requires several milliseconds, which is a very long time compared with the RAM access time. Therefore, disks are often the bottleneck in system performance. As a general rule, one of the policies already implemented in the earliest Unix system is to defer writing to disk as long as possible. As a result, data read previously from disk and no longer used by any process continue to stay in RAM. This strategy is based on the fact that there is a good chance that new processes will require data read from or written to disk by processes that no longer exist. When a process asks to access a disk, the kernel checks first whether the required data are in the cache. Each time this happens (a cache hit), the kernel is able to service the process request without accessing the disk. The sync( ) system call forces disk synchronization by writing all of the \"dirty\" buffers (i.e., all the buffers whose contents differ from that of the corresponding disk blocks) into disk. To avoid data loss, all operating systems take care to periodically write dirty buffers back to disk.","title":"1.6.8-Memory-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#168-memory-management","text":"Memory management is by far the most complex activity in a Unix kernel. More than a third of this book is dedicated just to describing how Linux handles memory management. This section illustrates some of the main issues related to memory management.","title":"1.6.8. Memory Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1681-virtual-memory","text":"All recent Unix systems provide a useful abstraction called virtual memory . Virtual memory acts as a logical layer between the application memory requests and the hardware Memory Management Unit (MMU). Virtual memory has many purposes and advantages: Several processes can be executed concurrently. It is possible to run applications whose memory needs are larger than the available physical memory. Processes can execute a program whose code is only partially loaded in memory. Each process is allowed to access a subset of the available physical memory. Processes can share a single memory image of a library or program. Programs can be relocatable that is, they can be placed anywhere in physical memory. The main ingredient of a virtual memory subsystem is the notion of virtual address space . The set of memory references that a process can use is different from physical memory addresses. When a process uses a virtual address , [*] the kernel and the MMU cooperate to find the actual physical location of the requested memory item. [*] These addresses have different nomenclatures, depending on the computer architecture. As we'll see in Chapter 2, Intel manuals refer to them as \"logical addresses.\" Today's CPUs include hardware circuits that automatically translate the virtual addresses into physical ones. To that end, the available RAM is partitioned into page frames typically 4 or 8 KB in length and a set of Page Tables is introduced to specify how virtual addresses correspond to physical addresses . These circuits make memory allocation simpler, because a request for a block of contiguous virtual addresses can be satisfied by allocating a group of page frames having noncontiguous physical addresses. NOTE: \u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u5bf9\u5982\u4e0b\u7ae0\u8282\u7684\u5185\u5bb9\u7684\u6982\u62ec\uff1a chapter 2.1. Memory Addresses chapter 2.4. Paging in Hardware chapter 2.5. Paging in Linux","title":"1.6.8.1. Virtual memory"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1682-random-access-memory-usage","text":"All Unix operating systems clearly distinguish between two portions of the random access memory (RAM). A few megabytes are dedicated to storing the kernel image (i.e., the kernel code and the kernel static data structures). The remaining portion of RAM is usually handled by the virtual memory system and is used in three possible ways: To satisfy kernel requests for buffers, descriptors, and other dynamic kernel data structures To satisfy process requests for generic memory areas and for memory mapping of files To get better performance from disks and other buffered devices by means of caches NOTE: \u5173\u4e8eRAM\u7684usage\uff0c\u5728Chapter 8. Memory Management\u6709\u7c7b\u4f3c\u4e0a\u9762\u8fd9\u6bb5\u7684\u63cf\u8ff0\u3002 Each request type is valuable. On the other hand, because the available RAM is limited, some balancing among request types must be done, particularly when little available memory is left. Moreover, when some critical threshold of available memory is reached and a page-frame-reclaiming algorithm is invoked to free additional memory, which are the page frames most suitable for reclaiming? As we will see in Chapter 17, there is no simple answer to this question and very little support from theory. The only available solution lies in developing carefully tuned empirical algorithms. One major problem that must be solved by the virtual memory system is memory fragmentation . Ideally, a memory request should fail only when the number of free page frames is too small. However, the kernel is often forced to use physically contiguous memory areas. Hence the memory request could fail even if there is enough memory available, but it is not available as one contiguous chunk.","title":"1.6.8.2. Random access memory usage"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1683-kernel-memory-allocator","text":"The Kernel Memory Allocator (KMA) is a subsystem that tries to satisfy the requests for memory areas from all parts of the system. Some of these requests come from other kernel subsystems needing memory for kernel use, and some requests come via system calls from user programs to increase their processes' address spaces. A good KMA should have the following features: It must be fast. Actually, this is the most crucial attribute, because it is invoked by all kernel subsystems (including the interrupt handlers). It should minimize the amount of wasted memory. It should try to reduce the memory fragmentation problem. It should be able to cooperate with the other memory management subsystems to borrow and release page frames from them. Several proposed KMAs, which are based on a variety of different algorithmic techniques, include: Resource map allocator Power-of-two free lists McKusick-Karels allocator Buddy system Mach's Zone allocator Dynix allocator Solaris 's Slab allocator As we will see in Chapter 8, Linux's KMA uses a Slab allocator on top of a buddy system. NOTE: Memory Allocation Guide","title":"1.6.8.3. Kernel Memory Allocator"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1684-process-virtual-address-space-handling","text":"The address space of a process contains all the virtual memory addresses that the process is allowed to reference. The kernel usually stores a process virtual address space as a list of memory area descriptors . For example, when a process starts the execution of some program via an exec( ) -like system call, the kernel assigns to the process a virtual address space that comprises memory areas for: The executable code of the program The initialized data of the program The uninitialized data of the program The initial program stack (i.e., the User Mode stack) The executable code and data of needed shared libraries The heap (the memory dynamically requested by the program) All recent Unix operating systems adopt a memory allocation strategy called demand paging . With demand paging, a process can start program execution with none of its pages in physical memory. As it accesses a nonpresent page, the MMU generates an exception; the exception handler finds the affected memory region, allocates a free page, and initializes it with the appropriate data. In a similar fashion, when the process dynamically requires memory by using malloc( ) , or the brk( ) system call (which is invoked internally by malloc( ) ), the kernel just updates the size of the heap memory region of the process. A page frame is assigned to the process only when it generates an exception by trying to refer its virtual memory addresses. Virtual address spaces also allow other efficient strategies, such as the Copy On Write strategy mentioned earlier. For example, when a new process is created, the kernel just assigns the parent's page frames to the child address space, but marks them read-only. An exception is raised as soon the parent or the child tries to modify the contents of a page. The exception handler assigns a new page frame to the affected process and initializes it with the contents of the original page.","title":"1.6.8.4. Process virtual address space handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.8-Memory-Management/#1685-caching","text":"A good part of the available physical memory is used as cache for hard disks and other block devices. This is because hard drives are very slow: a disk access requires several milliseconds, which is a very long time compared with the RAM access time. Therefore, disks are often the bottleneck in system performance. As a general rule, one of the policies already implemented in the earliest Unix system is to defer writing to disk as long as possible. As a result, data read previously from disk and no longer used by any process continue to stay in RAM. This strategy is based on the fact that there is a good chance that new processes will require data read from or written to disk by processes that no longer exist. When a process asks to access a disk, the kernel checks first whether the required data are in the cache. Each time this happens (a cache hit), the kernel is able to service the process request without accessing the disk. The sync( ) system call forces disk synchronization by writing all of the \"dirty\" buffers (i.e., all the buffers whose contents differ from that of the corresponding disk blocks) into disk. To avoid data loss, all operating systems take care to periodically write dirty buffers back to disk.","title":"1.6.8.5. Caching"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.9-Device-Drivers/","text":"1.6.9. Device Drivers # The kernel interacts with I/O devices by means of device drivers . Device drivers are included in the kernel and consist of data structures and functions that control one or more devices, such as hard disks, keyboards, mouses, monitors, network interfaces, and devices connected to an SCSI bus. Each driver interacts with the remaining part of the kernel (even with other drivers) through a specific interface. This approach has the following advantages: Device-specific code can be encapsulated in a specific module. Vendors can add new devices without knowing the kernel source code; only the interface specifications must be known. The kernel deals with all devices in a uniform way and accesses them through the same interface. It is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring the system to be rebooted. It is also possible to dynamically unload a module that is no longer needed, therefore minimizing the size of the kernel image stored in RAM. Figure 1-4 illustrates how device drivers interface with the rest of the kernel and with the processes. Figure 1-4. Device driver interface Some user programs (P) wish to operate on hardware devices. They make requests to the kernel using the usual file-related system calls and the device files normally found in the /dev directory. Actually, the device files are the user-visible portion of the device driver interface. Each device file refers to a specific device driver, which is invoked by the kernel to perform the requested operation on the hardware component. At the time Unix was introduced, graphical terminals were uncommon and expensive, so only alphanumeric terminals were handled directly by Unix kernels. When graphical terminals became widespread, ad hoc applications such as the X Window System were introduced that ran as standard processes and accessed the I/O ports of the graphics interface and the RAM video area directly. Some recent Unix kernels, such as Linux 2.6, provide an abstraction for the frame buffer of the graphic card and allow application software to access them without needing to know anything about the I/O ports of the graphics interface (see the section \"Levels of Kernel Support\" in Chapter 13.)","title":"1.6.9-Device-Drivers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/1.6.9-Device-Drivers/#169-device-drivers","text":"The kernel interacts with I/O devices by means of device drivers . Device drivers are included in the kernel and consist of data structures and functions that control one or more devices, such as hard disks, keyboards, mouses, monitors, network interfaces, and devices connected to an SCSI bus. Each driver interacts with the remaining part of the kernel (even with other drivers) through a specific interface. This approach has the following advantages: Device-specific code can be encapsulated in a specific module. Vendors can add new devices without knowing the kernel source code; only the interface specifications must be known. The kernel deals with all devices in a uniform way and accesses them through the same interface. It is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring the system to be rebooted. It is also possible to dynamically unload a module that is no longer needed, therefore minimizing the size of the kernel image stored in RAM. Figure 1-4 illustrates how device drivers interface with the rest of the kernel and with the processes. Figure 1-4. Device driver interface Some user programs (P) wish to operate on hardware devices. They make requests to the kernel using the usual file-related system calls and the device files normally found in the /dev directory. Actually, the device files are the user-visible portion of the device driver interface. Each device file refers to a specific device driver, which is invoked by the kernel to perform the requested operation on the hardware component. At the time Unix was introduced, graphical terminals were uncommon and expensive, so only alphanumeric terminals were handled directly by Unix kernels. When graphical terminals became widespread, ad hoc applications such as the X Window System were introduced that ran as standard processes and accessed the I/O ports of the graphics interface and the RAM video area directly. Some recent Unix kernels, such as Linux 2.6, provide an abstraction for the frame buffer of the graphic card and allow application software to access them without needing to know anything about the I/O ports of the graphics interface (see the section \"Levels of Kernel Support\" in Chapter 13.)","title":"1.6.9. Device Drivers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Chapter-1-Introduction/","text":"Chapter 1. Introduction # NOTE: \u5173\u4e8e\u66f4\u52a0\u8be6\u7ec6\u7684linux\u7684\u4ecb\u7ecd\uff0c\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a Linux Linux kernel","title":"Chapter-1-Introduction"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Chapter-1-Introduction/#chapter-1-introduction","text":"NOTE: \u5173\u4e8e\u66f4\u52a0\u8be6\u7ec6\u7684linux\u7684\u4ecb\u7ecd\uff0c\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a Linux Linux kernel","title":"Chapter 1. Introduction"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Giant-lock/","text":"Giant lock Linux Giant lock # In operating systems , a giant lock , also known as a big-lock or kernel-lock , is a lock that may be used in the kernel to provide concurrency control required by symmetric multiprocessing (SMP) systems. A giant lock is a solitary global lock that is held whenever a thread enters kernel space and released when the thread returns to user space ; a system call is the archetypal example. In this model, threads in user space can run concurrently on any available processors or processor cores , but no more than one thread can run in kernel space; any other threads that try to enter kernel space are forced to wait. In other words, the giant lock eliminates all concurrency in kernel space. By isolating the kernel from concurrency, many parts of the kernel no longer need to be modified to support SMP. However, as in giant-lock SMP systems only one processor can run the kernel code at a time, performance for applications spending significant amounts of time in the kernel is not much improved. Accordingly, the giant-lock approach is commonly seen as a preliminary means of bringing SMP support to an operating system, yielding benefits only in user space. Most modern operating systems use a fine-grained locking approach. Linux # The Linux kernel had a big kernel lock (BKL) since the introduction of SMP, until Arnd Bergmann removed it in 2011 in kernel version 2.6.39, with the remaining uses of the big lock removed or replaced by finer-grained locking. Linux distributions at or above CentOS 7 , Debian 7 (Wheezy) and Ubuntu 11.10 are therefore not using BKL.","title":"Giant-lock"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Giant-lock/#giant-lock","text":"In operating systems , a giant lock , also known as a big-lock or kernel-lock , is a lock that may be used in the kernel to provide concurrency control required by symmetric multiprocessing (SMP) systems. A giant lock is a solitary global lock that is held whenever a thread enters kernel space and released when the thread returns to user space ; a system call is the archetypal example. In this model, threads in user space can run concurrently on any available processors or processor cores , but no more than one thread can run in kernel space; any other threads that try to enter kernel space are forced to wait. In other words, the giant lock eliminates all concurrency in kernel space. By isolating the kernel from concurrency, many parts of the kernel no longer need to be modified to support SMP. However, as in giant-lock SMP systems only one processor can run the kernel code at a time, performance for applications spending significant amounts of time in the kernel is not much improved. Accordingly, the giant-lock approach is commonly seen as a preliminary means of bringing SMP support to an operating system, yielding benefits only in user space. Most modern operating systems use a fine-grained locking approach.","title":"Giant lock"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-1-Introduction/Giant-lock/#linux","text":"The Linux kernel had a big kernel lock (BKL) since the introduction of SMP, until Arnd Bergmann removed it in 2011 in kernel version 2.6.39, with the remaining uses of the big lock removed or replaced by finer-grained locking. Linux distributions at or above CentOS 7 , Debian 7 (Wheezy) and Ubuntu 11.10 are therefore not using BKL.","title":"Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/","text":"2.1. Memory Addresses Logical address Linear address (also known as virtual address) Physical address 2.1. Memory Addresses # Programmers casually refer to a memory address as the way to access the contents of a memory cell. But when dealing with 80 x 86 microprocessors, we have to distinguish three kinds of addresses: Logical address # Included in the machine language instructions to specify the address of an operand or of an instruction. This type of address embodies the well-known 80 x 86 segmented architecture that forces MS-DOS and Windows programmers to divide their programs into segments . Each logical address consists of a segment and an offset (or displacement) that denotes the distance from the start of the segment to the actual address. Linear address (also known as virtual address ) # A single 32-bit unsigned integer that can be used to address up to 4 GB that is, up to 4,294,967,296 memory cells. Linear addresses are usually represented in hexadecimal notation; their values range from 0x00000000 to 0xffffffff . NOTE: \u6bcf\u4e2aprocess\u90fd\u4e00\u4e2a\u72ec\u7acb\u7684 Virtual address space \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u5426\u53ef\u4ee5\u63a8\u65ad\u51fa\u6bcf\u4e2aprocess\u7684virtual address space\u7684address value range\u662f\u5426\u4e00\u81f4\uff1b\u53c2\u89c1\u8fd9\u7bc7\u6587\u7ae0 In virtual memory, can two different processes have the same address? Physical address # Used to address memory cells in memory chips. They correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus . Physical addresses are represented as 32-bit or 36-bit unsigned integers. The Memory Management Unit ( MMU ) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit ; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). In multiprocessor systems, all CPUs usually share the same memory; this means that RAM chips may be accessed concurrently by independent CPUs. Because read or write operations on a RAM chip must be performed serially, a hardware circuit called a memory arbiter is inserted between the bus and every RAM chip. Its role is to grant access to a CPU if the chip is free and to delay it if the chip is busy servicing a request by another processor. Even uniprocessor systems use memory arbiters , because they include specialized processors called DMA controllers that operate concurrently with the CPU (see the section \"Direct Memory Access (DMA)\" in Chapter 13). In the case of multiprocessor systems, the structure of the arbiter is more complex because it has more input ports. The dual Pentium, for instance, maintains a two-port arbiter at each chip entrance and requires that the two CPUs exchange synchronization messages before attempting to use the common bus. From the programming point of view, the arbiter is hidden because it is managed by hardware circuits.","title":"2.1-Memory-Addresses"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#21-memory-addresses","text":"Programmers casually refer to a memory address as the way to access the contents of a memory cell. But when dealing with 80 x 86 microprocessors, we have to distinguish three kinds of addresses:","title":"2.1. Memory Addresses"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#logical-address","text":"Included in the machine language instructions to specify the address of an operand or of an instruction. This type of address embodies the well-known 80 x 86 segmented architecture that forces MS-DOS and Windows programmers to divide their programs into segments . Each logical address consists of a segment and an offset (or displacement) that denotes the distance from the start of the segment to the actual address.","title":"Logical address"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#linear-address-also-known-as-virtual-address","text":"A single 32-bit unsigned integer that can be used to address up to 4 GB that is, up to 4,294,967,296 memory cells. Linear addresses are usually represented in hexadecimal notation; their values range from 0x00000000 to 0xffffffff . NOTE: \u6bcf\u4e2aprocess\u90fd\u4e00\u4e2a\u72ec\u7acb\u7684 Virtual address space \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u5426\u53ef\u4ee5\u63a8\u65ad\u51fa\u6bcf\u4e2aprocess\u7684virtual address space\u7684address value range\u662f\u5426\u4e00\u81f4\uff1b\u53c2\u89c1\u8fd9\u7bc7\u6587\u7ae0 In virtual memory, can two different processes have the same address?","title":"Linear address (also known as virtual address)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.1-Memory-Addresses/#physical-address","text":"Used to address memory cells in memory chips. They correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus . Physical addresses are represented as 32-bit or 36-bit unsigned integers. The Memory Management Unit ( MMU ) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit ; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). In multiprocessor systems, all CPUs usually share the same memory; this means that RAM chips may be accessed concurrently by independent CPUs. Because read or write operations on a RAM chip must be performed serially, a hardware circuit called a memory arbiter is inserted between the bus and every RAM chip. Its role is to grant access to a CPU if the chip is free and to delay it if the chip is busy servicing a request by another processor. Even uniprocessor systems use memory arbiters , because they include specialized processors called DMA controllers that operate concurrently with the CPU (see the section \"Direct Memory Access (DMA)\" in Chapter 13). In the case of multiprocessor systems, the structure of the arbiter is more complex because it has more input ports. The dual Pentium, for instance, maintains a two-port arbiter at each chip entrance and requires that the two CPUs exchange synchronization messages before attempting to use the common bus. From the programming point of view, the arbiter is hidden because it is managed by hardware circuits.","title":"Physical address"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/","text":"2.2. Segmentation in Hardware 2.2.1. Segment Selectors and Segmentation Registers 2.2.2. Segment Descriptors 2.2.3. Fast Access to Segment Descriptors 2.2.4. Segmentation Unit 2.2. Segmentation in Hardware # Starting with the 80286 model, Intel microprocessors perform address translation in two different ways called real mode and protected mode . We'll focus in the next sections on address translation when protected mode is enabled. Real mode exists mostly to maintain processor compatibility with older models and to allow the operating system to bootstrap (see Appendix A for a short description of real mode). 2.2.1. Segment Selectors and Segmentation Registers # A logical address consists of two parts: a segment identifier and an offset that specifies the relative address within the segment. The segment identifier is a 16-bit field called the Segment Selector (see Figure 2-2), while the offset is a 32-bit field. We'll describe the fields of Segment Selectors in the section \"Fast Access to Segment Descriptors\" later in this chapter. SUMMARY : \u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5730\u5740\u7684\u957f\u5ea6\u662f\uff1a16 + 32 = 48 SUMMARY : \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cSegment Selector\u9664\u4e86\u5305\u542b\u6709table index\u4e4b\u5916\uff0c\u8fd8\u5305\u542b\u6709\u5176\u4ed6\u7684\u4fe1\u606f\uff1b To make it easy to retrieve segment selectors quickly, the processor provides segmentation registers whose only purpose is to hold Segment Selectors; these registers are called cs , ss , ds , es , fs , and gs . Although there are only six of them, a program can reuse the same segmentation register for different purposes by saving its content in memory and then restoring it later. Three of the six segmentation registers have specific purposes: cs The code segment register, which points to a segment containing program instructions ss The stack segment register, which points to a segment containing the current program stack ds The data segment register, which points to a segment containing global and static data The remaining three segmentation registers are general purpose and may refer to arbitrary data segments. The cs register has another important function: it includes a 2-bit field that specifies the Current Privilege Level (CPL) of the CPU. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called Kernel Mode and User Mode . 2.2.2. Segment Descriptors # Each segment is represented by an 8-byte Segment Descriptor that describes the segment characteristics. Segment Descriptors are stored either in the Global Descriptor Table ( GDT ) or in the Local Descriptor Table( LDT ) . Usually only one GDT is defined, while each process is permitted to have its own LDT if it needs to create additional segments besides those stored in the GDT . The address and size of the GDT in main memory are contained in the gdtr control register, while the address and size of the currently used LDT are contained in the ldtr control register. Figure 2-3 illustrates the format of a Segment Descriptor ; the meaning of the various fields is explained in Table 2-1. Table 2-1. Segment Descriptor fields Field name Description Base Contains the linear address of the first byte of the segment. G Granularity flag : if it is cleared (equal to 0), the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes. Limit Holds the offset of the last memory cell in the segment, thus binding the segment length. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB. S System flag : if it is cleared, the segment is a system segment that stores critical data structures such as the Local Descriptor Table ; otherwise, it is a normal code or data segment. Type Characterizes the segment type and its access rights (see the text that follows this table). DPL Descriptor Privilege Level: used to restrict accesses to the segment. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 that is, in Kernel Mode while a segment with its DPL set to 3 is accessible with every CPL value. P Segment-Present flag : is equal to 0 if the segment is not stored currently in main memory. Linux always sets this flag (bit 47) to 1, because it never swaps out whole segments to disk. There are several types of segments, and thus several types of Segment Descriptors . The following list shows the types that are widely used in Linux. Code Segment Descriptor Indicates that the Segment Descriptor refers to a code segment; it may be included either in the GDT or in the LDT . The descriptor has the S flag set (non-system segment). Data Segment Descriptor Indicates that the Segment Descriptor refers to a data segment ; it may be included either in the GDT or in the LDT . The descriptor has the S flag set. Stack segments are implemented by means of generic data segments. Task State Segment Descriptor (TSSD) Indicates that the Segment Descriptor refers to a Task State Segment (TSS) that is, a segment used to save the contents of the processor registers (see the section \"Task State Segment\" in Chapter 3); it can appear only in the GDT. The corresponding Type field has the value 11 or 9, depending on whether the corresponding process is currently executing on a CPU. The S flag of such descriptors is set to 0. SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6ca1\u6709stack segment descriptor\uff1b\u6839\u636e\u7b2c2.3\u7ae0\u7684\u5185\u5bb9\u6765\u770b\uff0c\u8fd9\u662f\u56e0\u4e3astack segment\u662f inside data segment\u7684\uff1b 2.2.3. Fast Access to Segment Descriptors # We recall that logical addresses consist of a 16-bit Segment Selector and a 32-bit Offset, and that segmentation registers store only the Segment Selector. To speed up the translation of logical addresses into linear addresses , the 80 x 86 processor provides an additional nonprogrammable register that is, a register that cannot be set by a programmer for each of the six programmable segmentation registers. Each nonprogrammable register contains the 8-byte Segment Descriptor (described in the previous section) specified by the Segment Selector contained in the corresponding segmentation register . Every time a Segment Selector is loaded in a segmentation register , the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT or LDT stored in main memory; the processor can refer only directly to the CPU register containing the Segment Descriptor. Accesses to the GDT or LDT are necessary only when the contents of the segmentation registers change (see Figure 2-4). Any Segment Selector includes three fields that are described in Table 2-2. Table 2-2. Segment Selector fields Field name Description index Identifies the Segment Descriptor entry contained in the GDT or in the LDT (described further in the text following this table). TI Table Indicator : specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). RPL Requestor Privilege Level : specifies the Current Privilege Level of the CPU when the corresponding Segment Selector is loaded into the cs register; it also may be used to selectively weaken the processor privilege level when accessing data segments (see Intel documentation for details). Because a Segment Descriptor is 8 bytes long, its relative address inside the GDT or the LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8. For instance, if the GDT is at 0x00020000 (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is 0x00020000 + (2 x 8) , or 0x00020010 . 2.2.4. Segmentation Unit # Figure 2-5 shows in detail how a logical address is translated into a corresponding linear address . The segmentation unit performs the following operations: Examines the TI field of the Segment Selector to determine which Descriptor Table stores the Segment Descriptor . This field indicates that the Descriptor is either in the GDT (in which case the segmentation unit gets the base linear address of the GDT from the gdtr register) or in the active LDT (in which case the segmentation unit gets the base linear address of that LDT from the ldtr register). Computes the address of the Segment Descriptor from the index field of the Segment Selector. The index field is multiplied by 8 (the size of a Segment Descriptor), and the result is added to the content of the gdtr or ldtr register. Adds the offset of the logical address to the Base field of the Segment Descriptor, thus obtaining the linear address. Notice that, thanks to the nonprogrammable registers associated with the segmentation registers, the first two operations need to be performed only when a segmentation register has been changed.","title":"2.2 Segmentation in Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#22-segmentation-in-hardware","text":"Starting with the 80286 model, Intel microprocessors perform address translation in two different ways called real mode and protected mode . We'll focus in the next sections on address translation when protected mode is enabled. Real mode exists mostly to maintain processor compatibility with older models and to allow the operating system to bootstrap (see Appendix A for a short description of real mode).","title":"2.2. Segmentation in Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#221-segment-selectors-and-segmentation-registers","text":"A logical address consists of two parts: a segment identifier and an offset that specifies the relative address within the segment. The segment identifier is a 16-bit field called the Segment Selector (see Figure 2-2), while the offset is a 32-bit field. We'll describe the fields of Segment Selectors in the section \"Fast Access to Segment Descriptors\" later in this chapter. SUMMARY : \u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5730\u5740\u7684\u957f\u5ea6\u662f\uff1a16 + 32 = 48 SUMMARY : \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cSegment Selector\u9664\u4e86\u5305\u542b\u6709table index\u4e4b\u5916\uff0c\u8fd8\u5305\u542b\u6709\u5176\u4ed6\u7684\u4fe1\u606f\uff1b To make it easy to retrieve segment selectors quickly, the processor provides segmentation registers whose only purpose is to hold Segment Selectors; these registers are called cs , ss , ds , es , fs , and gs . Although there are only six of them, a program can reuse the same segmentation register for different purposes by saving its content in memory and then restoring it later. Three of the six segmentation registers have specific purposes: cs The code segment register, which points to a segment containing program instructions ss The stack segment register, which points to a segment containing the current program stack ds The data segment register, which points to a segment containing global and static data The remaining three segmentation registers are general purpose and may refer to arbitrary data segments. The cs register has another important function: it includes a 2-bit field that specifies the Current Privilege Level (CPL) of the CPU. The value 0 denotes the highest privilege level, while the value 3 denotes the lowest one. Linux uses only levels 0 and 3, which are respectively called Kernel Mode and User Mode .","title":"2.2.1. Segment Selectors and Segmentation Registers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#222-segment-descriptors","text":"Each segment is represented by an 8-byte Segment Descriptor that describes the segment characteristics. Segment Descriptors are stored either in the Global Descriptor Table ( GDT ) or in the Local Descriptor Table( LDT ) . Usually only one GDT is defined, while each process is permitted to have its own LDT if it needs to create additional segments besides those stored in the GDT . The address and size of the GDT in main memory are contained in the gdtr control register, while the address and size of the currently used LDT are contained in the ldtr control register. Figure 2-3 illustrates the format of a Segment Descriptor ; the meaning of the various fields is explained in Table 2-1. Table 2-1. Segment Descriptor fields Field name Description Base Contains the linear address of the first byte of the segment. G Granularity flag : if it is cleared (equal to 0), the segment size is expressed in bytes; otherwise, it is expressed in multiples of 4096 bytes. Limit Holds the offset of the last memory cell in the segment, thus binding the segment length. When G is set to 0, the size of a segment may vary between 1 byte and 1 MB; otherwise, it may vary between 4 KB and 4 GB. S System flag : if it is cleared, the segment is a system segment that stores critical data structures such as the Local Descriptor Table ; otherwise, it is a normal code or data segment. Type Characterizes the segment type and its access rights (see the text that follows this table). DPL Descriptor Privilege Level: used to restrict accesses to the segment. It represents the minimal CPU privilege level requested for accessing the segment. Therefore, a segment with its DPL set to 0 is accessible only when the CPL is 0 that is, in Kernel Mode while a segment with its DPL set to 3 is accessible with every CPL value. P Segment-Present flag : is equal to 0 if the segment is not stored currently in main memory. Linux always sets this flag (bit 47) to 1, because it never swaps out whole segments to disk. There are several types of segments, and thus several types of Segment Descriptors . The following list shows the types that are widely used in Linux. Code Segment Descriptor Indicates that the Segment Descriptor refers to a code segment; it may be included either in the GDT or in the LDT . The descriptor has the S flag set (non-system segment). Data Segment Descriptor Indicates that the Segment Descriptor refers to a data segment ; it may be included either in the GDT or in the LDT . The descriptor has the S flag set. Stack segments are implemented by means of generic data segments. Task State Segment Descriptor (TSSD) Indicates that the Segment Descriptor refers to a Task State Segment (TSS) that is, a segment used to save the contents of the processor registers (see the section \"Task State Segment\" in Chapter 3); it can appear only in the GDT. The corresponding Type field has the value 11 or 9, depending on whether the corresponding process is currently executing on a CPU. The S flag of such descriptors is set to 0. SUMMARY : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6ca1\u6709stack segment descriptor\uff1b\u6839\u636e\u7b2c2.3\u7ae0\u7684\u5185\u5bb9\u6765\u770b\uff0c\u8fd9\u662f\u56e0\u4e3astack segment\u662f inside data segment\u7684\uff1b","title":"2.2.2. Segment Descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#223-fast-access-to-segment-descriptors","text":"We recall that logical addresses consist of a 16-bit Segment Selector and a 32-bit Offset, and that segmentation registers store only the Segment Selector. To speed up the translation of logical addresses into linear addresses , the 80 x 86 processor provides an additional nonprogrammable register that is, a register that cannot be set by a programmer for each of the six programmable segmentation registers. Each nonprogrammable register contains the 8-byte Segment Descriptor (described in the previous section) specified by the Segment Selector contained in the corresponding segmentation register . Every time a Segment Selector is loaded in a segmentation register , the corresponding Segment Descriptor is loaded from memory into the matching nonprogrammable CPU register. From then on, translations of logical addresses referring to that segment can be performed without accessing the GDT or LDT stored in main memory; the processor can refer only directly to the CPU register containing the Segment Descriptor. Accesses to the GDT or LDT are necessary only when the contents of the segmentation registers change (see Figure 2-4). Any Segment Selector includes three fields that are described in Table 2-2. Table 2-2. Segment Selector fields Field name Description index Identifies the Segment Descriptor entry contained in the GDT or in the LDT (described further in the text following this table). TI Table Indicator : specifies whether the Segment Descriptor is included in the GDT (TI = 0) or in the LDT (TI = 1). RPL Requestor Privilege Level : specifies the Current Privilege Level of the CPU when the corresponding Segment Selector is loaded into the cs register; it also may be used to selectively weaken the processor privilege level when accessing data segments (see Intel documentation for details). Because a Segment Descriptor is 8 bytes long, its relative address inside the GDT or the LDT is obtained by multiplying the 13-bit index field of the Segment Selector by 8. For instance, if the GDT is at 0x00020000 (the value stored in the gdtr register) and the index specified by the Segment Selector is 2, the address of the corresponding Segment Descriptor is 0x00020000 + (2 x 8) , or 0x00020010 .","title":"2.2.3. Fast Access to Segment Descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.2-Segmentation-in-Hardware/#224-segmentation-unit","text":"Figure 2-5 shows in detail how a logical address is translated into a corresponding linear address . The segmentation unit performs the following operations: Examines the TI field of the Segment Selector to determine which Descriptor Table stores the Segment Descriptor . This field indicates that the Descriptor is either in the GDT (in which case the segmentation unit gets the base linear address of the GDT from the gdtr register) or in the active LDT (in which case the segmentation unit gets the base linear address of that LDT from the ldtr register). Computes the address of the Segment Descriptor from the index field of the Segment Selector. The index field is multiplied by 8 (the size of a Segment Descriptor), and the result is added to the content of the gdtr or ldtr register. Adds the offset of the logical address to the Base field of the Segment Descriptor, thus obtaining the linear address. Notice that, thanks to the nonprogrammable registers associated with the segmentation registers, the first two operations need to be performed only when a segmentation register has been changed.","title":"2.2.4. Segmentation Unit"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/","text":"2.3. Segmentation in Linux 2.3.1. The Linux GDT \u8865\u5145\u5185\u5bb9 Segmentation in Linux : Segmentation & Paging are redundant? A Does Linux not use segmentation but only paging? A A x86 memory segmentation Later developments 2.3. Segmentation in Linux # Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses . One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80 x 86 architecture. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. Notice that the linear addresses associated with such segments all start at 0 and reach the addressing limit of $2^{32} -1$. This means that all processes, either in User Mode or in Kernel Mode, may use the same logical addresses. SUMMARY : \u4e0a\u8ff0\u65ad\u8a00\u975e\u5e38\u5177\u6709\u4ef7\u503c Another important consequence of having all segments start at 0x00000000 is that in Linux, logical addresses coincide with linear addresses; that is, the value of the Offset field of a logical address always coincides with the value of the corresponding linear address . As stated earlier, the Current Privilege Level of the CPU indicates whether the processor is in User or Kernel Mode and is specified by the RPL field of the Segment Selector stored in the cs register. Whenever the CPL is changed, some segmentation registers must be correspondingly updated. For instance, when the CPL is equal to 3 (User Mode), the ds register must contain the Segment Selector of the user data segment, but when the CPL is equal to 0, the ds register must contain the Segment Selector of the kernel data segment. A similar situation occurs for the ss register. It must refer to a User Mode stack inside the user data segment when the CPL is 3, and it must refer to a Kernel Mode stack inside the kernel data segment when the CPL is 0. When switching from User Mode to Kernel Mode, Linux always makes sure that the ss register contains the Segment Selector of the kernel data segment . SUMMARY : stack\u662finside data segment\u7684\uff0c\u5e76\u4e14\u5b83\u4eec\u5171\u7528\u540c\u4e00\u4e2adescriptor\uff1b When saving a pointer to an instruction or to a data structure, the kernel does not need to store the Segment Selector component of the logical address , because the ss register contains the current Segment Selector . As an example, when the kernel invokes a function, it executes a call assembly language instruction specifying just the Offset component of its logical address; the Segment Selector is implicitly selected as the one referred to by the cs register. Because there is just one segment of type \"executable in Kernel Mode,\" namely the code segment identified by __KERNEL_CS , it is sufficient to load __ KERNEL_CS into cs whenever the CPU switches to Kernel Mode. The same argument goes for pointers to kernel data structures (implicitly using the ds register), as well as for pointers to user data structures (the kernel explicitly uses the es register). SUMMARY : \u6ca1\u6709\u8bfb\u61c2 Besides the four segments just described, Linux makes use of a few other specialized segments. We'll introduce them in the next section while describing the Linux GDT . 2.3.1. The Linux GDT # In uniprocessor systems there is only one GDT , while in multiprocessor systems there is one GDT for every CPU in the system. All GDT s are stored in the cpu_gdt_table array, while the addresses and sizes of the GDT s (used when initializing the gdtr registers) are stored in the cpu_gdt_descr array. If you look in the Source Code Index, you can see that these symbols are defined in the file arch/i386/kernel/head.S . Every macro, function, and other symbol in this book is listed in the Source Code Index, so you can quickly find it in the source code. \u8865\u5145\u5185\u5bb9 # Segmentation in Linux : Segmentation & Paging are redundant? # I'm reading \"Understanding Linux Kernel\". This is the snippet that explains how Linux uses Segmentation which I didn't understand. Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant , because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses. One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. I'm unable to understand 1st and last paragraph. A # The 80x86 family of CPUs generate a real address by adding the contents of a CPU register called a segment register to that of the program counter. Thus by changing the segment register contents you can change the physical addresses that the program accesses. Paging does something similar by mapping the same virtual address to different real addresses . Linux using uses the latter - the segment registers for Linux processes will always have the same unchanging contents. +1. Linux, and everyone else too nowadays. \u2013 Billy ONeal Jun 12 '10 at 15:14 7 In protected mode it's not actually the contents of the segment register itself that is added to addresses; the segment register contains a reference to a segment descriptor (stored in memory, in a descriptor table), and one of the fields of the segment descriptor is the base address of the segment, which is added to the offset to generate a linear address. \u2013 caf Jun 16 '10 at 7:42 Segmentation was dropped in x86-64 architecture (or amd64 is Linux calls it). This newer architecture uses the flat memory model. \u2013 hebbo May 19 '14 at 3:43 @caf Thanks for the elaboration about protected mode. Here's more info about the different CPU modes for those who are curious. \u2013 GDP2 Nov 12 '17 at 20:18 Does Linux not use segmentation but only paging? # The Linux Programming Interface shows the layout of a virtual address space of a process. Is each region in the diagram a segment? From Understanding The Linux Kernel , is it correct that the following means that the segmentation unit in MMU maps the segments and offsets within segments into the virtual memory address, and the paging unit then maps the virtual memory address to the physical memory address? The Memory Management Unit (MMU) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). Then why does it say that Linux doesn't use segmentation but only paging? Segmentation has been included in 80x86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: \u2022 Memory management is simpler when all processes use the same segment register values\u2014that is, when they share the same set of linear addresses. \u2022 One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures, in particular, have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80x86 architecture. A # The x86-64 architecture does not use segmentation in long mode (64-bit mode). Four of the segment registers: CS, SS, DS, and ES are forced to 0, and the limit to 2^64. https://en.wikipedia.org/wiki/X86_memory_segmentation#Later_developments It is no longer possible for the OS to limit which ranges of the \"linear addresses\" are available. Therefore it cannot use segmentation for memory protection; it must rely entirely on paging. Do not worry about the details of x86 CPUs which would only apply when running in the legacy 32-bit modes. Linux for the 32-bit modes is not used as much. It may even be considered \"in a state of benign neglect for several years\". See 32-Bit x86 support in Fedora [LWN.net, 2017]. (It happens that 32-bit Linux does not use segmentation either. But you don't need to trust me on that, you can just ignore it :-). A # As the x86 has segments, it is not possible to not use them. But both cs (code segment) and ds (data segment) base addresses are set to zero, so the segmentation is not really used. An exception is thread local data, one of the normally unused segment registers points to thread local data . But that is mainly to avoid reserving one of the general purpose registers for this task. It doesn't say that Linux doesn't use segmentation on the x86, as that would not be possible. You already highlighted one part, Linux uses segmentation in a very limited way . The second part is Linux uses segmentation only when required by the 80x86 architecture You already quoted the reasons, paging is easier and more portable. x86 memory segmentation Later developments #","title":"2.3 Segmentation in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#23-segmentation-in-linux","text":"Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses . One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80 x 86 architecture. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. Notice that the linear addresses associated with such segments all start at 0 and reach the addressing limit of $2^{32} -1$. This means that all processes, either in User Mode or in Kernel Mode, may use the same logical addresses. SUMMARY : \u4e0a\u8ff0\u65ad\u8a00\u975e\u5e38\u5177\u6709\u4ef7\u503c Another important consequence of having all segments start at 0x00000000 is that in Linux, logical addresses coincide with linear addresses; that is, the value of the Offset field of a logical address always coincides with the value of the corresponding linear address . As stated earlier, the Current Privilege Level of the CPU indicates whether the processor is in User or Kernel Mode and is specified by the RPL field of the Segment Selector stored in the cs register. Whenever the CPL is changed, some segmentation registers must be correspondingly updated. For instance, when the CPL is equal to 3 (User Mode), the ds register must contain the Segment Selector of the user data segment, but when the CPL is equal to 0, the ds register must contain the Segment Selector of the kernel data segment. A similar situation occurs for the ss register. It must refer to a User Mode stack inside the user data segment when the CPL is 3, and it must refer to a Kernel Mode stack inside the kernel data segment when the CPL is 0. When switching from User Mode to Kernel Mode, Linux always makes sure that the ss register contains the Segment Selector of the kernel data segment . SUMMARY : stack\u662finside data segment\u7684\uff0c\u5e76\u4e14\u5b83\u4eec\u5171\u7528\u540c\u4e00\u4e2adescriptor\uff1b When saving a pointer to an instruction or to a data structure, the kernel does not need to store the Segment Selector component of the logical address , because the ss register contains the current Segment Selector . As an example, when the kernel invokes a function, it executes a call assembly language instruction specifying just the Offset component of its logical address; the Segment Selector is implicitly selected as the one referred to by the cs register. Because there is just one segment of type \"executable in Kernel Mode,\" namely the code segment identified by __KERNEL_CS , it is sufficient to load __ KERNEL_CS into cs whenever the CPU switches to Kernel Mode. The same argument goes for pointers to kernel data structures (implicitly using the ds register), as well as for pointers to user data structures (the kernel explicitly uses the es register). SUMMARY : \u6ca1\u6709\u8bfb\u61c2 Besides the four segments just described, Linux makes use of a few other specialized segments. We'll introduce them in the next section while describing the Linux GDT .","title":"2.3. Segmentation in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#231-the-linux-gdt","text":"In uniprocessor systems there is only one GDT , while in multiprocessor systems there is one GDT for every CPU in the system. All GDT s are stored in the cpu_gdt_table array, while the addresses and sizes of the GDT s (used when initializing the gdtr registers) are stored in the cpu_gdt_descr array. If you look in the Source Code Index, you can see that these symbols are defined in the file arch/i386/kernel/head.S . Every macro, function, and other symbol in this book is listed in the Source Code Index, so you can quickly find it in the source code.","title":"2.3.1. The Linux GDT"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#_1","text":"","title":"\u8865\u5145\u5185\u5bb9"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#segmentation-in-linux-segmentation-paging-are-redundant","text":"I'm reading \"Understanding Linux Kernel\". This is the snippet that explains how Linux uses Segmentation which I didn't understand. Segmentation has been included in 80 x 86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant , because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: Memory management is simpler when all processes use the same segment register values that is, when they share the same set of linear addresses. One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures in particular have limited support for segmentation. All Linux processes running in User Mode use the same pair of segments to address instructions and data. These segments are called user code segment and user data segment , respectively. Similarly, all Linux processes running in Kernel Mode use the same pair of segments to address instructions and data: they are called kernel code segment and kernel data segment , respectively. Table 2-3 shows the values of the Segment Descriptor fields for these four crucial segments. I'm unable to understand 1st and last paragraph.","title":"Segmentation in Linux : Segmentation &amp; Paging are redundant?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#a","text":"The 80x86 family of CPUs generate a real address by adding the contents of a CPU register called a segment register to that of the program counter. Thus by changing the segment register contents you can change the physical addresses that the program accesses. Paging does something similar by mapping the same virtual address to different real addresses . Linux using uses the latter - the segment registers for Linux processes will always have the same unchanging contents. +1. Linux, and everyone else too nowadays. \u2013 Billy ONeal Jun 12 '10 at 15:14 7 In protected mode it's not actually the contents of the segment register itself that is added to addresses; the segment register contains a reference to a segment descriptor (stored in memory, in a descriptor table), and one of the fields of the segment descriptor is the base address of the segment, which is added to the offset to generate a linear address. \u2013 caf Jun 16 '10 at 7:42 Segmentation was dropped in x86-64 architecture (or amd64 is Linux calls it). This newer architecture uses the flat memory model. \u2013 hebbo May 19 '14 at 3:43 @caf Thanks for the elaboration about protected mode. Here's more info about the different CPU modes for those who are curious. \u2013 GDP2 Nov 12 '17 at 20:18","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#does-linux-not-use-segmentation-but-only-paging","text":"The Linux Programming Interface shows the layout of a virtual address space of a process. Is each region in the diagram a segment? From Understanding The Linux Kernel , is it correct that the following means that the segmentation unit in MMU maps the segments and offsets within segments into the virtual memory address, and the paging unit then maps the virtual memory address to the physical memory address? The Memory Management Unit (MMU) transforms a logical address into a linear address by means of a hardware circuit called a segmentation unit; subsequently, a second hardware circuit called a paging unit transforms the linear address into a physical address (see Figure 2-1). Then why does it say that Linux doesn't use segmentation but only paging? Segmentation has been included in 80x86 microprocessors to encourage programmers to split their applications into logically related entities, such as subroutines or global and local data areas. However, Linux uses segmentation in a very limited way. In fact, segmentation and paging are somewhat redundant, because both can be used to separate the physical address spaces of processes: segmentation can assign a different linear address space to each process, while paging can map the same linear address space into different physical address spaces. Linux prefers paging to segmentation for the following reasons: \u2022 Memory management is simpler when all processes use the same segment register values\u2014that is, when they share the same set of linear addresses. \u2022 One of the design objectives of Linux is portability to a wide range of architectures; RISC architectures, in particular, have limited support for segmentation. The 2.6 version of Linux uses segmentation only when required by the 80x86 architecture.","title":"Does Linux not use segmentation but only paging?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#a_1","text":"The x86-64 architecture does not use segmentation in long mode (64-bit mode). Four of the segment registers: CS, SS, DS, and ES are forced to 0, and the limit to 2^64. https://en.wikipedia.org/wiki/X86_memory_segmentation#Later_developments It is no longer possible for the OS to limit which ranges of the \"linear addresses\" are available. Therefore it cannot use segmentation for memory protection; it must rely entirely on paging. Do not worry about the details of x86 CPUs which would only apply when running in the legacy 32-bit modes. Linux for the 32-bit modes is not used as much. It may even be considered \"in a state of benign neglect for several years\". See 32-Bit x86 support in Fedora [LWN.net, 2017]. (It happens that 32-bit Linux does not use segmentation either. But you don't need to trust me on that, you can just ignore it :-).","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#a_2","text":"As the x86 has segments, it is not possible to not use them. But both cs (code segment) and ds (data segment) base addresses are set to zero, so the segmentation is not really used. An exception is thread local data, one of the normally unused segment registers points to thread local data . But that is mainly to avoid reserving one of the general purpose registers for this task. It doesn't say that Linux doesn't use segmentation on the x86, as that would not be possible. You already highlighted one part, Linux uses segmentation in a very limited way . The second part is Linux uses segmentation only when required by the 80x86 architecture You already quoted the reasons, paging is easier and more portable.","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.3-Segmentation-in-Linux/#x86-memory-segmentation-later-developments","text":"","title":"x86 memory segmentation Later developments"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/","text":"2.4. Paging in Hardware 2.4.1. Regular Paging 2.4.2. Extended Paging 2.4.3. Hardware Protection Scheme 2.4.4. An Example of Regular Paging 2.4.5. The Physical Address Extension (PAE) Paging Mechanism 2.4.6. Paging for 64-bit Architectures 2.4.7. Hardware Cache 2.4.8. Translation Lookaside Buffers (TLB) 2.4. Paging in Hardware # The paging unit translates linear addresses into physical ones . One key task in the unit is to check the requested access type against the access rights of the linear address . If the memory access is not valid, it generates a Page Fault exception (see Chapter 4 and Chapter 8). For the sake of efficiency, linear addresses are grouped in fixed-length intervals called pages ; contiguous linear addresses within a page are mapped into contiguous physical addresses. In this way, the kernel can specify the physical address and the access rights of a page instead of those of all the linear addresses included in it. Following the usual convention, we shall use the term \"page\" to refer both to a set of linear addresses and to the data contained in this group of addresses. The paging unit thinks of all RAM as partitioned into fixed-length page frames (sometimes referred to as physical pages ). Each page frame contains a page that is, the length of a page frame coincides with that of a page . A page frame is a constituent of main memory, and hence it is a storage area. It is important to distinguish a page from a page frame ; the former is just a block of data, which may be stored in any page frame or on disk . The data structures that map linear to physical addresses are called page tables ; they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit . NOTE: \u6709\u5fc5\u8981\u68b3\u7406\u4e00\u4e0bpaging unit\u548cpage tables\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a\u4e24\u8005\u7ec4\u5408\u8d77\u6765\u5b9e\u73b0\u5c06linear address\u8f6c\u6362\u4e3aphysical address\uff0cpaging unit\u9700\u8981\u4f9d\u8d56\u4e8epage tables\u4e2d\u7684\u6570\u636e\u3002 \u6839\u636e2.4.1. Regular Paging\u8282\u4e2d\u7684\u63cf\u8ff0\uff0c\u7cfb\u7edf\u662f\u53ef\u4ee5\u91c7\u7528\u591a\u5c42page table\uff08\u4e5f\u53eb\u505a translation table \uff09\u7684\u3002\u6bd4\u5982\u672c\u4e66\u4e2d\u6240\u63cf\u8ff0\u7684\u786c\u4ef6\u91c7\u7528\u4e86\u4e24\u5c42page table\uff1a \u7b2c\u4e00\u5c42\uff1a Page Directory \u7b2c\u4e8c\u5c42\uff1a Page Table Starting with the 80386, all 80 x 86 processors support paging ; it is enabled by setting the PG flag of a control register named cr0 . When PG = 0 , linear addresses are interpreted as physical addresses . 2.4.1. Regular Paging # Starting with the 80386, the paging unit of Intel processors handles 4 KB pages. The 32 bits of a linear address are divided into three fields: Directory The most significant 10 bits Table The intermediate 10 bits Offset The least significant 12 bits The translation of linear addresses is accomplished in two steps, each based on a type of translation table . The first translation table is called the Page Directory , and the second is called the Page Table . [ * ] [ * ] In the discussion that follows, the lowercase \"page table\" term denotes any page storing the mapping between linear and physical addresses, while the capitalized \"Page Table\" term denotes a page in the last level of page tables. The aim of this two-level scheme is to reduce the amount of RAM required for per-process Page Tables . If a simple one-level Page Table was used, then it would require up to 220 entries (i.e., at 4 bytes per entry, 4 MB of RAM) to represent the Page Table for each process (if the process used a full 4 GB linear address space), even though a process does not use all addresses in that range. The two-level scheme reduces the memory by requiring Page Tables only for those virtual memory regions actually used by a process. Each active process must have a Page Directory assigned to it. However, there is no need to allocate RAM for all Page Tables of a process at once; it is more efficient to allocate RAM for a Page Table only when the process effectively needs it. NOTE: \u6bcf\u4e2aprocess\u6709\u4e00\u4e2a Page Directory \uff0c\u800c\u4e0d\u662f Page Directory \u4e2d\u7684\u4e00\u6761\u8bb0\u5f55\uff1b - Multilevel Paging in Operating System - https://www.clear.rice.edu/comp425/slides/L31.pdf - How does multi-level page table save memory space? NOTE: The physical address of the Page Directory in use is stored in a control register named cr3 . The Directory field within the linear address determines the entry in the Page Directory that points to the proper Page Table . The address's Table field, in turn, determines the entry in the Page Table that contains the physical address of the page frame containing the page. The Offset field determines the relative position within the page frame (see Figure 2-7). Because it is 12 bits long, each page consists of 4096 bytes of data. Both the Directory and the Table fields are 10 bits long, so Page Directories and Page Tables can include up to 1,024\uff08$2^{10}=1024$\uff09 entries. It follows that a Page Directory can address up to $1024 * 1024 * 4096=2^{32}$ memory cells, as you'd expect in 32-bit addresses. The entries of Page Directories and Page Tables have the same structure. Each entry includes the following fields: Present flag If it is set, the referred-to page (or Page Table) is contained in main memory; if the flag is 0, the page is not contained in main memory and the remaining entry bits may be used by the operating system for its own purposes. If the entry of a Page Table or Page Directory needed to perform an address translation has the Present flag cleared, the paging unit stores the linear address in a control register named cr2 and generates exception 14: the Page Fault exception. (We will see in Chapter 17 how Linux uses this field.) NOTE: \u4ea7\u751f\u4e86Page Fault exception\u540e\uff0c\u5c31\u9700\u8981\u5c06demand page swap\u5230memory\u4e2d Field containing the 20 most significant bits of a page frame physical address Accessed flag Dirty flag 2.4.2. Extended Paging # Starting with the Pentium model, 80 x 86 microprocessors introduce extended paging , which allows page frames to be 4 MB instead of 4 KB in size (see Figure 2-8). Extended paging is used to translate large contiguous linear address ranges into corresponding physical ones; in these cases, the kernel can do without intermediate Page Tables and thus save memory and preserve TLB entries (see the section \"Translation Lookaside Buffers (TLB)\"). 2.4.3. Hardware Protection Scheme # 2.4.4. An Example of Regular Paging # 2.4.5. The Physical Address Extension (PAE) Paging Mechanism # 2.4.6. Paging for 64-bit Architectures # 2.4.7. Hardware Cache # 2.4.8. Translation Lookaside Buffers (TLB) #","title":"2.4-Paging-in-Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#24-paging-in-hardware","text":"The paging unit translates linear addresses into physical ones . One key task in the unit is to check the requested access type against the access rights of the linear address . If the memory access is not valid, it generates a Page Fault exception (see Chapter 4 and Chapter 8). For the sake of efficiency, linear addresses are grouped in fixed-length intervals called pages ; contiguous linear addresses within a page are mapped into contiguous physical addresses. In this way, the kernel can specify the physical address and the access rights of a page instead of those of all the linear addresses included in it. Following the usual convention, we shall use the term \"page\" to refer both to a set of linear addresses and to the data contained in this group of addresses. The paging unit thinks of all RAM as partitioned into fixed-length page frames (sometimes referred to as physical pages ). Each page frame contains a page that is, the length of a page frame coincides with that of a page . A page frame is a constituent of main memory, and hence it is a storage area. It is important to distinguish a page from a page frame ; the former is just a block of data, which may be stored in any page frame or on disk . The data structures that map linear to physical addresses are called page tables ; they are stored in main memory and must be properly initialized by the kernel before enabling the paging unit . NOTE: \u6709\u5fc5\u8981\u68b3\u7406\u4e00\u4e0bpaging unit\u548cpage tables\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a\u4e24\u8005\u7ec4\u5408\u8d77\u6765\u5b9e\u73b0\u5c06linear address\u8f6c\u6362\u4e3aphysical address\uff0cpaging unit\u9700\u8981\u4f9d\u8d56\u4e8epage tables\u4e2d\u7684\u6570\u636e\u3002 \u6839\u636e2.4.1. Regular Paging\u8282\u4e2d\u7684\u63cf\u8ff0\uff0c\u7cfb\u7edf\u662f\u53ef\u4ee5\u91c7\u7528\u591a\u5c42page table\uff08\u4e5f\u53eb\u505a translation table \uff09\u7684\u3002\u6bd4\u5982\u672c\u4e66\u4e2d\u6240\u63cf\u8ff0\u7684\u786c\u4ef6\u91c7\u7528\u4e86\u4e24\u5c42page table\uff1a \u7b2c\u4e00\u5c42\uff1a Page Directory \u7b2c\u4e8c\u5c42\uff1a Page Table Starting with the 80386, all 80 x 86 processors support paging ; it is enabled by setting the PG flag of a control register named cr0 . When PG = 0 , linear addresses are interpreted as physical addresses .","title":"2.4. Paging in Hardware"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#241-regular-paging","text":"Starting with the 80386, the paging unit of Intel processors handles 4 KB pages. The 32 bits of a linear address are divided into three fields: Directory The most significant 10 bits Table The intermediate 10 bits Offset The least significant 12 bits The translation of linear addresses is accomplished in two steps, each based on a type of translation table . The first translation table is called the Page Directory , and the second is called the Page Table . [ * ] [ * ] In the discussion that follows, the lowercase \"page table\" term denotes any page storing the mapping between linear and physical addresses, while the capitalized \"Page Table\" term denotes a page in the last level of page tables. The aim of this two-level scheme is to reduce the amount of RAM required for per-process Page Tables . If a simple one-level Page Table was used, then it would require up to 220 entries (i.e., at 4 bytes per entry, 4 MB of RAM) to represent the Page Table for each process (if the process used a full 4 GB linear address space), even though a process does not use all addresses in that range. The two-level scheme reduces the memory by requiring Page Tables only for those virtual memory regions actually used by a process. Each active process must have a Page Directory assigned to it. However, there is no need to allocate RAM for all Page Tables of a process at once; it is more efficient to allocate RAM for a Page Table only when the process effectively needs it. NOTE: \u6bcf\u4e2aprocess\u6709\u4e00\u4e2a Page Directory \uff0c\u800c\u4e0d\u662f Page Directory \u4e2d\u7684\u4e00\u6761\u8bb0\u5f55\uff1b - Multilevel Paging in Operating System - https://www.clear.rice.edu/comp425/slides/L31.pdf - How does multi-level page table save memory space? NOTE: The physical address of the Page Directory in use is stored in a control register named cr3 . The Directory field within the linear address determines the entry in the Page Directory that points to the proper Page Table . The address's Table field, in turn, determines the entry in the Page Table that contains the physical address of the page frame containing the page. The Offset field determines the relative position within the page frame (see Figure 2-7). Because it is 12 bits long, each page consists of 4096 bytes of data. Both the Directory and the Table fields are 10 bits long, so Page Directories and Page Tables can include up to 1,024\uff08$2^{10}=1024$\uff09 entries. It follows that a Page Directory can address up to $1024 * 1024 * 4096=2^{32}$ memory cells, as you'd expect in 32-bit addresses. The entries of Page Directories and Page Tables have the same structure. Each entry includes the following fields: Present flag If it is set, the referred-to page (or Page Table) is contained in main memory; if the flag is 0, the page is not contained in main memory and the remaining entry bits may be used by the operating system for its own purposes. If the entry of a Page Table or Page Directory needed to perform an address translation has the Present flag cleared, the paging unit stores the linear address in a control register named cr2 and generates exception 14: the Page Fault exception. (We will see in Chapter 17 how Linux uses this field.) NOTE: \u4ea7\u751f\u4e86Page Fault exception\u540e\uff0c\u5c31\u9700\u8981\u5c06demand page swap\u5230memory\u4e2d Field containing the 20 most significant bits of a page frame physical address Accessed flag Dirty flag","title":"2.4.1. Regular Paging"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#242-extended-paging","text":"Starting with the Pentium model, 80 x 86 microprocessors introduce extended paging , which allows page frames to be 4 MB instead of 4 KB in size (see Figure 2-8). Extended paging is used to translate large contiguous linear address ranges into corresponding physical ones; in these cases, the kernel can do without intermediate Page Tables and thus save memory and preserve TLB entries (see the section \"Translation Lookaside Buffers (TLB)\").","title":"2.4.2. Extended Paging"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#243-hardware-protection-scheme","text":"","title":"2.4.3. Hardware Protection Scheme"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#244-an-example-of-regular-paging","text":"","title":"2.4.4. An Example of Regular Paging"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#245-the-physical-address-extension-pae-paging-mechanism","text":"","title":"2.4.5. The Physical Address Extension (PAE) Paging Mechanism"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#246-paging-for-64-bit-architectures","text":"","title":"2.4.6. Paging for 64-bit Architectures"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#247-hardware-cache","text":"","title":"2.4.7. Hardware Cache"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.4-Paging-in-Hardware/#248-translation-lookaside-buffers-tlb","text":"","title":"2.4.8. Translation Lookaside Buffers (TLB)"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.5-Paging-in-Linux/","text":"2.5. Paging in Linux 2.5.1. The Linear Address Fields 2.5. Paging in Linux # Linux adopts a common paging model that fits both 32-bit and 64-bit architectures. As explained in the earlier section \"Paging for 64-bit Architectures,\" two paging levels are sufficient for 32-bit architectures, while 64-bit architectures require a higher number of paging levels. Up to version 2.6.10, the Linux paging model consisted of three paging levels . Starting with version 2.6.11, a four-level paging model has been adopted. [ * ] The four types of page tables illustrated in Figure 2-12 are called: [ * ] This change has been made to fully support the linear address bit splitting used by the x86_64 platform (see Table 2-4). Page Global Directory Page Upper Directory Page Middle Directory Page Table Figure 2-12. The Linux paging model The Page Global Directory includes the addresses of several Page Upper Directories , which in turn include the addresses of several Page Middle Directories , which in turn include the addresses of several Page Tables . Each Page Table entry points to a page frame . Thus the linear address can be split into up to five parts. Figure 2-12 does not show the bit numbers, because the size of each part depends on the computer architecture. For 32-bit architectures with no Physical Address Extension , two paging levels are sufficient. Linux essentially eliminates the Page Upper Directory and the Page Middle Directory fields by saying that they contain zero bits. However, the positions of the Page Upper Directory and the Page Middle Directory in the sequence of pointers are kept so that the same code can work on 32-bit and 64-bit architectures. The kernel keeps a position for the Page Upper Directory and the Page Middle Directory by setting the number of entries in them to 1 and mapping these two entries into the proper entry of the Page Global Directory . Finally, for 64-bit architectures three or four levels of paging are used depending on the linear address bit splitting performed by the hardware (see Table 2-2). Linux's handling of processes relies heavily on paging . In fact, the automatic translation of linear addresses into physical ones makes the following design objectives feasible: Assign a different physical address space to each process, ensuring an efficient protection against addressing errors. Distinguish pages (groups of data) from page frames (physical addresses in main memory). This allows the same page to be stored in a page frame, then saved to disk and later reloaded in a different page frame. This is the basic ingredient of the virtual memory mechanism (see Chapter 17). In the remaining part of this chapter, we will refer for the sake of concreteness to the paging circuitry used by the 80 x 86 processors. As we will see in Chapter 9, each process has its own Page Global Directory and its own set of Page Tables . When a process switch occurs (see the section \"Process Switch\" in Chapter 3), Linux saves the cr3 control register in the descriptor of the process previously in execution and then loads cr3 with the value stored in the descriptor of the process to be executed next. Thus, when the new process resumes its execution on the CPU, the paging unit refers to the correct set of Page Tables . Mapping linear to physical addresses now becomes a mechanical task, although it is still somewhat complex. The next few sections of this chapter are a rather tedious list of functions and macros that retrieve information the kernel needs to find addresses and manage the tables; most of the functions are one or two lines long. You may want to only skim these sections now, but it is useful to know the role of these functions and macros, because you'll see them often in discussions throughout this book. 2.5.1. The Linear Address Fields # The following macros simplify Page Table handling:","title":"2.5-Paging-in-Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.5-Paging-in-Linux/#25-paging-in-linux","text":"Linux adopts a common paging model that fits both 32-bit and 64-bit architectures. As explained in the earlier section \"Paging for 64-bit Architectures,\" two paging levels are sufficient for 32-bit architectures, while 64-bit architectures require a higher number of paging levels. Up to version 2.6.10, the Linux paging model consisted of three paging levels . Starting with version 2.6.11, a four-level paging model has been adopted. [ * ] The four types of page tables illustrated in Figure 2-12 are called: [ * ] This change has been made to fully support the linear address bit splitting used by the x86_64 platform (see Table 2-4). Page Global Directory Page Upper Directory Page Middle Directory Page Table Figure 2-12. The Linux paging model The Page Global Directory includes the addresses of several Page Upper Directories , which in turn include the addresses of several Page Middle Directories , which in turn include the addresses of several Page Tables . Each Page Table entry points to a page frame . Thus the linear address can be split into up to five parts. Figure 2-12 does not show the bit numbers, because the size of each part depends on the computer architecture. For 32-bit architectures with no Physical Address Extension , two paging levels are sufficient. Linux essentially eliminates the Page Upper Directory and the Page Middle Directory fields by saying that they contain zero bits. However, the positions of the Page Upper Directory and the Page Middle Directory in the sequence of pointers are kept so that the same code can work on 32-bit and 64-bit architectures. The kernel keeps a position for the Page Upper Directory and the Page Middle Directory by setting the number of entries in them to 1 and mapping these two entries into the proper entry of the Page Global Directory . Finally, for 64-bit architectures three or four levels of paging are used depending on the linear address bit splitting performed by the hardware (see Table 2-2). Linux's handling of processes relies heavily on paging . In fact, the automatic translation of linear addresses into physical ones makes the following design objectives feasible: Assign a different physical address space to each process, ensuring an efficient protection against addressing errors. Distinguish pages (groups of data) from page frames (physical addresses in main memory). This allows the same page to be stored in a page frame, then saved to disk and later reloaded in a different page frame. This is the basic ingredient of the virtual memory mechanism (see Chapter 17). In the remaining part of this chapter, we will refer for the sake of concreteness to the paging circuitry used by the 80 x 86 processors. As we will see in Chapter 9, each process has its own Page Global Directory and its own set of Page Tables . When a process switch occurs (see the section \"Process Switch\" in Chapter 3), Linux saves the cr3 control register in the descriptor of the process previously in execution and then loads cr3 with the value stored in the descriptor of the process to be executed next. Thus, when the new process resumes its execution on the CPU, the paging unit refers to the correct set of Page Tables . Mapping linear to physical addresses now becomes a mechanical task, although it is still somewhat complex. The next few sections of this chapter are a rather tedious list of functions and macros that retrieve information the kernel needs to find addresses and manage the tables; most of the functions are one or two lines long. You may want to only skim these sections now, but it is useful to know the role of these functions and macros, because you'll see them often in discussions throughout this book.","title":"2.5. Paging in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/2.5-Paging-in-Linux/#251-the-linear-address-fields","text":"The following macros simplify Page Table handling:","title":"2.5.1. The Linear Address Fields"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Chapter-2-Memory-Addressing/","text":"Chapter 2. Memory Addressing # This chapter deals with addressing techniques. Luckily, an operating system is not forced to keep track of physical memory all by itself; today's microprocessors include several hardware circuits to make memory management both more efficient and more robust so that programming errors cannot cause improper accesses to memory outside the program. As in the rest of this book, we offer details in this chapter on how 80 x 86 microprocessors address memory chips and how Linux uses the available addressing circuits. You will find, we hope, that when you learn the implementation details on Linux's most popular platform you will better understand both the general theory of paging and how to research the implementation on other platforms. This is the first of three chapters related to memory management; Chapter 8 discusses how the kernel allocates main memory to itself, while Chapter 9 considers how linear addresses are assigned to processes . NOTE: \u76ee\u524dlinux\u91c7\u7528\u7684\u662f\u57fa\u4e8epage\u7684memory management\uff0c\u6240\u4ee5\u672c\u7ae0\u76842.2. Segmentation in Hardware\u548c2.3. Segmentation in Linux\u7701\u7565\u4e86\u3002","title":"Chapter-2-Memory-Addressing"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Chapter-2-Memory-Addressing/#chapter-2-memory-addressing","text":"This chapter deals with addressing techniques. Luckily, an operating system is not forced to keep track of physical memory all by itself; today's microprocessors include several hardware circuits to make memory management both more efficient and more robust so that programming errors cannot cause improper accesses to memory outside the program. As in the rest of this book, we offer details in this chapter on how 80 x 86 microprocessors address memory chips and how Linux uses the available addressing circuits. You will find, we hope, that when you learn the implementation details on Linux's most popular platform you will better understand both the general theory of paging and how to research the implementation on other platforms. This is the first of three chapters related to memory management; Chapter 8 discusses how the kernel allocates main memory to itself, while Chapter 9 considers how linear addresses are assigned to processes . NOTE: \u76ee\u524dlinux\u91c7\u7528\u7684\u662f\u57fa\u4e8epage\u7684memory management\uff0c\u6240\u4ee5\u672c\u7ae0\u76842.2. Segmentation in Hardware\u548c2.3. Segmentation in Linux\u7701\u7565\u4e86\u3002","title":"Chapter 2. Memory Addressing"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/How-are-the-segment-registers-used-in-Linux/","text":"How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? #","title":"How are the segment registers used in Linux"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/How-are-the-segment-registers-used-in-Linux/#how-are-the-segment-registers-fs-gs-cs-ss-ds-es-used-in-linux","text":"","title":"How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Memory-Addresses-note/","text":"A logical address consists of two parts: a segment identifier and an offset that specifies the relative address within the segment. The segment identifier is a 16-bit field called the Segment Selector (see Figure 2-2), while the offset is a 32-bit field. the processor provides segmentation registers whose only purpose is to hold Segment Selectors SUMMARY : \u5728Segment Selector\u4e2d\u6709Request Privilege Level\u5b57\u6bb5\uff0cThe cs register has another important function: it includes a 2-bit field that specifies the Current Privilege Level (CPL) of the CPU. Segment Selector\u7684Request Privilege Level\u5b57\u6bb5\u4e0e cs register \u7684Current Privilege Level (CPL) \u5b57\u6bb5\u76f8\u5bf9\u5e94\uff1b\u53c2\u89c1Table 2-2. Segment Selector fields\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u63cf\u8ff0\uff1a Requestor Privilege Level : specifies the Current Privilege Level of the CPU when the corresponding Segment Selector is loaded into the cs register; it also may be used to selectively weaken the processor privilege level when accessing data segments (see Intel documentation for details).","title":"Memory Addresses note"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Multilevel-Paging-in-Operating-System/","text":"Multilevel Paging in Operating System Multilevel Paging in Operating System # Prerequisite \u2013 Paging Multilevel Paging is a paging scheme which consist of two or more levels of page tables in a hierarchical manner. It is also known as hierarchical paging. The entries of the level 1 page table are pointers to a level 2 page table and entries of the level 2 page tables are pointers to a level 3 page table and so on. The entries of the last level page table are stores actual frame information. Level 1 contain single page table and address of that table is stored in PTBR (Page Table Base Register). Virtual address: In multilevel paging whatever may be levels of paging all the page tables will be stored in main memory.So it requires more than one memory access to get the physical address of page frame. One access for each level needed. Each page table entry except the last level page table entry contains base address of the next level page table. Reference to actual page frame: Reference to PTE in level 1 page table = PTBR value + Level 1 offset present in virtual address. Reference to PTE in level 2 page table = Base address (present in Level 1 PTE) + Level 2 offset (present in VA). Reference to PTE in level 3 page table= Base address (present in Level 2 PTE) + Level 3 offset (present in VA). Actual page frame address = PTE (present in level 3). Generally the page table size will be equal to the size of page. Assumptions: Byte addressable memory, and n is the number of bits used to represent virtual address. Important formula: Number of entries in page table: = (virtual address space size) / (page size) = Number of pages Virtual address space size: = 2^{n} B Size of page table: <>= (number of entries in page table)*(size of PTE) If page table size > desired size then create 1 more level. Disadvantage: Extra memory references to access address translation tables can slow programs down by a factor of two or more. Use translation look aside buffer (TLB) to speed up address translation by storing page table entries. Example: Q. Consider a virtual memory system with physical memory of 8GB, a page size of 8KB and 46 bit virtual address. Assume every page table exactly fits into a single page . If page table entry size is 4B then how many levels of page tables would be required. Explanation: Page size = 8KB = 2^{13} B Virtual address space size = 2^{46} B PTE = 4B = 2^2 B Number of pages or number of entries in page table, = (virtual address space size) / (page size) = 2^{46} B/2^{13} B = 2^{33} Size of page table, = (number of entries in page table)*(size of PTE) = 2^{33} * 2^2 B = 2^{35} B To create one more level, Size of page table > page size Number of page tables in last level, = 2^{35} B / 2^{13} B = 2^{22}","title":"Multilevel Paging in Operating System"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-2-Memory-Addressing/Multilevel-Paging-in-Operating-System/#multilevel-paging-in-operating-system","text":"Prerequisite \u2013 Paging Multilevel Paging is a paging scheme which consist of two or more levels of page tables in a hierarchical manner. It is also known as hierarchical paging. The entries of the level 1 page table are pointers to a level 2 page table and entries of the level 2 page tables are pointers to a level 3 page table and so on. The entries of the last level page table are stores actual frame information. Level 1 contain single page table and address of that table is stored in PTBR (Page Table Base Register). Virtual address: In multilevel paging whatever may be levels of paging all the page tables will be stored in main memory.So it requires more than one memory access to get the physical address of page frame. One access for each level needed. Each page table entry except the last level page table entry contains base address of the next level page table. Reference to actual page frame: Reference to PTE in level 1 page table = PTBR value + Level 1 offset present in virtual address. Reference to PTE in level 2 page table = Base address (present in Level 1 PTE) + Level 2 offset (present in VA). Reference to PTE in level 3 page table= Base address (present in Level 2 PTE) + Level 3 offset (present in VA). Actual page frame address = PTE (present in level 3). Generally the page table size will be equal to the size of page. Assumptions: Byte addressable memory, and n is the number of bits used to represent virtual address. Important formula: Number of entries in page table: = (virtual address space size) / (page size) = Number of pages Virtual address space size: = 2^{n} B Size of page table: <>= (number of entries in page table)*(size of PTE) If page table size > desired size then create 1 more level. Disadvantage: Extra memory references to access address translation tables can slow programs down by a factor of two or more. Use translation look aside buffer (TLB) to speed up address translation by storing page table entries. Example: Q. Consider a virtual memory system with physical memory of 8GB, a page size of 8KB and 46 bit virtual address. Assume every page table exactly fits into a single page . If page table entry size is 4B then how many levels of page tables would be required. Explanation: Page size = 8KB = 2^{13} B Virtual address space size = 2^{46} B PTE = 4B = 2^2 B Number of pages or number of entries in page table, = (virtual address space size) / (page size) = 2^{46} B/2^{13} B = 2^{33} Size of page table, = (number of entries in page table)*(size of PTE) = 2^{33} * 2^2 B = 2^{35} B To create one more level, Size of page table > page size Number of page tables in last level, = 2^{35} B / 2^{13} B = 2^{22}","title":"Multilevel Paging in Operating System"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.1-Processes-and-Lightweight-Processes-and-Threads/","text":"3.1. Processes, Lightweight Processes, and Threads 3.1. Processes, Lightweight Processes, and Threads # The term \"process\" is often used with several different meanings. In this book, we stick to the usual OS textbook definition: a process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process Processes are like human beings: they are generated, they have a more or less significant life, they optionally generate one or more child processes, and eventually they die. A small difference is that sex is not really common among processes each process has just one parent. NOTE: process\u7684\u751f\u547d\u5468\u671f From the kernel's point of view, the purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated. When a process is created, it is almost identical to its parent. It receives a (logical) copy of the parent's address space and executes the same code as the parent, beginning at the next instruction following the process creation system call. Although the parent and child may share the pages containing the program code (text), they have separate copies of the data (stack and heap), so that changes by the child to a memory location are invisible to the parent (and vice versa). While earlier Unix kernels employed this simple model, modern Unix systems do not. They support multithreaded applications user programs having many relatively independent execution flows sharing a large portion of the application data structures. In such systems, a process is composed of several user threads (or simply threads ), each of which represents an execution flow of the process. Nowadays, most multithreaded applications are written using standard sets of library functions called pthread (POSIX thread) libraries . NOTE : \u8fd9\u6bb5\u8bdd\u5bf9\u6bd4\u4e86\u8fc7\u53bbUnix kernel model\u548c\u73b0\u5728Unix kernel model\u3002 \u8fd9\u6bb5\u8bdd\u4e2d\u7684 user threads \u5177\u6709\u7279\u6b8a\u7684\u542b\u4e49\uff0c\u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7406\u89e3\u5b83\u5bf9\u4e8e\u7406\u89e3\u672c\u6bb5\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 Older versions of the Linux kernel offered no support for multithreaded applications. From the kernel point of view, a multithreaded application was just a normal process . The multiple execution flows of a multithreaded application were created, handled, and scheduled entirely in User Mode , usually by means of a POSIX-compliant pthread library. NOTE: \u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u523b\u5206\u6790 However, such an implementation of multithreaded applications is not very satisfactory. For instance, suppose a chess program uses two threads: one of them controls the graphical chessboard, waiting for the moves of the human player and showing the moves of the computer, while the other thread ponders the next move of the game. While the first thread waits for the human move, the second thread should run continuously, thus exploiting the thinking time of the human player. However, if the chess program is just a single process, the first thread cannot simply issue a blocking system call waiting for a user action; otherwise, the second thread is blocked as well. Instead, the first thread must employ sophisticated nonblocking techniques to ensure that the process remains runnable. NOTE : \u663e\u7136Older versions of the Linux kernel offered no support for multithreaded applications. Linux uses lightweight processes to offer better support for multithreaded applications. Basically, two lightweight processes may share some resources, like the address space , the open files , and so on. Whenever one of them modifies a shared resource, the other immediately sees the change. Of course, the two processes must synchronize themselves when accessing the shared resource. A straightforward way to implement multithreaded applications is to associate a lightweight process with each thread . In this way, the threads can access the same set of application data structures by simply sharing the same memory address space, the same set of open files, and so on; at the same time, each thread can be scheduled independently by the kernel so that one may sleep while another remains runnable. Examples of POSIX-compliant pthread libraries that use Linux's lightweight processes are LinuxThreads, Native POSIX Thread Library (NPTL), and IBM's Next Generation Posix Threading Package (NGPT). NOTE: \u5173\u4e8eLinuxThreads\u3001Native POSIX Thread Library (NPTL)\uff0c\u53c2\u89c1 PTHREADS(7) \uff0c\u5176\u4e2d\u7684Linux implementations of POSIX threads\u7ae0\u8282\u63cf\u8ff0\u4e86linux\u7684GNU C library\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\u7684\u7ec6\u8282\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u5982\u4f55\u67e5\u770b\u7cfb\u7edf\u7684 pthread libraries\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u547d\u4ee4\u3002 POSIX-compliant multithreaded applications are best handled by kernels that support \" thread groups .\" In Linux a thread group is basically a set of lightweight processes that implement a multithreaded application and act as a whole with regards to some system calls such as getpid( ) , kill( ) , and _exit( ) . We are going to describe them at length later in this chapter. NOTE : \u65b0\u7248\u672c\u7684linux\u4f7f\u7528 lightweight process \u6765\u5b9e\u73b0thread\uff1b\u4f7f\u7528 thread group \u6765\u4f5c\u4e3aprocess\uff1b \u5176\u5b9elinux kernel\u63d0\u4f9b\u4e86\u975e\u5e38\u7075\u6d3b\u7684 CLONE(2) system call\u6765\u8ba9\u7528\u6237\u7075\u6d3b\u5730\u521b\u5efaprocess\u6216thread\u3002","title":"3.1-Processes-and-Lightweight-Processes-and-Threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.1-Processes-and-Lightweight-Processes-and-Threads/#31-processes-lightweight-processes-and-threads","text":"The term \"process\" is often used with several different meanings. In this book, we stick to the usual OS textbook definition: a process is an instance of a program in execution. You might think of it as the collection of data structures that fully describes how far the execution of the program has progressed. NOTE: \u672c\u8282\u4e2d\u7684process\u6307\u7684\u662f\u6807\u51c6 Process Processes are like human beings: they are generated, they have a more or less significant life, they optionally generate one or more child processes, and eventually they die. A small difference is that sex is not really common among processes each process has just one parent. NOTE: process\u7684\u751f\u547d\u5468\u671f From the kernel's point of view, the purpose of a process is to act as an entity to which system resources (CPU time, memory, etc.) are allocated. When a process is created, it is almost identical to its parent. It receives a (logical) copy of the parent's address space and executes the same code as the parent, beginning at the next instruction following the process creation system call. Although the parent and child may share the pages containing the program code (text), they have separate copies of the data (stack and heap), so that changes by the child to a memory location are invisible to the parent (and vice versa). While earlier Unix kernels employed this simple model, modern Unix systems do not. They support multithreaded applications user programs having many relatively independent execution flows sharing a large portion of the application data structures. In such systems, a process is composed of several user threads (or simply threads ), each of which represents an execution flow of the process. Nowadays, most multithreaded applications are written using standard sets of library functions called pthread (POSIX thread) libraries . NOTE : \u8fd9\u6bb5\u8bdd\u5bf9\u6bd4\u4e86\u8fc7\u53bbUnix kernel model\u548c\u73b0\u5728Unix kernel model\u3002 \u8fd9\u6bb5\u8bdd\u4e2d\u7684 user threads \u5177\u6709\u7279\u6b8a\u7684\u542b\u4e49\uff0c\u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7406\u89e3\u5b83\u5bf9\u4e8e\u7406\u89e3\u672c\u6bb5\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 Older versions of the Linux kernel offered no support for multithreaded applications. From the kernel point of view, a multithreaded application was just a normal process . The multiple execution flows of a multithreaded application were created, handled, and scheduled entirely in User Mode , usually by means of a POSIX-compliant pthread library. NOTE: \u5728\u300a VS-process-VS-thread-VS-lightweight process.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u6df1\u523b\u5206\u6790 However, such an implementation of multithreaded applications is not very satisfactory. For instance, suppose a chess program uses two threads: one of them controls the graphical chessboard, waiting for the moves of the human player and showing the moves of the computer, while the other thread ponders the next move of the game. While the first thread waits for the human move, the second thread should run continuously, thus exploiting the thinking time of the human player. However, if the chess program is just a single process, the first thread cannot simply issue a blocking system call waiting for a user action; otherwise, the second thread is blocked as well. Instead, the first thread must employ sophisticated nonblocking techniques to ensure that the process remains runnable. NOTE : \u663e\u7136Older versions of the Linux kernel offered no support for multithreaded applications. Linux uses lightweight processes to offer better support for multithreaded applications. Basically, two lightweight processes may share some resources, like the address space , the open files , and so on. Whenever one of them modifies a shared resource, the other immediately sees the change. Of course, the two processes must synchronize themselves when accessing the shared resource. A straightforward way to implement multithreaded applications is to associate a lightweight process with each thread . In this way, the threads can access the same set of application data structures by simply sharing the same memory address space, the same set of open files, and so on; at the same time, each thread can be scheduled independently by the kernel so that one may sleep while another remains runnable. Examples of POSIX-compliant pthread libraries that use Linux's lightweight processes are LinuxThreads, Native POSIX Thread Library (NPTL), and IBM's Next Generation Posix Threading Package (NGPT). NOTE: \u5173\u4e8eLinuxThreads\u3001Native POSIX Thread Library (NPTL)\uff0c\u53c2\u89c1 PTHREADS(7) \uff0c\u5176\u4e2d\u7684Linux implementations of POSIX threads\u7ae0\u8282\u63cf\u8ff0\u4e86linux\u7684GNU C library\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\u7684\u7ec6\u8282\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u5982\u4f55\u67e5\u770b\u7cfb\u7edf\u7684 pthread libraries\u7684\u5b9e\u73b0\u65b9\u5f0f\u7684\u547d\u4ee4\u3002 POSIX-compliant multithreaded applications are best handled by kernels that support \" thread groups .\" In Linux a thread group is basically a set of lightweight processes that implement a multithreaded application and act as a whole with regards to some system calls such as getpid( ) , kill( ) , and _exit( ) . We are going to describe them at length later in this chapter. NOTE : \u65b0\u7248\u672c\u7684linux\u4f7f\u7528 lightweight process \u6765\u5b9e\u73b0thread\uff1b\u4f7f\u7528 thread group \u6765\u4f5c\u4e3aprocess\uff1b \u5176\u5b9elinux kernel\u63d0\u4f9b\u4e86\u975e\u5e38\u7075\u6d3b\u7684 CLONE(2) system call\u6765\u8ba9\u7528\u6237\u7075\u6d3b\u5730\u521b\u5efaprocess\u6216thread\u3002","title":"3.1. Processes, Lightweight Processes, and Threads"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/","text":"3.2. Process Descriptor # To manage processes, the kernel must have a clear picture of what each process is doing. It must know, for instance, the process's priority\uff08\u8c03\u5ea6\u7684\u4f18\u5148\u7ea7\uff09, whether it is running on a CPU or blocked on an event, what address space has been assigned to it, which files it is allowed to address, and so on. This is the role of the process descriptor a task_struct type structure whose fields contain all the information related to a single process . [ * ] As the repository of so much information, the process descriptor is rather complex. In addition to a large number of fields containing process attributes, the process descriptor contains several pointers to other data structures that, in turn, contain pointers to other structures. Figure 3-1 describes the Linux process descriptor schematically\uff08\u6309\u7167\u56fe\u5f0f\uff09. [ * ] The kernel also defines the task_t data type to be equivalent to struct task_struct NOTE : task_struct \u7684\u6e90\u4ee3\u7801\u53c2\u89c1\uff1a https://elixir.bootlin.com/linux/latest/ident/task_struct https://github.com/torvalds/linux/blob/master/include/linux/sched.h \u5173\u4e8e task_struct \u548cprocess\uff0cthread\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728Chapter 3. Processes\u4e2d\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u5206\u6790\uff1b\u6b64\u5904\u4e0d\u518d\u8d58\u8ff0\uff1b The six data structures on the right side of the figure refer to specific resources owned by the process. Most of these resources will be covered in future chapters. This chapter focuses on two types of fields that refer to the process state and to process parent/child relationships . Figure 3-1. The Linux process descriptor 3.2.1. Process State # As its name implies, the state field of the process descriptor describes what is currently happening to the process. It consists of an array of flags, each of which describes a possible process state. In the current Linux version, these states are mutually exclusive, and hence exactly one flag of state always is set; the remaining flags are cleared. The following are the possible process states: TASK_RUNNING # The process is either executing on a CPU or waiting to be executed. TASK_INTERRUPTIBLE # The process is suspended ( sleeping ) until some condition becomes true. Raising a hardware interrupt, releasing a system resource the process is waiting for, or delivering a signal are examples of conditions that might wake up the process (put its state back to TASK_RUNNING ). NOTE \uff1a sleeping \u76f8\u5f53\u4e8esuspended TASK_UNINTERRUPTIBLE # Like TASK_INTERRUPTIBLE , except that delivering a signal to the sleeping process leaves its state unchanged. This process state is seldom used. It is valuable, however, under certain specific conditions in which a process must wait until a given event occurs without being interrupted. For instance, this state may be used when a process opens a device file and the corresponding device driver starts probing for a corresponding hardware device . The device driver must not be interrupted until the probing is complete, or the hardware device could be left in an unpredictable state. TASK_STOPPED # Process execution has been stopped; the process enters this state after receiving a SIGSTOP , SIGTSTP , SIGTTIN , or SIGTTOU signal. TASK_TRACED # Process execution has been stopped by a debugger. When a process is being monitored by another (such as when a debugger executes a ptrace( ) system call to monitor a test program), each signal may put the process in the TASK_TRACED state. Two additional states of the process can be stored both in the state field and in the exit_state field of the process descriptor; as the field name suggests, a process reaches one of these two states only when its execution is terminated: EXIT_ZOMBIE # Process execution is terminated, but the parent process has not yet issued a wait4( ) or waitpid( ) system call to return information about the dead process. [*] Before the wait( )-like call is issued, the kernel cannot discard the data contained in the dead process descriptor because the parent might need it. (See the section \"Process Removal\" near the end of this chapter.) [*] There are other wait( )-like library functions, such as wait3( ) and wait( ) , but in Linux they are implemented by means of the wait4( ) and waitpid( ) system calls. EXIT_DEAD # The final state: the process is being removed by the system because the parent process has just issued a wait4( ) or waitpid( ) system call for it. Changing its state from EXIT_ZOMBIE to EXIT_DEAD avoids race conditions due to other threads of execution that execute wait( ) -like calls on the same process (see Chapter 5). The value of the state field is usually set with a simple assignment. For instance: p->state = TASK_RUNNING; The kernel also uses the set_task_state and set_current_state macros: they set the state of a specified process and of the process currently executed, respectively. Moreover, these macros ensure that the assignment operation is not mixed with other instructions by the compiler or the CPU control unit . Mixing the instruction order may sometimes lead to catastrophic results (see Chapter 5). NOTE: process state \u8865\u5145\u5185\u5bb9\uff1a https://lwn.net/Articles/288056/","title":"3.2-Process-Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#32-process-descriptor","text":"To manage processes, the kernel must have a clear picture of what each process is doing. It must know, for instance, the process's priority\uff08\u8c03\u5ea6\u7684\u4f18\u5148\u7ea7\uff09, whether it is running on a CPU or blocked on an event, what address space has been assigned to it, which files it is allowed to address, and so on. This is the role of the process descriptor a task_struct type structure whose fields contain all the information related to a single process . [ * ] As the repository of so much information, the process descriptor is rather complex. In addition to a large number of fields containing process attributes, the process descriptor contains several pointers to other data structures that, in turn, contain pointers to other structures. Figure 3-1 describes the Linux process descriptor schematically\uff08\u6309\u7167\u56fe\u5f0f\uff09. [ * ] The kernel also defines the task_t data type to be equivalent to struct task_struct NOTE : task_struct \u7684\u6e90\u4ee3\u7801\u53c2\u89c1\uff1a https://elixir.bootlin.com/linux/latest/ident/task_struct https://github.com/torvalds/linux/blob/master/include/linux/sched.h \u5173\u4e8e task_struct \u548cprocess\uff0cthread\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728Chapter 3. Processes\u4e2d\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u5206\u6790\uff1b\u6b64\u5904\u4e0d\u518d\u8d58\u8ff0\uff1b The six data structures on the right side of the figure refer to specific resources owned by the process. Most of these resources will be covered in future chapters. This chapter focuses on two types of fields that refer to the process state and to process parent/child relationships . Figure 3-1. The Linux process descriptor","title":"3.2. Process Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#321-process-state","text":"As its name implies, the state field of the process descriptor describes what is currently happening to the process. It consists of an array of flags, each of which describes a possible process state. In the current Linux version, these states are mutually exclusive, and hence exactly one flag of state always is set; the remaining flags are cleared. The following are the possible process states:","title":"3.2.1. Process State"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#task_running","text":"The process is either executing on a CPU or waiting to be executed.","title":"TASK_RUNNING"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#task_interruptible","text":"The process is suspended ( sleeping ) until some condition becomes true. Raising a hardware interrupt, releasing a system resource the process is waiting for, or delivering a signal are examples of conditions that might wake up the process (put its state back to TASK_RUNNING ). NOTE \uff1a sleeping \u76f8\u5f53\u4e8esuspended","title":"TASK_INTERRUPTIBLE"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#task_uninterruptible","text":"Like TASK_INTERRUPTIBLE , except that delivering a signal to the sleeping process leaves its state unchanged. This process state is seldom used. It is valuable, however, under certain specific conditions in which a process must wait until a given event occurs without being interrupted. For instance, this state may be used when a process opens a device file and the corresponding device driver starts probing for a corresponding hardware device . The device driver must not be interrupted until the probing is complete, or the hardware device could be left in an unpredictable state.","title":"TASK_UNINTERRUPTIBLE"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#task_stopped","text":"Process execution has been stopped; the process enters this state after receiving a SIGSTOP , SIGTSTP , SIGTTIN , or SIGTTOU signal.","title":"TASK_STOPPED"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#task_traced","text":"Process execution has been stopped by a debugger. When a process is being monitored by another (such as when a debugger executes a ptrace( ) system call to monitor a test program), each signal may put the process in the TASK_TRACED state. Two additional states of the process can be stored both in the state field and in the exit_state field of the process descriptor; as the field name suggests, a process reaches one of these two states only when its execution is terminated:","title":"TASK_TRACED"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#exit_zombie","text":"Process execution is terminated, but the parent process has not yet issued a wait4( ) or waitpid( ) system call to return information about the dead process. [*] Before the wait( )-like call is issued, the kernel cannot discard the data contained in the dead process descriptor because the parent might need it. (See the section \"Process Removal\" near the end of this chapter.) [*] There are other wait( )-like library functions, such as wait3( ) and wait( ) , but in Linux they are implemented by means of the wait4( ) and waitpid( ) system calls.","title":"EXIT_ZOMBIE"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2-Process-Descriptor/#exit_dead","text":"The final state: the process is being removed by the system because the parent process has just issued a wait4( ) or waitpid( ) system call for it. Changing its state from EXIT_ZOMBIE to EXIT_DEAD avoids race conditions due to other threads of execution that execute wait( ) -like calls on the same process (see Chapter 5). The value of the state field is usually set with a simple assignment. For instance: p->state = TASK_RUNNING; The kernel also uses the set_task_state and set_current_state macros: they set the state of a specified process and of the process currently executed, respectively. Moreover, these macros ensure that the assignment operation is not mixed with other instructions by the compiler or the CPU control unit . Mixing the instruction order may sometimes lead to catastrophic results (see Chapter 5). NOTE: process state \u8865\u5145\u5185\u5bb9\uff1a https://lwn.net/Articles/288056/","title":"EXIT_DEAD"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/","text":"3.2.2. Identifying a Process 3.2.2.1. Process descriptors handling 3.2.2.2. Identifying the current process 3.2.2.3. Doubly linked lists 3.2.2.4. The process list 3.2.2.5. The lists of TASK_RUNNING processes 3.2.2. Identifying a Process # As a general rule, each execution context that can be independently scheduled must have its own process descriptor ; therefore, even lightweight processes , which share a large portion of their kernel data structures, have their own task_struct structures. SUMMARY : lightweight processes \u662fthread\uff1b\u9009\u7528\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u8981\u5c06 process descriptor \u548cprocess ID\u6df7\u6dc6\uff1b The strict one-to-one correspondence between the process and process descriptor makes the 32-bit address [ ] of the task_struct structure a useful means for the kernel to identify processes. These addresses are referred to as process descriptor pointers . Most of the references to processes that the kernel makes are through process descriptor pointers . [ ] As already noted in the section \"Segmentation in Linux\" in Chapter 2, although technically these 32 bits are only the offset component of a logical address, they coincide with the linear address On the other hand, Unix-like operating systems allow users to identify processes by means of a number called the Process ID (or PID ), which is stored in the pid field of the process descriptor . PIDs are numbered sequentially: the PID of a newly created process is normally the PID of the previously created process increased by one. Of course, there is an upper limit on the PID values; when the kernel reaches such limit, it must start recycling the lower, unused PIDs. By default, the maximum PID number is 32,767 ( PID_MAX_DEFAULT - 1 ); the system administrator may reduce this limit by writing a smaller value into the /proc/sys/kernel/pid_max file (/proc is the mount point of a special filesystem, see the section \"Special Filesystems\" in Chapter 12). In 64-bit architectures, the system administrator can enlarge the maximum PID number up to 4,194,303. When recycling PID numbers, the kernel must manage a pidmap_array bitmap that denotes which are the PIDs currently assigned and which are the free ones. Because a page frame contains 32,768\uff08 4K \uff09 bits, in 32-bit architectures the pidmap_array bitmap is stored in a single page. In 64-bit architectures, however, additional pages can be added to the bitmap when the kernel assigns a PID number too large for the current bitmap size. These pages are never released. SUMMARY : \u5173\u4e8e pidmap_array \uff0c\u53c2\u89c1 https://elixir.bootlin.com/linux/v2.6.17.7/source/kernel/pid.c#L46 https://blog.csdn.net/Jay14/article/details/54863073 Linux associates a different PID with each process or lightweight process in the system. (As we shall see later in this chapter, there is a tiny exception on multiprocessor systems.) This approach allows the maximum flexibility, because every execution context in the system can be uniquely identified. On the other hand, Unix programmers expect threads in the same group to have a common PID . For instance, it should be possible to a send a signal specifying a PID that affects all threads in the group. In fact, the POSIX 1003.1c standard states that all threads of a multithreaded application must have the same PID . To comply with this standard, Linux makes use of thread groups . The identifier shared by the threads is the PID of the thread group leader , that is, the PID of the first lightweight process in the group; it is stored in the tgid field of the process descriptors . The getpid( ) system call returns the value of tgid relative to the current process instead of the value of pid , so all the threads of a multithreaded application share the same identifier. Most processes belong to a thread group consisting of a single member; as thread group leaders, they have the tgid field equal to the pid field, thus the getpid( ) system call works as usual for this kind of process. SUMMARY : \u5728\u540c\u4e00\u4e2a thread group \u4e2d\u7684\u6bcf\u4e2a task_struct \u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684 pid \uff0c\u5b83\u4eec\u5171\u7528\u540c\u4e00\u4e2a tgid \u3002 SUMMARY : \u901a\u8fc7 getpid( ) \u83b7\u5f97\u5230\u7684\u503c\u662f tgid \u5b57\u6bb5\uff0c\u800c\u4e0d\u662f pid \u5b57\u6bb5\uff1b\u8fd9\u548c\u6211\u4eec\u7684\u76f4\u89c2\u7406\u89e3\u662f\u76f8\u6096\u7684\uff1b Later, we'll show you how it is possible to derive a true process descriptor pointer efficiently from its respective PID . Efficiency is important because many system calls such as kill( ) use the PID to denote the affected process. 3.2.2.1. Process descriptors handling # Processes are dynamic entities whose lifetimes range from a few milliseconds to months. Thus, the kernel must be able to handle many processes at the same time, and process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel. For each process, Linux packs two different data structures in a single per-process memory area: a small data structure linked to the process descriptor , namely the thread_info structure, and the Kernel Mode process stack . The length of this memory area is usually 8,192 bytes (two page frames). For reasons of efficiency the kernel stores the 8-KB memory area in two consecutive page frames with the first page frame aligned to a multiple of $2^{13}$ ; this may turn out to be a problem when little dynamic memory is available, because the free memory may become highly fragmented (see the section \"The Buddy System Algorithm\" in Chapter 8). Therefore, in the 80x86 architecture the kernel can be configured at compilation time so that the memory area including stack and thread_info structure spans a single page frame (4,096 bytes). SUMMARY : 1.6.3. Reentrant Kernels\u4e2d Kernel Mode process stack \u662f\u4e3akernel control path\u800c\u51c6\u5907\u7684\uff0ckernel control path\u7684\u6267\u884c\u662fReentrant\u7684\uff0c\u5e94\u8be5\u4e0d\u9700\u8981\u592a\u591a\u7684space\uff1b SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u4ecb\u7ecd\u7684 process descriptors \u7684\u5b58\u50a8\u4f4d\u7f6e\u975e\u5e38\u91cd\u8981\uff1a process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel;\u8fd8\u6709\u5c31\u662f thread_union \u7684\u5b58\u50a8\u4f4d\u7f6e\uff1a per-process memory area \uff1b\u9700\u8981\u6ce8\u610fper-process\u7684\u542b\u4e49\uff1b SUMMARY : https://elixir.bootlin.com/linux/v2.6.11/source/include/linux/sched.h union thread_union { struct thread_info thread_info; unsigned long stack[THREAD_SIZE/sizeof(long)]; }; SUMMARY : thread_union \u5c31\u662f\u4e0a\u8ff0\u7684memory area\uff1b In the section \"Segmentation in Linux\" in Chapter 2, we learned that a process in Kernel Mode accesses a stack contained in the kernel data segment , which is different from the stack used by the process in User Mode . Because kernel control paths make little use of the stack, only a few thousand bytes of kernel stack are required. Therefore, 8 KB is ample space for the stack and the thread_info structure. However, when stack and thread_info structure are contained in a single page frame, the kernel uses a few additional stacks to avoid the overflows caused by deeply nested interrupts and exceptions (see Chapter 4). SUMMARY : \u6839\u636e\u4e0a\u4e0a\u4e00\u6bb5\u4e2d\u63d0\u53ca\u7684\u5185\u5bb9\u53ef\u4ee5\u63a8\u6d4b\uff1a thread_union \u662f\u4fdd\u5b58\u5728per-process memory area\uff0c\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\uff1a Kernel Mode process stack \u4e5f\u4fdd\u5b58\u5728per-process memory area\u4e2d\uff1b\u800c\u8fd9\u4e00\u6bb5\u4e2d\u53c8\u63d0\u53ca\uff1aa process in Kernel Mode accesses a stack contained in the kernel data segment \uff1b\u90a3kernel data segment\u662f\u5b58\u653e\u5728\u4f55\u5904\u5462\uff1f\u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u8fdb\u884c\u4e86Google\uff1a is kernel data segment in process address space \uff1b\u76ee\u524d\u6240\u6709\u7684\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u5185\u5bb9\u90fd\u5728\u300a virtual-memory-address-space-thinking.md \u300b\u4e2d\uff1b\u5728\u9605\u8bfb\u8fd9\u4e00\u6bb5\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u5173\u952e\u70b9\u662f\u8981\u77e5\u9053\u672c\u4e66\u7684\u57fa\u4e8e i386 \u67b6\u6784\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u5728 i386 \u4e2d\uff0c\u4f7f\u7528\u4e86segmentation\uff0c\u4f46\u662f\u5728\u540e\u6765\u8fd9\u79cd\u65b9\u5f0f\u88ab\u53d6\u4ee3\u4e86\uff1b\u6240\u4ee5\u5f88\u591a\u67b6\u6784\u4e2d\u538b\u6839\u53ef\u80fd\u5c31\u6ca1\u6709 Kernel Mode stack contained in the kernel data segment \u7684\u8fd9\u79cd\u7ed3\u6784\uff1b\u5982\u5728 How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? \u4e2d\u6240\u8ff0\u7684\uff1b Figure 3-2 shows how the two data structures are stored in the 2-page (8 KB) memory area. The thread_info structure resides at the beginning of the memory area, and the stack grows downward from the end. The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and thread_info , respectively. SUMMARY \uff1a \u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u9700\u8981\u641e\u6e05\u695a struct thread_info \u7684\u5b9a\u4e49\uff0c\u4e00\u4e0b\u662f i386\u7684 struct thread_info struct thread_info { struct task_struct *task; /* main task structure */ struct exec_domain *exec_domain; /* execution domain */ unsigned long flags; /* low level flags */ unsigned long status; /* thread-synchronous flags */ __u32 cpu; /* current CPU */ __s32 preempt_count; /* 0 => preemptable, <0 => BUG */ mm_segment_t addr_limit; /* thread address space: 0-0xBFFFFFFF for user-thead 0-0xFFFFFFFF for kernel-thread */ struct restart_block restart_block; unsigned long previous_esp; /* ESP of the previous stack in case of nested (IRQ) stacks */ __u8 supervisor_stack[0]; }; \u53ef\u4ee5\u770b\u5230 struct thread_info \u6709\u6210\u5458\u53d8\u91cf struct task_struct *task \uff0c\u800c\u5728 struct task_struct \u4e2d\uff0c\u6709\u6210\u5458\u53d8\u91cf struct thread_info *thread_info; \uff0c\u8fd9\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u6240\u63cf\u8ff0\u7684\uff1a The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and tHRead_info , respectively. The esp register is the CPU stack pointer, which is used to address the stack's top location. On 80x86 systems, the stack starts at the end and grows toward the beginning of the memory area. Right after switching from User Mode to Kernel Mode , the kernel stack of a process is always empty, and therefore the esp register points to the byte immediately following the stack. The value of the esp is decreased as soon as data is written into the stack. Because the thread_info structure is 52 bytes long, the kernel stack can expand up to 8,140 bytes. The C language allows the thread_info structure and the kernel stack of a process to be conveniently represented by means of the following union construct: union thread_union { struct thread_info thread_info; unsigned long stack[2048]; /* 1024 for 4KB stacks */ }; The thread_info structure shown in Figure 3-2 is stored starting at address 0x015fa000 , and the stack is stored starting at address 0x015fc000 . The value of the esp register points to the current top of the stack at 0x015fa878 . The kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a thread_info structure and a kernel stack . 3.2.2.2. Identifying the current process # The close association between the thread_info structure and the Kernel Mode stack just described offers a key benefit in terms of efficiency: the kernel can easily obtain the address of the thread_info structure of the process currently running on a CPU from the value of the esp register. In fact, if the thread_union structure is 8 KB ($2^{13}$ bytes) long, the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure; on the other hand, if the thread_union structure is 4 KB long, the kernel masks out the 12 least significant bits of esp . This is done by the current_thread_info( ) function, which produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl %ecx,p After executing these three instructions, p contains the thread_info structure pointer of the process running on the CPU that executes the instruction. Most often the kernel needs the address of the process descriptor rather than the address of the thread_info structure. To get the process descriptor pointer of the process currently running on a CPU, the kernel makes use of the current macro, which is essentially equivalent to current_thread_info( )->task and produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl (%ecx),p Because the task field is at offset 0 in the thread_info structure, after executing these three instructions p contains the process descriptor pointer of the process running on the CPU. The current macro often appears in kernel code as a prefix to fields of the process descriptor . For example, current->pid returns the process ID of the process currently running on the CPU. Another advantage of storing the process descriptor with the stack emerges on multiprocessor systems: the correct current process for each hardware processor can be derived just by checking the stack, as shown previously. Earlier versions of Linux did not store the kernel stack and the process descriptor together. Instead, they were forced to introduce a global static variable called current to identify the process descriptor of the running process. On multiprocessor systems, it was necessary to define current as an array one element for each available CPU. SUMMARY : \u6240\u6709\u7684 process descriptor \u548c kernel stack \u90fd\u662f\u4f4d\u4e8ekernel\u4e2d\uff1b\u7531kernel\u6765\u6267\u884c\u8c03\u5ea6\uff1b\u5f53CPU\u9700\u8981\u6267\u884c\u67d0\u4e2a process descriptor \u7684\u65f6\u5019\uff0c\u5b83\u9700\u8981\u8bfb\u53d6\u8fd9\u4e2a process descriptor \u7684\u4e00\u4e9b\u6570\u636e\uff0c\u6bd4\u5982\u4e4b\u524d\u4fdd\u5b58\u7684register\u6570\u636e\u7b49\u4ee5\u4fbfresume\uff1b\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u5e76\u4e14\uff0c\u4ece\u524d\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u6bcf\u4e2a thread_union \u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 kernel stack \uff0c\u800c\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u6240\u4ee5CPU\u662f\u5728\u67d0\u4e2a`thread_union\u7684 kernel stack \u4e2d\u6267\u884c\uff0c\u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684 process descriptor \uff1b SUMMARY : \u56e0\u4e3ascheduler\u5728\u8c03\u5ea6\u4e00\u4e2atask\u5f00\u59cb\u8fd0\u884c\u4e4b\u524d\u4f1a\u5c06\u8fd9\u4e2atask\u7684\u6240\u6709\u7684register\u90fd\u6062\u590d\u5230CPU\u4e2d\uff0c\u6240\u4ee5\u5fc5\u7136\u4f1a\u5305\u542b esp \uff0c\u6240\u4ee5\u5b83\u5c31\u53ef\u4ee5\u6839\u636e esp \u5feb\u901f\u5730\u5b9a\u4f4d\u5230process descriptor\uff1b 3.2.2.3. Doubly linked lists # Before moving on and describing how the kernel keeps track of the various processes in the system, we would like to emphasize the role of special data structures that implement doubly linked lists. For each list, a set of primitive operations must be implemented: initializing the list, inserting and deleting an element, scanning the list, and so on. It would be both a waste of programmers' efforts and a waste of memory to replicate the primitive operations for each different list. Therefore, the Linux kernel defines the list_head data structure, whose only fields next and prev represent the forward and back pointers of a generic doubly linked list element, respectively. It is important to note, however, that the pointers in a list_head field store the addresses of other list_head fields rather than the addresses of the whole data structures in which the list_head structure is included; see Figure 3-3 (a). SUMMARY : list_head A new list is created by using the LIST_HEAD(list_name) macro. It declares a new variable named list_name of type list_head , which is a dummy first element that acts as a placeholder for the head of the new list, and initializes the prev and next fields of the list_head data structure so as to point to the list_name variable itself; see Figure 3-3 (b). Several functions and macros implement the primitives, including those shown in Table Table 3-1. The Linux kernel 2.6 sports another kind of doubly linked list, which mainly differs from a list_head list because it is not circular; it is mainly used for hash tables , where space is important, and finding the the last element in constant time is not. The list head is stored in an hlist_head data structure, which is simply a pointer to the first element in the list ( NULL if the list is empty). Each element is represented by an hlist_node data structure, which includes a pointer next to the next element, and a pointer pprev to the next field of the previous element. Because the list is not circular, the pprev field of the first element and the next field of the last element are set to NULL . The list can be handled by means of several helper functions and macros similar to those listed in Table 3-1: hlist_add_head( ) , hlist_del( ) , hlist_empty( ) , hlist_entry , hlist_for_each_entry , and so on. 3.2.2.4. The process list # The first example of a doubly linked list we will examine is the process list , a list that links together all existing process descriptors. Each task_struct structure includes a tasks field of type list_head whose prev and next fields point, respectively, to the previous and to the next task_struct element. The head of the process list is the init_task task_struct descriptor; it is the process descriptor of the so-called process 0 or swapper (see the section \"Kernel Threads\" later in this chapter). The tasks->prev field of init_task points to the tasks field of the process descriptor inserted last in the list. The SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process list , respectively. These macros also take care of the parenthood relationship of the process (see the section \"How Processes Are Organized\" later in this chapter). Another useful macro, called for_each_process , scans the whole process list. It is defined as: #define for_each_process(p) \\ for (p=&init_task; (p=list_entry((p)->tasks.next, \\ struct task_struct, tasks) \\ ) != &init_task; ) The macro is the loop control statement after which the kernel programmer supplies the loop. Notice how the init_task process descriptor just plays the role of list header . The macro starts by moving past init_task to the next task and continues until it reaches init_task again (thanks to the circularity of the list). At each iteration, the variable passed as the argument of the macro contains the address of the currently scanned process descriptor, as returned by the list_entry macro. SUMMARY : \u5728multiprocessor\u4e2d\uff0c\u662f\u5426\u662f\u6bcf\u4e2aprocessor\u90fd\u6709\u4e00\u4e2aprocess list\uff0c\u8fd8\u662f\u8bf4\u6240\u6709\u7684process descriptor\u90fd\u653e\u5728\u4e00\u4e2aprocess list\u4e2d\uff1f 3.2.2.5. The lists of TASK_RUNNING processes # When looking for a new process to run on a CPU, the kernel has to consider only the runnable processes (that is, the processes in the TASK_RUNNING state). Earlier Linux versions put all runnable processes in the same list called runqueue . Because it would be too costly to maintain the list ordered according to process priorities, the earlier schedulers were compelled to scan the whole list in order to select the \"best\" runnable process. Linux 2.6 implements the runqueue differently. The aim is to allow the scheduler to select the best runnable process in constant time, independently of the number of runnable processes. We'll defer to Chapter 7 a detailed description of this new kind of runqueue , and we'll provide here only some basic information. The trick used to achieve the scheduler speedup consists of splitting the runqueue in many lists of runnable processes, one list per process priority . Each task_struct descriptor includes a run_list field of type list_head . If the process priority is equal to k (a value ranging between 0 and 139), the run_list field links the process descriptor into the list of runnable processes having priority k . Furthermore, on a multiprocessor system, each CPU has its own runqueue , that is, its own set of lists of processes. This is a classic example of making a data structures more complex to improve performance: to make scheduler operations more efficient, the runqueue list has been split into 140 different lists! As we'll see, the kernel must preserve a lot of data for every runqueue in the system; however, the main data structures of a runqueue are the lists of process descriptors belonging to the runqueue ; all these lists are implemented by a single prio_array_t data structure, whose fields are shown in Table 3-2. SUMMARY : prio_array The enqueue_task(p,array) function inserts a process descriptor into a runqueue list; its code is essentially equivalent to: list_add_tail(&p->run_list, &array->queue[p->prio]); __set_bit(p->prio, array->bitmap); array->nr_active++; p->array = array; The prio field of the process descriptor stores the dynamic priority of the process, while the array field is a pointer to the prio_array_t data structure of its current runqueue . Similarly, the dequeue_task(p,array) function removes a process descriptor from a runqueue list.","title":"3.2.2-Identifying-a-Process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#322-identifying-a-process","text":"As a general rule, each execution context that can be independently scheduled must have its own process descriptor ; therefore, even lightweight processes , which share a large portion of their kernel data structures, have their own task_struct structures. SUMMARY : lightweight processes \u662fthread\uff1b\u9009\u7528\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u8981\u5c06 process descriptor \u548cprocess ID\u6df7\u6dc6\uff1b The strict one-to-one correspondence between the process and process descriptor makes the 32-bit address [ ] of the task_struct structure a useful means for the kernel to identify processes. These addresses are referred to as process descriptor pointers . Most of the references to processes that the kernel makes are through process descriptor pointers . [ ] As already noted in the section \"Segmentation in Linux\" in Chapter 2, although technically these 32 bits are only the offset component of a logical address, they coincide with the linear address On the other hand, Unix-like operating systems allow users to identify processes by means of a number called the Process ID (or PID ), which is stored in the pid field of the process descriptor . PIDs are numbered sequentially: the PID of a newly created process is normally the PID of the previously created process increased by one. Of course, there is an upper limit on the PID values; when the kernel reaches such limit, it must start recycling the lower, unused PIDs. By default, the maximum PID number is 32,767 ( PID_MAX_DEFAULT - 1 ); the system administrator may reduce this limit by writing a smaller value into the /proc/sys/kernel/pid_max file (/proc is the mount point of a special filesystem, see the section \"Special Filesystems\" in Chapter 12). In 64-bit architectures, the system administrator can enlarge the maximum PID number up to 4,194,303. When recycling PID numbers, the kernel must manage a pidmap_array bitmap that denotes which are the PIDs currently assigned and which are the free ones. Because a page frame contains 32,768\uff08 4K \uff09 bits, in 32-bit architectures the pidmap_array bitmap is stored in a single page. In 64-bit architectures, however, additional pages can be added to the bitmap when the kernel assigns a PID number too large for the current bitmap size. These pages are never released. SUMMARY : \u5173\u4e8e pidmap_array \uff0c\u53c2\u89c1 https://elixir.bootlin.com/linux/v2.6.17.7/source/kernel/pid.c#L46 https://blog.csdn.net/Jay14/article/details/54863073 Linux associates a different PID with each process or lightweight process in the system. (As we shall see later in this chapter, there is a tiny exception on multiprocessor systems.) This approach allows the maximum flexibility, because every execution context in the system can be uniquely identified. On the other hand, Unix programmers expect threads in the same group to have a common PID . For instance, it should be possible to a send a signal specifying a PID that affects all threads in the group. In fact, the POSIX 1003.1c standard states that all threads of a multithreaded application must have the same PID . To comply with this standard, Linux makes use of thread groups . The identifier shared by the threads is the PID of the thread group leader , that is, the PID of the first lightweight process in the group; it is stored in the tgid field of the process descriptors . The getpid( ) system call returns the value of tgid relative to the current process instead of the value of pid , so all the threads of a multithreaded application share the same identifier. Most processes belong to a thread group consisting of a single member; as thread group leaders, they have the tgid field equal to the pid field, thus the getpid( ) system call works as usual for this kind of process. SUMMARY : \u5728\u540c\u4e00\u4e2a thread group \u4e2d\u7684\u6bcf\u4e2a task_struct \u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684 pid \uff0c\u5b83\u4eec\u5171\u7528\u540c\u4e00\u4e2a tgid \u3002 SUMMARY : \u901a\u8fc7 getpid( ) \u83b7\u5f97\u5230\u7684\u503c\u662f tgid \u5b57\u6bb5\uff0c\u800c\u4e0d\u662f pid \u5b57\u6bb5\uff1b\u8fd9\u548c\u6211\u4eec\u7684\u76f4\u89c2\u7406\u89e3\u662f\u76f8\u6096\u7684\uff1b Later, we'll show you how it is possible to derive a true process descriptor pointer efficiently from its respective PID . Efficiency is important because many system calls such as kill( ) use the PID to denote the affected process.","title":"3.2.2. Identifying a Process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#3221-process-descriptors-handling","text":"Processes are dynamic entities whose lifetimes range from a few milliseconds to months. Thus, the kernel must be able to handle many processes at the same time, and process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel. For each process, Linux packs two different data structures in a single per-process memory area: a small data structure linked to the process descriptor , namely the thread_info structure, and the Kernel Mode process stack . The length of this memory area is usually 8,192 bytes (two page frames). For reasons of efficiency the kernel stores the 8-KB memory area in two consecutive page frames with the first page frame aligned to a multiple of $2^{13}$ ; this may turn out to be a problem when little dynamic memory is available, because the free memory may become highly fragmented (see the section \"The Buddy System Algorithm\" in Chapter 8). Therefore, in the 80x86 architecture the kernel can be configured at compilation time so that the memory area including stack and thread_info structure spans a single page frame (4,096 bytes). SUMMARY : 1.6.3. Reentrant Kernels\u4e2d Kernel Mode process stack \u662f\u4e3akernel control path\u800c\u51c6\u5907\u7684\uff0ckernel control path\u7684\u6267\u884c\u662fReentrant\u7684\uff0c\u5e94\u8be5\u4e0d\u9700\u8981\u592a\u591a\u7684space\uff1b SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u4ecb\u7ecd\u7684 process descriptors \u7684\u5b58\u50a8\u4f4d\u7f6e\u975e\u5e38\u91cd\u8981\uff1a process descriptors are stored in dynamic memory rather than in the memory area permanently assigned to the kernel;\u8fd8\u6709\u5c31\u662f thread_union \u7684\u5b58\u50a8\u4f4d\u7f6e\uff1a per-process memory area \uff1b\u9700\u8981\u6ce8\u610fper-process\u7684\u542b\u4e49\uff1b SUMMARY : https://elixir.bootlin.com/linux/v2.6.11/source/include/linux/sched.h union thread_union { struct thread_info thread_info; unsigned long stack[THREAD_SIZE/sizeof(long)]; }; SUMMARY : thread_union \u5c31\u662f\u4e0a\u8ff0\u7684memory area\uff1b In the section \"Segmentation in Linux\" in Chapter 2, we learned that a process in Kernel Mode accesses a stack contained in the kernel data segment , which is different from the stack used by the process in User Mode . Because kernel control paths make little use of the stack, only a few thousand bytes of kernel stack are required. Therefore, 8 KB is ample space for the stack and the thread_info structure. However, when stack and thread_info structure are contained in a single page frame, the kernel uses a few additional stacks to avoid the overflows caused by deeply nested interrupts and exceptions (see Chapter 4). SUMMARY : \u6839\u636e\u4e0a\u4e0a\u4e00\u6bb5\u4e2d\u63d0\u53ca\u7684\u5185\u5bb9\u53ef\u4ee5\u63a8\u6d4b\uff1a thread_union \u662f\u4fdd\u5b58\u5728per-process memory area\uff0c\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\uff1a Kernel Mode process stack \u4e5f\u4fdd\u5b58\u5728per-process memory area\u4e2d\uff1b\u800c\u8fd9\u4e00\u6bb5\u4e2d\u53c8\u63d0\u53ca\uff1aa process in Kernel Mode accesses a stack contained in the kernel data segment \uff1b\u90a3kernel data segment\u662f\u5b58\u653e\u5728\u4f55\u5904\u5462\uff1f\u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u8fdb\u884c\u4e86Google\uff1a is kernel data segment in process address space \uff1b\u76ee\u524d\u6240\u6709\u7684\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u5185\u5bb9\u90fd\u5728\u300a virtual-memory-address-space-thinking.md \u300b\u4e2d\uff1b\u5728\u9605\u8bfb\u8fd9\u4e00\u6bb5\u7684\u65f6\u5019\uff0c\u4e00\u4e2a\u5173\u952e\u70b9\u662f\u8981\u77e5\u9053\u672c\u4e66\u7684\u57fa\u4e8e i386 \u67b6\u6784\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u5728 i386 \u4e2d\uff0c\u4f7f\u7528\u4e86segmentation\uff0c\u4f46\u662f\u5728\u540e\u6765\u8fd9\u79cd\u65b9\u5f0f\u88ab\u53d6\u4ee3\u4e86\uff1b\u6240\u4ee5\u5f88\u591a\u67b6\u6784\u4e2d\u538b\u6839\u53ef\u80fd\u5c31\u6ca1\u6709 Kernel Mode stack contained in the kernel data segment \u7684\u8fd9\u79cd\u7ed3\u6784\uff1b\u5982\u5728 How are the segment registers (fs, gs, cs, ss, ds, es) used in Linux? \u4e2d\u6240\u8ff0\u7684\uff1b Figure 3-2 shows how the two data structures are stored in the 2-page (8 KB) memory area. The thread_info structure resides at the beginning of the memory area, and the stack grows downward from the end. The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and thread_info , respectively. SUMMARY \uff1a \u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u9700\u8981\u641e\u6e05\u695a struct thread_info \u7684\u5b9a\u4e49\uff0c\u4e00\u4e0b\u662f i386\u7684 struct thread_info struct thread_info { struct task_struct *task; /* main task structure */ struct exec_domain *exec_domain; /* execution domain */ unsigned long flags; /* low level flags */ unsigned long status; /* thread-synchronous flags */ __u32 cpu; /* current CPU */ __s32 preempt_count; /* 0 => preemptable, <0 => BUG */ mm_segment_t addr_limit; /* thread address space: 0-0xBFFFFFFF for user-thead 0-0xFFFFFFFF for kernel-thread */ struct restart_block restart_block; unsigned long previous_esp; /* ESP of the previous stack in case of nested (IRQ) stacks */ __u8 supervisor_stack[0]; }; \u53ef\u4ee5\u770b\u5230 struct thread_info \u6709\u6210\u5458\u53d8\u91cf struct task_struct *task \uff0c\u800c\u5728 struct task_struct \u4e2d\uff0c\u6709\u6210\u5458\u53d8\u91cf struct thread_info *thread_info; \uff0c\u8fd9\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u6240\u63cf\u8ff0\u7684\uff1a The figure also shows that the thread_info structure and the task_struct structure are mutually linked by means of the fields task and tHRead_info , respectively. The esp register is the CPU stack pointer, which is used to address the stack's top location. On 80x86 systems, the stack starts at the end and grows toward the beginning of the memory area. Right after switching from User Mode to Kernel Mode , the kernel stack of a process is always empty, and therefore the esp register points to the byte immediately following the stack. The value of the esp is decreased as soon as data is written into the stack. Because the thread_info structure is 52 bytes long, the kernel stack can expand up to 8,140 bytes. The C language allows the thread_info structure and the kernel stack of a process to be conveniently represented by means of the following union construct: union thread_union { struct thread_info thread_info; unsigned long stack[2048]; /* 1024 for 4KB stacks */ }; The thread_info structure shown in Figure 3-2 is stored starting at address 0x015fa000 , and the stack is stored starting at address 0x015fc000 . The value of the esp register points to the current top of the stack at 0x015fa878 . The kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a thread_info structure and a kernel stack .","title":"3.2.2.1. Process descriptors handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#3222-identifying-the-current-process","text":"The close association between the thread_info structure and the Kernel Mode stack just described offers a key benefit in terms of efficiency: the kernel can easily obtain the address of the thread_info structure of the process currently running on a CPU from the value of the esp register. In fact, if the thread_union structure is 8 KB ($2^{13}$ bytes) long, the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure; on the other hand, if the thread_union structure is 4 KB long, the kernel masks out the 12 least significant bits of esp . This is done by the current_thread_info( ) function, which produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl %ecx,p After executing these three instructions, p contains the thread_info structure pointer of the process running on the CPU that executes the instruction. Most often the kernel needs the address of the process descriptor rather than the address of the thread_info structure. To get the process descriptor pointer of the process currently running on a CPU, the kernel makes use of the current macro, which is essentially equivalent to current_thread_info( )->task and produces assembly language instructions like the following: movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */ andl %esp,%ecx movl (%ecx),p Because the task field is at offset 0 in the thread_info structure, after executing these three instructions p contains the process descriptor pointer of the process running on the CPU. The current macro often appears in kernel code as a prefix to fields of the process descriptor . For example, current->pid returns the process ID of the process currently running on the CPU. Another advantage of storing the process descriptor with the stack emerges on multiprocessor systems: the correct current process for each hardware processor can be derived just by checking the stack, as shown previously. Earlier versions of Linux did not store the kernel stack and the process descriptor together. Instead, they were forced to introduce a global static variable called current to identify the process descriptor of the running process. On multiprocessor systems, it was necessary to define current as an array one element for each available CPU. SUMMARY : \u6240\u6709\u7684 process descriptor \u548c kernel stack \u90fd\u662f\u4f4d\u4e8ekernel\u4e2d\uff1b\u7531kernel\u6765\u6267\u884c\u8c03\u5ea6\uff1b\u5f53CPU\u9700\u8981\u6267\u884c\u67d0\u4e2a process descriptor \u7684\u65f6\u5019\uff0c\u5b83\u9700\u8981\u8bfb\u53d6\u8fd9\u4e2a process descriptor \u7684\u4e00\u4e9b\u6570\u636e\uff0c\u6bd4\u5982\u4e4b\u524d\u4fdd\u5b58\u7684register\u6570\u636e\u7b49\u4ee5\u4fbfresume\uff1b\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u5e76\u4e14\uff0c\u4ece\u524d\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u6bcf\u4e2a thread_union \u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u7684 kernel stack \uff0c\u800c\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u662f\u6839\u636e esp \u7684\u503c\u6765\u83b7\u5f97 process descriptor \u7684\u5730\u5740\uff0c\u6240\u4ee5CPU\u662f\u5728\u67d0\u4e2a`thread_union\u7684 kernel stack \u4e2d\u6267\u884c\uff0c\u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684 process descriptor \uff1b SUMMARY : \u56e0\u4e3ascheduler\u5728\u8c03\u5ea6\u4e00\u4e2atask\u5f00\u59cb\u8fd0\u884c\u4e4b\u524d\u4f1a\u5c06\u8fd9\u4e2atask\u7684\u6240\u6709\u7684register\u90fd\u6062\u590d\u5230CPU\u4e2d\uff0c\u6240\u4ee5\u5fc5\u7136\u4f1a\u5305\u542b esp \uff0c\u6240\u4ee5\u5b83\u5c31\u53ef\u4ee5\u6839\u636e esp \u5feb\u901f\u5730\u5b9a\u4f4d\u5230process descriptor\uff1b","title":"3.2.2.2. Identifying the current process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#3223-doubly-linked-lists","text":"Before moving on and describing how the kernel keeps track of the various processes in the system, we would like to emphasize the role of special data structures that implement doubly linked lists. For each list, a set of primitive operations must be implemented: initializing the list, inserting and deleting an element, scanning the list, and so on. It would be both a waste of programmers' efforts and a waste of memory to replicate the primitive operations for each different list. Therefore, the Linux kernel defines the list_head data structure, whose only fields next and prev represent the forward and back pointers of a generic doubly linked list element, respectively. It is important to note, however, that the pointers in a list_head field store the addresses of other list_head fields rather than the addresses of the whole data structures in which the list_head structure is included; see Figure 3-3 (a). SUMMARY : list_head A new list is created by using the LIST_HEAD(list_name) macro. It declares a new variable named list_name of type list_head , which is a dummy first element that acts as a placeholder for the head of the new list, and initializes the prev and next fields of the list_head data structure so as to point to the list_name variable itself; see Figure 3-3 (b). Several functions and macros implement the primitives, including those shown in Table Table 3-1. The Linux kernel 2.6 sports another kind of doubly linked list, which mainly differs from a list_head list because it is not circular; it is mainly used for hash tables , where space is important, and finding the the last element in constant time is not. The list head is stored in an hlist_head data structure, which is simply a pointer to the first element in the list ( NULL if the list is empty). Each element is represented by an hlist_node data structure, which includes a pointer next to the next element, and a pointer pprev to the next field of the previous element. Because the list is not circular, the pprev field of the first element and the next field of the last element are set to NULL . The list can be handled by means of several helper functions and macros similar to those listed in Table 3-1: hlist_add_head( ) , hlist_del( ) , hlist_empty( ) , hlist_entry , hlist_for_each_entry , and so on.","title":"3.2.2.3. Doubly linked lists"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#3224-the-process-list","text":"The first example of a doubly linked list we will examine is the process list , a list that links together all existing process descriptors. Each task_struct structure includes a tasks field of type list_head whose prev and next fields point, respectively, to the previous and to the next task_struct element. The head of the process list is the init_task task_struct descriptor; it is the process descriptor of the so-called process 0 or swapper (see the section \"Kernel Threads\" later in this chapter). The tasks->prev field of init_task points to the tasks field of the process descriptor inserted last in the list. The SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process list , respectively. These macros also take care of the parenthood relationship of the process (see the section \"How Processes Are Organized\" later in this chapter). Another useful macro, called for_each_process , scans the whole process list. It is defined as: #define for_each_process(p) \\ for (p=&init_task; (p=list_entry((p)->tasks.next, \\ struct task_struct, tasks) \\ ) != &init_task; ) The macro is the loop control statement after which the kernel programmer supplies the loop. Notice how the init_task process descriptor just plays the role of list header . The macro starts by moving past init_task to the next task and continues until it reaches init_task again (thanks to the circularity of the list). At each iteration, the variable passed as the argument of the macro contains the address of the currently scanned process descriptor, as returned by the list_entry macro. SUMMARY : \u5728multiprocessor\u4e2d\uff0c\u662f\u5426\u662f\u6bcf\u4e2aprocessor\u90fd\u6709\u4e00\u4e2aprocess list\uff0c\u8fd8\u662f\u8bf4\u6240\u6709\u7684process descriptor\u90fd\u653e\u5728\u4e00\u4e2aprocess list\u4e2d\uff1f","title":"3.2.2.4. The process list"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.2.2-Identifying-a-Process/#3225-the-lists-of-task_running-processes","text":"When looking for a new process to run on a CPU, the kernel has to consider only the runnable processes (that is, the processes in the TASK_RUNNING state). Earlier Linux versions put all runnable processes in the same list called runqueue . Because it would be too costly to maintain the list ordered according to process priorities, the earlier schedulers were compelled to scan the whole list in order to select the \"best\" runnable process. Linux 2.6 implements the runqueue differently. The aim is to allow the scheduler to select the best runnable process in constant time, independently of the number of runnable processes. We'll defer to Chapter 7 a detailed description of this new kind of runqueue , and we'll provide here only some basic information. The trick used to achieve the scheduler speedup consists of splitting the runqueue in many lists of runnable processes, one list per process priority . Each task_struct descriptor includes a run_list field of type list_head . If the process priority is equal to k (a value ranging between 0 and 139), the run_list field links the process descriptor into the list of runnable processes having priority k . Furthermore, on a multiprocessor system, each CPU has its own runqueue , that is, its own set of lists of processes. This is a classic example of making a data structures more complex to improve performance: to make scheduler operations more efficient, the runqueue list has been split into 140 different lists! As we'll see, the kernel must preserve a lot of data for every runqueue in the system; however, the main data structures of a runqueue are the lists of process descriptors belonging to the runqueue ; all these lists are implemented by a single prio_array_t data structure, whose fields are shown in Table 3-2. SUMMARY : prio_array The enqueue_task(p,array) function inserts a process descriptor into a runqueue list; its code is essentially equivalent to: list_add_tail(&p->run_list, &array->queue[p->prio]); __set_bit(p->prio, array->bitmap); array->nr_active++; p->array = array; The prio field of the process descriptor stores the dynamic priority of the process, while the array field is a pointer to the prio_array_t data structure of its current runqueue . Similarly, the dequeue_task(p,array) function removes a process descriptor from a runqueue list.","title":"3.2.2.5. The lists of TASK_RUNNING processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4-Creating-Processes/","text":"3.4. Creating Processes # Unix operating systems rely heavily on process creation to satisfy user requests. For example, the shell creates a new process that executes another copy of the shell whenever the user enters a command. Traditional Unix systems treat all processes in the same way: resources owned by the parent process are duplicated in the child process. This approach makes process creation very slow and inefficient, because it requires copying the entire address space of the parent process. The child process rarely needs to read or modify all the resources inherited from the parent; in many cases, it issues an immediate execve( ) and wipes out the address space that was so carefully copied. Modern Unix kernels solve this problem by introducing three different mechanisms: The Copy On Write technique allows both the parent and the child to read the same physical pages. Whenever either one tries to write on a physical page, the kernel copies its contents into a new physical page that is assigned to the writing process. The implementation of this technique in Linux is fully explained in Chapter 9. Lightweight processes allow both the parent and the child to share many per-process kernel data structures , such as the paging tables (and therefore the entire User Mode address space ), the open file tables, and the signal dispositions . The vfork( ) system call creates a process that shares the memory address space of its parent. To prevent the parent from overwriting data needed by the child, the parent's execution is blocked until the child exits or executes a new program. We'll learn more about the vfork( ) system call in the following section. 3.4.1. The clone( ) , fork( ) , and vfork( ) System Calls # Lightweight processes are created in Linux by using a function named clone( ) , which uses the following parameters: NOTE: \u53c2\u89c1\u8be5\u51fd\u6570\u7684man page\u83b7\u53d6\u5173\u4e8e\u8be5\u51fd\u6570\u7684\u5404\u79cd\u4fe1\u606f\u3002 fn Specifies a function to be executed by the new process; when the function returns, the child terminates. The function returns an integer, which represents the exit code for the child process. arg Points to data passed to the fn( ) function. flags Miscellaneous information. The low byte specifies the signal number to be sent to the parent process when the child terminates; the SIGCHLD signal is generally selected. The remaining three bytes encode a group of clone flags , which are shown in Table 3-8. child_stack Specifies the User Mode stack pointer to be assigned to the esp register of the child process. The invoking process (the parent) should always allocate a new stack for the child. tls Specifies the address of a data structure that defines a Thread Local Storage segment for the new lightweight process (see the section \"The Linux GDT\" in Chapter 2). Meaningful only if the CLONE_SETTLS flag is set. ptid Specifies the address of a User Mode variable of the parent process that will hold the PID of the new lightweight process. Meaningful only if the CLONE_PARENT_SETTID flag is set. ctid Specifies the address of a User Mode variable of the new lightweight process that will hold the PID of such process. Meaningful only if the CLONE_CHILD_SETTID flag is set. Table 3-8. Clone flags Flag name Description CLONE_VM Shares the memory descriptor and all Page Tables (see Chapter 9). CLONE_FS Shares the table that identifies the root directory and the current working directory, as well as the value of the bitmask used to mask the initial file permissions of a new file (the so-called file umask ). CLONE_FILES Shares the table that identifies the open files (see Chapter 12). CLONE_SIGHAND Shares the tables that identify the signal handlers and the blocked and pending signals (see Chapter 11). If this flag is true, the CLONE_VM flag must also be set. CLONE_PTRACE If traced, the parent wants the child to be traced too. Furthermore, the debugger may want to trace the child on its own; in this case, the kernel forces the flag to 1. CLONE_VFORK Set when the system call issued is a vfork( ) (see later in this section). CLONE_PARENT Sets the parent of the child ( parent and real_parent fields in the process descriptor) to the parent of the calling process. CLONE_THREAD Inserts the child into the same thread group of the parent, and forces the child to share the signal descriptor of the parent. The child's tgid and group_leader fields are set accordingly. If this flag is true, the CLONE_SIGHAND flag must also be set. CLONE_NEWNS Set if the clone needs its own namespace, that is, its own view of the mounted filesystems (see Chapter 12); it is not possible to specify both CLONE_NEWNS and CLONE_FS . CLONE_SYSVSEM Shares the System V IPC undoable semaphore operations (see the section \"IPC Semaphores\" in Chapter 19). CLONE_SETTLS Creates a new Thread Local Storage (TLS) segment for the lightweight process; the segment is described in the structure pointed to by the tls parameter. CLONE_PARENT_SETTID Writes the PID of the child into the User Mode variable of the parent pointed to by the ptid parameter. CLONE_CHILD_CLEARTID When set, the kernel sets up a mechanism to be triggered when the child process will exit or when it will start executing a new program. In these cases, the kernel will clear the User Mode variable pointed to by the ctid parameter and will awaken any process waiting for this event. CLONE_DETACHED A legacy flag ignored by the kernel. CLONE_UNTRACED Set by the kernel to override the value of the CLONE_PTRACE flag (used for disabling tracing of kernel threads ; see the section \"Kernel Threads\" later in this chapter). CLONE_CHILD_SETTID Writes the PID of the child into the User Mode variable of the child pointed to by the ctid parameter. CLONE_STOPPED Forces the child to start in the TASK_STOPPED state. clone( ) is actually a wrapper function defined in the C library (see the section \"POSIX APIs and System Calls\" in Chapter 10), which sets up the stack of the new lightweight process and invokes a clone( ) system call hidden to the programmer. The sys_clone( ) service routine that implements the clone( ) system call does not have the fn and arg parameters. In fact, the wrapper function saves the pointer fn into the child's stack position corresponding to the return address of the wrapper function itself; the pointer arg is saved on the child's stack right below fn . When the wrapper function terminates, the CPU fetches the return address from the stack and executes the fn(arg) function. The traditional fork( ) system call is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and all the clone flags cleared, and whose child_stack parameter is the current parent stack pointer . Therefore, the parent and child temporarily share the same User Mode stack . But thanks to the Copy On Write mechanism, they usually get separate copies of the User Mode stack as soon as one tries to change the stack. The vfork( ) system call, introduced in the previous section, is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and the flags CLONE_VM and CLONE_VFORK , and whose child_stack parameter is equal to the current parent stack pointer .","title":"3.4-Creating-Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4-Creating-Processes/#34-creating-processes","text":"Unix operating systems rely heavily on process creation to satisfy user requests. For example, the shell creates a new process that executes another copy of the shell whenever the user enters a command. Traditional Unix systems treat all processes in the same way: resources owned by the parent process are duplicated in the child process. This approach makes process creation very slow and inefficient, because it requires copying the entire address space of the parent process. The child process rarely needs to read or modify all the resources inherited from the parent; in many cases, it issues an immediate execve( ) and wipes out the address space that was so carefully copied. Modern Unix kernels solve this problem by introducing three different mechanisms: The Copy On Write technique allows both the parent and the child to read the same physical pages. Whenever either one tries to write on a physical page, the kernel copies its contents into a new physical page that is assigned to the writing process. The implementation of this technique in Linux is fully explained in Chapter 9. Lightweight processes allow both the parent and the child to share many per-process kernel data structures , such as the paging tables (and therefore the entire User Mode address space ), the open file tables, and the signal dispositions . The vfork( ) system call creates a process that shares the memory address space of its parent. To prevent the parent from overwriting data needed by the child, the parent's execution is blocked until the child exits or executes a new program. We'll learn more about the vfork( ) system call in the following section.","title":"3.4. Creating Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/3.4-Creating-Processes/#341-the-clone-fork-and-vfork-system-calls","text":"Lightweight processes are created in Linux by using a function named clone( ) , which uses the following parameters: NOTE: \u53c2\u89c1\u8be5\u51fd\u6570\u7684man page\u83b7\u53d6\u5173\u4e8e\u8be5\u51fd\u6570\u7684\u5404\u79cd\u4fe1\u606f\u3002 fn Specifies a function to be executed by the new process; when the function returns, the child terminates. The function returns an integer, which represents the exit code for the child process. arg Points to data passed to the fn( ) function. flags Miscellaneous information. The low byte specifies the signal number to be sent to the parent process when the child terminates; the SIGCHLD signal is generally selected. The remaining three bytes encode a group of clone flags , which are shown in Table 3-8. child_stack Specifies the User Mode stack pointer to be assigned to the esp register of the child process. The invoking process (the parent) should always allocate a new stack for the child. tls Specifies the address of a data structure that defines a Thread Local Storage segment for the new lightweight process (see the section \"The Linux GDT\" in Chapter 2). Meaningful only if the CLONE_SETTLS flag is set. ptid Specifies the address of a User Mode variable of the parent process that will hold the PID of the new lightweight process. Meaningful only if the CLONE_PARENT_SETTID flag is set. ctid Specifies the address of a User Mode variable of the new lightweight process that will hold the PID of such process. Meaningful only if the CLONE_CHILD_SETTID flag is set. Table 3-8. Clone flags Flag name Description CLONE_VM Shares the memory descriptor and all Page Tables (see Chapter 9). CLONE_FS Shares the table that identifies the root directory and the current working directory, as well as the value of the bitmask used to mask the initial file permissions of a new file (the so-called file umask ). CLONE_FILES Shares the table that identifies the open files (see Chapter 12). CLONE_SIGHAND Shares the tables that identify the signal handlers and the blocked and pending signals (see Chapter 11). If this flag is true, the CLONE_VM flag must also be set. CLONE_PTRACE If traced, the parent wants the child to be traced too. Furthermore, the debugger may want to trace the child on its own; in this case, the kernel forces the flag to 1. CLONE_VFORK Set when the system call issued is a vfork( ) (see later in this section). CLONE_PARENT Sets the parent of the child ( parent and real_parent fields in the process descriptor) to the parent of the calling process. CLONE_THREAD Inserts the child into the same thread group of the parent, and forces the child to share the signal descriptor of the parent. The child's tgid and group_leader fields are set accordingly. If this flag is true, the CLONE_SIGHAND flag must also be set. CLONE_NEWNS Set if the clone needs its own namespace, that is, its own view of the mounted filesystems (see Chapter 12); it is not possible to specify both CLONE_NEWNS and CLONE_FS . CLONE_SYSVSEM Shares the System V IPC undoable semaphore operations (see the section \"IPC Semaphores\" in Chapter 19). CLONE_SETTLS Creates a new Thread Local Storage (TLS) segment for the lightweight process; the segment is described in the structure pointed to by the tls parameter. CLONE_PARENT_SETTID Writes the PID of the child into the User Mode variable of the parent pointed to by the ptid parameter. CLONE_CHILD_CLEARTID When set, the kernel sets up a mechanism to be triggered when the child process will exit or when it will start executing a new program. In these cases, the kernel will clear the User Mode variable pointed to by the ctid parameter and will awaken any process waiting for this event. CLONE_DETACHED A legacy flag ignored by the kernel. CLONE_UNTRACED Set by the kernel to override the value of the CLONE_PTRACE flag (used for disabling tracing of kernel threads ; see the section \"Kernel Threads\" later in this chapter). CLONE_CHILD_SETTID Writes the PID of the child into the User Mode variable of the child pointed to by the ctid parameter. CLONE_STOPPED Forces the child to start in the TASK_STOPPED state. clone( ) is actually a wrapper function defined in the C library (see the section \"POSIX APIs and System Calls\" in Chapter 10), which sets up the stack of the new lightweight process and invokes a clone( ) system call hidden to the programmer. The sys_clone( ) service routine that implements the clone( ) system call does not have the fn and arg parameters. In fact, the wrapper function saves the pointer fn into the child's stack position corresponding to the return address of the wrapper function itself; the pointer arg is saved on the child's stack right below fn . When the wrapper function terminates, the CPU fetches the return address from the stack and executes the fn(arg) function. The traditional fork( ) system call is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and all the clone flags cleared, and whose child_stack parameter is the current parent stack pointer . Therefore, the parent and child temporarily share the same User Mode stack . But thanks to the Copy On Write mechanism, they usually get separate copies of the User Mode stack as soon as one tries to change the stack. The vfork( ) system call, introduced in the previous section, is implemented by Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal and the flags CLONE_VM and CLONE_VFORK , and whose child_stack parameter is equal to the current parent stack pointer .","title":"3.4.1. The clone( ), fork( ), and vfork( ) System Calls"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/Chapter-3-Processes/","text":"Chapter 3. Processes # The concept of a process is fundamental to any multiprogramming operating system. A process is usually defined as an instance of a program in execution; thus, if 16 users are running vi at once, there are 16 separate processes (although they can share the same executable code). Processes are often called tasks or threads in the Linux source code. In this chapter, we discuss static properties of processes and then describe how process switching is performed by the kernel. The last two sections describe how processes can be created and destroyed. We also describe how Linux supports multithreaded applications as mentioned in Chapter 1, it relies on so-called lightweight processes (LWP) .","title":"Chapter-3-Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/Chapter-3-Processes/#chapter-3-processes","text":"The concept of a process is fundamental to any multiprogramming operating system. A process is usually defined as an instance of a program in execution; thus, if 16 users are running vi at once, there are 16 separate processes (although they can share the same executable code). Processes are often called tasks or threads in the Linux source code. In this chapter, we discuss static properties of processes and then describe how process switching is performed by the kernel. The last two sections describe how processes can be created and destroyed. We also describe how Linux supports multithreaded applications as mentioned in Chapter 1, it relies on so-called lightweight processes (LWP) .","title":"Chapter 3. Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/wikipedia-Copy-on-write/","text":"Copy-on-write In virtual memory management Copy-on-write # Copy-on-write ( CoW or COW ), sometimes referred to as implicit sharing or shadowing , is a resource-management technique used in computer programming to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write \uff08\u8fd9\u53e5\u8bdd\u662f\u5bf9copy on write\u7684\u6700\u597d\u7684\u89e3\u91ca\uff09. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations. NOTE : \u4e0a\u9762\u6240\u63d0\u53ca\u7684resource management\u8ba9\u6211\u60f3\u5230\u4e86 C++ \u4e2d\u7684RAII\uff0c\u548cpython\u4e2d\u7684with\uff0c\u663e\u7136\u8fd9\u4e24\u8005\u90fd\u4fa7\u91cd\u4e8e\u907f\u514dresource leak\u3002 In virtual memory management # Copy-on-write finds its main use in sharing the virtual memory of operating system processes , in the implementation of the fork system call . Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used. Copy-on-write can be implemented efficiently using the page table by marking\uff08\u6807\u5fd7\uff09 certain pages of memory as read-only and keeping a count of the number of references\uff08\u5f15\u7528\u8ba1\u6570\uff09 to the page. When data is written to these pages, the kernel intercepts\uff08\u62e6\u622a\uff09 the write attempt and allocates a new physical page, initialized with the copy-on-write data, although the allocation can be skipped if there is only one reference. The kernel then updates the page table with the new (writable) page, decrements the number of references, and performs the write. The new allocation ensures that a change in the memory of one process is not visible in another's. The copy-on-write technique can be extended to support efficient memory allocation by having a page of physical memory filled with zeros. When the memory is allocated, all the pages returned refer to the page of zeros and are all marked copy-on-write. This way, physical memory is not allocated for the process until data is written, allowing processes to reserve more virtual memory than physical memory and use memory sparsely, at the risk of running out of virtual address space. The combined algorithm is similar to demand paging . Copy-on-write pages are also used in the Linux kernel 's kernel same-page merging feature. Loading the libraries for an application is also a use of copy-on-write technique. The dynamic linker maps libraries as private like follows. Any writing action on the libraries will trigger a COW in virtual memory management. openat(AT_FDCWD, \"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 mmap(NULL, 3906144, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) mmap(0x7f8a3ced4000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1b0000)","title":"Copy-on-write"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/wikipedia-Copy-on-write/#copy-on-write","text":"Copy-on-write ( CoW or COW ), sometimes referred to as implicit sharing or shadowing , is a resource-management technique used in computer programming to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write \uff08\u8fd9\u53e5\u8bdd\u662f\u5bf9copy on write\u7684\u6700\u597d\u7684\u89e3\u91ca\uff09. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations. NOTE : \u4e0a\u9762\u6240\u63d0\u53ca\u7684resource management\u8ba9\u6211\u60f3\u5230\u4e86 C++ \u4e2d\u7684RAII\uff0c\u548cpython\u4e2d\u7684with\uff0c\u663e\u7136\u8fd9\u4e24\u8005\u90fd\u4fa7\u91cd\u4e8e\u907f\u514dresource leak\u3002","title":"Copy-on-write"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-3-Processes/wikipedia-Copy-on-write/#in-virtual-memory-management","text":"Copy-on-write finds its main use in sharing the virtual memory of operating system processes , in the implementation of the fork system call . Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used. Copy-on-write can be implemented efficiently using the page table by marking\uff08\u6807\u5fd7\uff09 certain pages of memory as read-only and keeping a count of the number of references\uff08\u5f15\u7528\u8ba1\u6570\uff09 to the page. When data is written to these pages, the kernel intercepts\uff08\u62e6\u622a\uff09 the write attempt and allocates a new physical page, initialized with the copy-on-write data, although the allocation can be skipped if there is only one reference. The kernel then updates the page table with the new (writable) page, decrements the number of references, and performs the write. The new allocation ensures that a change in the memory of one process is not visible in another's. The copy-on-write technique can be extended to support efficient memory allocation by having a page of physical memory filled with zeros. When the memory is allocated, all the pages returned refer to the page of zeros and are all marked copy-on-write. This way, physical memory is not allocated for the process until data is written, allowing processes to reserve more virtual memory than physical memory and use memory sparsely, at the risk of running out of virtual address space. The combined algorithm is similar to demand paging . Copy-on-write pages are also used in the Linux kernel 's kernel same-page merging feature. Loading the libraries for an application is also a use of copy-on-write technique. The dynamic linker maps libraries as private like follows. Any writing action on the libraries will trigger a COW in virtual memory management. openat(AT_FDCWD, \"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 mmap(NULL, 3906144, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) mmap(0x7f8a3ced4000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1b0000)","title":"In virtual memory management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.1-The-Role-of-Interrupt-Signals/","text":"4.1. The Role of Interrupt Signals 4.1. The Role of Interrupt Signals # As the name suggests, interrupt signals provide a way to divert\uff08\u8f6c\u6362\uff09 the processor to code outside the normal flow of control. When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter . NOTE: context switch There are some things in this chapter that will remind you of the context switch described in the previous chapter, carried out when a kernel substitutes one process for another. But there is a key difference between interrupt handling and process switching : the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a kernel control path , the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). SUMMARY : \u6267\u884csystem call\u4e5f\u662fkernel control path\uff0c\u90a3\u4e48\u662f\u5426system call\u7684\u6267\u884c\u6b65\u9aa4\u548c\u4e0a\u9762\u63cf\u8ff0\u7684\u7c7b\u4f3c\uff1f\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6709\u5982\u4e0b\u7684\u63d0\u95ee\uff1a Is the Unix process scheduler itself a process? Is the Unix process scheduler itself a process, or does it piggyback on other processes in the same way a system call does (running kernel code in the user process with the kernel bit set)? \u6309\u7167\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u63cf\u8ff0\u6765\u770b\uff0cinterrupt\u7684\u6267\u884c\u662fpiggyback on processes \uff1b\u6309\u71671.6.3. Reentrant Kernels\u4e2d\u6240\u5b9a\u4e49\u7684kernel control path\uff0c\u5b83\u652f\u6301A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call, an exception, or an interrupt.\u663e\u7136\uff0csystem call\u548cexception handler\u90fd\u662fkernel control path\uff1b\u4e0a\u9762\u6240\u63cf\u8ff0\u7684exception handler\u7684\u6267\u884c\u65b9\u5f0f\u662f\u5426\u4e5f\u9002\u7528\u4e8esystem call\uff1b Unix\u8fdb\u7a0b\u8c03\u5ea6\u7a0b\u5e8f\u672c\u8eab\u662f\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8fd8\u662f\u4ee5\u4e0e\u7cfb\u7edf\u8c03\u7528\u76f8\u540c\u7684\u65b9\u5f0f\u642d\u8f7d\u5728\u5176\u4ed6\u8fdb\u7a0b\u4e0a\uff08\u5728\u5185\u6838\u4f4d\u8bbe\u7f6e\u7684\u7528\u6237\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u5185\u6838\u4ee3\u7801\uff09\uff1f Interrupt handling is one of the most sensitive tasks performed by the kernel, because it must satisfy the following constraints: Interrupts can come anytime, when the kernel may want to finish something else it was trying to do. The kernel's goal is therefore to get the interrupt out of the way as soon as possible and defer as much processing as it can. For instance, suppose a block of data has arrived on a network line . When the hardware interrupts the kernel , it could simply mark the presence of data, give the processor back to whatever was running before, and do the rest of the processing later (such as moving the data into a buffer where its recipient process can find it, and then restarting the process). The activities that the kernel needs to perform in response to an interrupt are thus divided into a critical urgent part that the kernel executes right away and a deferrable part that is left for later. Because interrupts can come anytime, the kernel might be handling one of them while another one (of a different type) occurs. This should be allowed as much as possible, because it keeps the I/O devices busy (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a result, the interrupt handlers must be coded so that the corresponding kernel control paths can be executed in a nested manner. When the last kernel control path terminates, the kernel must be able to resume execution of the interrupted process or switch to another process if the interrupt signal has caused a rescheduling activity. Although the kernel may accept a new interrupt signal while handling a previous one, some critical regions exist inside the kernel code where interrupts must be disabled. Such critical regions must be limited as much as possible because, according to the previous requirement, the kernel, and particularly the interrupt handlers, should run most of the time with the interrupts enabled.","title":"4.1-The-Role-of-Interrupt-Signals"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.1-The-Role-of-Interrupt-Signals/#41-the-role-of-interrupt-signals","text":"As the name suggests, interrupt signals provide a way to divert\uff08\u8f6c\u6362\uff09 the processor to code outside the normal flow of control. When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter . NOTE: context switch There are some things in this chapter that will remind you of the context switch described in the previous chapter, carried out when a kernel substitutes one process for another. But there is a key difference between interrupt handling and process switching : the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a kernel control path , the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). SUMMARY : \u6267\u884csystem call\u4e5f\u662fkernel control path\uff0c\u90a3\u4e48\u662f\u5426system call\u7684\u6267\u884c\u6b65\u9aa4\u548c\u4e0a\u9762\u63cf\u8ff0\u7684\u7c7b\u4f3c\uff1f\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6709\u5982\u4e0b\u7684\u63d0\u95ee\uff1a Is the Unix process scheduler itself a process? Is the Unix process scheduler itself a process, or does it piggyback on other processes in the same way a system call does (running kernel code in the user process with the kernel bit set)? \u6309\u7167\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u63cf\u8ff0\u6765\u770b\uff0cinterrupt\u7684\u6267\u884c\u662fpiggyback on processes \uff1b\u6309\u71671.6.3. Reentrant Kernels\u4e2d\u6240\u5b9a\u4e49\u7684kernel control path\uff0c\u5b83\u652f\u6301A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call, an exception, or an interrupt.\u663e\u7136\uff0csystem call\u548cexception handler\u90fd\u662fkernel control path\uff1b\u4e0a\u9762\u6240\u63cf\u8ff0\u7684exception handler\u7684\u6267\u884c\u65b9\u5f0f\u662f\u5426\u4e5f\u9002\u7528\u4e8esystem call\uff1b Unix\u8fdb\u7a0b\u8c03\u5ea6\u7a0b\u5e8f\u672c\u8eab\u662f\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8fd8\u662f\u4ee5\u4e0e\u7cfb\u7edf\u8c03\u7528\u76f8\u540c\u7684\u65b9\u5f0f\u642d\u8f7d\u5728\u5176\u4ed6\u8fdb\u7a0b\u4e0a\uff08\u5728\u5185\u6838\u4f4d\u8bbe\u7f6e\u7684\u7528\u6237\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u5185\u6838\u4ee3\u7801\uff09\uff1f Interrupt handling is one of the most sensitive tasks performed by the kernel, because it must satisfy the following constraints: Interrupts can come anytime, when the kernel may want to finish something else it was trying to do. The kernel's goal is therefore to get the interrupt out of the way as soon as possible and defer as much processing as it can. For instance, suppose a block of data has arrived on a network line . When the hardware interrupts the kernel , it could simply mark the presence of data, give the processor back to whatever was running before, and do the rest of the processing later (such as moving the data into a buffer where its recipient process can find it, and then restarting the process). The activities that the kernel needs to perform in response to an interrupt are thus divided into a critical urgent part that the kernel executes right away and a deferrable part that is left for later. Because interrupts can come anytime, the kernel might be handling one of them while another one (of a different type) occurs. This should be allowed as much as possible, because it keeps the I/O devices busy (see the later section \"Nested Execution of Exception and Interrupt Handlers\"). As a result, the interrupt handlers must be coded so that the corresponding kernel control paths can be executed in a nested manner. When the last kernel control path terminates, the kernel must be able to resume execution of the interrupted process or switch to another process if the interrupt signal has caused a rescheduling activity. Although the kernel may accept a new interrupt signal while handling a previous one, some critical regions exist inside the kernel code where interrupts must be disabled. Such critical regions must be limited as much as possible because, according to the previous requirement, the kernel, and particularly the interrupt handlers, should run most of the time with the interrupts enabled.","title":"4.1. The Role of Interrupt Signals"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/","text":"4.2. Interrupts and Exceptions 4.2.1. IRQs and Interrupts 4.2.2. Exceptions 4.2.3. Interrupt Descriptor Table 4.2.4. Hardware Handling of Interrupts and Exceptions 4.2. Interrupts and Exceptions # The Intel documentation classifies interrupts and exceptions as follows: Interrupts: Maskable interrupts All Interrupt Requests (IRQs) issued by I/O devices give rise to maskable interrupts . A maskable interrupt can be in two states: masked or unmasked; a masked interrupt is ignored by the control unit as long as it remains masked. Nonmaskable interrupts Only a few critical events (such as hardware failures) give rise to nonmaskable interrupts . Nonmaskable interrupts are always recognized by the CPU. Exceptions: Processor-detected exceptions Generated when the CPU detects an anomalous condition while executing an instruction. These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception. Faults Can generally be corrected; once corrected, the program is allowed to restart with no loss of continuity. The saved value of eip is the address of the instruction that caused the fault , and hence that instruction can be resumed when the exception handler terminates. As we'll see in the section \"Page Fault Exception Handler\" in Chapter 9, resuming the same instruction is necessary whenever the handler is able to correct the anomalous condition that caused the exception. Traps Reported immediately following the execution of the trapping instruction ; after the kernel returns control to the program, it is allowed to continue its execution with no loss of continuity. The saved value of eip is the address of the instruction that should be executed after the one that caused the trap. A trap is triggered only when there is no need to reexecute the instruction that terminated. The main use of traps is for debugging purposes. The role of the interrupt signal in this case is to notify the debugger that a specific instruction has been executed (for instance, a breakpoint has been reached within a program). Once the user has examined the data provided by the debugger, she may ask that execution of the debugged program resume, starting from the next instruction. Aborts A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors , such as hardware failures and invalid or inconsistent values in system tables. The interrupt signal sent by the control unit is an emergency signal used to switch control to the corresponding abort exception handler . This handler has no choice but to force the affected process to terminate. Programmed exceptions Occur at the request of the programmer. They are triggered by int or int3 instructions; the into (check for overflow) and bound (check on address bound) instructions also give rise to a programmed exception when the condition they are checking is not true. Programmed exceptions are handled by the control unit as traps ; they are often called software interrupts . Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event (see Chapter 10). Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a vector. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller (see the next section). 4.2.1. IRQs and Interrupts # NOTE: Interrupt request (PC architecture) Programmable interrupt controller Interrupt vector table 4.2.2. Exceptions # The 80x86 microprocessors issue roughly 20 different exceptions . [*] The kernel must provide a dedicated exception handler for each exception type. For some exceptions, the CPU control unit also generates a hardware error code and pushes it on the Kernel Mode stack before starting the exception handler . [*] The exact number depends on the processor model. The following list gives the vector, the name, the type, and a brief description of the exceptions found in 80x86 processors. Additional information may be found in the Intel technical documentation. 4.2.3. Interrupt Descriptor Table # NOTE: Interrupt descriptor table 4.2.4. Hardware Handling of Interrupts and Exceptions #","title":"4.2-Interrupts-and-Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#42-interrupts-and-exceptions","text":"The Intel documentation classifies interrupts and exceptions as follows: Interrupts: Maskable interrupts All Interrupt Requests (IRQs) issued by I/O devices give rise to maskable interrupts . A maskable interrupt can be in two states: masked or unmasked; a masked interrupt is ignored by the control unit as long as it remains masked. Nonmaskable interrupts Only a few critical events (such as hardware failures) give rise to nonmaskable interrupts . Nonmaskable interrupts are always recognized by the CPU. Exceptions: Processor-detected exceptions Generated when the CPU detects an anomalous condition while executing an instruction. These are further divided into three groups, depending on the value of the eip register that is saved on the Kernel Mode stack when the CPU control unit raises the exception. Faults Can generally be corrected; once corrected, the program is allowed to restart with no loss of continuity. The saved value of eip is the address of the instruction that caused the fault , and hence that instruction can be resumed when the exception handler terminates. As we'll see in the section \"Page Fault Exception Handler\" in Chapter 9, resuming the same instruction is necessary whenever the handler is able to correct the anomalous condition that caused the exception. Traps Reported immediately following the execution of the trapping instruction ; after the kernel returns control to the program, it is allowed to continue its execution with no loss of continuity. The saved value of eip is the address of the instruction that should be executed after the one that caused the trap. A trap is triggered only when there is no need to reexecute the instruction that terminated. The main use of traps is for debugging purposes. The role of the interrupt signal in this case is to notify the debugger that a specific instruction has been executed (for instance, a breakpoint has been reached within a program). Once the user has examined the data provided by the debugger, she may ask that execution of the debugged program resume, starting from the next instruction. Aborts A serious error occurred; the control unit is in trouble, and it may be unable to store in the eip register the precise location of the instruction causing the exception. Aborts are used to report severe errors , such as hardware failures and invalid or inconsistent values in system tables. The interrupt signal sent by the control unit is an emergency signal used to switch control to the corresponding abort exception handler . This handler has no choice but to force the affected process to terminate. Programmed exceptions Occur at the request of the programmer. They are triggered by int or int3 instructions; the into (check for overflow) and bound (check on address bound) instructions also give rise to a programmed exception when the condition they are checking is not true. Programmed exceptions are handled by the control unit as traps ; they are often called software interrupts . Such exceptions have two common uses: to implement system calls and to notify a debugger of a specific event (see Chapter 10). Each interrupt or exception is identified by a number ranging from 0 to 255; Intel calls this 8-bit unsigned number a vector. The vectors of nonmaskable interrupts and exceptions are fixed, while those of maskable interrupts can be altered by programming the Interrupt Controller (see the next section).","title":"4.2. Interrupts and Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#421-irqs-and-interrupts","text":"NOTE: Interrupt request (PC architecture) Programmable interrupt controller Interrupt vector table","title":"4.2.1. IRQs and Interrupts"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#422-exceptions","text":"The 80x86 microprocessors issue roughly 20 different exceptions . [*] The kernel must provide a dedicated exception handler for each exception type. For some exceptions, the CPU control unit also generates a hardware error code and pushes it on the Kernel Mode stack before starting the exception handler . [*] The exact number depends on the processor model. The following list gives the vector, the name, the type, and a brief description of the exceptions found in 80x86 processors. Additional information may be found in the Intel technical documentation.","title":"4.2.2. Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#423-interrupt-descriptor-table","text":"NOTE: Interrupt descriptor table","title":"4.2.3. Interrupt Descriptor Table"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.2-Interrupts-and-Exceptions/#424-hardware-handling-of-interrupts-and-exceptions","text":"","title":"4.2.4. Hardware Handling of Interrupts and Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.3-Nested-Execution-of-Exception-and-Interrupt-Handlers/","text":"4.3. Nested Execution of Exception and Interrupt Handlers # Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process . For instance, when an I/O device raises an interrupt, the first instructions of the corresponding kernel control path are those that save the contents of the CPU registers in the Kernel Mode stack , while the last are those that restore the contents of the registers. Kernel control paths may be arbitrarily nested; an interrupt handler may be interrupted by another interrupt handler , thus giving rise to a nested execution of kernel control paths , as shown in Figure 4-3. As a result, the last instructions of a kernel control path that is taking care of an interrupt do not always put the current process back into User Mode: if the level of nesting is greater than 1, these instructions will put into execution the kernel control path that was interrupted last, and the CPU will continue to run in Kernel Mode. The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, no process switch can take place until an interrupt handler is running. In fact, all the data needed to resume a nested kernel control path is stored in the Kernel Mode stack, which is tightly bound to the current process. Assuming that the kernel is bug free, most exceptions can occur only while the CPU is in User Mode . Indeed, they are either caused by programming errors or triggered by debuggers. However, the \"Page Fault \" exception may occur in Kernel Mode. This happens when the process attempts to address a page that belongs to its address space but is not currently in RAM. While handling such an exception, the kernel may suspend the current process and replace it with another one until the requested page is available. The kernel control path that handles the \"Page Fault\" exception resumes execution as soon as the process gets the processor again. Because the \"Page Fault\" exception handler never gives rise to further exceptions, at most two kernel control paths associated with exceptions (the first one caused by a system call invocation, the second one caused by a Page Fault) may be stacked, one on top of the other. In contrast to exceptions, interrupts issued by I/O devices do not refer to data structures specific to the current process , although the kernel control paths that handle them run on behalf of that process. As a matter of fact, it is impossible to predict which process will be running when a given interrupt occurs. An interrupt handler may preempt both other interrupt handlers and exception handlers . Conversely, an exception handler never preempts an interrupt handler . The only exception that can be triggered in Kernel Mode is \"Page Fault,\" which we just described. But interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch. Linux interleaves kernel control paths for two major reasons: To improve the throughput of programmable interrupt controllers and device controllers . Assume that a device controller issues a signal on an IRQ line: the PIC transforms it into an external interrupt, and then both the PIC and the device controller remain blocked until the PIC receives an acknowledgment from the CPU. Thanks to kernel control path interleaving, the kernel is able to send the acknowledgment even when it is handling a previous interrupt. To implement an interrupt model without priority levels. Because each interrupt handler may be deferred by another one, there is no need to establish predefined priorities among hardware devices. This simplifies the kernel code and improves its portability. On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.","title":"4.3-Nested-Execution-of-Exception-and-Interrupt-Handlers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.3-Nested-Execution-of-Exception-and-Interrupt-Handlers/#43-nested-execution-of-exception-and-interrupt-handlers","text":"Every interrupt or exception gives rise to a kernel control path or separate sequence of instructions that execute in Kernel Mode on behalf of the current process . For instance, when an I/O device raises an interrupt, the first instructions of the corresponding kernel control path are those that save the contents of the CPU registers in the Kernel Mode stack , while the last are those that restore the contents of the registers. Kernel control paths may be arbitrarily nested; an interrupt handler may be interrupted by another interrupt handler , thus giving rise to a nested execution of kernel control paths , as shown in Figure 4-3. As a result, the last instructions of a kernel control path that is taking care of an interrupt do not always put the current process back into User Mode: if the level of nesting is greater than 1, these instructions will put into execution the kernel control path that was interrupted last, and the CPU will continue to run in Kernel Mode. The price to pay for allowing nested kernel control paths is that an interrupt handler must never block, that is, no process switch can take place until an interrupt handler is running. In fact, all the data needed to resume a nested kernel control path is stored in the Kernel Mode stack, which is tightly bound to the current process. Assuming that the kernel is bug free, most exceptions can occur only while the CPU is in User Mode . Indeed, they are either caused by programming errors or triggered by debuggers. However, the \"Page Fault \" exception may occur in Kernel Mode. This happens when the process attempts to address a page that belongs to its address space but is not currently in RAM. While handling such an exception, the kernel may suspend the current process and replace it with another one until the requested page is available. The kernel control path that handles the \"Page Fault\" exception resumes execution as soon as the process gets the processor again. Because the \"Page Fault\" exception handler never gives rise to further exceptions, at most two kernel control paths associated with exceptions (the first one caused by a system call invocation, the second one caused by a Page Fault) may be stacked, one on top of the other. In contrast to exceptions, interrupts issued by I/O devices do not refer to data structures specific to the current process , although the kernel control paths that handle them run on behalf of that process. As a matter of fact, it is impossible to predict which process will be running when a given interrupt occurs. An interrupt handler may preempt both other interrupt handlers and exception handlers . Conversely, an exception handler never preempts an interrupt handler . The only exception that can be triggered in Kernel Mode is \"Page Fault,\" which we just described. But interrupt handlers never perform operations that can induce page faults, and thus, potentially, a process switch. Linux interleaves kernel control paths for two major reasons: To improve the throughput of programmable interrupt controllers and device controllers . Assume that a device controller issues a signal on an IRQ line: the PIC transforms it into an external interrupt, and then both the PIC and the device controller remain blocked until the PIC receives an acknowledgment from the CPU. Thanks to kernel control path interleaving, the kernel is able to send the acknowledgment even when it is handling a previous interrupt. To implement an interrupt model without priority levels. Because each interrupt handler may be deferred by another one, there is no need to establish predefined priorities among hardware devices. This simplifies the kernel code and improves its portability. On multiprocessor systems, several kernel control paths may execute concurrently. Moreover, a kernel control path associated with an exception may start executing on a CPU and, due to a process switch, migrate to another CPU.","title":"4.3. Nested Execution of Exception and Interrupt Handlers"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.6-Interrupt-Handling/","text":"4.6. Interrupt Handling 4.6.1. I/O Interrupt Handling 4.6. Interrupt Handling # As we explained earlier, most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly. This approach does not hold for interrupts , because they frequently arrive long after the process to which they are related (for instance, a process that requested a data transfer) has been suspended and a completely unrelated process is running. So it would make no sense to send a Unix signal to the current process . Interrupt handling depends on the type of interrupt. For our purposes, we'll distinguish three main classes of interrupts: I/O interrupts An I/O device requires attention; the corresponding interrupt handler must query the device to determine the proper course of action. We cover this type of interrupt in the later section \"I/O Interrupt Handling.\" Timer interrupts Some timer, either a local APIC timer or an external timer , has issued an interrupt; this kind of interrupt tells the kernel that a fixed-time interval has elapsed. These interrupts are handled mostly as I/O interrupts; we discuss the peculiar characteristics of timer interrupts in Chapter 6. Interprocessor interrupts A CPU issued an interrupt to another CPU of a multiprocessor system. We cover such interrupts in the later section \"Interprocessor Interrupt Handling.\" 4.6.1. I/O Interrupt Handling # In general, an I/O interrupt handler must be flexible enough to service several devices at the same time. In the PCI bus architecture, for instance, several devices may share the same IRQ line . This means that the interrupt vector alone does not tell the whole story. In the example shown in Table 4-3, the same vector 43 is assigned to the USB port and to the sound card. However, some hardware devices found in older PC architectures (such as ISA) do not reliably operate if their IRQ line is shared with other devices. Interrupt handler flexibility is achieved in two distinct ways, as discussed in the following list.","title":"4.6-Interrupt-Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.6-Interrupt-Handling/#46-interrupt-handling","text":"As we explained earlier, most exceptions are handled simply by sending a Unix signal to the process that caused the exception. The action to be taken is thus deferred until the process receives the signal; as a result, the kernel is able to process the exception quickly. This approach does not hold for interrupts , because they frequently arrive long after the process to which they are related (for instance, a process that requested a data transfer) has been suspended and a completely unrelated process is running. So it would make no sense to send a Unix signal to the current process . Interrupt handling depends on the type of interrupt. For our purposes, we'll distinguish three main classes of interrupts: I/O interrupts An I/O device requires attention; the corresponding interrupt handler must query the device to determine the proper course of action. We cover this type of interrupt in the later section \"I/O Interrupt Handling.\" Timer interrupts Some timer, either a local APIC timer or an external timer , has issued an interrupt; this kind of interrupt tells the kernel that a fixed-time interval has elapsed. These interrupts are handled mostly as I/O interrupts; we discuss the peculiar characteristics of timer interrupts in Chapter 6. Interprocessor interrupts A CPU issued an interrupt to another CPU of a multiprocessor system. We cover such interrupts in the later section \"Interprocessor Interrupt Handling.\"","title":"4.6. Interrupt Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/4.6-Interrupt-Handling/#461-io-interrupt-handling","text":"In general, an I/O interrupt handler must be flexible enough to service several devices at the same time. In the PCI bus architecture, for instance, several devices may share the same IRQ line . This means that the interrupt vector alone does not tell the whole story. In the example shown in Table 4-3, the same vector 43 is assigned to the USB port and to the sound card. However, some hardware devices found in older PC architectures (such as ISA) do not reliably operate if their IRQ line is shared with other devices. Interrupt handler flexibility is achieved in two distinct ways, as discussed in the following list.","title":"4.6.1. I/O Interrupt Handling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/Chapter-4-Interrupts-and-Exceptions/","text":"Chapter 4. Interrupts and Exceptions # An interrupt is usually defined as an event that alters the sequence of instructions executed by a processor. Such events correspond to electrical signals generated by hardware circuits both inside and outside the CPU chip. Interrupts are often divided into synchronous and asynchronous interrupts : Synchronous interrupts are produced by the CPU control unit while executing instructions and are called synchronous because the control unit issues them only after terminating the execution of an instruction. Asynchronous interrupts are generated by other hardware devices at arbitrary times with respect to the CPU clock signals. NOTE : \u4e24\u8005\u7684\u6765\u6e90\u4e0d\u540c\uff0c\u4e00\u4e2a\u662f\u6e90\u81eaCPU\uff0c\u4e00\u4e2a\u662f\u6e90\u81ea\u5176\u4ed6\u7684 hardware devices Intel microprocessor manuals designate synchronous and asynchronous interrupts as exceptions and interrupts , respectively. We'll adopt this classification, although we'll occasionally use the term \" interrupt signal \" to designate both types together (synchronous as well as asynchronous). \u82f1\u7279\u5c14\u5fae\u5904\u7406\u5668\u624b\u518c\u5206\u522b\u5c06\u540c\u6b65\u548c\u5f02\u6b65\u4e2d\u65ad\u6307\u5b9a\u4e3a\u5f02\u5e38\u548c\u4e2d\u65ad\u3002 \u6211\u4eec\u5c06\u91c7\u7528\u8fd9\u79cd\u5206\u7c7b\uff0c\u5c3d\u7ba1\u6211\u4eec\u5076\u5c14\u4f1a\u4f7f\u7528\u672f\u8bed\u201c\u4e2d\u65ad\u4fe1\u53f7\u201d\u6765\u6307\u5b9a\u4e24\u79cd\u7c7b\u578b\uff08\u540c\u6b65\u548c\u5f02\u6b65\uff09\u3002 Interrupts are issued by interval timers and I/O devices ; for instance, the arrival of a keystroke from a user sets off an interrupt. Exceptions , on the other hand, are caused either by programming errors or by anomalous\uff08\u5f02\u5e38\u7684\uff09 conditions that must be handled by the kernel. In the first case, the kernel handles the exception by delivering to the current process one of the signals familiar to every Unix programmer . In the second case, the kernel performs all the steps needed to recover from the anomalous condition , such as a Page Fault or a request via an assembly language instruction such as int or sysenter for a kernel service. We start by describing in the next section the motivation for introducing such signals. We then show how the well-known IRQs (Interrupt Requests) issued by I/O devices give rise to interrupts, and we detail how 80x86 processors handle interrupts and exceptions at the hardware level . Then we illustrate, in the section \"Initializing the Interrupt Descriptor Table,\" how Linux initializes all the data structures required by the 80x86 interrupt architecture. The remaining three sections describe how Linux handles interrupt signals at the software level . NOTE: \u601d\u8003Unix signal\u548cexceptions\u548cinterrupts\u4e4b\u95f4\u7684\u5173\u7cfb\u3002Unix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53c2\u52a0\u300adocs/Programming/Signal/Signal.md\u300b","title":"Chapter-4-Interrupts-and-Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-4-Interrupts-and-Exceptions/Chapter-4-Interrupts-and-Exceptions/#chapter-4-interrupts-and-exceptions","text":"An interrupt is usually defined as an event that alters the sequence of instructions executed by a processor. Such events correspond to electrical signals generated by hardware circuits both inside and outside the CPU chip. Interrupts are often divided into synchronous and asynchronous interrupts : Synchronous interrupts are produced by the CPU control unit while executing instructions and are called synchronous because the control unit issues them only after terminating the execution of an instruction. Asynchronous interrupts are generated by other hardware devices at arbitrary times with respect to the CPU clock signals. NOTE : \u4e24\u8005\u7684\u6765\u6e90\u4e0d\u540c\uff0c\u4e00\u4e2a\u662f\u6e90\u81eaCPU\uff0c\u4e00\u4e2a\u662f\u6e90\u81ea\u5176\u4ed6\u7684 hardware devices Intel microprocessor manuals designate synchronous and asynchronous interrupts as exceptions and interrupts , respectively. We'll adopt this classification, although we'll occasionally use the term \" interrupt signal \" to designate both types together (synchronous as well as asynchronous). \u82f1\u7279\u5c14\u5fae\u5904\u7406\u5668\u624b\u518c\u5206\u522b\u5c06\u540c\u6b65\u548c\u5f02\u6b65\u4e2d\u65ad\u6307\u5b9a\u4e3a\u5f02\u5e38\u548c\u4e2d\u65ad\u3002 \u6211\u4eec\u5c06\u91c7\u7528\u8fd9\u79cd\u5206\u7c7b\uff0c\u5c3d\u7ba1\u6211\u4eec\u5076\u5c14\u4f1a\u4f7f\u7528\u672f\u8bed\u201c\u4e2d\u65ad\u4fe1\u53f7\u201d\u6765\u6307\u5b9a\u4e24\u79cd\u7c7b\u578b\uff08\u540c\u6b65\u548c\u5f02\u6b65\uff09\u3002 Interrupts are issued by interval timers and I/O devices ; for instance, the arrival of a keystroke from a user sets off an interrupt. Exceptions , on the other hand, are caused either by programming errors or by anomalous\uff08\u5f02\u5e38\u7684\uff09 conditions that must be handled by the kernel. In the first case, the kernel handles the exception by delivering to the current process one of the signals familiar to every Unix programmer . In the second case, the kernel performs all the steps needed to recover from the anomalous condition , such as a Page Fault or a request via an assembly language instruction such as int or sysenter for a kernel service. We start by describing in the next section the motivation for introducing such signals. We then show how the well-known IRQs (Interrupt Requests) issued by I/O devices give rise to interrupts, and we detail how 80x86 processors handle interrupts and exceptions at the hardware level . Then we illustrate, in the section \"Initializing the Interrupt Descriptor Table,\" how Linux initializes all the data structures required by the 80x86 interrupt architecture. The remaining three sections describe how Linux handles interrupt signals at the software level . NOTE: \u601d\u8003Unix signal\u548cexceptions\u548cinterrupts\u4e4b\u95f4\u7684\u5173\u7cfb\u3002Unix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53c2\u52a0\u300adocs/Programming/Signal/Signal.md\u300b","title":"Chapter 4. Interrupts and Exceptions"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-6-Timing-Measurements/Chapter-6-Timing-Measurements/","text":"Chapter 6. Timing Measurements Chapter 6. Timing Measurements # Countless computerized activities are driven by timing measurements , often behind the user's back. For instance, if the screen is automatically switched off after you have stopped using the computer's console, it is due to a timer that allows the kernel to keep track of how much time has elapsed since you pushed a key or moved the mouse. If you receive a warning from the system asking you to remove a set of unused files, it is the outcome of a program that identifies all user files that have not been accessed for a long time. To do these things, programs must be able to retrieve a timestamp identifying its last access time from each file. Such a timestamp must be automatically written by the kernel . More significantly, timing drives process switches along with even more visible kernel activities such as checking for time-outs. SUMMARY : CPU\u7684\u63a7\u5236\u5668\u4e5f\u662f\u53d7\u65f6\u949f\u63a7\u5236\u7684\uff1aclock generator\uff1b We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel: Keeping the current time and date so they can be returned to user programs through the time() , ftime( ) , and gettimeofday( ) APIs (see the section \"The time( ) and gettimeofday( ) System Calls\" later in this chapter) and used by the kernel itself as timestamps for files and network packets Maintaining timers mechanisms that are able to notify the kernel (see the later section \"Software Timers and Delay Functions\") or a user program (see the later sections \"The setitimer( ) and alarm( ) System Calls\" and \"System Calls for POSIX Timers\") that a certain interval of time has elapsed Timing measurements are performed by several hardware circuits based on fixed-frequency oscillators and counters. This chapter consists of four different parts. The first two sections describe the hardware devices that underly timing and give an overall picture of Linux timekeeping architecture . The following sections describe the main time-related duties of the kernel: implementing CPU time sharing , updating system time and resource usage statistics, and maintaining software timers . The last section discusses the system calls related to timing measurements and the corresponding service routines.","title":"Chapter-6-Timing-Measurements"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-6-Timing-Measurements/Chapter-6-Timing-Measurements/#chapter-6-timing-measurements","text":"Countless computerized activities are driven by timing measurements , often behind the user's back. For instance, if the screen is automatically switched off after you have stopped using the computer's console, it is due to a timer that allows the kernel to keep track of how much time has elapsed since you pushed a key or moved the mouse. If you receive a warning from the system asking you to remove a set of unused files, it is the outcome of a program that identifies all user files that have not been accessed for a long time. To do these things, programs must be able to retrieve a timestamp identifying its last access time from each file. Such a timestamp must be automatically written by the kernel . More significantly, timing drives process switches along with even more visible kernel activities such as checking for time-outs. SUMMARY : CPU\u7684\u63a7\u5236\u5668\u4e5f\u662f\u53d7\u65f6\u949f\u63a7\u5236\u7684\uff1aclock generator\uff1b We can distinguish two main kinds of timing measurement that must be performed by the Linux kernel: Keeping the current time and date so they can be returned to user programs through the time() , ftime( ) , and gettimeofday( ) APIs (see the section \"The time( ) and gettimeofday( ) System Calls\" later in this chapter) and used by the kernel itself as timestamps for files and network packets Maintaining timers mechanisms that are able to notify the kernel (see the later section \"Software Timers and Delay Functions\") or a user program (see the later sections \"The setitimer( ) and alarm( ) System Calls\" and \"System Calls for POSIX Timers\") that a certain interval of time has elapsed Timing measurements are performed by several hardware circuits based on fixed-frequency oscillators and counters. This chapter consists of four different parts. The first two sections describe the hardware devices that underly timing and give an overall picture of Linux timekeeping architecture . The following sections describe the main time-related duties of the kernel: implementing CPU time sharing , updating system time and resource usage statistics, and maintaining software timers . The last section discusses the system calls related to timing measurements and the corresponding service routines.","title":"Chapter 6. Timing Measurements"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-7-Process-Scheduling/Chapter-7-Process-Scheduling/","text":"Chapter 7. Process Scheduling # Like every time sharing system, Linux achieves the magical effect of an apparent simultaneous execution of multiple processes by switching from one process to another in a very short time frame. Process switching itself was discussed in Chapter 3; this chapter deals with scheduling , which is concerned with when to switch and which process to choose. The chapter consists of three parts. The section \"Scheduling Policy\" introduces the choices made by Linux in the abstract to schedule processes. The section \"The Scheduling Algorithm\" discusses the data structures used to implement scheduling and the corresponding algorithm. Finally, the section \"System Calls Related to Scheduling\" describes the system calls that affect process scheduling. To simplify the description, we refer as usual to the 80 x 86 architecture; in particular, we assume that the system uses the Uniform Memory Access model, and that the system tick is set to 1 ms.","title":"Chapter-7-Process-Scheduling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-7-Process-Scheduling/Chapter-7-Process-Scheduling/#chapter-7-process-scheduling","text":"Like every time sharing system, Linux achieves the magical effect of an apparent simultaneous execution of multiple processes by switching from one process to another in a very short time frame. Process switching itself was discussed in Chapter 3; this chapter deals with scheduling , which is concerned with when to switch and which process to choose. The chapter consists of three parts. The section \"Scheduling Policy\" introduces the choices made by Linux in the abstract to schedule processes. The section \"The Scheduling Algorithm\" discusses the data structures used to implement scheduling and the corresponding algorithm. Finally, the section \"System Calls Related to Scheduling\" describes the system calls that affect process scheduling. To simplify the description, we refer as usual to the 80 x 86 architecture; in particular, we assume that the system uses the Uniform Memory Access model, and that the system tick is set to 1 ms.","title":"Chapter 7. Process Scheduling"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/8.1-Page-Frame-Management/","text":"8.1. Page Frame Management # We saw in the section \"Paging in Hardware\" in Chapter 2 how the Intel Pentium processor can use two different page frame sizes: 4 KB and 4 MB (or 2 MB if PAE is enabled see the section \"The Physical Address Extension (PAE) Paging Mechanism\" in Chapter 2). Linux adopts the smaller 4 KB page frame size as the standard memory allocation unit . This makes things simpler for two reasons: The Page Fault exceptions issued by the paging circuitry are easily interpreted. Either the page requested exists but the process is not allowed to address it, or the page does not exist. In the second case, the memory allocator must find a free 4 KB page frame and assign it to the process. Although both 4 KB and 4 MB are multiples of all disk block sizes, transfers of data between main memory and disks are in most cases more efficient when the smaller size is used. 8.1.1. Page Descriptors # The kernel must keep track of the current status of each page frame . For instance, it must be able to distinguish the page frames that are used to contain pages that belong to processes from those that contain kernel code or kernel data structures. Similarly, it must be able to determine whether a page frame in dynamic memory is free. A page frame in dynamic memory is free if it does not contain any useful data. It is not free when the page frame contains data of a User Mode process, data of a software cache, dynamically allocated kernel data structures, buffered data of a device driver, code of a kernel module, and so on. State information of a page frame is kept in a page descriptor of type page , whose fields are shown in Table 8-1. All page descriptors are stored in the mem_map array. Because each descriptor is 32 bytes long, the space required by mem_map is slightly less than 1% of the whole RAM. The virt_to_page(addr) macro yields the address of the page descriptor associated with the linear address addr . The pfn_to_page(pfn) macro yields the address of the page descriptor associated with the page frame having number pfn . NOTE: \u6839\u636epage frame\u6765\u83b7\u5f97\u5176\u5bf9\u5e94\u7684page descriptor\u3002 Table 8-1. The fields of the page descriptor Type Name Description unsigned long flags Array of flags (see Table 8-2). Also encodes the zone number to which the page frame belongs. atomic_t _count Page frame's reference counter. atomic_t _mapcount Number of Page Table entries that refer to the page frame ( - 1 if none). unsigned long private Available to the kernel component that is using the page (for instance, it is a buffer head pointer in case of buffer page; see \"Block Buffers and Buffer Heads\" in Chapter 15). If the page is free, this field is used by the buddy system (see later in this chapter). struct address_space * mapping Used when the page is inserted into the page cache (see the section \"The Page Cache\" in Chapter 15), or when it belongs to an anonymous region (see the section \"Reverse Mapping for Anonymous Pages\" in Chapter 17). unsigned long index Used by several kernel components with different meanings. For instance, it identifies the position of the data stored in the page frame within the page's disk image or within an anonymous region (Chapter 15), or it stores a swapped-out page identifier (Chapter 17). struct list_head lru Contains pointers to the least recently used doubly linked list of pages. You don't have to fully understand the role of all fields in the page descriptor right now. In the following chapters, we often come back to the fields of the page descriptor. Moreover, several fields have different meaning, according to whether the page frame is free or what kernel component is using the page frame.","title":"8.1-Page-Frame-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/8.1-Page-Frame-Management/#81-page-frame-management","text":"We saw in the section \"Paging in Hardware\" in Chapter 2 how the Intel Pentium processor can use two different page frame sizes: 4 KB and 4 MB (or 2 MB if PAE is enabled see the section \"The Physical Address Extension (PAE) Paging Mechanism\" in Chapter 2). Linux adopts the smaller 4 KB page frame size as the standard memory allocation unit . This makes things simpler for two reasons: The Page Fault exceptions issued by the paging circuitry are easily interpreted. Either the page requested exists but the process is not allowed to address it, or the page does not exist. In the second case, the memory allocator must find a free 4 KB page frame and assign it to the process. Although both 4 KB and 4 MB are multiples of all disk block sizes, transfers of data between main memory and disks are in most cases more efficient when the smaller size is used.","title":"8.1. Page Frame Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/8.1-Page-Frame-Management/#811-page-descriptors","text":"The kernel must keep track of the current status of each page frame . For instance, it must be able to distinguish the page frames that are used to contain pages that belong to processes from those that contain kernel code or kernel data structures. Similarly, it must be able to determine whether a page frame in dynamic memory is free. A page frame in dynamic memory is free if it does not contain any useful data. It is not free when the page frame contains data of a User Mode process, data of a software cache, dynamically allocated kernel data structures, buffered data of a device driver, code of a kernel module, and so on. State information of a page frame is kept in a page descriptor of type page , whose fields are shown in Table 8-1. All page descriptors are stored in the mem_map array. Because each descriptor is 32 bytes long, the space required by mem_map is slightly less than 1% of the whole RAM. The virt_to_page(addr) macro yields the address of the page descriptor associated with the linear address addr . The pfn_to_page(pfn) macro yields the address of the page descriptor associated with the page frame having number pfn . NOTE: \u6839\u636epage frame\u6765\u83b7\u5f97\u5176\u5bf9\u5e94\u7684page descriptor\u3002 Table 8-1. The fields of the page descriptor Type Name Description unsigned long flags Array of flags (see Table 8-2). Also encodes the zone number to which the page frame belongs. atomic_t _count Page frame's reference counter. atomic_t _mapcount Number of Page Table entries that refer to the page frame ( - 1 if none). unsigned long private Available to the kernel component that is using the page (for instance, it is a buffer head pointer in case of buffer page; see \"Block Buffers and Buffer Heads\" in Chapter 15). If the page is free, this field is used by the buddy system (see later in this chapter). struct address_space * mapping Used when the page is inserted into the page cache (see the section \"The Page Cache\" in Chapter 15), or when it belongs to an anonymous region (see the section \"Reverse Mapping for Anonymous Pages\" in Chapter 17). unsigned long index Used by several kernel components with different meanings. For instance, it identifies the position of the data stored in the page frame within the page's disk image or within an anonymous region (Chapter 15), or it stores a swapped-out page identifier (Chapter 17). struct list_head lru Contains pointers to the least recently used doubly linked list of pages. You don't have to fully understand the role of all fields in the page descriptor right now. In the following chapters, we often come back to the fields of the page descriptor. Moreover, several fields have different meaning, according to whether the page frame is free or what kernel component is using the page frame.","title":"8.1.1. Page Descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/Chapter-8-Memory-Management/","text":"Chapter 8. Memory Management # We saw in Chapter 2 how Linux takes advantage of 80 x 86's segmentation and paging circuits to translate logical addresses into physical ones. We also mentioned that some portion of RAM is permanently assigned to the kernel and used to store both the kernel code and the static kernel data structures. NOTE: \u73b0\u4ee3\u5927\u591a\u6570\u90fd\u662f\u91c7\u7528\u7684\u57fa\u4e8epage\u7684memory management\u3002 The remaining part of the RAM is called dynamic memory . It is a valuable resource, needed not only by the processes but also by the kernel itself. In fact, the performance of the entire system depends on how efficiently dynamic memory is managed. Therefore, all current multitasking operating systems try to optimize the use of dynamic memory , assigning it only when it is needed and freeing it as soon as possible. Figure 8-1 shows schematically the page frames used as dynamic memory ; see the section \"Physical Memory Layout\" in Chapter 2 for details. This chapter, which consists of three main sections, describes how the kernel allocates dynamic memory for its own use. The sections \"Page Frame Management\" and \"Memory Area Management\" illustrate two different techniques for handling physically contiguous memory areas, while the section \"Noncontiguous Memory Area Management\" illustrates a third technique that handles noncontiguous memory areas. In these sections we'll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.","title":"Chapter-8-Memory-Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-8-Memory-Management/Chapter-8-Memory-Management/#chapter-8-memory-management","text":"We saw in Chapter 2 how Linux takes advantage of 80 x 86's segmentation and paging circuits to translate logical addresses into physical ones. We also mentioned that some portion of RAM is permanently assigned to the kernel and used to store both the kernel code and the static kernel data structures. NOTE: \u73b0\u4ee3\u5927\u591a\u6570\u90fd\u662f\u91c7\u7528\u7684\u57fa\u4e8epage\u7684memory management\u3002 The remaining part of the RAM is called dynamic memory . It is a valuable resource, needed not only by the processes but also by the kernel itself. In fact, the performance of the entire system depends on how efficiently dynamic memory is managed. Therefore, all current multitasking operating systems try to optimize the use of dynamic memory , assigning it only when it is needed and freeing it as soon as possible. Figure 8-1 shows schematically the page frames used as dynamic memory ; see the section \"Physical Memory Layout\" in Chapter 2 for details. This chapter, which consists of three main sections, describes how the kernel allocates dynamic memory for its own use. The sections \"Page Frame Management\" and \"Memory Area Management\" illustrate two different techniques for handling physically contiguous memory areas, while the section \"Noncontiguous Memory Area Management\" illustrates a third technique that handles noncontiguous memory areas. In these sections we'll cover topics such as memory zones, kernel mappings, the buddy system, the slab cache, and memory pools.","title":"Chapter 8. Memory Management"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.1-The-Process's-Address-Space/","text":"9.1. The Process's Address Space # The address space of a process consists of all linear addresses that the process is allowed to use. Each process sees a different set of linear addresses ; the address used by one process bears no relation to the address used by another. As we will see later, the kernel may dynamically modify a process address space by adding or removing intervals of linear addresses . The kernel represents intervals of linear addresses by means of resources called memory regions, which are characterized by an initial linear address , a length , and some access rights . For reasons of efficiency, both the initial address and the length of a memory region must be multiples of 4,096, so that the data identified by each memory region completely fills up the page frames allocated to it. Following are some typical situations in which a process gets new memory regions : When the user types a command at the console, the shell process creates a new process to execute the command. As a result, a fresh address space, and thus a set of memory regions , is assigned to the new process (see the section \"Creating and Deleting a Process Address Space\" later in this chapter; also, see Chapter 20). A running process may decide to load an entirely different program. In this case, the process ID remains unchanged, but the memory regions used before loading the program are released and a new set of memory regions is assigned to the process (see the section \"The exec Functions\" in Chapter 20). A running process may perform a \"memory mapping\" on a file (or on a portion of it). In such cases, the kernel assigns a new memory region to the process to map the file (see the section \"Memory Mapping\" in Chapter 16). A process may keep adding data on its User Mode stack until all addresses in the memory region that map the stack have been used. In this case, the kernel may decide to expand the size of that memory region (see the section \"Page Fault Exception Handler\" later in this chapter). A process may create an IPC-shared memory region to share data with other cooperating processes. In this case, the kernel assigns a new memory region to the process to implement this construct (see the section \"IPC Shared Memory\" in Chapter 19). A process may expand its dynamic area (the heap) through a function such as malloc( ) . As a result, the kernel may decide to expand the size of the memory region assigned to the heap (see the section \"Managing the Heap\" later in this chapter). Table 9-1 illustrates some of the system calls related to the previously mentioned tasks. brk( ) is discussed at the end of this chapter, while the remaining system calls are described in other chapters. Table 9-1. System calls related to memory region creation and deletion System call Description brk( ) Changes the heap size of the process execve( ) Loads a new executable file, thus changing the process address space _exit( ) Terminates the current process and destroys its address space fork( ) Creates a new process, and thus a new address space mmap( ) , mmap2( ) Creates a memory mapping for a file, thus enlarging the process address space mremap( ) Expands or shrinks a memory region remap_file_pages() Creates a non-linear mapping for a file (see Chapter 16) munmap( ) Destroys a memory mapping for a file, thus contracting the process address space shmat( ) Attaches a shared memory region shmdt( ) Detaches a shared memory region As we'll see in the later section \"Page Fault Exception Handler,\" it is essential for the kernel to identify the memory regions currently owned by a process (the address space of a process), because that allows the Page Fault exception handler to efficiently distinguish between two types of invalid linear addresses that cause it to be invoked: Those caused by programming errors. Those caused by a missing page; even though the linear address belongs to the process's address space, the page frame corresponding to that address has yet to be allocated. The latter addresses are not invalid from the process's point of view; the induced Page Faults are exploited by the kernel to implement demand paging : the kernel provides the missing page frame and lets the process continue.","title":"9.1-The-Process's-Address-Space."},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.1-The-Process's-Address-Space/#91-the-processs-address-space","text":"The address space of a process consists of all linear addresses that the process is allowed to use. Each process sees a different set of linear addresses ; the address used by one process bears no relation to the address used by another. As we will see later, the kernel may dynamically modify a process address space by adding or removing intervals of linear addresses . The kernel represents intervals of linear addresses by means of resources called memory regions, which are characterized by an initial linear address , a length , and some access rights . For reasons of efficiency, both the initial address and the length of a memory region must be multiples of 4,096, so that the data identified by each memory region completely fills up the page frames allocated to it. Following are some typical situations in which a process gets new memory regions : When the user types a command at the console, the shell process creates a new process to execute the command. As a result, a fresh address space, and thus a set of memory regions , is assigned to the new process (see the section \"Creating and Deleting a Process Address Space\" later in this chapter; also, see Chapter 20). A running process may decide to load an entirely different program. In this case, the process ID remains unchanged, but the memory regions used before loading the program are released and a new set of memory regions is assigned to the process (see the section \"The exec Functions\" in Chapter 20). A running process may perform a \"memory mapping\" on a file (or on a portion of it). In such cases, the kernel assigns a new memory region to the process to map the file (see the section \"Memory Mapping\" in Chapter 16). A process may keep adding data on its User Mode stack until all addresses in the memory region that map the stack have been used. In this case, the kernel may decide to expand the size of that memory region (see the section \"Page Fault Exception Handler\" later in this chapter). A process may create an IPC-shared memory region to share data with other cooperating processes. In this case, the kernel assigns a new memory region to the process to implement this construct (see the section \"IPC Shared Memory\" in Chapter 19). A process may expand its dynamic area (the heap) through a function such as malloc( ) . As a result, the kernel may decide to expand the size of the memory region assigned to the heap (see the section \"Managing the Heap\" later in this chapter). Table 9-1 illustrates some of the system calls related to the previously mentioned tasks. brk( ) is discussed at the end of this chapter, while the remaining system calls are described in other chapters. Table 9-1. System calls related to memory region creation and deletion System call Description brk( ) Changes the heap size of the process execve( ) Loads a new executable file, thus changing the process address space _exit( ) Terminates the current process and destroys its address space fork( ) Creates a new process, and thus a new address space mmap( ) , mmap2( ) Creates a memory mapping for a file, thus enlarging the process address space mremap( ) Expands or shrinks a memory region remap_file_pages() Creates a non-linear mapping for a file (see Chapter 16) munmap( ) Destroys a memory mapping for a file, thus contracting the process address space shmat( ) Attaches a shared memory region shmdt( ) Detaches a shared memory region As we'll see in the later section \"Page Fault Exception Handler,\" it is essential for the kernel to identify the memory regions currently owned by a process (the address space of a process), because that allows the Page Fault exception handler to efficiently distinguish between two types of invalid linear addresses that cause it to be invoked: Those caused by programming errors. Those caused by a missing page; even though the linear address belongs to the process's address space, the page frame corresponding to that address has yet to be allocated. The latter addresses are not invalid from the process's point of view; the induced Page Faults are exploited by the kernel to implement demand paging : the kernel provides the missing page frame and lets the process continue.","title":"9.1. The Process's Address Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.2-The-Memory-Descriptor/","text":"9.2. The Memory Descriptor # All information related to the process address space is included in an object called the memory descriptor of type mm_struct . This object is referenced by the mm field of the process descriptor . The fields of a memory descriptor are listed in Table 9-2. Table 9-2. The fields of the memory descriptor Type Field Description \u6ce8\u91ca struct vm_area_struct * mmap Pointer to the head of the list of memory region objects \u53c2\u89c1chapter 9.3. Memory Regions struct rb_root mm_rb Pointer to the root of the red-black tree of memory region objects struct vm_area_struct * mmap_cache Pointer to the last referenced memory region object unsigned long (*)( ) get_unmapped_area Method that searches an available linear address interval in the process address space void (*)( ) unmap_area Method invoked when releasing a linear address interval unsigned long mmap_base Identifies the linear address of the first allocated anonymous memory region or file memory mapping (see the section \"Program Segments and Process Memory Regions\" in Chapter 20) unsigned long free_area_cache Address from which the kernel will look for a free interval of linear addresses in the process address space pgd_t * pgd Pointer to the Page Global Directory \u5173\u4e8ePage Global Directory\uff0c\u53c2\u89c1Section 2.4. Paging in Hardware\u3001Section 2.5. Paging in Linux atomic_t mm_users Secondary usage counter atomic_t mm_count Main usage counter All memory descriptors are stored in a doubly linked list. Each descriptor stores the address of the adjacent list items in the mmlist field. The first element of the list is the mmlist field of init_mm , the memory descriptor used by process 0 in the initialization phase. The list is protected against concurrent accesses in multiprocessor systems by the mmlist_lock spin lock. The mm_users field stores the number of lightweight processes that share the mm_struct data structure (see the section \"The clone( ) , fork( ) , and vfork( ) System Calls\" in Chapter 3). The mm_count field is the main usage counter of the memory descriptor; all \"users\" in mm_users count as one unit in mm_count . Every time the mm_count field is decreased, the kernel checks whether it becomes zero; if so, the memory descriptor is deallocated because it is no longer in use.","title":"9.2-The-Memory-Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/9.2-The-Memory-Descriptor/#92-the-memory-descriptor","text":"All information related to the process address space is included in an object called the memory descriptor of type mm_struct . This object is referenced by the mm field of the process descriptor . The fields of a memory descriptor are listed in Table 9-2. Table 9-2. The fields of the memory descriptor Type Field Description \u6ce8\u91ca struct vm_area_struct * mmap Pointer to the head of the list of memory region objects \u53c2\u89c1chapter 9.3. Memory Regions struct rb_root mm_rb Pointer to the root of the red-black tree of memory region objects struct vm_area_struct * mmap_cache Pointer to the last referenced memory region object unsigned long (*)( ) get_unmapped_area Method that searches an available linear address interval in the process address space void (*)( ) unmap_area Method invoked when releasing a linear address interval unsigned long mmap_base Identifies the linear address of the first allocated anonymous memory region or file memory mapping (see the section \"Program Segments and Process Memory Regions\" in Chapter 20) unsigned long free_area_cache Address from which the kernel will look for a free interval of linear addresses in the process address space pgd_t * pgd Pointer to the Page Global Directory \u5173\u4e8ePage Global Directory\uff0c\u53c2\u89c1Section 2.4. Paging in Hardware\u3001Section 2.5. Paging in Linux atomic_t mm_users Secondary usage counter atomic_t mm_count Main usage counter All memory descriptors are stored in a doubly linked list. Each descriptor stores the address of the adjacent list items in the mmlist field. The first element of the list is the mmlist field of init_mm , the memory descriptor used by process 0 in the initialization phase. The list is protected against concurrent accesses in multiprocessor systems by the mmlist_lock spin lock. The mm_users field stores the number of lightweight processes that share the mm_struct data structure (see the section \"The clone( ) , fork( ) , and vfork( ) System Calls\" in Chapter 3). The mm_count field is the main usage counter of the memory descriptor; all \"users\" in mm_users count as one unit in mm_count . Every time the mm_count field is decreased, the kernel checks whether it becomes zero; if so, the memory descriptor is deallocated because it is no longer in use.","title":"9.2. The Memory Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/Chapter-9-Process-Address-Space/","text":"Chapter 9. Process Address Space # As seen in the previous chapter, a kernel function gets dynamic memory in a fairly straightforward manner by invoking one of a variety of functions: __get_free_pages( ) or alloc_pages( ) to get pages from the zoned page frame allocator, kmem_cache_alloc( ) or kmalloc( ) to use the slab allocator for specialized or general-purpose objects, and vmalloc( ) or vmalloc_32( ) to get a noncontiguous memory area. If the request can be satisfied, each of these functions returns a page descriptor address or a linear address identifying the beginning of the allocated dynamic memory area. NOTE: \u666e\u901a\u7684process\u662f\u4e0d\u4f1a\u63a5\u89e6\u5230page descriptor\u7684 These simple approaches work for two reasons: The kernel is the highest-priority component of the operating system. If a kernel function makes a request for dynamic memory , it must have a valid reason to issue that request, and there is no point in trying to defer it. The kernel trusts itself. All kernel functions are assumed to be error-free, so the kernel does not need to insert any protection against programming errors. When allocating memory to User Mode processes, the situation is entirely different: Process requests for dynamic memory are considered non-urgent. When a process's executable file is loaded, for instance, it is unlikely that the process will address all the pages of code in the near future. Similarly, when a process invokes malloc( ) to get additional dynamic memory, it doesn't mean the process will soon access all the additional memory obtained. Thus, as a general rule, the kernel tries to defer allocating dynamic memory to User Mode processes. Because user programs cannot be trusted, the kernel must be prepared to catch all addressing errors caused by processes in User Mode. As this chapter describes, the kernel succeeds in deferring the allocation of dynamic memory to processes by using a new kind of resource. When a User Mode process asks for dynamic memory , it doesn't get additional page frames ; instead, it gets the right to use a new range of linear addresses , which become part of its address space . This interval is called a \" memory region .\" In the next section, we discuss how the process views dynamic memory . We then describe the basic components of the process address space in the section \"Memory Regions.\" Next, we examine in detail the role played by the Page Fault exception handler in deferring the allocation of page frames to processes and illustrate how the kernel creates and deletes whole process address spaces . Last, we discuss the APIs and system calls related to address space management .","title":"Chapter-9-Process-Address-Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Chapter-9-Process-Address-Space/Chapter-9-Process-Address-Space/#chapter-9-process-address-space","text":"As seen in the previous chapter, a kernel function gets dynamic memory in a fairly straightforward manner by invoking one of a variety of functions: __get_free_pages( ) or alloc_pages( ) to get pages from the zoned page frame allocator, kmem_cache_alloc( ) or kmalloc( ) to use the slab allocator for specialized or general-purpose objects, and vmalloc( ) or vmalloc_32( ) to get a noncontiguous memory area. If the request can be satisfied, each of these functions returns a page descriptor address or a linear address identifying the beginning of the allocated dynamic memory area. NOTE: \u666e\u901a\u7684process\u662f\u4e0d\u4f1a\u63a5\u89e6\u5230page descriptor\u7684 These simple approaches work for two reasons: The kernel is the highest-priority component of the operating system. If a kernel function makes a request for dynamic memory , it must have a valid reason to issue that request, and there is no point in trying to defer it. The kernel trusts itself. All kernel functions are assumed to be error-free, so the kernel does not need to insert any protection against programming errors. When allocating memory to User Mode processes, the situation is entirely different: Process requests for dynamic memory are considered non-urgent. When a process's executable file is loaded, for instance, it is unlikely that the process will address all the pages of code in the near future. Similarly, when a process invokes malloc( ) to get additional dynamic memory, it doesn't mean the process will soon access all the additional memory obtained. Thus, as a general rule, the kernel tries to defer allocating dynamic memory to User Mode processes. Because user programs cannot be trusted, the kernel must be prepared to catch all addressing errors caused by processes in User Mode. As this chapter describes, the kernel succeeds in deferring the allocation of dynamic memory to processes by using a new kind of resource. When a User Mode process asks for dynamic memory , it doesn't get additional page frames ; instead, it gets the right to use a new range of linear addresses , which become part of its address space . This interval is called a \" memory region .\" In the next section, we discuss how the process views dynamic memory . We then describe the basic components of the process address space in the section \"Memory Regions.\" Next, we examine in detail the role played by the Page Fault exception handler in deferring the allocation of page frames to processes and illustrate how the kernel creates and deletes whole process address spaces . Last, we discuss the APIs and system calls related to address space management .","title":"Chapter 9. Process Address Space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/Debugger/","text":"3.2.1. Process State TASK_TRACED ptrace Say this five times fast: strace, ptrace, dtrace, dtruss","title":"Debugger"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-01-OS-kernel-is-event-driven/","text":"OS kernel is event-driven # \u5173\u4e8eOS\u7684\u4f5c\u7528\uff0c\u672c\u4e66\u4e2d\u5df2\u7ecf\u82b1\u4e86\u975e\u5e38\u591a\u7684\u7bc7\u5e45\u6765\u8fdb\u884c\u4ecb\u7ecd\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u7ae0\u8282\uff1a 1.4. Basic Operating System Concepts 1.6. An Overview of Unix Kernels \u5728\u9605\u8bfb\u8fc7\u7a0b\u4e2d\uff0c\u6709\u5fc5\u8981\u5efa\u7acb\u8d77\u5bf9OS\u8fd0\u884c\u6982\u51b5\u7684\u9ad8\u5c4b\u5efa\u74f4\u7684\u3001\u6574\u4f53\u7684\u8ba4\u77e5\uff08big picture\uff09\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u68b3\u7406\u6e05\u695a\u4e66\u4e2d\u5404\u4e2a\u7ae0\u8282\u4e4b\u95f4\u7684\u5173\u8054\u3002\u4ece\u4e00\u4e2asoftware engineer\u7684\u89c6\u89d2\u6765\u770b\uff0c\u6211\u89c9\u5f97OS kernel\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2aevent-driven system\uff0c\u5373\u6574\u4e2aOS kernel\u7684\u8fd0\u884c\u662f event \u9a71\u52a8\u7684\uff0clinux OS kernel\u7684\u5b9e\u73b0\u91c7\u7528\uff08\u90e8\u5206\uff09\u7684\u662f Event-driven architecture \uff0c Event-driven programming \u3002\u4e0b\u9762\u5bf9\u8fd9\u4e2a\u8bba\u65ad\u7684\u5206\u6790\u3002 \u6b63\u5982\u57281.4. Basic Operating System Concepts\u6240\u4ecb\u7ecd\u7684\uff1a The operating system interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. A Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. \u663e\u7136\uff0cOS kernel\u76f4\u63a5\u548chardware\u6253\u4ea4\u9053\uff0c\u90a3\u6709\u54ea\u4e9bhardware\u5462\uff1f\u5982\u4e0b\uff1a I/O devices Chapter 13. I/O Architecture and Device Drivers \u8865\u5145\uff1a Operating System - I/O Hardware Clock and Timer Circuits Chapter 6. Timing Measurements \u76ee\u524d\uff0c\u57fa\u672c\u4e0a\u6240\u6709\u7684hardware\u90fd\u662f\u901a\u8fc7 interrupt \u6765\u901a\u77e5OS kernel\u7684\uff0c\u7136\u540e\u5176\u5bf9\u5e94\u7684 Interrupt handler \u4f1a\u88ab\u89e6\u53d1\u6267\u884c\uff0c\u4e5f\u5c31\u662fOS kernel\u662f interrupt-driven \u7684\u3002\u62e5\u6709\u8fd9\u6837\u7684\u8ba4\u77e5\u5bf9\u4e8e\u5b8c\u6574\u5730\u638c\u63e1\u672c\u4e66\u7684\u5185\u5bb9\u5341\u5206\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u63cf\u8ff0\u4e86OS kernel\u8fd0\u884c\u7684\u6982\u51b5\u3002\u672c\u4e66\u7684Chapter 4. Interrupts and Exceptions\u4e13\u95e8\u63cf\u8ff0\u4e2d\u65ad\u76f8\u5173\u5185\u5bb9\uff0c\u5b83\u662f\u540e\u9762\u5f88\u591a\u7ae0\u8282\u7684\u57fa\u7840\uff0c\u56e0\u4e3aOS\u4e2d\u6709\u592a\u591a\u592a\u591a\u7684\u6d3b\u52a8\u90fd\u662finterrupt\u89e6\u53d1\u7684\uff0c\u6bd4\u5982\uff1a TODO: \u6b64\u5904\u6dfb\u52a0\u4e00\u4e9b\u4f8b\u5b50 Chapter 6. Timing Measurements\u4e3b\u8981\u63cf\u8ff0\u7684\u662ftiming measurements\u76f8\u5173\u7684hardware\uff08\u4e3b\u8981\u5305\u62ecClock and Timer Circuits\uff09\u4ee5\u53caOS kernel\u4e2d\u7531timing measurement\u9a71\u52a8\u7684\u91cd\u8981\u7684\u6d3b\u52a8\uff08\u4e0b\u9762\u4f1a\u6709\u4ecb\u7ecd\uff09\uff0c\u6b63\u5982\u672c\u7ae0\u5f00\u5934\u6240\u8ff0\uff1a Countless computerized activities are driven by timing measurements OS kernel\u7684\u4f17\u591a\u6838\u5fc3activity\u662fdriven by timing measurements\uff0c\u6b63\u59826.2. The Linux Timekeeping Architecture\u4e2d\u6240\u603b\u7ed3\u7684\uff1a Updates the time elapsed since system startup Updates the time and date Determines, for every CPU, how long the current process has been running, and preempts it if it has exceeded the time allocated to it. The allocation of time slots (also called \"quanta\") is discussed in Chapter 7. NOTE: \u8fd9\u4e2a\u6d3b\u52a8\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662fOS\u5b9e\u73b0 Time-sharing \uff0c\u8fdb\u800c\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5728linux kernel\u7684\u5b9e\u73b0\u4e2d\uff0c\u5b83\u7684\u5165\u53e3\u51fd\u6570\u662f scheduler_tick \uff0c\u641c\u7d22\u8fd9\u4e2a\u51fd\u6570\uff0c\u53ef\u4ee5\u67e5\u8be2\u5230\u975e\u5e38\u591a\u5173\u4e8e\u5b83\u7684\u5206\u6790\u3002 \u672c\u4e66\u4e2d\u5173\u4e8e\u8fd9\u4e2a\u51fd\u6570\u7684\u7ae0\u8282\uff1a 6.4. Updating System Statistics 7.4. Functions Used by the Scheduler Updates resource usage statistics. Checks whether the interval of time associated with each software timer (see the later section \"Software Timers and Delay Functions\") has elapsed. \u6211\u4eec\u60ca\u559c\u7684\u53d1\u73b0\u7ad9\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u63cf\u8ff0\u672c\u8d28\u4e0a\u975e\u5e38\u7c7b\u4f3c\u7684\u4e8b\u52a1\u6709\u7740\u4e0d\u540c\u7684\u8bf4\u6cd5\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a Hardware Software Interrupt-driven Event-driven architecture / Event-driven programming Interrupt Event (computing) Interrupt handler / Interrupt service routine Event handler / Callback function \u5404\u79cdinterrupt\u5c31\u662f\u6240\u8c13\u7684event\u3002 timer # timer interrupt\u5bf9\u7cfb\u7edf\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u7cfb\u7edf\u7684heartbeat\uff0c\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0ctimer\u5c31\u76f8\u5f53\u4e8e\u76f8\u540c\u7684heart\u3002\u56e0\u4e3a\u5b83\u5b83\u89e6\u53d1\u8fd9\u7cfb\u7edf\u7684\u8fd0\u8f6c\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u76f8\u540c\u7684\u7cfb\u7edf\u7684 Electric motor \uff0c\u6bd4\u5982\u5185\u71c3\u673a\u8fd0\u8f6c\u5e26\u52a8\u6574\u4e2a\u7cfb\u7edf\u8fd0\u8f6c\u8d77\u6765\u3002 system call\u4e5f\u76f8\u5f53\u4e8einterrupt # \u4e0a\u9762\u4f7f\u7528\u7684\u662f\u201c\u76f8\u5f53\u4e8e\u201d\uff0c\u800c\u4e0d\u662f\u201c\u662f\u201d\uff0c\u8fd9\u662f\u56e0\u4e3a\u968f\u7740\u6280\u672f\u7684\u66f4\u65b0\u8fed\u4ee3\uff0c\u5b9e\u73b0system call\u7684assembly instruction\u4e5f\u5728\u8fdb\u884c\u66f4\u65b0\u8fed\u4ee3\uff0c\u53ef\u80fd\u539f\u6765\u4f7f\u7528\u7684\u4e2d\u65ad\u6307\u4ee4\uff08 int assembly instruction\uff09\u4f1a\u66ff\u6362\u4e3a\u66f4\u52a0\u9ad8\u6548\u7684assembly instruction\u3002\u572810.3. Entering and Exiting a System Call\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u8bf4\u660e\uff0c\u5982\u4e0b\uff1a Applications can invoke a system call in two different ways: By executing the int $0x80 assembly language instruction; in older versions of the Linux kernel, this was the only way to switch from User Mode to Kernel Mode. By executing the sysenter assembly language instruction, introduced in the Intel Pentium II microprocessors; this instruction is now supported by the Linux 2.6 kernel. \u4f7f\u7528 int $0x80 \u7684\u65b9\u5f0f\u662finterrupt\uff0c\u4f7f\u7528 sysenter \u7684\u65b9\u5f0f\u5219\u4e0d\u662finterrupt\uff0c\u4f46\u662f\u5b83\u7684\u4f5c\u7528\u5176\u5b9e\u548cinterrupt\u975e\u5e38\u7c7b\u4f3c\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u770b\u505a\u662finterrupt\u3002 \u5173\u4e8e sysenter \uff0c\u53c2\u52a0\uff1a https://wiki.osdev.org/Sysenter \u4e0a\u9762\u63cf\u8ff0\u7684interrupt\u4e3b\u8981\u6765\u81ea\u4e8ehardware\uff0c\u5176\u5b9esystem call\u7684\u5b9e\u73b0\u4e5f\u662f\u4f9d\u8d56\u4e8einterrupt\u3002 \u603b\u7ed3 # \u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230OS kernel\u7684\u6240\u6709activity\u5176\u5b9e\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662fevent-driven\u7684\uff1aOS kernel\u7ba1\u7406\u7740hardware\u3001process\uff0c\u5b83\u4f5c\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u4e2d\u95f4\u5c42\uff0c\u53ef\u4ee5\u8ba4\u4e3aOS\u7684\u6240\u6709\u7684activity\u90fd\u662f\u7531\u5b83\u4eec\u89e6\u53d1\u7684\u3002 \u5efa\u7acb\u8fd9\u6837\u7684\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u5bf9\u4e8e\u540e\u9762\u8ba8\u8bbaOS kernel\u7684\u5b9e\u73b0\u601d\u8def\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002","title":"How-OS-run-01-OS-kernel-is-event-driven"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-01-OS-kernel-is-event-driven/#os-kernel-is-event-driven","text":"\u5173\u4e8eOS\u7684\u4f5c\u7528\uff0c\u672c\u4e66\u4e2d\u5df2\u7ecf\u82b1\u4e86\u975e\u5e38\u591a\u7684\u7bc7\u5e45\u6765\u8fdb\u884c\u4ecb\u7ecd\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u7ae0\u8282\uff1a 1.4. Basic Operating System Concepts 1.6. An Overview of Unix Kernels \u5728\u9605\u8bfb\u8fc7\u7a0b\u4e2d\uff0c\u6709\u5fc5\u8981\u5efa\u7acb\u8d77\u5bf9OS\u8fd0\u884c\u6982\u51b5\u7684\u9ad8\u5c4b\u5efa\u74f4\u7684\u3001\u6574\u4f53\u7684\u8ba4\u77e5\uff08big picture\uff09\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u68b3\u7406\u6e05\u695a\u4e66\u4e2d\u5404\u4e2a\u7ae0\u8282\u4e4b\u95f4\u7684\u5173\u8054\u3002\u4ece\u4e00\u4e2asoftware engineer\u7684\u89c6\u89d2\u6765\u770b\uff0c\u6211\u89c9\u5f97OS kernel\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2aevent-driven system\uff0c\u5373\u6574\u4e2aOS kernel\u7684\u8fd0\u884c\u662f event \u9a71\u52a8\u7684\uff0clinux OS kernel\u7684\u5b9e\u73b0\u91c7\u7528\uff08\u90e8\u5206\uff09\u7684\u662f Event-driven architecture \uff0c Event-driven programming \u3002\u4e0b\u9762\u5bf9\u8fd9\u4e2a\u8bba\u65ad\u7684\u5206\u6790\u3002 \u6b63\u5982\u57281.4. Basic Operating System Concepts\u6240\u4ecb\u7ecd\u7684\uff1a The operating system interact with the hardware components, servicing all low-level programmable elements included in the hardware platform. A Unix-like operating system hides all low-level details concerning the physical organization of the computer from applications run by the user. \u663e\u7136\uff0cOS kernel\u76f4\u63a5\u548chardware\u6253\u4ea4\u9053\uff0c\u90a3\u6709\u54ea\u4e9bhardware\u5462\uff1f\u5982\u4e0b\uff1a I/O devices Chapter 13. I/O Architecture and Device Drivers \u8865\u5145\uff1a Operating System - I/O Hardware Clock and Timer Circuits Chapter 6. Timing Measurements \u76ee\u524d\uff0c\u57fa\u672c\u4e0a\u6240\u6709\u7684hardware\u90fd\u662f\u901a\u8fc7 interrupt \u6765\u901a\u77e5OS kernel\u7684\uff0c\u7136\u540e\u5176\u5bf9\u5e94\u7684 Interrupt handler \u4f1a\u88ab\u89e6\u53d1\u6267\u884c\uff0c\u4e5f\u5c31\u662fOS kernel\u662f interrupt-driven \u7684\u3002\u62e5\u6709\u8fd9\u6837\u7684\u8ba4\u77e5\u5bf9\u4e8e\u5b8c\u6574\u5730\u638c\u63e1\u672c\u4e66\u7684\u5185\u5bb9\u5341\u5206\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u63cf\u8ff0\u4e86OS kernel\u8fd0\u884c\u7684\u6982\u51b5\u3002\u672c\u4e66\u7684Chapter 4. Interrupts and Exceptions\u4e13\u95e8\u63cf\u8ff0\u4e2d\u65ad\u76f8\u5173\u5185\u5bb9\uff0c\u5b83\u662f\u540e\u9762\u5f88\u591a\u7ae0\u8282\u7684\u57fa\u7840\uff0c\u56e0\u4e3aOS\u4e2d\u6709\u592a\u591a\u592a\u591a\u7684\u6d3b\u52a8\u90fd\u662finterrupt\u89e6\u53d1\u7684\uff0c\u6bd4\u5982\uff1a TODO: \u6b64\u5904\u6dfb\u52a0\u4e00\u4e9b\u4f8b\u5b50 Chapter 6. Timing Measurements\u4e3b\u8981\u63cf\u8ff0\u7684\u662ftiming measurements\u76f8\u5173\u7684hardware\uff08\u4e3b\u8981\u5305\u62ecClock and Timer Circuits\uff09\u4ee5\u53caOS kernel\u4e2d\u7531timing measurement\u9a71\u52a8\u7684\u91cd\u8981\u7684\u6d3b\u52a8\uff08\u4e0b\u9762\u4f1a\u6709\u4ecb\u7ecd\uff09\uff0c\u6b63\u5982\u672c\u7ae0\u5f00\u5934\u6240\u8ff0\uff1a Countless computerized activities are driven by timing measurements OS kernel\u7684\u4f17\u591a\u6838\u5fc3activity\u662fdriven by timing measurements\uff0c\u6b63\u59826.2. The Linux Timekeeping Architecture\u4e2d\u6240\u603b\u7ed3\u7684\uff1a Updates the time elapsed since system startup Updates the time and date Determines, for every CPU, how long the current process has been running, and preempts it if it has exceeded the time allocated to it. The allocation of time slots (also called \"quanta\") is discussed in Chapter 7. NOTE: \u8fd9\u4e2a\u6d3b\u52a8\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662fOS\u5b9e\u73b0 Time-sharing \uff0c\u8fdb\u800c\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5728linux kernel\u7684\u5b9e\u73b0\u4e2d\uff0c\u5b83\u7684\u5165\u53e3\u51fd\u6570\u662f scheduler_tick \uff0c\u641c\u7d22\u8fd9\u4e2a\u51fd\u6570\uff0c\u53ef\u4ee5\u67e5\u8be2\u5230\u975e\u5e38\u591a\u5173\u4e8e\u5b83\u7684\u5206\u6790\u3002 \u672c\u4e66\u4e2d\u5173\u4e8e\u8fd9\u4e2a\u51fd\u6570\u7684\u7ae0\u8282\uff1a 6.4. Updating System Statistics 7.4. Functions Used by the Scheduler Updates resource usage statistics. Checks whether the interval of time associated with each software timer (see the later section \"Software Timers and Delay Functions\") has elapsed. \u6211\u4eec\u60ca\u559c\u7684\u53d1\u73b0\u7ad9\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u63cf\u8ff0\u672c\u8d28\u4e0a\u975e\u5e38\u7c7b\u4f3c\u7684\u4e8b\u52a1\u6709\u7740\u4e0d\u540c\u7684\u8bf4\u6cd5\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff1a Hardware Software Interrupt-driven Event-driven architecture / Event-driven programming Interrupt Event (computing) Interrupt handler / Interrupt service routine Event handler / Callback function \u5404\u79cdinterrupt\u5c31\u662f\u6240\u8c13\u7684event\u3002","title":"OS kernel is event-driven"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-01-OS-kernel-is-event-driven/#timer","text":"timer interrupt\u5bf9\u7cfb\u7edf\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u7cfb\u7edf\u7684heartbeat\uff0c\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0ctimer\u5c31\u76f8\u5f53\u4e8e\u76f8\u540c\u7684heart\u3002\u56e0\u4e3a\u5b83\u5b83\u89e6\u53d1\u8fd9\u7cfb\u7edf\u7684\u8fd0\u8f6c\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8e\u76f8\u540c\u7684\u7cfb\u7edf\u7684 Electric motor \uff0c\u6bd4\u5982\u5185\u71c3\u673a\u8fd0\u8f6c\u5e26\u52a8\u6574\u4e2a\u7cfb\u7edf\u8fd0\u8f6c\u8d77\u6765\u3002","title":"timer"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-01-OS-kernel-is-event-driven/#system-callinterrupt","text":"\u4e0a\u9762\u4f7f\u7528\u7684\u662f\u201c\u76f8\u5f53\u4e8e\u201d\uff0c\u800c\u4e0d\u662f\u201c\u662f\u201d\uff0c\u8fd9\u662f\u56e0\u4e3a\u968f\u7740\u6280\u672f\u7684\u66f4\u65b0\u8fed\u4ee3\uff0c\u5b9e\u73b0system call\u7684assembly instruction\u4e5f\u5728\u8fdb\u884c\u66f4\u65b0\u8fed\u4ee3\uff0c\u53ef\u80fd\u539f\u6765\u4f7f\u7528\u7684\u4e2d\u65ad\u6307\u4ee4\uff08 int assembly instruction\uff09\u4f1a\u66ff\u6362\u4e3a\u66f4\u52a0\u9ad8\u6548\u7684assembly instruction\u3002\u572810.3. Entering and Exiting a System Call\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u8bf4\u660e\uff0c\u5982\u4e0b\uff1a Applications can invoke a system call in two different ways: By executing the int $0x80 assembly language instruction; in older versions of the Linux kernel, this was the only way to switch from User Mode to Kernel Mode. By executing the sysenter assembly language instruction, introduced in the Intel Pentium II microprocessors; this instruction is now supported by the Linux 2.6 kernel. \u4f7f\u7528 int $0x80 \u7684\u65b9\u5f0f\u662finterrupt\uff0c\u4f7f\u7528 sysenter \u7684\u65b9\u5f0f\u5219\u4e0d\u662finterrupt\uff0c\u4f46\u662f\u5b83\u7684\u4f5c\u7528\u5176\u5b9e\u548cinterrupt\u975e\u5e38\u7c7b\u4f3c\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u770b\u505a\u662finterrupt\u3002 \u5173\u4e8e sysenter \uff0c\u53c2\u52a0\uff1a https://wiki.osdev.org/Sysenter \u4e0a\u9762\u63cf\u8ff0\u7684interrupt\u4e3b\u8981\u6765\u81ea\u4e8ehardware\uff0c\u5176\u5b9esystem call\u7684\u5b9e\u73b0\u4e5f\u662f\u4f9d\u8d56\u4e8einterrupt\u3002","title":"system call\u4e5f\u76f8\u5f53\u4e8einterrupt"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-01-OS-kernel-is-event-driven/#_1","text":"\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230OS kernel\u7684\u6240\u6709activity\u5176\u5b9e\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662fevent-driven\u7684\uff1aOS kernel\u7ba1\u7406\u7740hardware\u3001process\uff0c\u5b83\u4f5c\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u4e2d\u95f4\u5c42\uff0c\u53ef\u4ee5\u8ba4\u4e3aOS\u7684\u6240\u6709\u7684activity\u90fd\u662f\u7531\u5b83\u4eec\u89e6\u53d1\u7684\u3002 \u5efa\u7acb\u8fd9\u6837\u7684\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u5bf9\u4e8e\u540e\u9762\u8ba8\u8bbaOS kernel\u7684\u5b9e\u73b0\u601d\u8def\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002","title":"\u603b\u7ed3"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-02-kernel-control-path-and-reentrant-kernel/","text":"kernel control path and reentrant kernel # \u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81eachapter 1.6.3. Reentrant Kernels \u5728 How-OS-run-01-OS-kernel-is-event-driven \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5efa\u7acb\u8d77\u6765\u4e86OS kernel\u7684\u8fd0\u884c\u6982\u51b5\uff0c\u5373OS kernel\u662fevent-driven\u7684\uff0c\u90a3\u73b0\u5728\u8ba9\u6211\u4eec\u7ad9\u5728\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u5982\u4f55\u6765\u5b9e\u73b0\uff1f \u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2asystem call\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2asystem call\uff1b\u5f53\u8be5system call\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u6302\u8d77\u5f53\u524d\u7684system call\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684interrupt handler\uff0c\u5f53\u8fd9\u4e2ainterrupt handler\u6267\u884c\u5b8c\u6210\u540e\uff0c\u5728\u91cd\u542f\u4e4b\u524d\u88ab\u4e2d\u65ad\u7684system call\uff08\u662f\u5426\u4f1a\u91cd\u542f\u5176\u5b9e\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\uff0c\u540e\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u4e13\u95e8\u5206\u6790\uff09\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662fchapter 1.6.3. Reentrant Kernels\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002 kernel control path # \u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\u601d\u8def\uff0c\u5728chapter 1.6.3. Reentrant Kernels\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684activity\uff0c\u5728 How-OS-run-01-OS-kernel-is-event-driven \u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\uff0ckernel\u7684activity\u53ef\u80fd\u6709\u5982\u4e0b\u51e0\u79cd\u60c5\u51b5\u89e6\u53d1\uff1a system call interrupt and exception(\u5728Chapter 4. Interrupts and Exceptions\u533a\u5206\u8fd9\u4e24\u8005) \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff08\u6302\u8d77\uff09\uff0cresume\uff08\u91cd\u542f\uff09\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u4f7f\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728chapter 1.6.3. Reentrant Kernels\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83\u5bf9kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization \u5173\u4e8e\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u7684\u4e00\u4e9b\u6267\u884c\u7ec6\u8282\uff0c\u6bd4\u5982kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662fkernel control path runs on behalf of process\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff1a Kernel Control Path Definition","title":"How-OS-run-02-kernel-control-path-and-reentrant-kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-02-kernel-control-path-and-reentrant-kernel/#kernel-control-path-and-reentrant-kernel","text":"\u672c\u8282\u7684\u5185\u5bb9\u4e3b\u8981\u6e90\u81eachapter 1.6.3. Reentrant Kernels \u5728 How-OS-run-01-OS-kernel-is-event-driven \u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u5efa\u7acb\u8d77\u6765\u4e86OS kernel\u7684\u8fd0\u884c\u6982\u51b5\uff0c\u5373OS kernel\u662fevent-driven\u7684\uff0c\u90a3\u73b0\u5728\u8ba9\u6211\u4eec\u7ad9\u5728\u5185\u6838\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003\u5982\u4f55\u6765\u5b9e\u73b0\uff1f \u5185\u6838\u7684\u8bbe\u8ba1\u8005\u4f1a\u8ffd\u6c42\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5730\u54cd\u5e94\u7528\u6237\u7684\u8bf7\u6c42\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5730\u8fd0\u884c\uff0c\u7cfb\u7edf\u9700\u8981\u5c3d\u53ef\u80fd\u7684\u538b\u7f29CPU\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u8ba9CPU\u66f4\u591a\u5730\u8fdb\u884c\u8fd0\u8f6c\u3002\u6240\u4ee5\uff0c\u5b83\u5c31\u9700\u8981\u5728\u67d0\u4e2asystem call\u6682\u65f6\u65e0\u6cd5\u5b8c\u6210\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5b83\u6302\u8d77\u5e76\u8f6c\u5411\u53e6\u5916\u4e00\u4e2asystem call\uff1b\u5f53\u8be5system call\u7684\u6267\u884c\u6761\u4ef6\u6ee1\u8db3\u7684\u65f6\u5019\u518d\u5c06\u5b83\u91cd\u542f\uff1b\u53e6\u5916\uff0ckernel\u8fd8\u9700\u8981\u5904\u7406\u65e0\u6cd5\u9884\u6d4b\u4f55\u65f6\u4f1a\u51fa\u73b0\u7684\u5404\u79cdinterrupt\u548cexception\uff0c\u6302\u8d77\u5f53\u524d\u7684system call\u8f6c\u53bb\u6267\u884c\u76f8\u5e94\u7684interrupt handler\uff0c\u5f53\u8fd9\u4e2ainterrupt handler\u6267\u884c\u5b8c\u6210\u540e\uff0c\u5728\u91cd\u542f\u4e4b\u524d\u88ab\u4e2d\u65ad\u7684system call\uff08\u662f\u5426\u4f1a\u91cd\u542f\u5176\u5b9e\u662f\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\uff0c\u540e\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u4e13\u95e8\u5206\u6790\uff09\u3002\u8fd9\u79cd\u80fd\u529b\u5c31\u662fchapter 1.6.3. Reentrant Kernels\u6240\u8ff0\u7684 reentrant \u3002\u663e\u7136\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u591f\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9ad8\u6548\u3002","title":"kernel control path and reentrant kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-02-kernel-control-path-and-reentrant-kernel/#kernel-control-path","text":"\u4e3a\u4e86\u4fbf\u4e8e\u63cf\u8ff0reentrant kernel\u7684\u5b9e\u73b0\u601d\u8def\uff0c\u5728chapter 1.6.3. Reentrant Kernels\u4e2d\u4f5c\u8005\u63d0\u51fa\u4e86 kernel control path \u7684\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u4e86kernel\u6240\u6709\u7684\u53ef\u80fd\u7684activity\uff0c\u5728 How-OS-run-01-OS-kernel-is-event-driven \u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\uff0ckernel\u7684activity\u53ef\u80fd\u6709\u5982\u4e0b\u51e0\u79cd\u60c5\u51b5\u89e6\u53d1\uff1a system call interrupt and exception(\u5728Chapter 4. Interrupts and Exceptions\u533a\u5206\u8fd9\u4e24\u8005) \u4e5f\u5c31\u662f\u8bf4\uff1a \u5f53process\u5411kernel\u8bf7\u6c42\u4e00\u4e2asystem call\uff0c\u6b64\u65f6kernel\u4e2d\u5c31\u6267\u884c\u6b64system call\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u5219\u662fkernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2asystem call\u7684kernel control path\uff1b \u5f53\u4ea7\u751finterrupt\u6216exception\uff0c\u6b64\u65f6kernel\u8f6c\u53bb\u6267\u884c\u5b83\u4eec\u5bf9\u5e94\u7684handler\uff0c\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0c\u53ef\u4ee5\u8ba4\u4e3akernel\u521b\u5efa\u4e86\u4e00\u4e2a\u6267\u884c\u8fd9\u4e2ahandler\u7684kernel control path\uff1b \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528 kernel control path \u6982\u5ff5\u6765\u8fdb\u884c\u63cf\u8ff0\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u77e5\u9053\uff0coperating system\u7684kernel\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5b83\u9700\u8981\u540c\u65f6\u5904\u7406\u975e\u5e38\u591a\u7684\u4e8b\u60c5\uff0c\u6bd4\u5982process\u8bf7\u6c42\u7684system call\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u662f\u4f1a\u4f34\u968f\u4e2d\u968f\u65f6\u53ef\u80fd\u53d1\u751f\u7684interrupt\u548cexception\u7684\u3002\u524d\u9762\u6211\u4eec\u5df2\u7ecf\u94fa\u57ab\u4e86\uff0ckernel\u4e3a\u4e86\u4fdd\u6301\u9ad8\u6548\uff0c\u53ef\u80fd\u9700\u8981\u6302\u8d77\u6b63\u5728\u6267\u884c\u7684\u6d41\u7a0b\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2a\u6d41\u7a0b\uff0c\u800c\u540e\u5728\u91cd\u542f\u4e4b\u524d\u6302\u8d77\u7684\u6d41\u7a0b\u3002\u6b64\u5904\u6240\u8c13\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u66f4\u52a0\u4e13\u4e1a\u7684\u672f\u8bed\u5c31\u662fkernel control path\u3002\u663e\u7136\u4e0efunction\u76f8\u6bd4\uff0ckernel control path\u8574\u542b\u7740\u66f4\u52a0\u4e30\u5bcc\u7684\uff0c\u66f4\u52a0\u7b26\u5408kernel\u8c03\u5ea6\u60c5\u51b5\u7684\u5185\u6db5\uff0c\u6bd4\u5982\u5b83\u80fd\u591f\u8868\u793akernel\u7684suspend\uff08\u6302\u8d77\uff09\uff0cresume\uff08\u91cd\u542f\uff09\uff0c\u80fd\u591f\u8868\u793a\u591a\u4e2acontrol path\u7684interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\u3002\u8fd9\u79cd\u901a\u8fc7\u521b\u9020\u65b0\u7684\u6982\u5ff5\u6765\u4f7f\u8868\u8ff0\u66f4\u52a0\u4fbf\u5229\u7684\u505a\u6cd5\u662f\u5728\u5404\u79cd\u5b66\u79d1\u975e\u5e38\u666e\u904d\u7684\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u5bfc\u81f4\u7cfb\u7edf\u7684\u590d\u6742\uff0c\u6b63\u5982\u5728chapter 1.6.3. Reentrant Kernels\u540e\u9762\u6240\u8ff0\u7684\uff0c \u7cfb\u7edf\u662f\u5728\u591a\u4e2a kernel control path \u4e2d\u4ea4\u9519\u8fd0\u884c\u7684\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f1a\u6d3e\u751f\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u5c06\u57281.6.5. Synchronization and Critical Regions\u4e2d\u4ecb\u7ecd\u7684race condition\uff0c\u6240\u4ee5\u5b83\u5bf9kernel\u7684\u5b9e\u73b0\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u5f53\u7136\u53ef\u4ee5\u9884\u671f\u7684\u662f\uff0c\u7cfb\u7edf\u662f\u5728\u8fd9\u6837\u7684\u4ea4\u9519\u4e2d\u4e0d\u65ad\u5411\u524d\u8fdb\u7684\u3002 \u5982\u4f55\u6765\u5b9e\u73b0reentrant kernel\u5462\uff1f\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u7cfb\u7edf\u5730\u8fdb\u884c\u8bbe\u8ba1\u624d\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e0b\u9762\u603b\u7ed3\u4e86\u548c\u8fd9\u4e2a\u95ee\u9898\u76f8\u5173\u7684\u4e00\u4e9b\u7ae0\u8282\uff1a 1.6.4. Process Address Space Kernel control path refers to its own private kernel stack. 1.6.5. Synchronization and Critical Regions \u63cf\u8ff0\u4e86kernel control path\u7684Synchronization \u5173\u4e8e\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u7684\u4e00\u4e9b\u6267\u884c\u7ec6\u8282\uff0c\u6bd4\u5982kernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u8054\u662f\u672c\u4e66\u4e2d\u4f1a\u4e00\u76f4\u5f3a\u8c03\u7684\u5185\u5bb9\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff0c\u5176\u4e2d\u6700\u6700\u5178\u578b\u7684\u5c31\u662fkernel control path runs on behalf of process\u3002\u4e3a\u4e86\u4eca\u540e\u4fbf\u4e8e\u5feb\u901f\u5730\u68c0\u7d22\u5230\u8fd9\u4e9b\u5185\u5bb9\uff0c\u73b0\u5c06\u672c\u4e66\u4e2d\u6240\u6709\u7684\u4e0e\u6b64\u76f8\u5173\u5185\u5bb9\u7684\u4f4d\u7f6e\u5168\u90e8\u90fd\u6574\u7406\u5230\u8fd9\u91cc\uff1a chapter 1.6.3. Reentrant Kernels \u672c\u8282\u7684\u540e\u534a\u90e8\u5206\u5bf9kernel control path\u7684\u4e00\u4e9b\u53ef\u80fd\u60c5\u51b5\u8fdb\u884c\u4e86\u679a\u4e3e\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0ckernel control path\u548cprocess\u4e4b\u95f4\u7684\u5173\u7cfb Chapter 4. Interrupts and Exceptions \u4e3b\u8981\u63cf\u8ff0\u4e86Interrupts and Exceptions\u89e6\u53d1\u7684kernel control path\u7684\u6267\u884c\u60c5\u51b5\u3002\u5e76\u4e14\u5176\u4e2d\u8fd8\u5bf9\u6bd4\u4e86interrupt \u89e6\u53d1\u7684kernel control path\u548csystem call\u89e6\u53d1\u7684kernel control path\u4e4b\u95f4\u7684\u5dee\u5f02\u7b49\u5185\u5bb9\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff1a Kernel Control Path Definition","title":"kernel control path"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-03-Control-path-&-Context-&-Context-switch/","text":"Control path # Control path\u8fd9\u4e2a\u6982\u5ff5\u662f\u6211\u7531kernel control path\u542f\u53d1\u800c\u521b\u5efa\u7684\uff0c\u5b83\u8868\u793aOS\u4e2d\u6240\u6709\u53ef\u80fd\u7684\u6d3b\u52a8/\u6267\u884c\u6d41\u7a0b\uff0c\u4e4b\u6240\u4ee5\u521b\u5efa\u8fd9\u4e2a\u6982\u5ff5\uff0c\u662f\u56e0\u4e3a\u5b83\u5177\u5907\u4e00\u4e9b\u7279\u5f81\u53ef\u4ee5\u65b9\u4fbf\u6211\u4eec\u6765\u7edf\u4e00\u5730\u3001\u6982\u62ec\u5730\u63cf\u8ff0\u95ee\u9898\uff08\u4e00\u4e2a\u62bd\u8c61\u8fc7\u7a0b\uff09\u3002 OS\u4e2d\u6709\u5982\u4e0bcontrol path\uff1a kernel control path kernel thread task\uff08process/thread\uff0c\u73b0\u4ee3OS\u9700\u8981\u652f\u6301 multitasking \uff09 Control path\u7684\u6267\u884c\u90fd\u53ef\u80fd\u4f1a\u88abinterrupted\uff1a \u4e00\u65e6\u53d1\u751f\u4e86hardware interrupt\uff0cOS kernel\u4f1a\u7acb\u5373\u53bb\u54cd\u5e94\uff0c\u4ece\u800cinterrupt\uff08suspend\uff09\u5f53\u524d\u6267\u884c\u7684kernel control path\uff0c\u8f6c\u53bb\u6267\u884c\u65b0\u7684kernel control path\u3002\u5373\u539fkernel control path\u4f1a\u88abinterrupted\u3002 task\u662f\u73b0\u4ee3OS\u4e3a\u652f\u6301 multitasking \u800c\u521b\u5efa\u7684\uff0c\u5b83\u7531 scheduler \u8fdb\u884c\u8c03\u5ea6\u6267\u884c\u7684\uff0c\u76ee\u524dlinux\u91c7\u53d6\u7684\u8c03\u5ea6\u7b56\u7565\u662f Preemptive multitasking \uff0c\u8fd9\u79cd\u7b56\u7565\u7684\u672c\u8d28\u662f\uff1a It is normally carried out by a privileged task or part of the system known as a preemptive scheduler , which has the power to preempt , or interrupt, and later resume, other tasks in the system. \u5373\u5b83\u53ef\u80fd\u4f1ainterrupt\uff08suspend\uff09\u6b63\u5728\u6267\u884c\u7684task\uff0c\u7136\u540e\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2atask\u3002 \u663e\u7136\u8fd9\u662fOS\u4e3a\u4e86\u9ad8\u6548\uff0c\u8ba9\u591a\u4e2acontrol path interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\uff0c\u4e3a\u4e86\u5b9e\u73b0 Reentrancy \uff0c\u6bcf\u4e2acontrol path\u90fd\u8981\u6709\u81ea\u5df1private\u7684context\u3001address space\uff0c\u8fd9\u5176\u5b9e\u662f\u4e00\u4e2aseparation\u673a\u5236\u3002\u5b83\u80fd\u591f\u4f7f\u4e00\u4e2acontrol path\u5728\u88absuspend\u540e\uff0c\u8fc7\u540e\u80fd\u591f\u88abresume\uff0c\u5176\u5b9e\u8fd9\u662f\u5728 How-OS-run-02-kernel-control-path-and-reentrant-kernel \u4e2d\u63d0\u51fa\u7684reentrant\u601d\u60f3\u3002 \u5f53\u5b83\u4eec\u88abinterrupted\u7684\u65f6\u5019\uff0c\u90fd\u4f1a\u6d89\u53ca\u5230context switch\uff0c\u56e0\u4e3aOS\u4e3a\u4e86\u9ad8\u6548\uff0c\u80af\u5b9a\u4f1a\u8ba9\u591a\u4e2acontrol path interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\uff0c\u5c31\u5fc5\u7136\u9700\u8981\u7ef4\u62a4\u6bcf\u4e2acontrol path\u7684context\uff0ccontext\u5176\u5b9e\u662f\u4e00\u79cdseparation\u673a\u5236\uff0c \u663e\u7136context\u5305\u62ec\u6bcf\u4e2acontrol path\u7684private\u6570\u636e\uff0c\u5982\u4e0b\uff1a call stack context switch # \u6267\u884ccontext switch\u7684\u76ee\u7684\uff1a Computer multitasking \u8fdb\u7a0b\u662foperating system\u7684\u6982\u5ff5\uff0c\u5b83\u662f\u4e3a\u4e86\u5b9e\u73b0 Computer multitasking \uff0c\u4ee5\u5145\u5206\u5229\u7528hardware\uff0c\u5728hardware\u4e2d\uff0c\u5e76\u6ca1\u6709\u8fdb\u7a0b\u7684\u6982\u5ff5\u3002 \u53d1\u751fcontext switch\u7684\u573a\u666f\uff1a Scheduler\u89e6\u53d1Process Switch # 3.3. Process Switch kernel substitutes one process for another process Interrupt Signals\u89e6\u53d1Switch # 4.1. The Role of Interrupt Signals the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). 4.3. Nested Execution of Exception and Interrupt Handlers \u601d\u8003\uff1a\u53d1\u751fcontext switch\u7684\u65f6\u5019\uff0c\u8981\u628acontext\u7f6e\u4e8e\u4f55\u5904\u5462\uff1f","title":"How-OS-run-03-Control-path-&-Context-&-Context-switch"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-03-Control-path-&-Context-&-Context-switch/#control-path","text":"Control path\u8fd9\u4e2a\u6982\u5ff5\u662f\u6211\u7531kernel control path\u542f\u53d1\u800c\u521b\u5efa\u7684\uff0c\u5b83\u8868\u793aOS\u4e2d\u6240\u6709\u53ef\u80fd\u7684\u6d3b\u52a8/\u6267\u884c\u6d41\u7a0b\uff0c\u4e4b\u6240\u4ee5\u521b\u5efa\u8fd9\u4e2a\u6982\u5ff5\uff0c\u662f\u56e0\u4e3a\u5b83\u5177\u5907\u4e00\u4e9b\u7279\u5f81\u53ef\u4ee5\u65b9\u4fbf\u6211\u4eec\u6765\u7edf\u4e00\u5730\u3001\u6982\u62ec\u5730\u63cf\u8ff0\u95ee\u9898\uff08\u4e00\u4e2a\u62bd\u8c61\u8fc7\u7a0b\uff09\u3002 OS\u4e2d\u6709\u5982\u4e0bcontrol path\uff1a kernel control path kernel thread task\uff08process/thread\uff0c\u73b0\u4ee3OS\u9700\u8981\u652f\u6301 multitasking \uff09 Control path\u7684\u6267\u884c\u90fd\u53ef\u80fd\u4f1a\u88abinterrupted\uff1a \u4e00\u65e6\u53d1\u751f\u4e86hardware interrupt\uff0cOS kernel\u4f1a\u7acb\u5373\u53bb\u54cd\u5e94\uff0c\u4ece\u800cinterrupt\uff08suspend\uff09\u5f53\u524d\u6267\u884c\u7684kernel control path\uff0c\u8f6c\u53bb\u6267\u884c\u65b0\u7684kernel control path\u3002\u5373\u539fkernel control path\u4f1a\u88abinterrupted\u3002 task\u662f\u73b0\u4ee3OS\u4e3a\u652f\u6301 multitasking \u800c\u521b\u5efa\u7684\uff0c\u5b83\u7531 scheduler \u8fdb\u884c\u8c03\u5ea6\u6267\u884c\u7684\uff0c\u76ee\u524dlinux\u91c7\u53d6\u7684\u8c03\u5ea6\u7b56\u7565\u662f Preemptive multitasking \uff0c\u8fd9\u79cd\u7b56\u7565\u7684\u672c\u8d28\u662f\uff1a It is normally carried out by a privileged task or part of the system known as a preemptive scheduler , which has the power to preempt , or interrupt, and later resume, other tasks in the system. \u5373\u5b83\u53ef\u80fd\u4f1ainterrupt\uff08suspend\uff09\u6b63\u5728\u6267\u884c\u7684task\uff0c\u7136\u540e\u8f6c\u53bb\u6267\u884c\u53e6\u5916\u4e00\u4e2atask\u3002 \u663e\u7136\u8fd9\u662fOS\u4e3a\u4e86\u9ad8\u6548\uff0c\u8ba9\u591a\u4e2acontrol path interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\uff0c\u4e3a\u4e86\u5b9e\u73b0 Reentrancy \uff0c\u6bcf\u4e2acontrol path\u90fd\u8981\u6709\u81ea\u5df1private\u7684context\u3001address space\uff0c\u8fd9\u5176\u5b9e\u662f\u4e00\u4e2aseparation\u673a\u5236\u3002\u5b83\u80fd\u591f\u4f7f\u4e00\u4e2acontrol path\u5728\u88absuspend\u540e\uff0c\u8fc7\u540e\u80fd\u591f\u88abresume\uff0c\u5176\u5b9e\u8fd9\u662f\u5728 How-OS-run-02-kernel-control-path-and-reentrant-kernel \u4e2d\u63d0\u51fa\u7684reentrant\u601d\u60f3\u3002 \u5f53\u5b83\u4eec\u88abinterrupted\u7684\u65f6\u5019\uff0c\u90fd\u4f1a\u6d89\u53ca\u5230context switch\uff0c\u56e0\u4e3aOS\u4e3a\u4e86\u9ad8\u6548\uff0c\u80af\u5b9a\u4f1a\u8ba9\u591a\u4e2acontrol path interleave\uff08\u4ea4\u9519\u8fd0\u884c\uff09\uff0c\u5c31\u5fc5\u7136\u9700\u8981\u7ef4\u62a4\u6bcf\u4e2acontrol path\u7684context\uff0ccontext\u5176\u5b9e\u662f\u4e00\u79cdseparation\u673a\u5236\uff0c \u663e\u7136context\u5305\u62ec\u6bcf\u4e2acontrol path\u7684private\u6570\u636e\uff0c\u5982\u4e0b\uff1a call stack","title":"Control path"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-03-Control-path-&-Context-&-Context-switch/#context-switch","text":"\u6267\u884ccontext switch\u7684\u76ee\u7684\uff1a Computer multitasking \u8fdb\u7a0b\u662foperating system\u7684\u6982\u5ff5\uff0c\u5b83\u662f\u4e3a\u4e86\u5b9e\u73b0 Computer multitasking \uff0c\u4ee5\u5145\u5206\u5229\u7528hardware\uff0c\u5728hardware\u4e2d\uff0c\u5e76\u6ca1\u6709\u8fdb\u7a0b\u7684\u6982\u5ff5\u3002 \u53d1\u751fcontext switch\u7684\u573a\u666f\uff1a","title":"context switch"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-03-Control-path-&-Context-&-Context-switch/#schedulerprocess-switch","text":"3.3. Process Switch kernel substitutes one process for another process","title":"Scheduler\u89e6\u53d1Process Switch"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-03-Control-path-&-Context-&-Context-switch/#interrupt-signalsswitch","text":"4.1. The Role of Interrupt Signals the code executed by an interrupt or by an exception handler is not a process. Rather, it is a kernel control path that runs at the expense of the same process that was running when the interrupt occurred As a kernel control path, the interrupt handler is lighter than a process (it has less context and requires less time to set up or tear down). 4.3. Nested Execution of Exception and Interrupt Handlers \u601d\u8003\uff1a\u53d1\u751fcontext switch\u7684\u65f6\u5019\uff0c\u8981\u628acontext\u7f6e\u4e8e\u4f55\u5904\u5462\uff1f","title":"Interrupt Signals\u89e6\u53d1Switch"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-04-multitask/","text":"\u524d\u8a00 # multitasking \u5373\u591a\u4efb\u52a1\uff0c\u662f\u73b0\u4ee3OS\u7684\u5fc5\u5907feature\u3002\u672c\u7ae0\u5c31\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff08\u5176\u5b9e\u5728\u4e0a\u7bc7\u4e2d\u5df2\u7ecf\u6d89\u53ca\u5230\u4e86\u8fd9\u4e9b\u5185\u5bb9\u4e86\uff09\u3002 \u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u6587\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb Computer multitasking # \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7 Computer multitasking \u603b\u7ed3\u5730\u975e\u5e38\u597d\uff0c\u4e0b\u9762\u662f\u6211\u7684\u9605\u8bfb\u7b14\u8bb0\u3002 NOTE: # \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5e94\u8be5\u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85multitask\u7684\u53d1\u5c55\uff0cmultitask\u662f\u4e00\u4e2a\u5f88\u65e9\u63d0\u51fa\u7684 \u6982\u5ff5 \uff1a In computing , multitasking is the concurrent execution of multiple tasks \u663e\u7136\u8fd9\u4e2a\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662ftask\u7684concurrent\uff08\u5e76\u53d1\uff09\u6267\u884c\u3002\u81f3\u4e8etask\u6240\u6307\u4e3a\u4f55\uff1f\u662fprocess\uff08\u8fdb\u7a0b\uff09\u8fd8\u662fthread\uff08\u7ebf\u7a0b\uff09\uff1f\u4e0d\u540c\u7684\u5b9e\u73b0\u80af\u5b9a\u7b54\u6848\u5c31\u4e0d\u540c\u4e86\u3002\u5728\u65e9\u671f\uff0cthread\u8fd8\u6ca1\u6709\u51fa\u73b0\u7684\u65f6\u5019\uff0c\u663e\u7136task\u6240\u6307\u4e3aprocess\u3002\u4f46\u662f\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86thread\u7684\u6982\u5ff5\uff0c\u5982\u679cOS\u7684\u5b9e\u73b0\u652f\u6301\u7684thread\u7684\u8bdd\uff0c\u90a3\u4e48task\u5c31\u53ef\u80fd\u662f\u6307thread\u4e86\uff08\u663e\u7136task\u662f\u4e00\u79cd\u62bd\u8c61\u7684\u63cf\u8ff0\uff0c\u7c7b\u4f3c\u4e8ekernel control path\uff09\u3002 \u5728\u672c\u6587\u7684 Multithreading \u7ae0\u8282\u5c31\u8bf4\u660e\u4e86\u8fd9\u79cd\u6f14\u8fdb\uff1a\u4eceprocess\u5230thread\u3002\u8fd9\u4e00\u6bb5\u7684\u8bba\u8ff0\u662f\u6bd4\u8f83\u597d\u7684\uff0c\u5b83\u8bf4\u660e\u4e86thread\u7684\u4ef7\u503c\u6240\u5728\u3002\u5728\u672c\u6587\u7684\u5f00\u5934\u4e5f\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\uff1a Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program. \u53e6\u5916\u4e00\u4e2a\u5173\u4e8emultitask\u9700\u8981\u8fdb\u884c\u5f3a\u8c03\u7684\u662f\uff1amultitask\u662foperating system\u5c42\u7684\u6982\u5ff5\uff0c\u5728hardware\u5c42\u6ca1\u6709multitask\u7684\u6982\u5ff5\uff0c\u6240\u4ee5multitask\u7531OS\u5382\u5546\u5b9e\u73b0\uff0c\u5728hardware\u5c42\u6bd4\u5982CPU\u538b\u6839\u5c31\u6ca1\u6709\u8fd9\u6837\u7684\u6982\u5ff5\u3002\u4e0d\u8fc7CPU\u5382\u5546\u80af\u5b9a\u4f1a\u4e3aOS\u63d0\u4f9b\u4fbf\u4e8e\u5b9e\u73b0multitask\u7684\u786c\u4ef6\u652f\u6301\uff0c\u6bd4\u5982\u63d0\u4f9b\u4e00\u4e9b\u4e13\u95e8\u7684\u6307\u4ee4\u7b49\u3002 OS\u4e3a\u4e86\u652f\u6301 multitasking \u90fd\u4f1a\u884d\u751f\u51fa\u4e00\u4e9b\u5217\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5b9e\u73b0 multitasking \u5f80\u5f80\u9700\u8981hardware\u548cOS\u540c\u65f6\u652f\u6301\uff1a \u95ee\u9898\u4e00\uff1a\u6b63\u5982 multitasking \u7684\u5b9a\u4e49\u6240\u652f\u6301\uff0cOS\u4e2d\u4f1a\u5b58\u5728\u591a\u4e2atask\uff0c\u90a3\u5982\u4f55\u4fdd\u8bc1task\u4e4b\u95f4\u5f7c\u6b64\u7684\u9694\u79bb\u3001\u4e92\u4e0d\u4fb5\u72af\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728\u672c\u6587\u7684 Memory protection \u8fdb\u884c\u4e86\u8bf4\u660e\uff0c\u5176\u5b9e\u6700\u6839\u672c\u7684\u63aa\u65bd\u662f\u6bcf\u4e2aprocess\u90fd\u6709\u81ea\u5df1\u7684 address space \u3002 \u95ee\u9898\u4e8c\uff1aoperating system's scheduler \u5982\u4f55\u5b9e\u73b0\u6765\u652f\u6301 multitasking \uff1f scheduler \u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\uff0c\u5728\u6b64\u6211\u4eec\u4ec5\u4ec5\u8ba8\u8bba scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u6839\u636e scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u53ef\u4ee5\u5c06 multitasking \u5206\u4e3a\u5982\u4e0b\u4e24\u79cd\uff1a pre-emptive multitasking cooperative multitasking \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u662f\u666e\u904d\u5b58\u5728\u7684\uff0c\u4e24\u8005\u5404\u6709\u5343\u79cb\uff0cOS\u7684\u5b9e\u73b0\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5176\u4e2d\u4efb\u610f\u4e00\u79cd\u3002 \u65e0\u8bba\u54ea\u79cd multitasking \uff0c\u5728\u8fdb\u884c\u8c03\u5ea6\u7684\u65f6\u5019\uff0c\u90fd\u6d89\u53ca context switch \u3002","title":"How-OS-run-04-multitask"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-04-multitask/#_1","text":"multitasking \u5373\u591a\u4efb\u52a1\uff0c\u662f\u73b0\u4ee3OS\u7684\u5fc5\u5907feature\u3002\u672c\u7ae0\u5c31\u5bf9\u6b64\u8fdb\u884c\u5206\u6790\uff08\u5176\u5b9e\u5728\u4e0a\u7bc7\u4e2d\u5df2\u7ecf\u6d89\u53ca\u5230\u4e86\u8fd9\u4e9b\u5185\u5bb9\u4e86\uff09\u3002 \u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u6587\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb","title":"\u524d\u8a00"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-04-multitask/#computer-multitasking","text":"\u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7 Computer multitasking \u603b\u7ed3\u5730\u975e\u5e38\u597d\uff0c\u4e0b\u9762\u662f\u6211\u7684\u9605\u8bfb\u7b14\u8bb0\u3002","title":"Computer multitasking"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-04-multitask/#note","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5e94\u8be5\u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85multitask\u7684\u53d1\u5c55\uff0cmultitask\u662f\u4e00\u4e2a\u5f88\u65e9\u63d0\u51fa\u7684 \u6982\u5ff5 \uff1a In computing , multitasking is the concurrent execution of multiple tasks \u663e\u7136\u8fd9\u4e2a\u6982\u5ff5\u6240\u5f3a\u8c03\u7684\u662ftask\u7684concurrent\uff08\u5e76\u53d1\uff09\u6267\u884c\u3002\u81f3\u4e8etask\u6240\u6307\u4e3a\u4f55\uff1f\u662fprocess\uff08\u8fdb\u7a0b\uff09\u8fd8\u662fthread\uff08\u7ebf\u7a0b\uff09\uff1f\u4e0d\u540c\u7684\u5b9e\u73b0\u80af\u5b9a\u7b54\u6848\u5c31\u4e0d\u540c\u4e86\u3002\u5728\u65e9\u671f\uff0cthread\u8fd8\u6ca1\u6709\u51fa\u73b0\u7684\u65f6\u5019\uff0c\u663e\u7136task\u6240\u6307\u4e3aprocess\u3002\u4f46\u662f\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86thread\u7684\u6982\u5ff5\uff0c\u5982\u679cOS\u7684\u5b9e\u73b0\u652f\u6301\u7684thread\u7684\u8bdd\uff0c\u90a3\u4e48task\u5c31\u53ef\u80fd\u662f\u6307thread\u4e86\uff08\u663e\u7136task\u662f\u4e00\u79cd\u62bd\u8c61\u7684\u63cf\u8ff0\uff0c\u7c7b\u4f3c\u4e8ekernel control path\uff09\u3002 \u5728\u672c\u6587\u7684 Multithreading \u7ae0\u8282\u5c31\u8bf4\u660e\u4e86\u8fd9\u79cd\u6f14\u8fdb\uff1a\u4eceprocess\u5230thread\u3002\u8fd9\u4e00\u6bb5\u7684\u8bba\u8ff0\u662f\u6bd4\u8f83\u597d\u7684\uff0c\u5b83\u8bf4\u660e\u4e86thread\u7684\u4ef7\u503c\u6240\u5728\u3002\u5728\u672c\u6587\u7684\u5f00\u5934\u4e5f\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\uff1a Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program. \u53e6\u5916\u4e00\u4e2a\u5173\u4e8emultitask\u9700\u8981\u8fdb\u884c\u5f3a\u8c03\u7684\u662f\uff1amultitask\u662foperating system\u5c42\u7684\u6982\u5ff5\uff0c\u5728hardware\u5c42\u6ca1\u6709multitask\u7684\u6982\u5ff5\uff0c\u6240\u4ee5multitask\u7531OS\u5382\u5546\u5b9e\u73b0\uff0c\u5728hardware\u5c42\u6bd4\u5982CPU\u538b\u6839\u5c31\u6ca1\u6709\u8fd9\u6837\u7684\u6982\u5ff5\u3002\u4e0d\u8fc7CPU\u5382\u5546\u80af\u5b9a\u4f1a\u4e3aOS\u63d0\u4f9b\u4fbf\u4e8e\u5b9e\u73b0multitask\u7684\u786c\u4ef6\u652f\u6301\uff0c\u6bd4\u5982\u63d0\u4f9b\u4e00\u4e9b\u4e13\u95e8\u7684\u6307\u4ee4\u7b49\u3002 OS\u4e3a\u4e86\u652f\u6301 multitasking \u90fd\u4f1a\u884d\u751f\u51fa\u4e00\u4e9b\u5217\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5b9e\u73b0 multitasking \u5f80\u5f80\u9700\u8981hardware\u548cOS\u540c\u65f6\u652f\u6301\uff1a \u95ee\u9898\u4e00\uff1a\u6b63\u5982 multitasking \u7684\u5b9a\u4e49\u6240\u652f\u6301\uff0cOS\u4e2d\u4f1a\u5b58\u5728\u591a\u4e2atask\uff0c\u90a3\u5982\u4f55\u4fdd\u8bc1task\u4e4b\u95f4\u5f7c\u6b64\u7684\u9694\u79bb\u3001\u4e92\u4e0d\u4fb5\u72af\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728\u672c\u6587\u7684 Memory protection \u8fdb\u884c\u4e86\u8bf4\u660e\uff0c\u5176\u5b9e\u6700\u6839\u672c\u7684\u63aa\u65bd\u662f\u6bcf\u4e2aprocess\u90fd\u6709\u81ea\u5df1\u7684 address space \u3002 \u95ee\u9898\u4e8c\uff1aoperating system's scheduler \u5982\u4f55\u5b9e\u73b0\u6765\u652f\u6301 multitasking \uff1f scheduler \u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u975e\u5e38\u5b8f\u5927\u7684\u95ee\u9898\uff0c\u5728\u6b64\u6211\u4eec\u4ec5\u4ec5\u8ba8\u8bba scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u6839\u636e scheduler \u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u53ef\u4ee5\u5c06 multitasking \u5206\u4e3a\u5982\u4e0b\u4e24\u79cd\uff1a pre-emptive multitasking cooperative multitasking \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u662f\u666e\u904d\u5b58\u5728\u7684\uff0c\u4e24\u8005\u5404\u6709\u5343\u79cb\uff0cOS\u7684\u5b9e\u73b0\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5176\u4e2d\u4efb\u610f\u4e00\u79cd\u3002 \u65e0\u8bba\u54ea\u79cd multitasking \uff0c\u5728\u8fdb\u884c\u8c03\u5ea6\u7684\u65f6\u5019\uff0c\u90fd\u6d89\u53ca context switch \u3002","title":"NOTE:"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/","text":"\u524d\u8a00 # Thread \u548c Process \u662f\u73b0\u4ee3OS\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5efa\u7acb\u8d77\u5b8c\u6574\u3001\u6b63\u786e\u7684process model\u5bf9\u4e8e\u5728linux-like OS\u4e0b\u8fdb\u884c\u5f00\u53d1\u3001\u7406\u89e3linux kernel\u7684\u5b9e\u73b0\u81f3\u5173\u91cd\u8981\u3002 Process model # Process model\u56fe\u793a\u5982\u4e0b\uff1a Process # \u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Process In computing, a process is the instance of a computer program that is being executed by one or many threads. It contains the program code and its activity. Depending on the operating system (OS), a process may be made up of multiple threads of execution that execute instructions concurrently . \u663e\u7136thread\u662fprocess\u7684\u201c\u6210\u5206\u201d\uff0c\u4e0b\u9762\u770b\u770bthread\u3002 Thread # \u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Thread (computing) In computer science , a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler . Multiple threads can exist within one process, executing concurrently and sharing resources \u7efc\u5408\u4e0a\u9762\u63cf\u8ff0\uff0c\u4ee5\u4e0b\u6211\u6240\u6982\u62ecProcess model\uff1a \u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0c\u57fa\u4e8e thread \u7684\u8c03\u5ea6\u3002\u4e00\u4e2a process \u53ef\u80fd\u7531\u591a\u4e2a thread \u7ec4\u6210\uff0c thread \u5171\u4eabprocess\u7684resource\u3001 \u5e76\u53d1 \u6267\u884c \u3002\u201d \u6ce8\u610f\uff1a\u4e0a\u8ff0\u6982\u62ec\u7684\u662f\u73b0\u4ee3\u5927\u591a\u6570OS\u7684process model\uff0c\u5e76\u975e\u6240\u6709OS\u7684process model\u90fd\u662f\u5982\u6b64\uff0c\u5b9e\u73b0\u4e0a\u662f\u5b58\u5728\u5dee\u5f02\u7684\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u867d\u7136\u7b80\u77ed\uff0c\u4f46\u662f\u8574\u542b\u7740\u4e30\u5bcc\u7684\u5185\u6db5\uff0c\u9700\u8981\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff1a \u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\u201d # \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u610f\u5473\u7740\uff1aprocess\u662fOS\u7684\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0cprocess\u4e4b\u95f4\u662f\u5f7c\u6b64\u9694\u79bb\u7684\u3002 NOTE: \u5bf9\u4e8e\u4e00\u4e9b\u7279\u6b8a\u7684\u60c5\u51b5\uff0c\u5982process\u4e4b\u95f4\u5171\u4eabmemory\u7684\u60c5\u51b5\u9664\u5916\u3002 OS\u4e2d\u6709\u54ea\u4e9bresource\uff1fProcess\u9700\u8981\u54ea\u4e9bresource\uff1f # OS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0cresource\u5305\u62ec\uff1a Memory address space Multiple threads can exist within one process, executing concurrently Multiple threads share the process resource \u3002 Thread \u7684\u7b2c\u4e00\u6bb5\u57fa\u672c\u4e0a\u662f\u6309\u7167\u8fd9\u4e2a\u5173\u7cfb\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\u3002 Thread In computer science , a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler , which is typically a part of the operating system . # process model\u7684\u6f14\u8fdb\u5386\u53f2 # \u5728 stanford CS 140: Operating Systems (Spring 2014) \u7684lecture\u4e2d\u603b\u7ed3\u4e86Evolution of operating system process model : Early operating systems supported a single process with a single thread at a time ( single tasking ). They ran batch jobs (one user at a time). Some early personal computer operating systems used single-tasking (e.g. MS-DOS), but these systems are almost unheard of today. By late 1970's most operating systems were multitasking systems: they supported multiple processes , but each process had only a single thread. In the 1990's most systems converted to multithreading : multiple threads within each process. \u663e\u7136\uff0c\u65e9\u671f\u7684\u65f6\u5019\uff0c\u5e76\u6ca1\u6709 multithreading : multiple threads within each process\uff0c\u6240\u4ee5\u65e9\u671f\u7684\u65f6\u5019multitasking\u7684task\u6240\u6307 processes \u3002\u800c\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u540e\u6765\u624d\u51fa\u73b0\u4e86 multithreading : multiple threads within each process\u3002 multithreading \u76f8\u8f83\u4e8e Multiprocessing \u4f18\u52bf\u662f\u4ec0\u4e48\uff1f # \u4e0d\u7981\u8981\u95ee\uff1a multithreading \u76f8\u8f83\u4e8e Multiprocessing \u4f18\u52bf\u662f\u4ec0\u4e48\uff1f \u8fd9\u4e2a\u95ee\u9898\uff0c\u5728 Computer multitasking \u7684 Multithreading \u7ae0\u8282\u7ed9\u51fa\u4e86\u7b54\u6848\u89e3\u7b54\u3002 Threads vs. processes # Linux OS\u4e2dProcess model\u7684\u5b9e\u73b0 # \u53c2\u89c1\u7ae0\u8282\uff1a 1.6.2. Process Implementation 1.6.4. Process Address Space process\u7684\u4e00\u7cfb\u5217\u95ee\u9898 # \u751f # \u521b\u5efaprocess \u5360\u7528\u4e86\u54ea\u4e9b\u8d44\u6e90 # \u5982\u4f55\u6765\u63a7\u5236process # \u5982\u4f55\u9650\u5236\u8d44\u6e90 # process\u4e4b\u95f4\u7684\u5173\u7cfb # process\u4e4b\u95f4\u5982\u4f55\u8fdb\u884c\u6c9f\u901a # \u65f6\u7a7a\u7684\u89d2\u5ea6 # process\u7684\u72b6\u6001 # \u6b7b #","title":"How-OS-run-05-process-model"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#_1","text":"Thread \u548c Process \u662f\u73b0\u4ee3OS\u5b9e\u73b0 multitasking \u7684\u5173\u952e\u6240\u5728\u3002\u5efa\u7acb\u8d77\u5b8c\u6574\u3001\u6b63\u786e\u7684process model\u5bf9\u4e8e\u5728linux-like OS\u4e0b\u8fdb\u884c\u5f00\u53d1\u3001\u7406\u89e3linux kernel\u7684\u5b9e\u73b0\u81f3\u5173\u91cd\u8981\u3002","title":"\u524d\u8a00"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process-model","text":"Process model\u56fe\u793a\u5982\u4e0b\uff1a","title":"Process model"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process","text":"\u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Process In computing, a process is the instance of a computer program that is being executed by one or many threads. It contains the program code and its activity. Depending on the operating system (OS), a process may be made up of multiple threads of execution that execute instructions concurrently . \u663e\u7136thread\u662fprocess\u7684\u201c\u6210\u5206\u201d\uff0c\u4e0b\u9762\u770b\u770bthread\u3002","title":"Process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#thread","text":"\u4e3b\u8981\u53c2\u8003\u6587\u7ae0\uff1a Thread (computing) In computer science , a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler . Multiple threads can exist within one process, executing concurrently and sharing resources \u7efc\u5408\u4e0a\u9762\u63cf\u8ff0\uff0c\u4ee5\u4e0b\u6211\u6240\u6982\u62ecProcess model\uff1a \u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0c\u57fa\u4e8e thread \u7684\u8c03\u5ea6\u3002\u4e00\u4e2a process \u53ef\u80fd\u7531\u591a\u4e2a thread \u7ec4\u6210\uff0c thread \u5171\u4eabprocess\u7684resource\u3001 \u5e76\u53d1 \u6267\u884c \u3002\u201d \u6ce8\u610f\uff1a\u4e0a\u8ff0\u6982\u62ec\u7684\u662f\u73b0\u4ee3\u5927\u591a\u6570OS\u7684process model\uff0c\u5e76\u975e\u6240\u6709OS\u7684process model\u90fd\u662f\u5982\u6b64\uff0c\u5b9e\u73b0\u4e0a\u662f\u5b58\u5728\u5dee\u5f02\u7684\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u867d\u7136\u7b80\u77ed\uff0c\u4f46\u662f\u8574\u542b\u7740\u4e30\u5bcc\u7684\u5185\u6db5\uff0c\u9700\u8981\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff1a","title":"Thread"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#osprocessresource","text":"\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u610f\u5473\u7740\uff1aprocess\u662fOS\u7684\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0cprocess\u4e4b\u95f4\u662f\u5f7c\u6b64\u9694\u79bb\u7684\u3002 NOTE: \u5bf9\u4e8e\u4e00\u4e9b\u7279\u6b8a\u7684\u60c5\u51b5\uff0c\u5982process\u4e4b\u95f4\u5171\u4eabmemory\u7684\u60c5\u51b5\u9664\u5916\u3002","title":"\u201cOS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\u201d"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#osresourceprocessresource","text":"OS\u662f\u57fa\u4e8eprocess\u7684resource\u5206\u914d\uff0cresource\u5305\u62ec\uff1a Memory address space Multiple threads can exist within one process, executing concurrently Multiple threads share the process resource \u3002 Thread \u7684\u7b2c\u4e00\u6bb5\u57fa\u672c\u4e0a\u662f\u6309\u7167\u8fd9\u4e2a\u5173\u7cfb\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\u3002 Thread In computer science , a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler , which is typically a part of the operating system .","title":"OS\u4e2d\u6709\u54ea\u4e9bresource\uff1fProcess\u9700\u8981\u54ea\u4e9bresource\uff1f"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process-model_1","text":"\u5728 stanford CS 140: Operating Systems (Spring 2014) \u7684lecture\u4e2d\u603b\u7ed3\u4e86Evolution of operating system process model : Early operating systems supported a single process with a single thread at a time ( single tasking ). They ran batch jobs (one user at a time). Some early personal computer operating systems used single-tasking (e.g. MS-DOS), but these systems are almost unheard of today. By late 1970's most operating systems were multitasking systems: they supported multiple processes , but each process had only a single thread. In the 1990's most systems converted to multithreading : multiple threads within each process. \u663e\u7136\uff0c\u65e9\u671f\u7684\u65f6\u5019\uff0c\u5e76\u6ca1\u6709 multithreading : multiple threads within each process\uff0c\u6240\u4ee5\u65e9\u671f\u7684\u65f6\u5019multitasking\u7684task\u6240\u6307 processes \u3002\u800c\u968f\u7740\u6280\u672f\u7684\u53d1\u5c55\uff0c\u540e\u6765\u624d\u51fa\u73b0\u4e86 multithreading : multiple threads within each process\u3002","title":"process model\u7684\u6f14\u8fdb\u5386\u53f2"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#multithreadingmultiprocessing","text":"\u4e0d\u7981\u8981\u95ee\uff1a multithreading \u76f8\u8f83\u4e8e Multiprocessing \u4f18\u52bf\u662f\u4ec0\u4e48\uff1f \u8fd9\u4e2a\u95ee\u9898\uff0c\u5728 Computer multitasking \u7684 Multithreading \u7ae0\u8282\u7ed9\u51fa\u4e86\u7b54\u6848\u89e3\u7b54\u3002","title":"multithreading\u76f8\u8f83\u4e8eMultiprocessing\u4f18\u52bf\u662f\u4ec0\u4e48\uff1f"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#threads-vs-processes","text":"","title":"Threads vs. processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#linux-osprocess-model","text":"\u53c2\u89c1\u7ae0\u8282\uff1a 1.6.2. Process Implementation 1.6.4. Process Address Space","title":"Linux OS\u4e2dProcess model\u7684\u5b9e\u73b0"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process_1","text":"","title":"process\u7684\u4e00\u7cfb\u5217\u95ee\u9898"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#_3","text":"\u521b\u5efaprocess","title":"\u751f"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#_4","text":"","title":"\u5360\u7528\u4e86\u54ea\u4e9b\u8d44\u6e90"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process_2","text":"","title":"\u5982\u4f55\u6765\u63a7\u5236process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#_5","text":"","title":"\u5982\u4f55\u9650\u5236\u8d44\u6e90"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process_3","text":"","title":"process\u4e4b\u95f4\u7684\u5173\u7cfb"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process_4","text":"","title":"process\u4e4b\u95f4\u5982\u4f55\u8fdb\u884c\u6c9f\u901a"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#_6","text":"","title":"\u65f6\u7a7a\u7684\u89d2\u5ea6"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#process_5","text":"","title":"process\u7684\u72b6\u6001"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-05-process-model/#_7","text":"","title":"\u6b7b"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-06-process-address-space/","text":"\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0bvirtual address\u7684\u91cd\u8981\u610f\u4e49 #","title":"How-OS-run-06-process-address-space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-06-process-address-space/#virtual-address","text":"","title":"\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0bvirtual address\u7684\u91cd\u8981\u610f\u4e49"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-07-thread/","text":"\u4e0d\u540c\u7684OS\u6709\u7740\u4e0d\u540c\u7684\u5b9e\u73b0\uff0c\u4f46\u662f\u5b83\u4eec\u80af\u5b9a\u90fd\u4f1a\u7b26\u5408\u6807\u51c6\u3002 \u6309\u7167\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\u6d41\u7a0b\u6765\u770b\uff0c\u5e94\u8be5\u662f\u9996\u5148\u6709\u8ba1\u7b97\u673a\u7406\u8bba\u5b66\u5bb6\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\uff0c\u7136\u540e\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u518d\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\u3002\u6240\u4ee5\u4ece\u6807\u51c6\u7684\u51fa\u73b0\u5230\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u5b9e\u73b0\u8fd9\u4e9b\u6807\u51c6\uff0c\u4e24\u8005\u4e4b\u95f4\u662f\u6709\u4e00\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u3002\u4e0d\u540c\u5382\u5546\u7684\u5bf9\u540c\u4e00\u6982\u5ff5/\u6807\u51c6\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u4e0d\u65ad\u5730\u6f14\u8fdb\u3002\u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u4e66\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb \u4e0b\u9762\u4ee5operating system\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u76ee\u524d\u5b58\u5728\u7740\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a user level thread\uff0c\u5e38\u79f0\u4e3auser thread kernel level thread \u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/ What is a user thread and a kernel thread? \u663e\u7136\uff0c\u5bf9\u4e8e\u6807\u51c6\u6240\u63d0\u51fa\u7684 Thread (computing) \uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5173\u4e8e\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Thread (computing) \u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002","title":"How-OS-run-07-thread"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-08-Entry-point-of-kernel/","text":"Dose kernel have main function # Linux Kernel And Its Functions Does the kernel have a main() function? closed Does kernel have main function?","title":"How-OS-run-08-Entry-point-of-kernel"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-OS-run-08-Entry-point-of-kernel/#dose-kernel-have-main-function","text":"Linux Kernel And Its Functions Does the kernel have a main() function? closed Does kernel have main function?","title":"Dose kernel have main function"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/","text":"\u89c2\u70b9 # \u5982\u4f55\u9605\u8bfblinux kernel\u7684source code\uff1f\u5728\u62ff\u8d77\u672c\u4e66\u7684\u65f6\u5019\u6211\u601d\u8003\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u540e\u6765\u6211\u53d1\u73b0\u6709\u4eba\u548c\u6211\u6709\u76f8\u540c\u7684\u7591\u60d1\uff0c\u4e0b\u9762\u662f\u6211\u68c0\u7d22\u5230\u7684\u6211\u89c9\u5f97\u6709\u9053\u7406\u7684\u89c2\u70b9\uff1a Focus on data structures . Understanding data structures is usually more important than code . If you are only shown data structures but no code, you still get the big picture of the system. Vice versa, if shown only code but not data structures, it's very hard to understand the system. \"I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.\" -- Linus Torvalds \"Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won't usually need your flowcharts; they'll be obvious.\" -- Fred Brooks. How to understand Linux kernel source code for a beginner? \u4ecestructure\u5165\u624b\uff0c\u672c\u4e66\u4e5f\u662f\u5982\u6b64\u3002 \u4e0b\u9762\u603b\u7ed3\u4e86\u4e00\u4e9bstructure\uff1a \u5404\u79cd\u5404\u6837\u7684descriptor # \u5404\u79cd\u5404\u6837\u7684descriptor\uff0c\u4ee5\u53ca\u5176\u5bf9\u5e94\u7684\u6570\u636e\u7ed3\u6784 Descriptor Chapter Struct Source Code Process Descriptor 3.2. Process Descriptor task_struct - https://github.com/torvalds/linux/blob/master/include/linux/sched.h - https://elixir.bootlin.com/linux/latest/ident/task_struct Memory Descriptor 9.2. The Memory Descriptor mm_struct - https://elixir.bootlin.com/linux/latest/ident/mm_struct - https://github.com/torvalds/linux/blob/master/include/linux/mm_types.h Page Descriptor 8.1.1. Page Descriptors page - https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h#L68 Process Descriptor # 3.2. Process Descriptor Task State Segment Descriptor # 3.3. Process Switch Global Descriptor Table # memory descriptor # signal descriptor # file descriptors # Interrupt Descriptor Table #","title":"How-to-understand-linux-kernel-source-code"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#_1","text":"\u5982\u4f55\u9605\u8bfblinux kernel\u7684source code\uff1f\u5728\u62ff\u8d77\u672c\u4e66\u7684\u65f6\u5019\u6211\u601d\u8003\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u540e\u6765\u6211\u53d1\u73b0\u6709\u4eba\u548c\u6211\u6709\u76f8\u540c\u7684\u7591\u60d1\uff0c\u4e0b\u9762\u662f\u6211\u68c0\u7d22\u5230\u7684\u6211\u89c9\u5f97\u6709\u9053\u7406\u7684\u89c2\u70b9\uff1a Focus on data structures . Understanding data structures is usually more important than code . If you are only shown data structures but no code, you still get the big picture of the system. Vice versa, if shown only code but not data structures, it's very hard to understand the system. \"I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships.\" -- Linus Torvalds \"Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won't usually need your flowcharts; they'll be obvious.\" -- Fred Brooks. How to understand Linux kernel source code for a beginner? \u4ecestructure\u5165\u624b\uff0c\u672c\u4e66\u4e5f\u662f\u5982\u6b64\u3002 \u4e0b\u9762\u603b\u7ed3\u4e86\u4e00\u4e9bstructure\uff1a","title":"\u89c2\u70b9"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#descriptor","text":"\u5404\u79cd\u5404\u6837\u7684descriptor\uff0c\u4ee5\u53ca\u5176\u5bf9\u5e94\u7684\u6570\u636e\u7ed3\u6784 Descriptor Chapter Struct Source Code Process Descriptor 3.2. Process Descriptor task_struct - https://github.com/torvalds/linux/blob/master/include/linux/sched.h - https://elixir.bootlin.com/linux/latest/ident/task_struct Memory Descriptor 9.2. The Memory Descriptor mm_struct - https://elixir.bootlin.com/linux/latest/ident/mm_struct - https://github.com/torvalds/linux/blob/master/include/linux/mm_types.h Page Descriptor 8.1.1. Page Descriptors page - https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h#L68","title":"\u5404\u79cd\u5404\u6837\u7684descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#process-descriptor","text":"3.2. Process Descriptor","title":"Process Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#task-state-segment-descriptor","text":"3.3. Process Switch","title":"Task State Segment Descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#global-descriptor-table","text":"","title":"Global Descriptor Table"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#memory-descriptor","text":"","title":"memory descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#signal-descriptor","text":"","title":"signal descriptor"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#file-descriptors","text":"","title":"file descriptors"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/How-to-understand-linux-kernel-source-code/#interrupt-descriptor-table","text":"","title":"Interrupt Descriptor Table"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/","text":"\u524d\u8a00 # \u5728\u672c\u4e66\u4e2d\u9891\u7e41\u51fa\u73b0process\uff0clightweight process\uff0cthread \u8fd9\u4e9b\u8bcd\u8bed\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u533a\u522b\uff0c\u5426\u5219\u5f88\u96be\u51c6\u786e\u7406\u89e3\u4e66\u4e2d\u5185\u5bb9\uff1b \u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0 # Thread (computing) \u548c Process (computing) \u662fsoftware engineer\u975e\u5e38\u719f\u7cfb\u7684\u6982\u5ff5\uff0c\u5b83\u4eec\u662f\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u4e24\u4e2a\u6982\u5ff5\uff0c\u6709\u7740\u51c6\u786e\u7684\u542b\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u4e5f\u662f\u975e\u5e38\u6e05\u695a\u7684\u3002\u6309\u7167\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\u6d41\u7a0b\u6765\u770b\uff0c\u5e94\u8be5\u662f\u9996\u5148\u6709\u8ba1\u7b97\u673a\u7406\u8bba\u5b66\u5bb6\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\uff0c\u7136\u540e\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u518d\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\u3002\u6240\u4ee5\u4ece\u6807\u51c6\u7684\u51fa\u73b0\u5230\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u5b9e\u73b0\u8fd9\u4e9b\u6807\u51c6\uff0c\u4e24\u8005\u4e4b\u95f4\u662f\u6709\u4e00\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u3002\u4e0d\u540c\u5382\u5546\u7684\u5bf9\u540c\u4e00\u6982\u5ff5/\u6807\u51c6\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u4e0d\u65ad\u5730\u6f14\u8fdb\u3002\u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u4e66\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb \u4e0b\u9762\u4ee5operating system\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u76ee\u524d\u5b58\u5728\u7740\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a user level thread\uff0c\u5e38\u79f0\u4e3auser thread kernel level thread \u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/ What is a user thread and a kernel thread? \u663e\u7136\uff0c\u5bf9\u4e8e\u6807\u51c6\u6240\u63d0\u51fa\u7684 Thread (computing) \uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5173\u4e8e\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Thread (computing) \u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002 \u7406\u89e3\u6807\u51c6 # \u63cf\u8ff0\u6807\u51c6\u7684process\u548cthread\u5b9a\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e9b\u9700\u8981\u7740\u91cd\u5f3a\u8c03\u7684\uff1a process\u662fOS\u7684\u6982\u5ff5\uff0c\u5728instruction\u5c42\u7ea7\u5e76\u6ca1\u6709process\u7684\u6982\u5ff5\u3002OS\u4f7f\u7528process\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b9e\u73b0 multitasking \uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528hardware\u3002process\u662fprogram\u7684\u6267\u884c\uff0c\u5b83\u662fOS\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0c\u4e0d\u540cprocess\u4e4b\u95f4\u7684\u8d44\u6e90\u9700\u8981\u5b8c\u5168\u9694\u79bb\uff08\u7279\u6b8a\u60c5\u51b5\u9664\u5916\uff09\uff0cOS\u4e2d\u7684\u6240\u6709process\u5171\u4eabOS\u6240\u7ba1\u7406\u7684hardware\u8d44\u6e90\u3002OS\u9700\u8981\u6e05\u695a\u5730\u77e5\u9053process\u548c\u8d44\u6e90\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5373\u4e00\u4e2aprocess\u62e5\u6709\u54ea\u4e9bresource\u3002 linux kernel\u7684\u5b9e\u73b0 # \u90a3linux kernel\u662f\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u7684\u5462\uff1f\u4e0b\u9762\u662f\u4ece\u672c\u4e66\u7684\u4e00\u4e9b\u4ecb\u7ecd\uff1a chapter 1.1. Linux Versus Other Unix-Like Kernels Multithreaded application support Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes, which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads, Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u5176\u5b9e\u66f4\u597d\u7684\u65b9\u5f0f\u4e0d\u662f\u6839\u636e\u672c\u4e66\u4e2d\u7684\u5185\u5bb9\u6765\u63a8\u65adLinux OS\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\uff0c\u6700\u597d\u7684\u65b9\u5f0f\u662f\u9605\u8bfblinux\u7684man\uff0c\u5728 PTHREADS(7) \u7684Linux implementations of POSIX threads\u7ae0\u8282\u7ed9\u51fa\u4e86linux\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5176\u4e2d\u4e5f\u7ed9\u51fa\u4e86\u67e5\u770b\u76f8\u5173\u5b9e\u73b0\u7684\u547d\u4ee4\u3002\u53ef\u4ee5\u786e\u5b9a\u7684\u662f\uff0c\u65e0\u8bba\u91c7\u7528\u54ea\u79cd\u65b9\u5f0f\uff0c\u6700\u7ec8\u90fd\u662f\u4f9d\u8d56 clone(2) \u3002 \u5173\u4e8e\u672c\u6bb5\uff0c\u6709\u7591\u95ee\uff1aLWP VS thread VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy \u5728linux\u4e2d\uff0clightweight process\u5bf9\u5e94\u7684\u662fthread\u5417\uff1f \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u672c\u4e66\u4e2d\u6709\u65f6\u5019\u4f1a\u5c06lightweight process\u7b80\u79f0\u4e3aprocess\uff0c\u6bd4\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\u8fd9\u53e5\uff1a A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. \u6240\u4ee5\u5728\u672c\u4e66\u4e2d\uff0cprocess\u4e0d\u4e00\u5b9a\u6307\u7684\u662f\u6807\u51c6\u7684 Process (computing) \uff0c\u6709\u7684\u65f6\u5019\u6307\u7684\u662flightweight process\uff0c\u4e3a\u4e86\u4fbf\u4e8e\u533a\u5206\uff0c\u4f1a\u5728note\u4e2d\u8fdb\u884c\u7279\u6b8a\u8bf4\u660e\u3002 \u5173\u4e8elightweight process\uff0c\u53c2\u89c1\uff1a Light-weight process linux kernel\u5982\u4f55\u5b9e\u73b0process\u4e0ethread # \u53c2\u89c13.1. Processes, Lightweight Processes, and Threads \u6211\u89c9\u5f97\u8981\u60f3\u89e3\u91ca\u597d\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u68b3\u7406\u4e00\u4e0blinux\u7684fork\uff0cclone\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5728 Fork (system call) \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u68b3\u7406\u5730\u975e\u5e38\u597d\u3002\u57283.4. Creating Processes\u4e2d\u4e5f\u6709\u76f8\u5173\u7684\u63cf\u8ff0\u3002 per-process kernel data structures # \u57283.4. Creating Processes\u4e2d\u63d0\u51fa\u4e86\u8fd9\u4e2a\u8bf4\u6cd5\uff0c\u5b83\u8ba9\u6211\u60f3\u8d77\u4e86\u4e24\u4ef6\u4e8b\u60c5\uff1a process\u4f5c\u4e3asystem resource\u5206\u914d\u5355\u4f4d\uff0c\u5b83\u6709\u54ea\u4e9bresource\u5462\uff1f\u663e\u7136\uff0c\u5b83\u7684\u6240\u6709\u7684resource\u90fd\u9700\u8981\u4f7f\u7528\u4e00\u4e2a kernel data structures\u6765\u8fdb\u884c\u63cf\u8ff0\u3002\u6709\u5fc5\u8981\u603b\u7ed3per-process\u7684resource\u4ee5\u53ca\u5bf9\u5e94\u7684kernel data structures\u3002\u4e0e\u6b64\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff0c\u8fd9\u4e9bresource\u54ea\u4e9b\u662fchild process\u53ef\u4ee5\u7ee7\u627f\u7684\uff0c\u54ea\u4e9b\u662f\u65e0\u6cd5\u7ee7\u627f\u7684\u3002 \u663e\u7136\uff0c\u591a\u4e2alightweight process\u662f\u53ef\u4ee5\u5171\u4eabper-process kernel data structure\u7684\uff08\u8fd9\u662f\u6807\u51c6\u89c4\u5b9a\u7684\uff09\uff0c\u8fd9\u79cd\u5171\u4eab\uff0c\u6211\u89c9\u5f97\u5b9e\u73b0\u4e0a\u5e94\u8be5\u4e5f\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c\u65e0\u975e\u5c31\u662f\u4f20\u5165\u4e00\u4e2a\u6307\u9488\u3002 Address space # \u8fd9\u4e2a\u95ee\u9898\u662f\u7531\u524d\u9762\u7684\u5173\u4e8eprocess\u7684resource\u7684\u601d\u8003\u884d\u751f\u51fa\u6765\u7684\u3002Address space\u662f\u4e00\u4e2aprocess\u975e\u5e38\u91cd\u8981\u7684resource\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u662fprocess\u8fdb\u884c\u6d3b\u52a8\u7684\u7a7a\u95f4\u3002\u76ee\u524d\u7684OS\u90fd\u662f\u91c7\u7528\u7684virtual address\uff0c\u5373process\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u6240\u4f7f\u7528\u7684\u662fvirtual memory\uff0c\u6240\u4ee5\u4e5f\u53ef\u4ee5\u5c06Address space\u79f0\u4e3aVirtual address space\u3002\u5173\u4e8eprocess\u7684Virtual address space\uff0c\u6211\u6709\u5982\u4e0b\u7591\u95ee\uff1a Question: process\u4f7f\u7528virtual memory\uff0c\u5e76\u4e14\u4f7f\u7528\u57fa\u4e8epage\u7684memory management\uff0c\u90a3\u5b83\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u57fa\u4e8epage\u7684virtual memory\u5462\uff1f\u662f\u5206\u5272\u4e3a\u4e00\u4e2a\u4e00\u4e2a\u7684page\uff1f \u7ecf\u8fc7\u7b80\u5355\u7684\u601d\u8003\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u7f16\u8bd1\u5668\u5728\u7ed9\u751f\u6210\u4ee3\u7801\u7684\u65f6\u5019\u5176\u5b9e\u662f\u4e0d\u9700\u8981\u8003\u8651\u8fd9\u4e2a\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u662fOS\u5728\u8fd0\u884cprogram\u7684\u65f6\u5019\u6309\u7167page\u8fdb\u884cmemory management\uff0c\u65e0\u8bba\u7f16\u8bd1\u5668\u751f\u6210\u7684program\u662f\u600e\u6837\u7684\uff0c\u662fOS\u8d1f\u8d23\u5c06\u8fd9\u4e9bprogram\u88c5\u5165\u5230memory\u4e2d\uff0c\u8fd9\u4e00\u5207\u5bf9compiler\u800c\u8a00\u90fd\u662f\u900f\u660e\u7684\u3002 \u4f46\u662f\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u5ef6\u4f38\u4e00\u4e0b\uff1a\u6211\u4eec\u77e5\u9053\uff0c\u7f16\u8bd1\u5668\u751f\u6210\u7684\u4ee3\u7801\u80af\u5b9a\u662f\u9700\u8981\u9075\u5faaalignment\u7684\uff0c\u90a3\u8fd9\u5c31\u6d89\u53ca\u5230alignment\u548cpage size\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u5e94\u8be5\u53ea\u8981\u7b26\u5408alignment\uff0c\u90a3\u4e48\u5e94\u8be5\u5c31\u4e0d\u4f1a\u5b58\u5728\u4e00\u4e2a\u6570\u636e\u5b58\u50a8\u8de8\u8d8a\u4e86\u591a\u4e2apage\u7684\u60c5\u51b5\u4e86\u3002 Question: \u8fdb\u7a0b\u7684virtual address space\u90fd\u662f\u76f8\u540c\u7684\uff0c\u90a3virtual address\u662f\u5982\u4f55\u6620\u5c04\u5230physical memory address\u7684\u5462\uff1f \u65e2\u7136\u4f7f\u7528\u7684\u662fdemand page\uff0c\u4e5f\u5c31\u662f\u5728process\u8fd0\u884c\u7684\u65f6\u5019\u9700\u8981\u8bbf\u95ee\u8be5virtual memory\u7684\u65f6\u5019\uff0c\u624dallocate physical memory\u6216\u8005swap-in\uff0c\u624d\u5c06virtual address\u6620\u5c04\u5230physical memory\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4fdd\u5b58\u5230\u8be5process\u7684page table\u4e2d\u3002 \u5176\u5b9e\u901a\u8fc7\u8fd9\u4e2a\u601d\u8003\u624d\u53d1\u73b0virtual memory\u7684\u91cd\u8981\u4ef7\u503c\u6240\u5728\uff0c\u5b83\u662f\u5b9e\u73b0demand page\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0\u6269\u5145memory\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0copy on write\u7684\u57fa\u7840\u3002 Question: \u59821.6.8.4. Process virtual address space handling\u8282\u6240\u53d9\u8ff0\u7684 The kernel usually stores a process virtual address space as a list of memory area descriptors . \u5373\u6211\u4eec\u901a\u5e38\u5c06virtual address space\u5206\u5272\u4e3a\u591a\u5757\uff0c\u90a3\u662f\u5728\u4ec0\u4e48\u5730\u65b9\u5c06virtual address space\u5206\u5272\u4e3a\u5982\u4e0a\u6240\u8ff0\u7684a list of memory area descriptors \uff1f operating system\u91c7\u7528\u7684\u662fdemand paging\uff0c\u5e76\u4e14stack\u7684\u589e\u957f\u65b9\u5411\u548cheap\u7684\u589e\u957f\u65b9\u5411\u76f8\u53cd\uff0c\u90a3\u8fd9\u4e9b\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u8981\u60f3\u5b8c\u5168\u7406\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9605\u8bfbcalling convention\u3002\u6211\u89c9\u5f97process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5bf9call stack\u7684\u7ef4\u62a4\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6d3b\u52a8\uff0c\u6bcf\u6b21new\u4e00\u4e2a\u6808\u5e27\u90fd\u9700\u8981\u5206\u914d\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\u91cd\u8981\u624d\u80fd\u591f\u4fdd\u8bc1process\u8fd0\u884c\u4e0b\u53bb\u3002 \u53e6\u5916\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u4e3a\u4ec0\u4e48\u9700\u8981\u7533\u8bf7memory\uff1f \u5176\u5b9e\u5982\u679c\u8fd9\u4e2a\u7cfb\u7edf\u4e2d\u53ea\u6709\u4e00\u4e2a\u7a0b\u5e8f\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u60f3\u600e\u4e48\u6837\u4f7f\u7528memory\u5c31\u600e\u4e48\u6837\u4f7f\u7528memory\uff0c\u4f46\u662f\u95ee\u9898\u662f\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u662f\u9700\u8981\u652f\u6301\u591a\u4efb\u52a1\u7684\uff0c\u90a3\u5b83\u5c31\u9700\u8981\u505a\u597d\u4e0d\u540c\u7684process\u4e4b\u95f4\u7684\u9694\u79bb\uff0cA process\u4e0d\u80fd\u591f\u4f7f\u7528B process\u7684\u4e1c\u897f\u3002\u6240\u4ee5\uff0c\u6240\u6709\u7684process\u90fd\u5fc5\u987b\u8981\u5148\u60f3OS\u7533\u8bf7memory\uff0c\u7136\u540e\u624d\u80fd\u591f\u4f7f\u7528\uff0cOS\u4f1a\u8bb0\u4f4fmemory\u7684\u6240\u5c5e\uff0c\u8fd9\u6837\u5c31\u80fd\u591f\u4fdd\u8bc1\u4e0d\u51b2\u7a81\u4e86\u3002\u5176\u6b21\u662fprocess\u7684\u8fd0\u884c\u662f\u9700\u8981\u4e00\u5b9a\u7684memory space\u6765\u5b58\u653e\u5b83\u7684\u76f8\u5173\u7684\u6570\u636e\u7684\uff0c\u6bd4\u5982\u5728\u53d1\u751fcontext switch\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u5c06\u5b83\u7684context\u76f8\u5173\u7684\u6570\u636e\u90fd\u4fdd\u5b58\u5230\u5b83\u7684memory space\u4e2d\u6765\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fprocess\u7684call stack\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u9700\u8981memory space\u7684\u573a\u6240\u3002 Question: \u5982\u524d\u6240\u8ff0\uff0c\u6808\u4e5f\u662fvirtual address space\u7684\u6210\u5206\u4e4b\u4e00\uff0c\u6bcf\u4e2athread\u90fd\u6709\u5404\u81ea \u72ec\u7acb \u7684call stack\uff0c\u800c\u6240\u6709\u7684thread\u7406\u8bba\u4e0a\u90fd\u662f\u5171\u4eabprocess\u7684virtual address space\u7684\uff0c\u90a3\u8fd9\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u5176\u5b9e\u6700\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u662f\u67e5\u770b task_descriptor \u7684\u6210\u5458\u53d8\u91cf \u8865\u5145\u5185\u5bb9 # \u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u4e00\u4e9b\u5206\u6790\u5730\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\u3002 What the difference between lightweight process and thread? # I found an answer to the question here . But I don't understand some ideas in the answer. For instance, lightweight process is said to share its logical address space with other processes. What does it mean? I can understand the same situation with 2 threads: both of them share one address space, so both of them can read any variables from bss segment (for example). But we've got a lot of different processes with different bss sections and I dunno how to share all of them. A # From MSDN, Threads and Processes : Processes exist in the operating system and correspond to what users see as programs or applications. A thread, on the other hand, exists within a process. For this reason, threads are sometimes referred to as light-weight processes. Each process consists of one or more threads. A # I am not sure that answers are correct here, so let me post my version. There is a difference between process - LWP (lightweight process) and user thread . I will leave process definition aside since that's more or less known and focus on LWP vs user threads . LWP is what essentially are called today threads . Originally, user thread meant a thread that is managed by the application itself and the kernel does not know anything about it. LWP , on the other hand, is a unit of scheduling and execution by the kernel . Example: Let's assume that system has 3 other processes running and scheduling is round-robin without priorities. And you have 1 processor/core. Option 1 . You have 2 user threads using one LWP. That means that from OS perspective you have ONE scheduling unit. Totally there are 4 LWP running (3 others + 1 yours). Your LWP gets 1/4 of total CPU time and since you have 2 user threads, each of them gets 1/8 of total CPU time (depends on your implementation) Option2 . You have 2 LWP. From OS perspective, you have TWO scheduling units. Totally there are 5 LWP running. Your LWP gets 1/5 of total CPU time EACH and your application get's 2/5 of CPU. Another rough difference - LWP has pid (process id), user threads do not. For some reason, naming got little messed and we refer to LWP as threads. There are definitely more differences, but please, refer to slides.http://www.cosc.brocku.ca/Offerings/4P13/slides/threads.ppt EDIT: After posting, I found a good article that explains everything in more details and is in better English than I write. http://www.thegeekstuff.com/2013/11/linux-process-and-threads/ What is the difference between LWP and threads? # This explains the difference between LWP-Process-Thread: A light-weight process (LWP) is a means of achieving multitasking. In contrast to a regular (full-blown) process, an LWP shares all (or most of) its logical address space and system resources with other process(es) \uff08\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684process\u6240\u6307\u7684\u662flight weight process\uff0c\u800c\u4e0d\u662f\u6211\u4eec\u5bfb\u5e38\u610f\u4e49\u7684process\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f20\u7edf\u610f\u4e49\u7684process\u4f7f\u7528full-blown process\u6765\u8868\u793a\uff09; in contrast to a thread, a light-weight process has its own private process identifier and parenthood relationships with other processes. Moreover, while a thread can either be managed at the application level or by the kernel , an LWP is always managed by the kernel and it is scheduled as a regular process. One significant example of a kernel that supports LWPs is the Linux kernel . On most systems, a light-weight process also differs from a full-blown process , in that it only consists of the bare minimum execution context and accounting information that is needed by the scheduler, hence the term light-weight . Generally, a process \uff08full-blown process\uff09 refers to an instance of a program, while an LWP represents a thread of execution of a program (indeed, LWP s can be conveniently used to implement thread s, if the underlying kernel does not directly support them). Since a thread of execution does not need as much state information as a process, a light-weight process does not carry such information. As a consequence of the fact that LWPs share most of their resources with other LWPs, they are unsuitable for certain applications, where multiple full-blown processes are needed, e.g. to avoid memory leaks (a process can be replaced by another one) or to achieve privilege separation (processes can run under other credentials and have other permissions). Using multiple processes also allows the application to more easily survive if a process of the pool crashes or is exploited. What are Linux Processes, Threads, Light Weight Processes, and Process State # Linux has evolved a lot since its inception. It has become the most widely used operating system when in comes to servers and mission critical work. Though its not easy to understand Linux as a whole but there are aspects which are fundamental to Linux and worth understanding. In this article, we will discuss about Linux processes, threads and light weight processes and understand the difference between them. Towards the end, we will also discuss various states for Linux processes. Linux Processes # In a very basic form, Linux process can be visualized as running instance of a program. For example, just open a text editor on your Linux box and a text editor process will be born. Here is an example when I opened gedit on my machine : $ gedit & [1] 5560 $ ps -aef | grep gedit 1000 5560 2684 9 17:34 pts/0 00:00:00 gedit First command ( gedit &) opens gedit window while second ps command ( ps -aef | grep gedit ) checks if there is an associated process. In the result you can see that there is a process associated with gedit . Processes are fundamental to Linux as each and every work done by the OS is done in terms of and by the processes. Just think of anything and you will see that it is a process. This is because any work that is intended to be done requires system resources ( that are provided by kernel) and it is a process that is viewed by kernel as an entity to which it can provide system resources. Processes have priority based on which kernel context switches them. A process can be pre-empted if a process with higher priority is ready to be executed. For example, if a process is waiting for a system resource like some text from text file kept on disk then kernel can schedule a higher priority process and get back to the waiting process when data is available. This keeps the ball rolling for an operating system as a whole and gives user a feeling that tasks are being run in parallel. Processes can talk to other processes using Inter process communication methods and can share data using techniques like shared memory. In Linux, fork() is used to create new processes. These new processes are called as child processes and each child process initially shares all the segments like text, stack, heap etc until child tries to make any change to stack or heap. In case of any change, a separate copy of stack and heap segments are prepared for child so that changes remain child specific. The text segment is read-only so both parent and child share the same text segment. C fork function article explains more about fork(). Linux Threads vs Light Weight Processes # Threads in Linux are nothing but a flow of execution of the process. A process containing multiple execution flows is known as multi-threaded process. For a non multi-threaded process there is only execution flow that is the main execution flow and hence it is also known as single threaded process. For Linux kernel , there is no concept of thread . Each thread is viewed by kernel as a separate process but these processes are somewhat different from other normal processes. I will explain the difference in following paragraphs. Threads are often mixed with the term Light Weight Processes or LWPs. The reason dates back to those times when Linux supported threads at user level only . This means that even a multi-threaded application was viewed by kernel as a single process only. This posed big challenges for the library that managed these user level threads because it had to take care of cases that a thread execution did not hinder if any other thread issued a blocking call. Later on the implementation changed and processes were attached to each thread so that kernel can take care of them. But, as discussed earlier, Linux kernel does not see them as threads, each thread is viewed as a process inside kernel. These processes are known as light weight processes . The main difference between a light weight process (LWP) and a normal process is that LWPs share same address space and other resources like open files etc. As some resources are shared so these processes are considered to be light weight as compared to other normal processes and hence the name light weight processes. So, effectively we can say that threads and light weight processes are same. It\u2019s just that thread is a term that is used at user level while light weight process is a term used at kernel level. From implementation point of view, threads are created using functions exposed by POSIX compliant pthread library in Linux. Internally, the clone() function is used to create a normal as well as a light weight process . This means that to create a normal process fork() is used that further calls clone() with appropriate arguments while to create a thread or LWP, a function from pthread library calls clone() with relevant flags. So, the main difference is generated by using different flags that can be passed to clone() function. Read more about fork() and clone() on their respective man pages. How to Create Threads in Linux explains more about threads. Linux Process States # Life cycle of a normal Linux process seems pretty much like real life. Processes are born, share resources with parents for sometime, get their own copy of resources when they are ready to make changes, go through various states depending upon their priority and then finally die. In this section will will discuss various states of Linux processes : RUNNING \u2013 This state specifies that the process is either in execution or waiting to get executed. INTERRUPTIBLE \u2013 This state specifies that the process is waiting to get interrupted as it is in sleep mode and waiting for some action to happen that can wake this process up. The action can be a hardware interrupt, signal etc. UN-INTERRUPTIBLE \u2013 It is just like the INTERRUPTIBLE state, the only difference being that a process in this state cannot be waken up by delivering a signal. STOPPED \u2013 This state specifies that the process has been stopped. This may happen if a signal like SIGSTOP, SIGTTIN etc is delivered to the process. TRACED \u2013 This state specifies that the process is being debugged. Whenever the process is stopped by debugger (to help user debug the code) the process enters this state. ZOMBIE \u2013 This state specifies that the process is terminated but still hanging around in kernel process table because the parent of this process has still not fetched the termination status of this process. Parent uses wait() family of functions to fetch the termination status. DEAD \u2013 This state specifies that the process is terminated and entry is removed from process table. This state is achieved when the parent successfully fetches the termination status as explained in ZOMBIE state. What are the relations between processes, kernel threads, lightweight processes and user threads in Unix? [closed] #","title":"VS-process-VS-thread-VS-lightweight-process"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#_1","text":"\u5728\u672c\u4e66\u4e2d\u9891\u7e41\u51fa\u73b0process\uff0clightweight process\uff0cthread \u8fd9\u4e9b\u8bcd\u8bed\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u533a\u522b\uff0c\u5426\u5219\u5f88\u96be\u51c6\u786e\u7406\u89e3\u4e66\u4e2d\u5185\u5bb9\uff1b","title":"\u524d\u8a00"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#_2","text":"Thread (computing) \u548c Process (computing) \u662fsoftware engineer\u975e\u5e38\u719f\u7cfb\u7684\u6982\u5ff5\uff0c\u5b83\u4eec\u662f\u6807\u51c6\u6240\u5b9a\u4e49\u7684\u4e24\u4e2a\u6982\u5ff5\uff0c\u6709\u7740\u51c6\u786e\u7684\u542b\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u4e5f\u662f\u975e\u5e38\u6e05\u695a\u7684\u3002\u6309\u7167\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\u6d41\u7a0b\u6765\u770b\uff0c\u5e94\u8be5\u662f\u9996\u5148\u6709\u8ba1\u7b97\u673a\u7406\u8bba\u5b66\u5bb6\u63d0\u51fa\u4e86\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\uff0c\u7136\u540e\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u518d\u5b9e\u73b0\u8fd9\u4e9b\u6982\u5ff5/\u6807\u51c6\u3002\u6240\u4ee5\u4ece\u6807\u51c6\u7684\u51fa\u73b0\u5230\u64cd\u4f5c\u7cfb\u7edf\u5382\u5546\u5b9e\u73b0\u8fd9\u4e9b\u6807\u51c6\uff0c\u4e24\u8005\u4e4b\u95f4\u662f\u6709\u4e00\u4e2a\u65f6\u95f4\u95f4\u9694\u7684\u3002\u4e0d\u540c\u5382\u5546\u7684\u5bf9\u540c\u4e00\u6982\u5ff5/\u6807\u51c6\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e5f\u4f1a\u4e0d\u65ad\u5730\u6f14\u8fdb\u3002\u6240\u4ee5\u5728\u5f00\u59cb\u8fdb\u5165\u5230\u672c\u4e66\u7684\u5185\u5bb9\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5efa\u7acb\u5982\u4e0b\u89c2\u5ff5\uff1a \u6807\u51c6\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb \u4ee5\u53d1\u5c55\u7684\u773c\u5149\u6765\u770b\u5f85\u8f6f\u4ef6\u7684\u6f14\u8fdb \u4e0b\u9762\u4ee5operating system\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u76ee\u524d\u5b58\u5728\u7740\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a user level thread\uff0c\u5e38\u79f0\u4e3auser thread kernel level thread \u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0\uff1a https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/ What is a user thread and a kernel thread? \u663e\u7136\uff0c\u5bf9\u4e8e\u6807\u51c6\u6240\u63d0\u51fa\u7684 Thread (computing) \uff0c\u53ef\u4ee5\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002\u5173\u4e8e\u6b64\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Thread (computing) \u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002","title":"\u7406\u89e3\u6807\u51c6\u4e0e\u5b9e\u73b0"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#_3","text":"\u63cf\u8ff0\u6807\u51c6\u7684process\u548cthread\u5b9a\u4e49\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 \u4ee5\u4e0b\u662f\u4e00\u4e9b\u9700\u8981\u7740\u91cd\u5f3a\u8c03\u7684\uff1a process\u662fOS\u7684\u6982\u5ff5\uff0c\u5728instruction\u5c42\u7ea7\u5e76\u6ca1\u6709process\u7684\u6982\u5ff5\u3002OS\u4f7f\u7528process\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u5b9e\u73b0 multitasking \uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528hardware\u3002process\u662fprogram\u7684\u6267\u884c\uff0c\u5b83\u662fOS\u8fdb\u884cresource\u5206\u914d\u7684\u5355\u4f4d\uff0c\u4e0d\u540cprocess\u4e4b\u95f4\u7684\u8d44\u6e90\u9700\u8981\u5b8c\u5168\u9694\u79bb\uff08\u7279\u6b8a\u60c5\u51b5\u9664\u5916\uff09\uff0cOS\u4e2d\u7684\u6240\u6709process\u5171\u4eabOS\u6240\u7ba1\u7406\u7684hardware\u8d44\u6e90\u3002OS\u9700\u8981\u6e05\u695a\u5730\u77e5\u9053process\u548c\u8d44\u6e90\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5373\u4e00\u4e2aprocess\u62e5\u6709\u54ea\u4e9bresource\u3002","title":"\u7406\u89e3\u6807\u51c6"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#linux-kernel","text":"\u90a3linux kernel\u662f\u5982\u4f55\u6765\u5b9e\u73b0 Thread (computing) \u7684\u5462\uff1f\u4e0b\u9762\u662f\u4ece\u672c\u4e66\u7684\u4e00\u4e9b\u4ecb\u7ecd\uff1a chapter 1.1. Linux Versus Other Unix-Like Kernels Multithreaded application support Most modern operating systems have some kind of support for multithreaded applications that is, user programs that are designed in terms of many relatively independent execution flows that share a large portion of the application data structures. A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. Linux defines its own version of lightweight processes, which is different from the types used on other systems such as SVR4 and Solaris. While all the commercial Unix variants of LWP are based on kernel threads, Linux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u5176\u5b9e\u66f4\u597d\u7684\u65b9\u5f0f\u4e0d\u662f\u6839\u636e\u672c\u4e66\u4e2d\u7684\u5185\u5bb9\u6765\u63a8\u65adLinux OS\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\uff0c\u6700\u597d\u7684\u65b9\u5f0f\u662f\u9605\u8bfblinux\u7684man\uff0c\u5728 PTHREADS(7) \u7684Linux implementations of POSIX threads\u7ae0\u8282\u7ed9\u51fa\u4e86linux\u5b9e\u73b0POSIX threads\u7684\u65b9\u5f0f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5176\u4e2d\u4e5f\u7ed9\u51fa\u4e86\u67e5\u770b\u76f8\u5173\u5b9e\u73b0\u7684\u547d\u4ee4\u3002\u53ef\u4ee5\u786e\u5b9a\u7684\u662f\uff0c\u65e0\u8bba\u91c7\u7528\u54ea\u79cd\u65b9\u5f0f\uff0c\u6700\u7ec8\u90fd\u662f\u4f9d\u8d56 clone(2) \u3002 \u5173\u4e8e\u672c\u6bb5\uff0c\u6709\u7591\u95ee\uff1aLWP VS thread VS kernel thread? \u4e0a\u4e00\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux kernel threads do not represent the basic execution context abstraction. \u672c\u6bb5\u4e2d\u6240\u63cf\u8ff0\u7684\uff1aLinux regards lightweight processes as the basic execution context and handles them via the nonstandard clone( ) system call. \u663e\u7136\uff0ckernel thread\u4e0d\u662flinux\u7684lightweight process\u3002 \u663e\u7136linux\u7684lightweight process\u662f\u9700\u8981\u7531linux\u7684scheduler\u6765\u8fdb\u884c\u8c03\u5ea6\u7684\uff0c\u90a3kernel thread\u662f\u7531\u8c01\u6765\u8fdb\u884c\u8c03\u5ea6\u5462\uff1f\u4e0b\u9762\u662f\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Are kernel threads processes and daemons? Difference between user-level and kernel-supported threads? Kernel threads made easy \u5728linux\u4e2d\uff0clightweight process\u5bf9\u5e94\u7684\u662fthread\u5417\uff1f \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u672c\u4e66\u4e2d\u6709\u65f6\u5019\u4f1a\u5c06lightweight process\u7b80\u79f0\u4e3aprocess\uff0c\u6bd4\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\u8fd9\u53e5\uff1a A multithreaded user application could be composed of many lightweight processes (LWP), which are processes that can operate on a common address space, common physical memory pages, common opened files, and so on. \u6240\u4ee5\u5728\u672c\u4e66\u4e2d\uff0cprocess\u4e0d\u4e00\u5b9a\u6307\u7684\u662f\u6807\u51c6\u7684 Process (computing) \uff0c\u6709\u7684\u65f6\u5019\u6307\u7684\u662flightweight process\uff0c\u4e3a\u4e86\u4fbf\u4e8e\u533a\u5206\uff0c\u4f1a\u5728note\u4e2d\u8fdb\u884c\u7279\u6b8a\u8bf4\u660e\u3002 \u5173\u4e8elightweight process\uff0c\u53c2\u89c1\uff1a Light-weight process","title":"linux kernel\u7684\u5b9e\u73b0"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#linux-kernelprocessthread","text":"\u53c2\u89c13.1. Processes, Lightweight Processes, and Threads \u6211\u89c9\u5f97\u8981\u60f3\u89e3\u91ca\u597d\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u68b3\u7406\u4e00\u4e0blinux\u7684fork\uff0cclone\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5728 Fork (system call) \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u68b3\u7406\u5730\u975e\u5e38\u597d\u3002\u57283.4. Creating Processes\u4e2d\u4e5f\u6709\u76f8\u5173\u7684\u63cf\u8ff0\u3002","title":"linux kernel\u5982\u4f55\u5b9e\u73b0process\u4e0ethread"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#per-process-kernel-data-structures","text":"\u57283.4. Creating Processes\u4e2d\u63d0\u51fa\u4e86\u8fd9\u4e2a\u8bf4\u6cd5\uff0c\u5b83\u8ba9\u6211\u60f3\u8d77\u4e86\u4e24\u4ef6\u4e8b\u60c5\uff1a process\u4f5c\u4e3asystem resource\u5206\u914d\u5355\u4f4d\uff0c\u5b83\u6709\u54ea\u4e9bresource\u5462\uff1f\u663e\u7136\uff0c\u5b83\u7684\u6240\u6709\u7684resource\u90fd\u9700\u8981\u4f7f\u7528\u4e00\u4e2a kernel data structures\u6765\u8fdb\u884c\u63cf\u8ff0\u3002\u6709\u5fc5\u8981\u603b\u7ed3per-process\u7684resource\u4ee5\u53ca\u5bf9\u5e94\u7684kernel data structures\u3002\u4e0e\u6b64\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff0c\u8fd9\u4e9bresource\u54ea\u4e9b\u662fchild process\u53ef\u4ee5\u7ee7\u627f\u7684\uff0c\u54ea\u4e9b\u662f\u65e0\u6cd5\u7ee7\u627f\u7684\u3002 \u663e\u7136\uff0c\u591a\u4e2alightweight process\u662f\u53ef\u4ee5\u5171\u4eabper-process kernel data structure\u7684\uff08\u8fd9\u662f\u6807\u51c6\u89c4\u5b9a\u7684\uff09\uff0c\u8fd9\u79cd\u5171\u4eab\uff0c\u6211\u89c9\u5f97\u5b9e\u73b0\u4e0a\u5e94\u8be5\u4e5f\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c\u65e0\u975e\u5c31\u662f\u4f20\u5165\u4e00\u4e2a\u6307\u9488\u3002","title":"per-process kernel data structures"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#address-space","text":"\u8fd9\u4e2a\u95ee\u9898\u662f\u7531\u524d\u9762\u7684\u5173\u4e8eprocess\u7684resource\u7684\u601d\u8003\u884d\u751f\u51fa\u6765\u7684\u3002Address space\u662f\u4e00\u4e2aprocess\u975e\u5e38\u91cd\u8981\u7684resource\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u662fprocess\u8fdb\u884c\u6d3b\u52a8\u7684\u7a7a\u95f4\u3002\u76ee\u524d\u7684OS\u90fd\u662f\u91c7\u7528\u7684virtual address\uff0c\u5373process\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u6240\u4f7f\u7528\u7684\u662fvirtual memory\uff0c\u6240\u4ee5\u4e5f\u53ef\u4ee5\u5c06Address space\u79f0\u4e3aVirtual address space\u3002\u5173\u4e8eprocess\u7684Virtual address space\uff0c\u6211\u6709\u5982\u4e0b\u7591\u95ee\uff1a Question: process\u4f7f\u7528virtual memory\uff0c\u5e76\u4e14\u4f7f\u7528\u57fa\u4e8epage\u7684memory management\uff0c\u90a3\u5b83\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u57fa\u4e8epage\u7684virtual memory\u5462\uff1f\u662f\u5206\u5272\u4e3a\u4e00\u4e2a\u4e00\u4e2a\u7684page\uff1f \u7ecf\u8fc7\u7b80\u5355\u7684\u601d\u8003\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u662f\u7f16\u8bd1\u5668\u5728\u7ed9\u751f\u6210\u4ee3\u7801\u7684\u65f6\u5019\u5176\u5b9e\u662f\u4e0d\u9700\u8981\u8003\u8651\u8fd9\u4e2a\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u662fOS\u5728\u8fd0\u884cprogram\u7684\u65f6\u5019\u6309\u7167page\u8fdb\u884cmemory management\uff0c\u65e0\u8bba\u7f16\u8bd1\u5668\u751f\u6210\u7684program\u662f\u600e\u6837\u7684\uff0c\u662fOS\u8d1f\u8d23\u5c06\u8fd9\u4e9bprogram\u88c5\u5165\u5230memory\u4e2d\uff0c\u8fd9\u4e00\u5207\u5bf9compiler\u800c\u8a00\u90fd\u662f\u900f\u660e\u7684\u3002 \u4f46\u662f\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u5ef6\u4f38\u4e00\u4e0b\uff1a\u6211\u4eec\u77e5\u9053\uff0c\u7f16\u8bd1\u5668\u751f\u6210\u7684\u4ee3\u7801\u80af\u5b9a\u662f\u9700\u8981\u9075\u5faaalignment\u7684\uff0c\u90a3\u8fd9\u5c31\u6d89\u53ca\u5230alignment\u548cpage size\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u5e94\u8be5\u53ea\u8981\u7b26\u5408alignment\uff0c\u90a3\u4e48\u5e94\u8be5\u5c31\u4e0d\u4f1a\u5b58\u5728\u4e00\u4e2a\u6570\u636e\u5b58\u50a8\u8de8\u8d8a\u4e86\u591a\u4e2apage\u7684\u60c5\u51b5\u4e86\u3002 Question: \u8fdb\u7a0b\u7684virtual address space\u90fd\u662f\u76f8\u540c\u7684\uff0c\u90a3virtual address\u662f\u5982\u4f55\u6620\u5c04\u5230physical memory address\u7684\u5462\uff1f \u65e2\u7136\u4f7f\u7528\u7684\u662fdemand page\uff0c\u4e5f\u5c31\u662f\u5728process\u8fd0\u884c\u7684\u65f6\u5019\u9700\u8981\u8bbf\u95ee\u8be5virtual memory\u7684\u65f6\u5019\uff0c\u624dallocate physical memory\u6216\u8005swap-in\uff0c\u624d\u5c06virtual address\u6620\u5c04\u5230physical memory\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4fdd\u5b58\u5230\u8be5process\u7684page table\u4e2d\u3002 \u5176\u5b9e\u901a\u8fc7\u8fd9\u4e2a\u601d\u8003\u624d\u53d1\u73b0virtual memory\u7684\u91cd\u8981\u4ef7\u503c\u6240\u5728\uff0c\u5b83\u662f\u5b9e\u73b0demand page\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0\u6269\u5145memory\u7684\u57fa\u7840\uff0c\u5b83\u662f\u5b9e\u73b0copy on write\u7684\u57fa\u7840\u3002 Question: \u59821.6.8.4. Process virtual address space handling\u8282\u6240\u53d9\u8ff0\u7684 The kernel usually stores a process virtual address space as a list of memory area descriptors . \u5373\u6211\u4eec\u901a\u5e38\u5c06virtual address space\u5206\u5272\u4e3a\u591a\u5757\uff0c\u90a3\u662f\u5728\u4ec0\u4e48\u5730\u65b9\u5c06virtual address space\u5206\u5272\u4e3a\u5982\u4e0a\u6240\u8ff0\u7684a list of memory area descriptors \uff1f operating system\u91c7\u7528\u7684\u662fdemand paging\uff0c\u5e76\u4e14stack\u7684\u589e\u957f\u65b9\u5411\u548cheap\u7684\u589e\u957f\u65b9\u5411\u76f8\u53cd\uff0c\u90a3\u8fd9\u4e9b\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u8981\u60f3\u5b8c\u5168\u7406\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9605\u8bfbcalling convention\u3002\u6211\u89c9\u5f97process\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5bf9call stack\u7684\u7ef4\u62a4\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6d3b\u52a8\uff0c\u6bcf\u6b21new\u4e00\u4e2a\u6808\u5e27\u90fd\u9700\u8981\u5206\u914d\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\u91cd\u8981\u624d\u80fd\u591f\u4fdd\u8bc1process\u8fd0\u884c\u4e0b\u53bb\u3002 \u53e6\u5916\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u4e3a\u4ec0\u4e48\u9700\u8981\u7533\u8bf7memory\uff1f \u5176\u5b9e\u5982\u679c\u8fd9\u4e2a\u7cfb\u7edf\u4e2d\u53ea\u6709\u4e00\u4e2a\u7a0b\u5e8f\u7684\u8bdd\uff0c\u90a3\u4e48\u5b83\u60f3\u600e\u4e48\u6837\u4f7f\u7528memory\u5c31\u600e\u4e48\u6837\u4f7f\u7528memory\uff0c\u4f46\u662f\u95ee\u9898\u662f\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u662f\u9700\u8981\u652f\u6301\u591a\u4efb\u52a1\u7684\uff0c\u90a3\u5b83\u5c31\u9700\u8981\u505a\u597d\u4e0d\u540c\u7684process\u4e4b\u95f4\u7684\u9694\u79bb\uff0cA process\u4e0d\u80fd\u591f\u4f7f\u7528B process\u7684\u4e1c\u897f\u3002\u6240\u4ee5\uff0c\u6240\u6709\u7684process\u90fd\u5fc5\u987b\u8981\u5148\u60f3OS\u7533\u8bf7memory\uff0c\u7136\u540e\u624d\u80fd\u591f\u4f7f\u7528\uff0cOS\u4f1a\u8bb0\u4f4fmemory\u7684\u6240\u5c5e\uff0c\u8fd9\u6837\u5c31\u80fd\u591f\u4fdd\u8bc1\u4e0d\u51b2\u7a81\u4e86\u3002\u5176\u6b21\u662fprocess\u7684\u8fd0\u884c\u662f\u9700\u8981\u4e00\u5b9a\u7684memory space\u6765\u5b58\u653e\u5b83\u7684\u76f8\u5173\u7684\u6570\u636e\u7684\uff0c\u6bd4\u5982\u5728\u53d1\u751fcontext switch\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u5c06\u5b83\u7684context\u76f8\u5173\u7684\u6570\u636e\u90fd\u4fdd\u5b58\u5230\u5b83\u7684memory space\u4e2d\u6765\u3002\u53e6\u5916\u4e00\u4e2a\u5c31\u662fprocess\u7684call stack\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u9700\u8981memory space\u7684\u573a\u6240\u3002 Question: \u5982\u524d\u6240\u8ff0\uff0c\u6808\u4e5f\u662fvirtual address space\u7684\u6210\u5206\u4e4b\u4e00\uff0c\u6bcf\u4e2athread\u90fd\u6709\u5404\u81ea \u72ec\u7acb \u7684call stack\uff0c\u800c\u6240\u6709\u7684thread\u7406\u8bba\u4e0a\u90fd\u662f\u5171\u4eabprocess\u7684virtual address space\u7684\uff0c\u90a3\u8fd9\u53c8\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f \u5176\u5b9e\u6700\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u662f\u67e5\u770b task_descriptor \u7684\u6210\u5458\u53d8\u91cf","title":"Address space"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#_4","text":"\u4e0b\u9762\u662f\u68c0\u7d22\u5230\u7684\u4e00\u4e9b\u5206\u6790\u5730\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\u3002","title":"\u8865\u5145\u5185\u5bb9"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#what-the-difference-between-lightweight-process-and-thread","text":"I found an answer to the question here . But I don't understand some ideas in the answer. For instance, lightweight process is said to share its logical address space with other processes. What does it mean? I can understand the same situation with 2 threads: both of them share one address space, so both of them can read any variables from bss segment (for example). But we've got a lot of different processes with different bss sections and I dunno how to share all of them.","title":"What the difference between lightweight process and thread?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#a","text":"From MSDN, Threads and Processes : Processes exist in the operating system and correspond to what users see as programs or applications. A thread, on the other hand, exists within a process. For this reason, threads are sometimes referred to as light-weight processes. Each process consists of one or more threads.","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#a_1","text":"I am not sure that answers are correct here, so let me post my version. There is a difference between process - LWP (lightweight process) and user thread . I will leave process definition aside since that's more or less known and focus on LWP vs user threads . LWP is what essentially are called today threads . Originally, user thread meant a thread that is managed by the application itself and the kernel does not know anything about it. LWP , on the other hand, is a unit of scheduling and execution by the kernel . Example: Let's assume that system has 3 other processes running and scheduling is round-robin without priorities. And you have 1 processor/core. Option 1 . You have 2 user threads using one LWP. That means that from OS perspective you have ONE scheduling unit. Totally there are 4 LWP running (3 others + 1 yours). Your LWP gets 1/4 of total CPU time and since you have 2 user threads, each of them gets 1/8 of total CPU time (depends on your implementation) Option2 . You have 2 LWP. From OS perspective, you have TWO scheduling units. Totally there are 5 LWP running. Your LWP gets 1/5 of total CPU time EACH and your application get's 2/5 of CPU. Another rough difference - LWP has pid (process id), user threads do not. For some reason, naming got little messed and we refer to LWP as threads. There are definitely more differences, but please, refer to slides.http://www.cosc.brocku.ca/Offerings/4P13/slides/threads.ppt EDIT: After posting, I found a good article that explains everything in more details and is in better English than I write. http://www.thegeekstuff.com/2013/11/linux-process-and-threads/","title":"A"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#what-is-the-difference-between-lwp-and-threads","text":"This explains the difference between LWP-Process-Thread: A light-weight process (LWP) is a means of achieving multitasking. In contrast to a regular (full-blown) process, an LWP shares all (or most of) its logical address space and system resources with other process(es) \uff08\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u7684process\u6240\u6307\u7684\u662flight weight process\uff0c\u800c\u4e0d\u662f\u6211\u4eec\u5bfb\u5e38\u610f\u4e49\u7684process\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f20\u7edf\u610f\u4e49\u7684process\u4f7f\u7528full-blown process\u6765\u8868\u793a\uff09; in contrast to a thread, a light-weight process has its own private process identifier and parenthood relationships with other processes. Moreover, while a thread can either be managed at the application level or by the kernel , an LWP is always managed by the kernel and it is scheduled as a regular process. One significant example of a kernel that supports LWPs is the Linux kernel . On most systems, a light-weight process also differs from a full-blown process , in that it only consists of the bare minimum execution context and accounting information that is needed by the scheduler, hence the term light-weight . Generally, a process \uff08full-blown process\uff09 refers to an instance of a program, while an LWP represents a thread of execution of a program (indeed, LWP s can be conveniently used to implement thread s, if the underlying kernel does not directly support them). Since a thread of execution does not need as much state information as a process, a light-weight process does not carry such information. As a consequence of the fact that LWPs share most of their resources with other LWPs, they are unsuitable for certain applications, where multiple full-blown processes are needed, e.g. to avoid memory leaks (a process can be replaced by another one) or to achieve privilege separation (processes can run under other credentials and have other permissions). Using multiple processes also allows the application to more easily survive if a process of the pool crashes or is exploited.","title":"What is the difference between LWP and threads?"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#what-are-linux-processes-threads-light-weight-processes-and-process-state","text":"Linux has evolved a lot since its inception. It has become the most widely used operating system when in comes to servers and mission critical work. Though its not easy to understand Linux as a whole but there are aspects which are fundamental to Linux and worth understanding. In this article, we will discuss about Linux processes, threads and light weight processes and understand the difference between them. Towards the end, we will also discuss various states for Linux processes.","title":"What are Linux Processes, Threads, Light Weight Processes, and Process State"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#linux-processes","text":"In a very basic form, Linux process can be visualized as running instance of a program. For example, just open a text editor on your Linux box and a text editor process will be born. Here is an example when I opened gedit on my machine : $ gedit & [1] 5560 $ ps -aef | grep gedit 1000 5560 2684 9 17:34 pts/0 00:00:00 gedit First command ( gedit &) opens gedit window while second ps command ( ps -aef | grep gedit ) checks if there is an associated process. In the result you can see that there is a process associated with gedit . Processes are fundamental to Linux as each and every work done by the OS is done in terms of and by the processes. Just think of anything and you will see that it is a process. This is because any work that is intended to be done requires system resources ( that are provided by kernel) and it is a process that is viewed by kernel as an entity to which it can provide system resources. Processes have priority based on which kernel context switches them. A process can be pre-empted if a process with higher priority is ready to be executed. For example, if a process is waiting for a system resource like some text from text file kept on disk then kernel can schedule a higher priority process and get back to the waiting process when data is available. This keeps the ball rolling for an operating system as a whole and gives user a feeling that tasks are being run in parallel. Processes can talk to other processes using Inter process communication methods and can share data using techniques like shared memory. In Linux, fork() is used to create new processes. These new processes are called as child processes and each child process initially shares all the segments like text, stack, heap etc until child tries to make any change to stack or heap. In case of any change, a separate copy of stack and heap segments are prepared for child so that changes remain child specific. The text segment is read-only so both parent and child share the same text segment. C fork function article explains more about fork().","title":"Linux Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#linux-threads-vs-light-weight-processes","text":"Threads in Linux are nothing but a flow of execution of the process. A process containing multiple execution flows is known as multi-threaded process. For a non multi-threaded process there is only execution flow that is the main execution flow and hence it is also known as single threaded process. For Linux kernel , there is no concept of thread . Each thread is viewed by kernel as a separate process but these processes are somewhat different from other normal processes. I will explain the difference in following paragraphs. Threads are often mixed with the term Light Weight Processes or LWPs. The reason dates back to those times when Linux supported threads at user level only . This means that even a multi-threaded application was viewed by kernel as a single process only. This posed big challenges for the library that managed these user level threads because it had to take care of cases that a thread execution did not hinder if any other thread issued a blocking call. Later on the implementation changed and processes were attached to each thread so that kernel can take care of them. But, as discussed earlier, Linux kernel does not see them as threads, each thread is viewed as a process inside kernel. These processes are known as light weight processes . The main difference between a light weight process (LWP) and a normal process is that LWPs share same address space and other resources like open files etc. As some resources are shared so these processes are considered to be light weight as compared to other normal processes and hence the name light weight processes. So, effectively we can say that threads and light weight processes are same. It\u2019s just that thread is a term that is used at user level while light weight process is a term used at kernel level. From implementation point of view, threads are created using functions exposed by POSIX compliant pthread library in Linux. Internally, the clone() function is used to create a normal as well as a light weight process . This means that to create a normal process fork() is used that further calls clone() with appropriate arguments while to create a thread or LWP, a function from pthread library calls clone() with relevant flags. So, the main difference is generated by using different flags that can be passed to clone() function. Read more about fork() and clone() on their respective man pages. How to Create Threads in Linux explains more about threads.","title":"Linux Threads vs Light Weight Processes"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#linux-process-states","text":"Life cycle of a normal Linux process seems pretty much like real life. Processes are born, share resources with parents for sometime, get their own copy of resources when they are ready to make changes, go through various states depending upon their priority and then finally die. In this section will will discuss various states of Linux processes : RUNNING \u2013 This state specifies that the process is either in execution or waiting to get executed. INTERRUPTIBLE \u2013 This state specifies that the process is waiting to get interrupted as it is in sleep mode and waiting for some action to happen that can wake this process up. The action can be a hardware interrupt, signal etc. UN-INTERRUPTIBLE \u2013 It is just like the INTERRUPTIBLE state, the only difference being that a process in this state cannot be waken up by delivering a signal. STOPPED \u2013 This state specifies that the process has been stopped. This may happen if a signal like SIGSTOP, SIGTTIN etc is delivered to the process. TRACED \u2013 This state specifies that the process is being debugged. Whenever the process is stopped by debugger (to help user debug the code) the process enters this state. ZOMBIE \u2013 This state specifies that the process is terminated but still hanging around in kernel process table because the parent of this process has still not fetched the termination status of this process. Parent uses wait() family of functions to fetch the termination status. DEAD \u2013 This state specifies that the process is terminated and entry is removed from process table. This state is achieved when the parent successfully fetches the termination status as explained in ZOMBIE state.","title":"Linux Process States"},{"location":"Kernel/Book-Understanding-the-Linux-Kernel/Summary/VS-process-VS-thread-VS-lightweight-process/#what-are-the-relations-between-processes-kernel-threads-lightweight-processes-and-user-threads-in-unix-closed","text":"","title":"What are the relations between processes, kernel threads, lightweight processes and user threads in Unix? [closed]"},{"location":"Programming/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u4e3b\u8981\u8ba8\u8bbaprogramming in Linux OS\uff0c\u6240\u4ee5\u4e3b\u8981\u5173\u6ce8\u7684\u662fLinux OS\u63d0\u4f9b\u7684interface\u3002 man # TODO: \u5bf9Linux OS\u7684man\u8fdb\u884c\u4ecb\u7ecd\u3002 Linux OS\u5728\u591a\u4e2a\u5c42\u6b21\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7c7b\u4f3c\u529f\u80fd\u7684Interface\uff0c\u4e3b\u8981\u7531\u5982\u4e0b\u5c42\u6b21\uff1a system call\uff0c man(2) \u3001 man(3) user command\uff0c man(1) \u3001 man(8) user command\u5176\u5b9e\u4e5f\u662fkernel\u7684interface","title":"Introduction"},{"location":"Programming/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u8ba8\u8bbaprogramming in Linux OS\uff0c\u6240\u4ee5\u4e3b\u8981\u5173\u6ce8\u7684\u662fLinux OS\u63d0\u4f9b\u7684interface\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Programming/#man","text":"TODO: \u5bf9Linux OS\u7684man\u8fdb\u884c\u4ecb\u7ecd\u3002 Linux OS\u5728\u591a\u4e2a\u5c42\u6b21\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7c7b\u4f3c\u529f\u80fd\u7684Interface\uff0c\u4e3b\u8981\u7531\u5982\u4e0b\u5c42\u6b21\uff1a system call\uff0c man(2) \u3001 man(3) user command\uff0c man(1) \u3001 man(8) user command\u5176\u5b9e\u4e5f\u662fkernel\u7684interface","title":"man"},{"location":"Programming/01-01-Convention/Conventions/","text":"unix handler and start_rtn # FORWORD # \u4eca\u5929\u5728\u9605\u8bfbAPUE\u7684chapter 7.3 Process Termination\uff0c\u770b\u5176 atexit Function\uff0c\u901a\u8fc7\u6b64\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u6765\u6ce8\u518c exit handlers \uff0c\u8fd9\u79cd\u901a\u8fc7\u4f7f\u7528\u51fd\u6570\u6307\u9488\u6765\u4f5c\u4e3a\u53c2\u6570\u7684\u65b9\u5f0f\u5728Unix-like\u7684API\u4e2d\u975e\u5e38\u5e38\u89c1\uff0c\u5e76\u4e14\uff0c\u5b83\u4eec\u7684\u547d\u540d\u5f80\u5f80\u4e5f\u662f\u7c7b\u4f3c\u7684\uff0c\u6bd4\u5982\u5e38\u5e38\u5e26handler\u7b49\uff1b\u6240\u4ee5\u6211\u51b3\u5b9a\u8fdb\u884c\u6574\u7406\uff1b atexit and exit handler # \u53c2\u89c1 APUE chapter 7.3 Process Termination sigaction Function and signal handler # \u53c2\u89c1APUE 10.14 sigaction Function signal Function and signal handler # \u53c2\u89c1APUE 10.3 signal Function pthread_create and start_rtn # \u53c2\u89c1APUE 11.4 Thread Creation","title":"Conventions"},{"location":"Programming/01-01-Convention/Conventions/#unix-handler-and-start_rtn","text":"","title":"unix handler and start_rtn"},{"location":"Programming/01-01-Convention/Conventions/#forword","text":"\u4eca\u5929\u5728\u9605\u8bfbAPUE\u7684chapter 7.3 Process Termination\uff0c\u770b\u5176 atexit Function\uff0c\u901a\u8fc7\u6b64\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u6765\u6ce8\u518c exit handlers \uff0c\u8fd9\u79cd\u901a\u8fc7\u4f7f\u7528\u51fd\u6570\u6307\u9488\u6765\u4f5c\u4e3a\u53c2\u6570\u7684\u65b9\u5f0f\u5728Unix-like\u7684API\u4e2d\u975e\u5e38\u5e38\u89c1\uff0c\u5e76\u4e14\uff0c\u5b83\u4eec\u7684\u547d\u540d\u5f80\u5f80\u4e5f\u662f\u7c7b\u4f3c\u7684\uff0c\u6bd4\u5982\u5e38\u5e38\u5e26handler\u7b49\uff1b\u6240\u4ee5\u6211\u51b3\u5b9a\u8fdb\u884c\u6574\u7406\uff1b","title":"FORWORD"},{"location":"Programming/01-01-Convention/Conventions/#atexit-and-exit-handler","text":"\u53c2\u89c1 APUE chapter 7.3 Process Termination","title":"atexit and exit handler"},{"location":"Programming/01-01-Convention/Conventions/#sigaction-function-and-signal-handler","text":"\u53c2\u89c1APUE 10.14 sigaction Function","title":"sigaction Function and  signal handler"},{"location":"Programming/01-01-Convention/Conventions/#signal-function-and-signal-handler","text":"\u53c2\u89c1APUE 10.3 signal Function","title":"signal Function and  signal handler"},{"location":"Programming/01-01-Convention/Conventions/#pthread_create-and-start_rtn","text":"\u53c2\u89c1APUE 11.4 Thread Creation","title":"pthread_create  and start_rtn"},{"location":"Programming/01-02-Lib/organization-gnu/","text":"\u524d\u8a00 # \u5728Unix-like OS\u4e2d\u8fdb\u884cprogramming\uff0c\u5c31\u4e0d\u5f97\u4e0d\u638c\u63e1 GNU Project \u6240\u63d0\u4f9b\u7684\u4e00\u7cfb\u5217\u5de5\u5177\u4e86\u3002 GNU\u5b98\u7f51\uff1a GNU Operating System \u5206\u7c7b # GNU\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\uff08\u5f88\u591a\uff09software\uff0c\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e9bsoftware\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u5176\u5b98\u7f51\u7684 GNU Manuals Online \u9875\u9762\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5927\u5206\u7c7b\uff1a Archiving Audio Business and productivity Database Dictionaries Documentation translation Editors Education Email Fonts GNU organization Games Graphics Health Interface Internet applications Live communications Localization Mathematics Music Printing Science Security Software development Software libraries Spreadsheets System administration Telephony Text creation and manipulation Version control Video Web authoring Software development # \u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u8986\u76d6\u4e86\u975e\u5e38\u591a\u7684\u9886\u57df\u3002 \u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u800c\u8a00\uff0c\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u662f Software development \u8fd9\u4e00\u5927\u7c7b\u3002\u5176\u4e2d\u7684\u5de5\u5177\u5f88\u591a\u662f\u6211\u4eec\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u662f\u79bb\u4e0d\u5f00\u7684\u3002 GNU Build System # \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cgnu build system\u7684\u53e6\u5916\u4e00\u79cd\u8bf4\u6cd5\u662fautotools\uff1a \u5b98\u65b9\u4ecb\u7ecd\uff1a An Introduction to the Autotools \u7ec4\u6210 # GNU Autoconf # \u5b98\u7f51\uff1a Autoconf \u6587\u6863\uff1a GNU Autoconf - Creating Automatic Configuration Scripts \u529f\u80fd\uff1a\u521b\u5efa configure \u6587\u4ef6 \u5173\u4e8econfigure\u6587\u4ef6\uff0c\u53c2\u89c1 configure script \u4f7f\u7528configure\u6587\u4ef6\u6765\u521b\u5efamakefile\u6587\u4ef6 GNU Automake # \u5b98\u7f51\uff1a Automake \u6587\u6863\uff1a automake GNU Libtool # Gnulib # \u5b66\u4e60\u8d44\u6e90 # Autotools Mythbuster GNU toolchain #","title":"Introduction"},{"location":"Programming/01-02-Lib/organization-gnu/#_1","text":"\u5728Unix-like OS\u4e2d\u8fdb\u884cprogramming\uff0c\u5c31\u4e0d\u5f97\u4e0d\u638c\u63e1 GNU Project \u6240\u63d0\u4f9b\u7684\u4e00\u7cfb\u5217\u5de5\u5177\u4e86\u3002 GNU\u5b98\u7f51\uff1a GNU Operating System","title":"\u524d\u8a00"},{"location":"Programming/01-02-Lib/organization-gnu/#_2","text":"GNU\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\uff08\u5f88\u591a\uff09software\uff0c\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e9bsoftware\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u5176\u5b98\u7f51\u7684 GNU Manuals Online \u9875\u9762\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5927\u5206\u7c7b\uff1a Archiving Audio Business and productivity Database Dictionaries Documentation translation Editors Education Email Fonts GNU organization Games Graphics Health Interface Internet applications Live communications Localization Mathematics Music Printing Science Security Software development Software libraries Spreadsheets System administration Telephony Text creation and manipulation Version control Video Web authoring","title":"\u5206\u7c7b"},{"location":"Programming/01-02-Lib/organization-gnu/#software-development","text":"\u53ef\u4ee5\u770b\u5230\uff0c\u5b83\u8986\u76d6\u4e86\u975e\u5e38\u591a\u7684\u9886\u57df\u3002 \u5bf9\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u800c\u8a00\uff0c\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u662f Software development \u8fd9\u4e00\u5927\u7c7b\u3002\u5176\u4e2d\u7684\u5de5\u5177\u5f88\u591a\u662f\u6211\u4eec\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u662f\u79bb\u4e0d\u5f00\u7684\u3002","title":"Software development"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-build-system","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cgnu build system\u7684\u53e6\u5916\u4e00\u79cd\u8bf4\u6cd5\u662fautotools\uff1a \u5b98\u65b9\u4ecb\u7ecd\uff1a An Introduction to the Autotools","title":"GNU Build System"},{"location":"Programming/01-02-Lib/organization-gnu/#_3","text":"","title":"\u7ec4\u6210"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-autoconf","text":"\u5b98\u7f51\uff1a Autoconf \u6587\u6863\uff1a GNU Autoconf - Creating Automatic Configuration Scripts \u529f\u80fd\uff1a\u521b\u5efa configure \u6587\u4ef6 \u5173\u4e8econfigure\u6587\u4ef6\uff0c\u53c2\u89c1 configure script \u4f7f\u7528configure\u6587\u4ef6\u6765\u521b\u5efamakefile\u6587\u4ef6","title":"GNU Autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-automake","text":"\u5b98\u7f51\uff1a Automake \u6587\u6863\uff1a automake","title":"GNU Automake"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-libtool","text":"","title":"GNU Libtool"},{"location":"Programming/01-02-Lib/organization-gnu/#gnulib","text":"","title":"Gnulib"},{"location":"Programming/01-02-Lib/organization-gnu/#_4","text":"Autotools Mythbuster","title":"\u5b66\u4e60\u8d44\u6e90"},{"location":"Programming/01-02-Lib/organization-gnu/#gnu-toolchain","text":"","title":"GNU toolchain"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/","text":"Autoconf # 1 Introduction # Autoconf is a tool for producing shell scripts that automatically configure software source code packages to adapt to many kinds of Posix-like systems . The configuration scripts produced by Autoconf are independent of Autoconf when they are run, so their users do not need to have Autoconf. The configuration scripts produced by Autoconf require no manual user intervention when run; they do not normally even need an argument specifying the system type. Instead, they individually test for the presence of each feature that the software package they are for might need. (Before each check, they print a one-line message stating what they are checking for, so the user doesn't get too bored while waiting for the script to finish.) As a result, they deal well with systems that are hybrids or customized from the more common Posix variants. There is no need to maintain files that list the features supported by each release of each variant of Posix. For each software package that Autoconf is used with, it creates a configuration script from a template file that lists the system features that the package needs or can use. After the shell code to recognize and respond to a system feature has been written, Autoconf allows it to be shared by many software packages that can use (or need) that feature. If it later turns out that the shell code needs adjustment for some reason, it needs to be changed in only one place; all of the configuration scripts can be regenerated automatically to take advantage of the updated code. NOTE: Autoconf \u7684\u8f93\u5165\uff1atemplate file\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure.ac Autoconf \u7684\u8f93\u51fa\uff1aconfiguration script\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure 2 The GNU Build System # 2.1 Automake 2.2 Gnulib 2.3 Libtool 2.4 Pointers 3 Making configure Scripts # The configuration scripts that Autoconf produces are by convention called configure . When run, configure creates several files, replacing configuration parameters in them with appropriate values. The files that configure creates are: one or more Makefile files, usually one in each subdirectory of the package (see Makefile Substitutions ); optionally, a C header file, the name of which is configurable, containing #define directives (see Configuration Headers ); a shell script called config.status that, when run, recreates the files listed above (see config.status Invocation ); an optional shell script normally called config.cache (created when using \u2018 configure --config-cache \u2019) that saves the results of running many of the tests (see Cache Files ); a file called config.log containing any messages produced by compilers, to help debugging if configure makes a mistake. To create a configure script with Autoconf, you need to write an Autoconf input file configure.ac (or configure.in ) and run autoconf on it. If you write your own feature tests to supplement those that come with Autoconf, you might also write files called aclocal.m4 and acsite.m4 . If you use a C header file to contain #define directives, you might also run autoheader , and you can distribute the generated file config.h.in with the package. Here is a diagram showing how the files that can be used in configuration are produced. Programs that are executed are suffixed by \u2018 * \u2019. Optional files are enclosed in square brackets (\u2018 [] \u2019). autoconf and autoheader also read the installed Autoconf macro files (by reading autoconf.m4 ). Files used in preparing a software package for distribution, when using just Autoconf: your source files --> [autoscan*] --> [configure.scan] --> configure.ac configure.ac --. | .------> autoconf* -----> configure [aclocal.m4] --+---+ | `-----> [autoheader*] --> [config.h.in] [acsite.m4] ---' Makefile.in Additionally, if you use Automake, the following additional productions come into play: [acinclude.m4] --. | [local macros] --+--> aclocal* --> aclocal.m4 | configure.ac ----' configure.ac --. +--> automake* --> Makefile.in Makefile.am ---' 3.1 Writing configure.ac # To produce a configure script for a software package, create a file called configure.ac that contains invocations of the Autoconf macros that test the system features your package needs or can use. Autoconf macros already exist to check for many features; see Existing Tests , for their descriptions. For most other features, you can use Autoconf template macros to produce custom checks ; see Writing Tests , for information about them. For especially tricky or specialized features, configure.ac might need to contain some hand-crafted shell commands; see Portable Shell Programming . The autoscan program can give you a good start in writing configure.ac (see autoscan Invocation , for more information). Shell Script Compiler : Autoconf as solution of a problem Autoconf Language : Programming in Autoconf Autoconf Input Layout : Standard organization of configure.ac 3.1.1 A Shell Script Compiler # Just as for any other computer language, in order to properly program configure.ac in Autoconf you must understand what problem the language tries to address and how it does so. The problem Autoconf addresses is that the world is a mess. After all, you are using Autoconf in order to have your package compile easily on all sorts of different systems, some of them being extremely hostile. Autoconf itself bears the price for these differences: configure must run on all those systems, and thus configure must limit itself to their lowest common denominator of features. Naturally, you might then think of shell scripts; who needs autoconf? A set of properly written shell functions is enough to make it easy to write configure scripts by hand. Sigh! Unfortunately, even in 2008, where shells without any function support are far and few between, there are pitfalls to avoid when making use of them. Also, finding a Bourne shell that accepts shell functions is not trivial, even though there is almost always one on interesting porting targets. So, what is really needed is some kind of compiler, autoconf , that takes an Autoconf program, configure.ac , and transforms it into a portable shell script, configure . How does autoconf perform this task? There are two obvious possibilities: creating a brand new language or extending an existing one. The former option is attractive: all sorts of optimizations could easily be implemented in the compiler and many rigorous checks could be performed on the Autoconf program (e.g., rejecting any non-portable construct). Alternatively, you can extend an existing language, such as the sh (Bourne shell) language. Autoconf does the latter: it is a layer on top of sh . It was therefore most convenient to implement autoconf as a macro expander : a program that repeatedly performs macro expansions on text input, replacing macro calls with macro bodies and producing a pure sh script in the end. Instead of implementing a dedicated Autoconf macro expander , it is natural to use an existing general-purpose macro language , such as M4, and implement the extensions as a set of M4 macros. 3.1.2 The Autoconf Language # The Autoconf language differs from many other computer languages because it treats actual code the same as plain text. Whereas in C, for instance, data and instructions have different syntactic status, in Autoconf their status is rigorously the same. Therefore, we need a means to distinguish literal strings from text to be expanded: quotation. When calling macros that take arguments, there must not be any white space between the macro name and the open parenthesis. AC_INIT ([oops], [1.0]) # incorrect AC_INIT([hello], [1.0]) # good Arguments should be enclosed within the quote characters \u2018 [ \u2019 and \u2018 ] \u2019, and be separated by commas. Any leading blanks or newlines in arguments are ignored, unless they are quoted. You should always quote an argument that might contain a macro name, comma, parenthesis, or a leading blank or newline. This rule applies recursively for every macro call, including macros called from other macros. For more details on quoting rules, see Programming in M4 . For instance: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], [1], [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) is quoted properly. You may safely simplify its quotation to: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) because \u20181\u2019 cannot contain a macro call. Here, the argument of AC_MSG_ERROR must be quoted; otherwise, its comma would be interpreted as an argument separator. Also, the second and third arguments of \u2018 AC_CHECK_HEADER \u2019 must be quoted, since they contain macro calls. The three arguments \u2018 HAVE_STDIO_H \u2019, \u2018 stdio.h \u2019, and \u2018 Define to 1 if you have <stdio.h>. \u2019 do not need quoting, but if you unwisely defined a macro with a name like \u2018 Define \u2019 or \u2018 stdio \u2019 then they would need quoting. Cautious Autoconf users would keep the quotes, but many Autoconf users find such precautions annoying, and would rewrite the example as follows: AC_CHECK_HEADER(stdio.h, [AC_DEFINE(HAVE_STDIO_H, 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) 3.1.3 Standard configure.ac Layout # The order in which configure.ac calls the Autoconf macros is not important, with a few exceptions. Every configure.ac must contain a call to AC_INIT before the checks, and a call to AC_OUTPUT at the end (see Output ). Additionally, some macros rely on other macros having been called first, because they check previously set values of some variables to decide what to do. These macros are noted in the individual descriptions (see Existing Tests ), and they also warn you when configure is created if they are called out of order. To encourage consistency, here is a suggested order for calling the Autoconf macros. Generally speaking, the things near the end of this list are those that could depend on things earlier in it. For example, library functions could be affected by types and libraries. Autoconf requirements AC_INIT(package, version, bug-report-address) information on the package checks for programs checks for libraries checks for header files checks for types checks for structures checks for compiler characteristics checks for library functions checks for system services AC_CONFIG_FILES([file...]) AC_OUTPUT 3.2 Using autoscan to Create configure.ac # 3.3 Using ifnames to List Conditionals # 3.4 Using autoconf to Create configure 3.5 Using autoreconf to Update configure Scripts","title":"Autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#autoconf","text":"","title":"Autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#1-introduction","text":"Autoconf is a tool for producing shell scripts that automatically configure software source code packages to adapt to many kinds of Posix-like systems . The configuration scripts produced by Autoconf are independent of Autoconf when they are run, so their users do not need to have Autoconf. The configuration scripts produced by Autoconf require no manual user intervention when run; they do not normally even need an argument specifying the system type. Instead, they individually test for the presence of each feature that the software package they are for might need. (Before each check, they print a one-line message stating what they are checking for, so the user doesn't get too bored while waiting for the script to finish.) As a result, they deal well with systems that are hybrids or customized from the more common Posix variants. There is no need to maintain files that list the features supported by each release of each variant of Posix. For each software package that Autoconf is used with, it creates a configuration script from a template file that lists the system features that the package needs or can use. After the shell code to recognize and respond to a system feature has been written, Autoconf allows it to be shared by many software packages that can use (or need) that feature. If it later turns out that the shell code needs adjustment for some reason, it needs to be changed in only one place; all of the configuration scripts can be regenerated automatically to take advantage of the updated code. NOTE: Autoconf \u7684\u8f93\u5165\uff1atemplate file\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure.ac Autoconf \u7684\u8f93\u51fa\uff1aconfiguration script\uff0c\u6309\u7167\u60ef\u4f8b\uff0c\u6587\u4ef6\u540d\u4e3a configure","title":"1 Introduction"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#2-the-gnu-build-system","text":"2.1 Automake 2.2 Gnulib 2.3 Libtool 2.4 Pointers","title":"2 The GNU Build System"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#3-making-configure-scripts","text":"The configuration scripts that Autoconf produces are by convention called configure . When run, configure creates several files, replacing configuration parameters in them with appropriate values. The files that configure creates are: one or more Makefile files, usually one in each subdirectory of the package (see Makefile Substitutions ); optionally, a C header file, the name of which is configurable, containing #define directives (see Configuration Headers ); a shell script called config.status that, when run, recreates the files listed above (see config.status Invocation ); an optional shell script normally called config.cache (created when using \u2018 configure --config-cache \u2019) that saves the results of running many of the tests (see Cache Files ); a file called config.log containing any messages produced by compilers, to help debugging if configure makes a mistake. To create a configure script with Autoconf, you need to write an Autoconf input file configure.ac (or configure.in ) and run autoconf on it. If you write your own feature tests to supplement those that come with Autoconf, you might also write files called aclocal.m4 and acsite.m4 . If you use a C header file to contain #define directives, you might also run autoheader , and you can distribute the generated file config.h.in with the package. Here is a diagram showing how the files that can be used in configuration are produced. Programs that are executed are suffixed by \u2018 * \u2019. Optional files are enclosed in square brackets (\u2018 [] \u2019). autoconf and autoheader also read the installed Autoconf macro files (by reading autoconf.m4 ). Files used in preparing a software package for distribution, when using just Autoconf: your source files --> [autoscan*] --> [configure.scan] --> configure.ac configure.ac --. | .------> autoconf* -----> configure [aclocal.m4] --+---+ | `-----> [autoheader*] --> [config.h.in] [acsite.m4] ---' Makefile.in Additionally, if you use Automake, the following additional productions come into play: [acinclude.m4] --. | [local macros] --+--> aclocal* --> aclocal.m4 | configure.ac ----' configure.ac --. +--> automake* --> Makefile.in Makefile.am ---'","title":"3 Making configure Scripts"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#31-writing-configureac","text":"To produce a configure script for a software package, create a file called configure.ac that contains invocations of the Autoconf macros that test the system features your package needs or can use. Autoconf macros already exist to check for many features; see Existing Tests , for their descriptions. For most other features, you can use Autoconf template macros to produce custom checks ; see Writing Tests , for information about them. For especially tricky or specialized features, configure.ac might need to contain some hand-crafted shell commands; see Portable Shell Programming . The autoscan program can give you a good start in writing configure.ac (see autoscan Invocation , for more information). Shell Script Compiler : Autoconf as solution of a problem Autoconf Language : Programming in Autoconf Autoconf Input Layout : Standard organization of configure.ac","title":"3.1 Writing configure.ac"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#311-a-shell-script-compiler","text":"Just as for any other computer language, in order to properly program configure.ac in Autoconf you must understand what problem the language tries to address and how it does so. The problem Autoconf addresses is that the world is a mess. After all, you are using Autoconf in order to have your package compile easily on all sorts of different systems, some of them being extremely hostile. Autoconf itself bears the price for these differences: configure must run on all those systems, and thus configure must limit itself to their lowest common denominator of features. Naturally, you might then think of shell scripts; who needs autoconf? A set of properly written shell functions is enough to make it easy to write configure scripts by hand. Sigh! Unfortunately, even in 2008, where shells without any function support are far and few between, there are pitfalls to avoid when making use of them. Also, finding a Bourne shell that accepts shell functions is not trivial, even though there is almost always one on interesting porting targets. So, what is really needed is some kind of compiler, autoconf , that takes an Autoconf program, configure.ac , and transforms it into a portable shell script, configure . How does autoconf perform this task? There are two obvious possibilities: creating a brand new language or extending an existing one. The former option is attractive: all sorts of optimizations could easily be implemented in the compiler and many rigorous checks could be performed on the Autoconf program (e.g., rejecting any non-portable construct). Alternatively, you can extend an existing language, such as the sh (Bourne shell) language. Autoconf does the latter: it is a layer on top of sh . It was therefore most convenient to implement autoconf as a macro expander : a program that repeatedly performs macro expansions on text input, replacing macro calls with macro bodies and producing a pure sh script in the end. Instead of implementing a dedicated Autoconf macro expander , it is natural to use an existing general-purpose macro language , such as M4, and implement the extensions as a set of M4 macros.","title":"3.1.1 A Shell Script Compiler"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#312-the-autoconf-language","text":"The Autoconf language differs from many other computer languages because it treats actual code the same as plain text. Whereas in C, for instance, data and instructions have different syntactic status, in Autoconf their status is rigorously the same. Therefore, we need a means to distinguish literal strings from text to be expanded: quotation. When calling macros that take arguments, there must not be any white space between the macro name and the open parenthesis. AC_INIT ([oops], [1.0]) # incorrect AC_INIT([hello], [1.0]) # good Arguments should be enclosed within the quote characters \u2018 [ \u2019 and \u2018 ] \u2019, and be separated by commas. Any leading blanks or newlines in arguments are ignored, unless they are quoted. You should always quote an argument that might contain a macro name, comma, parenthesis, or a leading blank or newline. This rule applies recursively for every macro call, including macros called from other macros. For more details on quoting rules, see Programming in M4 . For instance: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], [1], [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) is quoted properly. You may safely simplify its quotation to: AC_CHECK_HEADER([stdio.h], [AC_DEFINE([HAVE_STDIO_H], 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])]) because \u20181\u2019 cannot contain a macro call. Here, the argument of AC_MSG_ERROR must be quoted; otherwise, its comma would be interpreted as an argument separator. Also, the second and third arguments of \u2018 AC_CHECK_HEADER \u2019 must be quoted, since they contain macro calls. The three arguments \u2018 HAVE_STDIO_H \u2019, \u2018 stdio.h \u2019, and \u2018 Define to 1 if you have <stdio.h>. \u2019 do not need quoting, but if you unwisely defined a macro with a name like \u2018 Define \u2019 or \u2018 stdio \u2019 then they would need quoting. Cautious Autoconf users would keep the quotes, but many Autoconf users find such precautions annoying, and would rewrite the example as follows: AC_CHECK_HEADER(stdio.h, [AC_DEFINE(HAVE_STDIO_H, 1, [Define to 1 if you have <stdio.h>.])], [AC_MSG_ERROR([sorry, can't do anything for you])])","title":"3.1.2 The Autoconf Language"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#313-standard-configureac-layout","text":"The order in which configure.ac calls the Autoconf macros is not important, with a few exceptions. Every configure.ac must contain a call to AC_INIT before the checks, and a call to AC_OUTPUT at the end (see Output ). Additionally, some macros rely on other macros having been called first, because they check previously set values of some variables to decide what to do. These macros are noted in the individual descriptions (see Existing Tests ), and they also warn you when configure is created if they are called out of order. To encourage consistency, here is a suggested order for calling the Autoconf macros. Generally speaking, the things near the end of this list are those that could depend on things earlier in it. For example, library functions could be affected by types and libraries. Autoconf requirements AC_INIT(package, version, bug-report-address) information on the package checks for programs checks for libraries checks for header files checks for types checks for structures checks for compiler characteristics checks for library functions checks for system services AC_CONFIG_FILES([file...]) AC_OUTPUT","title":"3.1.3 Standard configure.ac Layout"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#32-using-autoscan-to-create-configureac","text":"","title":"3.2 Using autoscan to Create configure.ac"},{"location":"Programming/01-02-Lib/organization-gnu/Autoconf/Autoconf/#33-using-ifnames-to-list-conditionals","text":"3.4 Using autoconf to Create configure 3.5 Using autoreconf to Update configure Scripts","title":"3.3 Using ifnames to List Conditionals"},{"location":"Programming/01-02-Lib/organization-gnu/Automake/Automake/","text":"automake #","title":"Automake"},{"location":"Programming/01-02-Lib/organization-gnu/Automake/Automake/#automake","text":"","title":"automake"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/","text":"Autotools Mythbuster # Chapter 1. Configuring The Build \u2014 autoconf # Configuring the build consists of running a series of tests to identify the build environment and the presence of the required tools and libraries. It is a crucial step in allowing portability between different operating systems to detect this build environment system. In the autotools chain, this is done by the autoconf tool. The autoconf tool translates a configure.ac file, written in a mixture of m4 and shell scripting, into a configure POSIX shell script that executes the tests that determines what the build environment is. 1. M4sh # The language used to write the configure.ac is called M4sh , to make clear that it's based off both sh and the macro language M4 . 2. Canonical Systems # When using autoconf, there are three system definitions (or machine definitions ) that are used to identify the \u201cactors\u201d in the build process; each definition relates to a similarly-named variable which will be illustrated in detail later. These three definitions are: host ( CHOST ) The system that is going to run the software once it is built, which is the main actor. Once the software has been built, it will execute on this particular system. build ( CBUILD ) The system where the build process is being executed. For most uses this would be the same as the host system, but in case of cross-compilation the two obviously differ. target ( CTARGET ) The system against which the software being built will run on. This actor only exists, or rather has a meaning, when the software being built may interact specifically with a system that differs from the one it's being executed on (our host ). This is the case for compilers, debuggers, profilers and analyzers and other tools in general. To identify the current actors involved in the build process, autoconf provides three macros that take care of finding the so-called \u201ccanonical\u201d values (see Section 2.1, \u201cThe System Definition Tuples\u201d for their format): AC_CANONICAL_HOST , AC_CANONICAL_BUILD and AC_CANONICAL_TARGET . These three macros then provide to the configure script the sh variables with the name of the actor ( $host , $build and $target ), and three parameters with the same name to the configure script so that the user can override the default discovered values. The most basic autoconf based build systems won't need to know any of these values, at least directly. Some other tools, such as libtool, will require discovery of canonical systems by themselves. Since adding these macros unconditionally adds direct and indirect code to the configure script (and a dependency on the two support files config.sub and config.guess ); it is recommended not to call them unconditionally. It is actually quite easy to decide whether canonical system definitions are needed or not. We just have to look for the use of the related actor variable. For instance if the configure.ac script uses the $build variable, we would need to call AC_CANONICAL_BUILD to discover its value. If the system definition variables are used in a macro instead, we should use the AC_REQUIRE macro to ensure that they are executed before entering. Don't fear calling them in more than one place. See Section 6.2, \u201cOnce-Expansion\u201d for more details. One common mistake is to \u201cgo all the way\u201d and always use the AC_CANONICAL_TARGET macro, or its misnamed predecessor AC_CANONICAL_SYSTEM . This is particularly a problem; because most of the software will not have a target actor at all. This actor is only meaningful when the software that is being built manages data that is specific to a different system than the one it is being executed on (the host system). In practice, the only places where the target actor is meaningful are to the parts of a compile toolchain: assemblers, linkers, compilers, debuggers, profilers, analysers, \u2026 For the rest of the software, the presence of an extraneous --target option to configure is likely to just be confusing. Especially for software that processes the output of the script to identify some information about the package being built. 2.1. The System Definition Tuples # The system definitions used by autoconf (but also by other packages like GCC and Binutils) are simple tuples in the form of strings. These are designed to provide, in a format easy to parse with \u201cglob masks\u201d; the major details that describe a computer system . The number of elements in these tuples is variable, for some uses that only deal with very low-level code, there can be just a single element, the system architecture ( i386 , x86_64 , powerpc , \u2026); others will have two, defining either the operating system or, most often for definition pairs, the executable format ( elf , coff , \u2026). These two formats though are usually, only related to components of the toolchain and not to autoconf directly. The tuples commonly used with autoconf are triples and quadruples, which define three components: architecture , vendor and operating system . These three components usually map directly into the triples, but for quadruple you have to split the operating system into kernel and userland (usually the C library). While the architecture is most obvious; and operating systems differ slightly from one another (still being probably the most important data), the vendor value is usually just ignored. It is meant to actually be the vendor of the hardware system, rather than the vendor of the software, although presently it is mostly used by distributions to brand their toolchain ( i386-redhat-linux-gnu ) or their special systems ( i386-gentoo-freebsd7.0 ) and by vendors that provide their own specific toolchain ( i686-apple-darwin9 ). Most operating systems don't split their definitions further in kernel and userland because they only work as an \u201censemble\u201d: FreeBSD, (Open)Solaris, Darwin, \u2026 There are, though, a few operating systems that have a split between kernel and userland, being managed by different projects or even being replaceable independently. This is the case for instance of Linux, which can use (among others) the GNU C Library (GNU/Linux) or uClibc, which become respectively *-linux-gnu and *-linux-uclibc . Also, most operating systems using triples also have a single standardised version for both kernel and userland, and thus provide it as a suffix to the element ( *-freebsd7.0 , *-netbsd4.0 ). For a few operating systems, this value might differ from the one that is used as the \u201cproduct version\u201d used in public. For instance Solaris 10 uses as a definition *-solaris2.10 and Apple's Mac OS X 10.5 uses *-darwin9 . 2.2. When To Use System Definitions # To be extended 3. Adding Options # 4. Finding Libraries # 5. Custom Autoconf Tests # 6. Autoconf Building Blocks: Macros # 7. Caching Results #","title":"Introduction"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#autotools-mythbuster","text":"","title":"Autotools Mythbuster"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#chapter-1-configuring-the-build-autoconf","text":"Configuring the build consists of running a series of tests to identify the build environment and the presence of the required tools and libraries. It is a crucial step in allowing portability between different operating systems to detect this build environment system. In the autotools chain, this is done by the autoconf tool. The autoconf tool translates a configure.ac file, written in a mixture of m4 and shell scripting, into a configure POSIX shell script that executes the tests that determines what the build environment is.","title":"Chapter 1. Configuring The Build \u2014 autoconf"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#1-m4sh","text":"The language used to write the configure.ac is called M4sh , to make clear that it's based off both sh and the macro language M4 .","title":"1. M4sh"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#2-canonical-systems","text":"When using autoconf, there are three system definitions (or machine definitions ) that are used to identify the \u201cactors\u201d in the build process; each definition relates to a similarly-named variable which will be illustrated in detail later. These three definitions are: host ( CHOST ) The system that is going to run the software once it is built, which is the main actor. Once the software has been built, it will execute on this particular system. build ( CBUILD ) The system where the build process is being executed. For most uses this would be the same as the host system, but in case of cross-compilation the two obviously differ. target ( CTARGET ) The system against which the software being built will run on. This actor only exists, or rather has a meaning, when the software being built may interact specifically with a system that differs from the one it's being executed on (our host ). This is the case for compilers, debuggers, profilers and analyzers and other tools in general. To identify the current actors involved in the build process, autoconf provides three macros that take care of finding the so-called \u201ccanonical\u201d values (see Section 2.1, \u201cThe System Definition Tuples\u201d for their format): AC_CANONICAL_HOST , AC_CANONICAL_BUILD and AC_CANONICAL_TARGET . These three macros then provide to the configure script the sh variables with the name of the actor ( $host , $build and $target ), and three parameters with the same name to the configure script so that the user can override the default discovered values. The most basic autoconf based build systems won't need to know any of these values, at least directly. Some other tools, such as libtool, will require discovery of canonical systems by themselves. Since adding these macros unconditionally adds direct and indirect code to the configure script (and a dependency on the two support files config.sub and config.guess ); it is recommended not to call them unconditionally. It is actually quite easy to decide whether canonical system definitions are needed or not. We just have to look for the use of the related actor variable. For instance if the configure.ac script uses the $build variable, we would need to call AC_CANONICAL_BUILD to discover its value. If the system definition variables are used in a macro instead, we should use the AC_REQUIRE macro to ensure that they are executed before entering. Don't fear calling them in more than one place. See Section 6.2, \u201cOnce-Expansion\u201d for more details. One common mistake is to \u201cgo all the way\u201d and always use the AC_CANONICAL_TARGET macro, or its misnamed predecessor AC_CANONICAL_SYSTEM . This is particularly a problem; because most of the software will not have a target actor at all. This actor is only meaningful when the software that is being built manages data that is specific to a different system than the one it is being executed on (the host system). In practice, the only places where the target actor is meaningful are to the parts of a compile toolchain: assemblers, linkers, compilers, debuggers, profilers, analysers, \u2026 For the rest of the software, the presence of an extraneous --target option to configure is likely to just be confusing. Especially for software that processes the output of the script to identify some information about the package being built.","title":"2. Canonical Systems"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#21-the-system-definition-tuples","text":"The system definitions used by autoconf (but also by other packages like GCC and Binutils) are simple tuples in the form of strings. These are designed to provide, in a format easy to parse with \u201cglob masks\u201d; the major details that describe a computer system . The number of elements in these tuples is variable, for some uses that only deal with very low-level code, there can be just a single element, the system architecture ( i386 , x86_64 , powerpc , \u2026); others will have two, defining either the operating system or, most often for definition pairs, the executable format ( elf , coff , \u2026). These two formats though are usually, only related to components of the toolchain and not to autoconf directly. The tuples commonly used with autoconf are triples and quadruples, which define three components: architecture , vendor and operating system . These three components usually map directly into the triples, but for quadruple you have to split the operating system into kernel and userland (usually the C library). While the architecture is most obvious; and operating systems differ slightly from one another (still being probably the most important data), the vendor value is usually just ignored. It is meant to actually be the vendor of the hardware system, rather than the vendor of the software, although presently it is mostly used by distributions to brand their toolchain ( i386-redhat-linux-gnu ) or their special systems ( i386-gentoo-freebsd7.0 ) and by vendors that provide their own specific toolchain ( i686-apple-darwin9 ). Most operating systems don't split their definitions further in kernel and userland because they only work as an \u201censemble\u201d: FreeBSD, (Open)Solaris, Darwin, \u2026 There are, though, a few operating systems that have a split between kernel and userland, being managed by different projects or even being replaceable independently. This is the case for instance of Linux, which can use (among others) the GNU C Library (GNU/Linux) or uClibc, which become respectively *-linux-gnu and *-linux-uclibc . Also, most operating systems using triples also have a single standardised version for both kernel and userland, and thus provide it as a suffix to the element ( *-freebsd7.0 , *-netbsd4.0 ). For a few operating systems, this value might differ from the one that is used as the \u201cproduct version\u201d used in public. For instance Solaris 10 uses as a definition *-solaris2.10 and Apple's Mac OS X 10.5 uses *-darwin9 .","title":"2.1. The System Definition Tuples"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#22-when-to-use-system-definitions","text":"To be extended","title":"2.2. When To Use System Definitions"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#3-adding-options","text":"","title":"3. Adding Options"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#4-finding-libraries","text":"","title":"4. Finding Libraries"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#5-custom-autoconf-tests","text":"","title":"5. Custom Autoconf Tests"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#6-autoconf-building-blocks-macros","text":"","title":"6. Autoconf Building Blocks: Macros"},{"location":"Programming/01-02-Lib/organization-gnu/Autotools-Mythbuster/#7-caching-results","text":"","title":"7. Caching Results"},{"location":"Programming/01-02-Lib/organiztion-posix/","text":"the open group # \u7ad9\u70b9\u4e8c # http://pubs.opengroup.org/onlinepubs/9699919799/","title":"Introduction"},{"location":"Programming/01-02-Lib/organiztion-posix/#the-open-group","text":"","title":"the open group"},{"location":"Programming/01-02-Lib/organiztion-posix/#_1","text":"http://pubs.opengroup.org/onlinepubs/9699919799/","title":"\u7ad9\u70b9\u4e8c"},{"location":"Programming/01-03-Common/Capabilities/","text":"CAPABILITIES(7) # Linux capabilities 101 # Linux Capabilities: Why They Exist and How They Work # Taking Advantage of Linux Capabilities #","title":"Capabilities"},{"location":"Programming/01-03-Common/Capabilities/#capabilities7","text":"","title":"CAPABILITIES(7)"},{"location":"Programming/01-03-Common/Capabilities/#linux-capabilities-101","text":"","title":"Linux capabilities 101"},{"location":"Programming/01-03-Common/Capabilities/#linux-capabilities-why-they-exist-and-how-they-work","text":"","title":"Linux Capabilities: Why They Exist and How They Work"},{"location":"Programming/01-03-Common/Capabilities/#taking-advantage-of-linux-capabilities","text":"","title":"Taking Advantage of Linux Capabilities"},{"location":"Programming/01-03-Common/Path-resolution/","text":"PATH_RESOLUTION(7) #","title":"Path-resolution"},{"location":"Programming/01-03-Common/Path-resolution/#path_resolution7","text":"","title":"PATH_RESOLUTION(7)"},{"location":"Programming/01-03-Common/Unix-programming-time-and-space/","text":"\u65f6\u95f4\u4e0e\u7a7a\u95f4 # \u65f6\u95f4\u4e0e\u7a7a\u95f4\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u662f\u7ecf\u5e38\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\uff0c\u5728\u8fdb\u884c\u7cfb\u7edfprogram\u7684\u65f6\u5019\uff0c\u9700\u8981\u4ecetime\u548cspace\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u8003\u8651\u3002 space\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86race condition\u3002 time\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86\u51fd\u6570\u6267\u884c\u7684\u65f6\u95f4\uff0c\u7cfb\u7edf\u8c03\u7528\u963b\u585e\u7684\u65f6\u5019\u3002\u5e76\u4e14\u5f88\u591a\u7684\u7cfb\u7edf\u8c03\u7528\u90fd\u6b63\u5e38\u8bbe\u7f6emax blocked time\u3002 \u6bd4\u5982\u5728multiple thread\u73af\u5883\u4e2d\u8fdb\u884cprogram\u7684\u65f6\u5019\uff0c\u4ece\u7a7a\u95f4\u7684\u89d2\u5ea6\u8fdb\u884c\u8003\u8651\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u9632\u6b62thread\u4e4b\u95f4\u7684race condition\u3002\u5c31\u6d89\u53ca\u5230\u539f\u5b50\u64cd\u4f5c\u3002 \u539f\u5b50\u64cd\u4f5c\u7684\u8bdd\uff0c\u6709\u4e9bprogramming language\u63d0\u4f9b\u4e86\u539f\u5b50\u64cd\u4f5c\u5e93\uff0cOS\u4e5f\u63d0\u4f9b\u4e86\u539f\u5b50\u51fd\u6570\uff0c\u5982APUE\u7684chapter12.10\u4e2d\u6240\u63d0\u53ca\u7684 pread \uff0c pwrite \u7b49\u3002 time-of-check-to-time-of-use\u5c31\u5c5e\u4e8etime\u7684\u89d2\u5ea6\u4e86","title":"Time-and-space"},{"location":"Programming/01-03-Common/Unix-programming-time-and-space/#_1","text":"\u65f6\u95f4\u4e0e\u7a7a\u95f4\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u662f\u7ecf\u5e38\u9700\u8981\u8003\u8651\u7684\u95ee\u9898\uff0c\u5728\u8fdb\u884c\u7cfb\u7edfprogram\u7684\u65f6\u5019\uff0c\u9700\u8981\u4ecetime\u548cspace\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u8003\u8651\u3002 space\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86race condition\u3002 time\u89d2\u5ea6\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5230\u4e86\u51fd\u6570\u6267\u884c\u7684\u65f6\u95f4\uff0c\u7cfb\u7edf\u8c03\u7528\u963b\u585e\u7684\u65f6\u5019\u3002\u5e76\u4e14\u5f88\u591a\u7684\u7cfb\u7edf\u8c03\u7528\u90fd\u6b63\u5e38\u8bbe\u7f6emax blocked time\u3002 \u6bd4\u5982\u5728multiple thread\u73af\u5883\u4e2d\u8fdb\u884cprogram\u7684\u65f6\u5019\uff0c\u4ece\u7a7a\u95f4\u7684\u89d2\u5ea6\u8fdb\u884c\u8003\u8651\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u9632\u6b62thread\u4e4b\u95f4\u7684race condition\u3002\u5c31\u6d89\u53ca\u5230\u539f\u5b50\u64cd\u4f5c\u3002 \u539f\u5b50\u64cd\u4f5c\u7684\u8bdd\uff0c\u6709\u4e9bprogramming language\u63d0\u4f9b\u4e86\u539f\u5b50\u64cd\u4f5c\u5e93\uff0cOS\u4e5f\u63d0\u4f9b\u4e86\u539f\u5b50\u51fd\u6570\uff0c\u5982APUE\u7684chapter12.10\u4e2d\u6240\u63d0\u53ca\u7684 pread \uff0c pwrite \u7b49\u3002 time-of-check-to-time-of-use\u5c31\u5c5e\u4e8etime\u7684\u89d2\u5ea6\u4e86","title":"\u65f6\u95f4\u4e0e\u7a7a\u95f4"},{"location":"Programming/01-03-Common/Unix-system-call/","text":"\u524d\u8a00 \u4eceblocking\u7684\u89d2\u5ea6\u6765\u5bf9system call\u8fdb\u884c\u5206\u7c7b APUE 10.5 Interrupted System Calls man SIGNAL(7) Difference between slow system calls and fast system calls \u901a\u8fc7Nonblocking I/O\u6765\u8f6c\u6362slow system call \u524d\u8a00 APUE 14.2 Nonblocking I/O \u5e26\u6709max blocking time\u7684system call\u975e\u5e38\u91cd\u8981\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u603b\u7ed3\uff1a \u548c\u963b\u585e\u76f8\u5173\u95ee\u9898 \u975e\u963b\u585eI\u4e0e\u5f02\u6b65IO socket\u662f\u5982\u4f55\u5b9e\u73b0\u7684IO with timeout\uff1f \u524d\u8a00 # \u603b\u7ed3Unix system call\u7684\u76f8\u5173\u77e5\u8bc6 \u4eceblocking\u7684\u89d2\u5ea6\u6765\u5bf9system call\u8fdb\u884c\u5206\u7c7b # \u5728APUE\u768410.5 Interrupted System Calls\u4e2d\u4ecb\u7ecd\u4e86slow system call\uff0c\u8fd9\u63d0\u793a\u6709\u5fc5\u8981\u5bf9linux\u7684system call\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff1b APUE 10.5 Interrupted System Calls # A characteristic of earlier UNIX systems was that if a process caught a signal while the process was blocked in a \u2018\u2018 slow \u2019\u2019 system call, the system call was interrupted. The system call returned an error and errno was set to EINTR . This was done under the assumption that since a signal occurred and the process caught it, there is a good chance that something has happened that should wake up the blocked system call. To support this feature, the system calls are divided into two categories: the \u2018\u2018slow\u2019\u2019 system calls and all the others. The slow system calls are those that can block forever . Included in this category are \u2022 Reads that can block the caller forever if data isn\u2019t present with certain file types (pipes, terminal devices, and network devices) \u2022 Writes that can block the caller forever if the data can\u2019t be accepted immediately by these same file types \u2022 Opens on certain file types that block the caller until some condition occurs (such as a terminal device open waiting until an attached modem answers the phone) \u2022 The pause function (which by definition puts the calling process to sleep until a signal is caught) and the wait function \u2022 Certain ioctl operations \u2022 Some of the interprocess communication functions (Chapter 15) The notable exception to these slow system calls is anything related to disk I/O. Although a read or a write of a disk file can block the caller temporarily (while the disk driver queues the request and then the request is executed), unless a hardware error occurs, the I/O operation always returns and unblocks the caller quickly. SUMMARY : \u663e\u7136\uff0c\u4e0a\u8ff0\u5bf9system call\u7684\u5206\u7c7b\u65b9\u6cd5\u662f\u6839\u636e\u8fd9\u4e2asystem call\u662f\u5426\u53ef\u80fd\u4f1a\u5c06process block forever \u7684\uff0c\u77ed\u6682\u7684block\u662f\u4e0d\u7b97slow\u7684\uff0c\u8fd9\u4e2a\u77ed\u6682\u7684block\u5c31\u662fa read or a write of a disk file\u3002\u5e76\u4e14slow system call\u662f\u548csignal\u5bc6\u5207\u76f8\u5173\u7684\uff1b \u4e0a\u8ff0\u63cf\u8ff0\u7684\u662f\u5728Unix system\u4e2d\u7684\u60c5\u51b5\uff0c\u90a3\u4e48linux\u4e2d\u7684\u60c5\u51b5\u662f\u600e\u6837\u7684\u5462\uff1f\u53c2\u89c1 man SIGNAL(7) \uff0c\u5176\u4e2d\u5bf9linux\u4e2d\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u8bf4\u660e\uff1b man SIGNAL(7) # Difference between slow system calls and fast system calls # \u901a\u8fc7Nonblocking I/O\u6765\u8f6c\u6362slow system call # \u524d\u8a00 # \u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5176\u5b9e\u6240\u8c13\u7684slow system call\u662f\u8ddf\u5b83\u6240\u64cd\u4f5c\u7684device\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u800cUnix OS\u7684everything in Unix is file\u7684philosophy\uff0c\u5c06\u5f88\u591adevice\u90fd\u62bd\u8c61\u6210\u4e86file\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u8fd9\u4e9bfile\u7684file descriptor\u8fdb\u884c\u64cd\u4f5c\u6765\u5b9e\u73b0\u5bf9device\u7684\u64cd\u4f5c\uff0c\u56e0\u6b64\u5f88\u591a\u64cd\u4f5c\u90fd\u662f\u7c7b\u4f3c\u4e8eIO\uff1b\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f53\u6211\u4eec\u5bf9slow device\u6267\u884csystem call\u7684\u65f6\u5019\uff0c\u5c31\u975e\u5e38\u53ef\u80fd\u51fa\u73b0slow system call\u7684\u60c5\u51b5\uff1bUnix OS\u662f\u975e\u5e38\u7075\u6d3b\u7684\uff0c\u5b83\u662f\u6709\u63d0\u4f9bsystem call\u6765\u5141\u8bb8\u7528\u6237\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\u7684\uff1a\u8fd9\u5c31\u662fUnix\u4e2d\u7684nonblocking I/O\uff0c\u901a\u8fc7\u5728\u6307\u5b9a\u7684file descriptor\u4e0a\u8bbe\u7f6enonblocking\u6807\u5fd7\uff0c\u6765\u544a\u8bc9kernel\u4e0d\u8981block\u5bf9\u8be5file descriptor\u8fdb\u884c\u64cd\u4f5c\u7684thread\uff1b APUE 14.2 Nonblocking I/O # \u5728\u8fd9\u4e00\u8282\u5bf9\u8fd9\u4e2a\u4e3b\u9898\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u4ecb\u7ecd\uff1b \u5e26\u6709max blocking time\u7684system call\u975e\u5e38\u91cd\u8981\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u603b\u7ed3\uff1a # pthread_mutex_timedlock pthread_rwlock_timedrdlock pthread_cond_timedwait select poll epoll_wait \u548c\u963b\u585e\u76f8\u5173\u95ee\u9898 # \u5982\u4f55\u53d6\u6d88\u963b\u585e\u7684\u7cfb\u7edf\u8c03\u7528\uff08\u53ef\u4ee5\u4f7f\u7528\u4fe1\u53f7\uff09 sleep \u662f\u5426\u662f\u963b\u585e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u5b83\u548c\u963b\u585e\u6709\u4ec0\u4e48\u5f02\u540c\uff1f \u975e\u963b\u585eI\u4e0e\u5f02\u6b65IO # \u975e\u963b\u585eIO\u53c2\u89c1APUE 14.2 \u901a\u8fc7\u8bbe\u7f6e\u6807\u5fd7\u6765\u5b9e\u73b0\u975e\u963b\u585e \u5f02\u6b65IO\u53c2\u89c114.5 socket\u662f\u5982\u4f55\u5b9e\u73b0\u7684IO with timeout\uff1f #","title":"System-call"},{"location":"Programming/01-03-Common/Unix-system-call/#_1","text":"\u603b\u7ed3Unix system call\u7684\u76f8\u5173\u77e5\u8bc6","title":"\u524d\u8a00"},{"location":"Programming/01-03-Common/Unix-system-call/#blockingsystem-call","text":"\u5728APUE\u768410.5 Interrupted System Calls\u4e2d\u4ecb\u7ecd\u4e86slow system call\uff0c\u8fd9\u63d0\u793a\u6709\u5fc5\u8981\u5bf9linux\u7684system call\u8fdb\u884c\u4e00\u4e0b\u603b\u7ed3\uff1b","title":"\u4eceblocking\u7684\u89d2\u5ea6\u6765\u5bf9system call\u8fdb\u884c\u5206\u7c7b"},{"location":"Programming/01-03-Common/Unix-system-call/#apue-105-interrupted-system-calls","text":"A characteristic of earlier UNIX systems was that if a process caught a signal while the process was blocked in a \u2018\u2018 slow \u2019\u2019 system call, the system call was interrupted. The system call returned an error and errno was set to EINTR . This was done under the assumption that since a signal occurred and the process caught it, there is a good chance that something has happened that should wake up the blocked system call. To support this feature, the system calls are divided into two categories: the \u2018\u2018slow\u2019\u2019 system calls and all the others. The slow system calls are those that can block forever . Included in this category are \u2022 Reads that can block the caller forever if data isn\u2019t present with certain file types (pipes, terminal devices, and network devices) \u2022 Writes that can block the caller forever if the data can\u2019t be accepted immediately by these same file types \u2022 Opens on certain file types that block the caller until some condition occurs (such as a terminal device open waiting until an attached modem answers the phone) \u2022 The pause function (which by definition puts the calling process to sleep until a signal is caught) and the wait function \u2022 Certain ioctl operations \u2022 Some of the interprocess communication functions (Chapter 15) The notable exception to these slow system calls is anything related to disk I/O. Although a read or a write of a disk file can block the caller temporarily (while the disk driver queues the request and then the request is executed), unless a hardware error occurs, the I/O operation always returns and unblocks the caller quickly. SUMMARY : \u663e\u7136\uff0c\u4e0a\u8ff0\u5bf9system call\u7684\u5206\u7c7b\u65b9\u6cd5\u662f\u6839\u636e\u8fd9\u4e2asystem call\u662f\u5426\u53ef\u80fd\u4f1a\u5c06process block forever \u7684\uff0c\u77ed\u6682\u7684block\u662f\u4e0d\u7b97slow\u7684\uff0c\u8fd9\u4e2a\u77ed\u6682\u7684block\u5c31\u662fa read or a write of a disk file\u3002\u5e76\u4e14slow system call\u662f\u548csignal\u5bc6\u5207\u76f8\u5173\u7684\uff1b \u4e0a\u8ff0\u63cf\u8ff0\u7684\u662f\u5728Unix system\u4e2d\u7684\u60c5\u51b5\uff0c\u90a3\u4e48linux\u4e2d\u7684\u60c5\u51b5\u662f\u600e\u6837\u7684\u5462\uff1f\u53c2\u89c1 man SIGNAL(7) \uff0c\u5176\u4e2d\u5bf9linux\u4e2d\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u8bf4\u660e\uff1b","title":"APUE 10.5 Interrupted System Calls"},{"location":"Programming/01-03-Common/Unix-system-call/#man-signal7","text":"","title":"man SIGNAL(7)"},{"location":"Programming/01-03-Common/Unix-system-call/#difference-between-slow-system-calls-and-fast-system-calls","text":"","title":"Difference between slow system calls and fast system calls"},{"location":"Programming/01-03-Common/Unix-system-call/#nonblocking-ioslow-system-call","text":"","title":"\u901a\u8fc7Nonblocking I/O\u6765\u8f6c\u6362slow system call"},{"location":"Programming/01-03-Common/Unix-system-call/#_2","text":"\u4ece\u4e0a\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u5176\u5b9e\u6240\u8c13\u7684slow system call\u662f\u8ddf\u5b83\u6240\u64cd\u4f5c\u7684device\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u800cUnix OS\u7684everything in Unix is file\u7684philosophy\uff0c\u5c06\u5f88\u591adevice\u90fd\u62bd\u8c61\u6210\u4e86file\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u8fd9\u4e9bfile\u7684file descriptor\u8fdb\u884c\u64cd\u4f5c\u6765\u5b9e\u73b0\u5bf9device\u7684\u64cd\u4f5c\uff0c\u56e0\u6b64\u5f88\u591a\u64cd\u4f5c\u90fd\u662f\u7c7b\u4f3c\u4e8eIO\uff1b\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f53\u6211\u4eec\u5bf9slow device\u6267\u884csystem call\u7684\u65f6\u5019\uff0c\u5c31\u975e\u5e38\u53ef\u80fd\u51fa\u73b0slow system call\u7684\u60c5\u51b5\uff1bUnix OS\u662f\u975e\u5e38\u7075\u6d3b\u7684\uff0c\u5b83\u662f\u6709\u63d0\u4f9bsystem call\u6765\u5141\u8bb8\u7528\u6237\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\u7684\uff1a\u8fd9\u5c31\u662fUnix\u4e2d\u7684nonblocking I/O\uff0c\u901a\u8fc7\u5728\u6307\u5b9a\u7684file descriptor\u4e0a\u8bbe\u7f6enonblocking\u6807\u5fd7\uff0c\u6765\u544a\u8bc9kernel\u4e0d\u8981block\u5bf9\u8be5file descriptor\u8fdb\u884c\u64cd\u4f5c\u7684thread\uff1b","title":"\u524d\u8a00"},{"location":"Programming/01-03-Common/Unix-system-call/#apue-142-nonblocking-io","text":"\u5728\u8fd9\u4e00\u8282\u5bf9\u8fd9\u4e2a\u4e3b\u9898\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u4ecb\u7ecd\uff1b","title":"APUE 14.2 Nonblocking I/O"},{"location":"Programming/01-03-Common/Unix-system-call/#max-blocking-timesystem-call","text":"pthread_mutex_timedlock pthread_rwlock_timedrdlock pthread_cond_timedwait select poll epoll_wait","title":"\u5e26\u6709max blocking time\u7684system call\u975e\u5e38\u91cd\u8981\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u603b\u7ed3\uff1a"},{"location":"Programming/01-03-Common/Unix-system-call/#_3","text":"\u5982\u4f55\u53d6\u6d88\u963b\u585e\u7684\u7cfb\u7edf\u8c03\u7528\uff08\u53ef\u4ee5\u4f7f\u7528\u4fe1\u53f7\uff09 sleep \u662f\u5426\u662f\u963b\u585e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u5b83\u548c\u963b\u585e\u6709\u4ec0\u4e48\u5f02\u540c\uff1f","title":"\u548c\u963b\u585e\u76f8\u5173\u95ee\u9898"},{"location":"Programming/01-03-Common/Unix-system-call/#iio","text":"\u975e\u963b\u585eIO\u53c2\u89c1APUE 14.2 \u901a\u8fc7\u8bbe\u7f6e\u6807\u5fd7\u6765\u5b9e\u73b0\u975e\u963b\u585e \u5f02\u6b65IO\u53c2\u89c114.5","title":"\u975e\u963b\u585eI\u4e0e\u5f02\u6b65IO"},{"location":"Programming/01-03-Common/Unix-system-call/#socketio-with-timeout","text":"","title":"socket\u662f\u5982\u4f55\u5b9e\u73b0\u7684IO with timeout\uff1f"},{"location":"Programming/01-03-Common/safety/","text":"SIGNAL-SAFETY(7) # Reentrancy (computing) Async-cancel-safe functions # Thread-safe functions # Thread safety race condition #","title":"Safety"},{"location":"Programming/01-03-Common/safety/#signal-safety7","text":"Reentrancy (computing)","title":"SIGNAL-SAFETY(7)"},{"location":"Programming/01-03-Common/safety/#async-cancel-safe-functions","text":"","title":"Async-cancel-safe functions"},{"location":"Programming/01-03-Common/safety/#thread-safe-functions","text":"Thread safety","title":"Thread-safe functions"},{"location":"Programming/01-03-Common/safety/#race-condition","text":"","title":"race condition"},{"location":"Programming/01-philosophy/","text":"\u524d\u8a00 # \u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86 linux kernel \u6240\u91c7\u7528\u7684\u67b6\u6784\u662f monolithic kernel \uff0c\u4e5f\u5c31\u662flinux kernel\u638c\u7ba1\u7740\u8fd9\u4e2a\u7cfb\u7edf\uff0capplication\u63d0\u4f9b\u8c03\u7528system call\u6765\u83b7\u5f97\u7cfb\u7edf\u670d\u52a1\u3002\u6309\u7167linux\u7684\u7ba1\u7406\uff0csystem call\u6240\u8fd4\u56de\u7684\u53eb\u505adescriptor\uff0c\u5728Windows\u7cfb\u7edf\u4e2d\u4e00\u822c\u53eb\u505ahandler\u3002 20200128 # \u4f7f\u7528\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300b1.6. An Overview of Unix Kernels\u4e2d\u7684\u5185\u5bb9\u4f5c\u4e3a\u5165\u573a\u662f\u975e\u5e38\u597d\u7684\u3002 \u5173\u4e8eeverything is a file\uff0c\u9700\u8981\u6dfb\u52a0\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300bchapter 1.6.9. Device Drivers\u7684\u5185\u5bb9\u3002","title":"Introduction"},{"location":"Programming/01-philosophy/#_1","text":"\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86 linux kernel \u6240\u91c7\u7528\u7684\u67b6\u6784\u662f monolithic kernel \uff0c\u4e5f\u5c31\u662flinux kernel\u638c\u7ba1\u7740\u8fd9\u4e2a\u7cfb\u7edf\uff0capplication\u63d0\u4f9b\u8c03\u7528system call\u6765\u83b7\u5f97\u7cfb\u7edf\u670d\u52a1\u3002\u6309\u7167linux\u7684\u7ba1\u7406\uff0csystem call\u6240\u8fd4\u56de\u7684\u53eb\u505adescriptor\uff0c\u5728Windows\u7cfb\u7edf\u4e2d\u4e00\u822c\u53eb\u505ahandler\u3002","title":"\u524d\u8a00"},{"location":"Programming/01-philosophy/#20200128","text":"\u4f7f\u7528\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300b1.6. An Overview of Unix Kernels\u4e2d\u7684\u5185\u5bb9\u4f5c\u4e3a\u5165\u573a\u662f\u975e\u5e38\u597d\u7684\u3002 \u5173\u4e8eeverything is a file\uff0c\u9700\u8981\u6dfb\u52a0\u300aUnderstanding.The.Linux.kernel.3rd.Edition\u300bchapter 1.6.9. Device Drivers\u7684\u5185\u5bb9\u3002","title":"20200128"},{"location":"Programming/01-philosophy/File-descriptor/","text":"File descriptor Overview Operations on file descriptors Creating file descriptors Deriving file descriptors Operations on a single file descriptor Operations on multiple file descriptors Operations on the file descriptor table Operations that modify process state File locking Sockets Miscellaneous Upcoming operations File descriptors as capabilities File descriptor # In Unix and related computer operating systems, a file descriptor ( FD , less frequently fildes ) is an abstract indicator ( handle ) used to access a file or other input/output resource , such as a pipe or network socket . File descriptors form part of the POSIX application programming interface . A file descriptor is a non-negative integer , generally represented in the C programming language as the type int (negative values being reserved to indicate \"no value\" or an error condition). Each Unix process (except perhaps a daemon ) should expect to have three standard POSIX file descriptors, corresponding to the three standard streams : Integer value Name unistd.h symbolic constant[ 1] stdio.h file stream[ 2] 0 Standard input STDIN_FILENO stdin 1 Standard output STDOUT_FILENO stdout 2 Standard error STDERR_FILENO stderr Overview # In the traditional implementation of Unix, file descriptors index into a per-process file descriptor table maintained by the kernel, that in turn indexes into a system-wide table of files opened by all processes, called the file table . This table records the mode with which the file (or other resource) has been opened: for reading, writing, appending, and possibly other modes. It also indexes into a third table called the inode table that describes the actual underlying files.[ 3] To perform input or output, the process passes the file descriptor to the kernel through a system call , and the kernel will access the file on behalf of the process. The process does not have direct access to the file or inode tables . SUMMARY : \u5728APUE\u76843.10 File Sharing\u4e5f\u63cf\u8ff0\u4e86\u8fd9\u90e8\u5206\u5185\u5bb9\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1athe data structures used by the kernel for all I/O.\u5373\u6240\u6709\u7684IO\u90fd\u662f\u91c7\u7528\u7684\u7c7b\u4f3c\u4e8e\u4e0a\u8ff0\u7684\u7ed3\u6784\uff1b\u5e76\u4e14\u4e0a\u8ff0\u7ed3\u6784\u9700\u8981\u548c Process control block \u4e00\u8d77\u6765\u7406\u89e3\u624d\u80fd\u591f\u5f88\u597d\u7684\u5bf9Unix OS\u7684IO\u6709\u4e00\u4e2a\u6574\u4f53\u7684\u8ba4\u77e5\uff1b SUMMARY : \u9700\u8981\u6ce8\u610f\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63d0\u53ca file descriptor table \u548c file table \u65f6\uff0c\u524d\u9762\u5206\u522b\u52a0\u4e0a\u4e86\u4fee\u9970\u8bed\uff1a per-process \u548c system-wide \uff1b\u8fd9\u4e24\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u9700\u8981\u5c06\u5b83\u4eec\u548cthe data structures used by the kernel for all I/O\u4e00\u8d77\u6765\u8fdb\u884c\u7406\u89e3\uff1b\u56e0\u4e3a file descriptor table \u7684scope\u662fprocess\uff0c\u5373\u6bcf\u4e2aprocess\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u7684 file descriptor table \uff0c\u6240\u4ee5\u6bcf\u4e2aprocess\u7684file descriptor\u90fd\u662f\u4ece0\u5f00\u59cb\u589e\u957f\uff1b\u663e\u7136\u6bd4\u8f83\u4e24\u4e2aprocess\u7684file descriptor\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff08\u5904\u74060,1,2\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5df2\u7ecf\u88ab\u9ed8\u8ba4\u7ed1\u5b9a\u5230 STDIN_FILENO , STDOUT_FILENO , STDERR_FILENO \uff09\uff1b\u800cfile table\u7684scope\u662fsystem\uff0c\u5373\u6240\u6709\u7684process\u90fd\u5c06\u5171\u4eabfile table\uff1b SUMMARY : \u6bcf\u6b21\u8c03\u7528 open \u7cfb\u7edf\u8c03\u7528\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2afile table entry On Linux , the set of file descriptors open in a process can be accessed under the path /proc/PID/fd/ , where PID is the process identifier . In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b The FILE data structure in the C standard I/O library usually includes a low level file descriptor for the object in question on Unix-like systems. The overall data structure provides additional abstraction and is instead known as a file handle. File descriptors for a single process, file table and inode table. Note that multiple file descriptors can refer to the same file table entry (e.g., as a result of the dup system call[ 3] :104 and that multiple file table entries can in turn refer to the same inode (if it has been opened multiple times; the table is still simplified because it represents inodes by file names, even though an inode can have multiple names ). File descriptor 3 does not refer to anything in the file table, signifying that it has been closed. SUMMARY : \u4e0a\u8ff0\u7684\u4e09\u5c42\u5bf9\u5e94\u5173\u7cfb\u5b58\u5728\u7740\u591a\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\uff0c\u518d\u52a0\u4e0aOS\u63d0\u4f9b\u7684fork\u673a\u5236\uff08\u5b50\u8fdb\u7a0b\u7ee7\u627f\u7236\u8fdb\u7a0b\u7684file descriptor\u548cfile tableentry\uff09\uff0c\u5404\u79cdIO\u64cd\u4f5c\uff08\u6bd4\u5982dup\uff0cread\uff0cwrite\uff09\u7b49\u7b49\u90fd\u5bfc\u81f4\u4e86\u95ee\u9898\u7684\u590d\u6742\u6027\uff1b \u6bd4\u5982\u5b58\u5728\u7740\u8fd9\u4e9b\u53ef\u80fd\u7684\u60c5\u51b5\uff1a dup \uff0c\u540c\u4e00\u8fdb\u7a0b\u4e2d\uff0c\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry fork \u540e\uff0c\u7236\u8fdb\u7a0b\uff0c\u5b50\u8fdb\u7a0b\u7684\u540c\u4e00\u4e2afile descriptor\u5171\u4eab\u540c\u4e00\u4e2afile table entry\uff08\u56e0\u4e3afile descriptor table\u662f\u6bcf\u4e2a\u8fdb\u7a0b\u79c1\u6709\u7684\uff0c\u6240\u4ee5\u8fd9\u79cd\u60c5\u51b5\u5176\u5b9e\u7c7b\u4f3c\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5\uff0c\u5373\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry\uff09 \u4e0a\u9762\u63cf\u8ff0\u4e86file descriptor\u548cfile table entry\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e0b\u9762\u63cf\u8ff0file table entry\u548ciNode\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a \u662f\u6709\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u4e0d\u540c\u7684file table entry\u6307\u5411\u4e86\u540c\u4e00\u4e2aiNode\u7684\uff1b \u663e\u7136OS\u7684\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u5c31\u5bfc\u81f4\u5f53\u4e00\u4e2a\u6587\u4ef6\u88ab\u591a\u4e2a\u4e0d\u540c\u7684process\u8fdb\u884cshare\u7684\u65f6\u5019\uff0c\u800c\u6bcf\u4e2aprocess\u90fd\u53ef\u4ee5\u6267\u884c\u4e00\u7cfb\u5217\u7684IO\u64cd\u4f5c\uff0c\u8fd9\u5c31\u5bfc\u81f4\u4e86\u53ef\u80fd\u5b58\u5728\u7684\u6570\u636e\u51b2\u7a81\u95ee\u9898\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u6309\u7167OS\u7684\u8fd9\u603b\u7ed3\u6784\u8bbe\u8ba1\uff0c\u4ee5\u53caOS\u63d0\u4f9b\u7684\u5404\u79cd\u64cd\u4f5c\uff0c\u662f\u53ef\u4ee5\u603b\u7ed3\u51fa\u53ef\u80fd\u7684\u6240\u6709\u60c5\u5f62\u7684\uff1b Operations on file descriptors # The following lists typical operations on file descriptors on modern Unix-like systems. Most of these functions are declared in the <unistd.h> header, but some are in the <fcntl.h> header instead. Creating file descriptors # open () creat() [ 4] socket() accept() socketpair () pipe() opendir() open_by_handle_at() (Linux) signalfd() (Linux) eventfd() (Linux) timerfd_create() (Linux) memfd_create() (Linux) userfaultfd() (Linux) Deriving file descriptors # dirfd() fileno() Operations on a single file descriptor # read (), write () readv(), writev() pread(), pwrite() recv(), send() recvmsg(), sendmsg() (including allowing sending FDs) sendfile() lseek() fstat() fchmod() fchown() fdopen() ftruncate() fsync() fdatasync() fstatvfs() dprintf() vmsplice() (Linux) Operations on multiple file descriptors # select() , pselect() poll() epoll() (for Linux) kqueue() (for BSD-based systems). sendfile() splice() , tee() (for Linux) Operations on the file descriptor table # The fcntl() function is used to perform various operations on a file descriptor, depending on the command argument passed to it. There are commands to get and set attributes associated with a file descriptor, including F_GETFD , F_SETFD , F_GETFL and F_SETFL . close() closefrom() (BSD and Solaris only; deletes all file descriptors greater than or equal to specified number) dup() (duplicates an existing file descriptor guaranteeing to be the lowest number available file descriptor) dup2() (the new file descriptor will have the value passed as an argument) fcntl ( F_DUPFD ) Operations that modify process state # fchdir() (sets the process's current working directory based on a directory file descriptor) mmap() (maps ranges of a file into the process's address space) File locking # flock() fcntl() ( F_GETLK , F_SETLK ) and F_SETLKW lockf() Sockets # connect() bind() listen() accept() (creates a new file descriptor for an incoming connection) getsockname() getpeername() getsockopt() setsockopt() shutdown() (shuts down one or both halves of a full duplex connection) Miscellaneous # ioctl() (a large collection of miscellaneous operations on a single file descriptor, often associated with a device) Upcoming operations # A series of new operations on file descriptors has been added to many modern Unix-like systems, as well as numerous C libraries, to be standardized in a future version of POSIX .[ 5] The at suffix signifies that the function takes an additional first argument supplying a file descriptor from which relative paths are resolved, the forms lacking the at suffix thus becoming equivalent to passing a file descriptor corresponding to the current working directory . The purpose of these new operations is to defend against a certain class of TOCTTOU attacks. openat() faccessat() fchmodat() fchownat() fstatat() futimesat() linkat() mkdirat() mknodat() readlinkat() renameat() symlinkat() unlinkat() mkfifoat() fdopendir() File descriptors as capabilities # Unix file descriptors behave in many ways as capabilities . They can be passed between processes across Unix domain sockets using the sendmsg() system call. Note, however, that what is actually passed is a reference to an \"open file description\" that has mutable state (the file offset, and the file status and access flags). This complicates the secure use of file descriptors as capabilities, since when programs share access to the same open file description, they can interfere with each other's use of it by changing its offset or whether it is blocking or non-blocking, for example.[ 6] [ 7] In operating systems that are specifically designed as capability systems, there is very rarely any mutable state associated with a capability itself. A Unix process' file descriptor table is an example of a C-list .","title":"File-descriptor"},{"location":"Programming/01-philosophy/File-descriptor/#file-descriptor","text":"In Unix and related computer operating systems, a file descriptor ( FD , less frequently fildes ) is an abstract indicator ( handle ) used to access a file or other input/output resource , such as a pipe or network socket . File descriptors form part of the POSIX application programming interface . A file descriptor is a non-negative integer , generally represented in the C programming language as the type int (negative values being reserved to indicate \"no value\" or an error condition). Each Unix process (except perhaps a daemon ) should expect to have three standard POSIX file descriptors, corresponding to the three standard streams : Integer value Name unistd.h symbolic constant[ 1] stdio.h file stream[ 2] 0 Standard input STDIN_FILENO stdin 1 Standard output STDOUT_FILENO stdout 2 Standard error STDERR_FILENO stderr","title":"File descriptor"},{"location":"Programming/01-philosophy/File-descriptor/#overview","text":"In the traditional implementation of Unix, file descriptors index into a per-process file descriptor table maintained by the kernel, that in turn indexes into a system-wide table of files opened by all processes, called the file table . This table records the mode with which the file (or other resource) has been opened: for reading, writing, appending, and possibly other modes. It also indexes into a third table called the inode table that describes the actual underlying files.[ 3] To perform input or output, the process passes the file descriptor to the kernel through a system call , and the kernel will access the file on behalf of the process. The process does not have direct access to the file or inode tables . SUMMARY : \u5728APUE\u76843.10 File Sharing\u4e5f\u63cf\u8ff0\u4e86\u8fd9\u90e8\u5206\u5185\u5bb9\uff1b\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1athe data structures used by the kernel for all I/O.\u5373\u6240\u6709\u7684IO\u90fd\u662f\u91c7\u7528\u7684\u7c7b\u4f3c\u4e8e\u4e0a\u8ff0\u7684\u7ed3\u6784\uff1b\u5e76\u4e14\u4e0a\u8ff0\u7ed3\u6784\u9700\u8981\u548c Process control block \u4e00\u8d77\u6765\u7406\u89e3\u624d\u80fd\u591f\u5f88\u597d\u7684\u5bf9Unix OS\u7684IO\u6709\u4e00\u4e2a\u6574\u4f53\u7684\u8ba4\u77e5\uff1b SUMMARY : \u9700\u8981\u6ce8\u610f\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63d0\u53ca file descriptor table \u548c file table \u65f6\uff0c\u524d\u9762\u5206\u522b\u52a0\u4e0a\u4e86\u4fee\u9970\u8bed\uff1a per-process \u548c system-wide \uff1b\u8fd9\u4e24\u4e2a\u4fee\u9970\u8bed\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u9700\u8981\u5c06\u5b83\u4eec\u548cthe data structures used by the kernel for all I/O\u4e00\u8d77\u6765\u8fdb\u884c\u7406\u89e3\uff1b\u56e0\u4e3a file descriptor table \u7684scope\u662fprocess\uff0c\u5373\u6bcf\u4e2aprocess\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u7684 file descriptor table \uff0c\u6240\u4ee5\u6bcf\u4e2aprocess\u7684file descriptor\u90fd\u662f\u4ece0\u5f00\u59cb\u589e\u957f\uff1b\u663e\u7136\u6bd4\u8f83\u4e24\u4e2aprocess\u7684file descriptor\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff08\u5904\u74060,1,2\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5df2\u7ecf\u88ab\u9ed8\u8ba4\u7ed1\u5b9a\u5230 STDIN_FILENO , STDOUT_FILENO , STDERR_FILENO \uff09\uff1b\u800cfile table\u7684scope\u662fsystem\uff0c\u5373\u6240\u6709\u7684process\u90fd\u5c06\u5171\u4eabfile table\uff1b SUMMARY : \u6bcf\u6b21\u8c03\u7528 open \u7cfb\u7edf\u8c03\u7528\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2afile table entry On Linux , the set of file descriptors open in a process can be accessed under the path /proc/PID/fd/ , where PID is the process identifier . In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b The FILE data structure in the C standard I/O library usually includes a low level file descriptor for the object in question on Unix-like systems. The overall data structure provides additional abstraction and is instead known as a file handle. File descriptors for a single process, file table and inode table. Note that multiple file descriptors can refer to the same file table entry (e.g., as a result of the dup system call[ 3] :104 and that multiple file table entries can in turn refer to the same inode (if it has been opened multiple times; the table is still simplified because it represents inodes by file names, even though an inode can have multiple names ). File descriptor 3 does not refer to anything in the file table, signifying that it has been closed. SUMMARY : \u4e0a\u8ff0\u7684\u4e09\u5c42\u5bf9\u5e94\u5173\u7cfb\u5b58\u5728\u7740\u591a\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\uff0c\u518d\u52a0\u4e0aOS\u63d0\u4f9b\u7684fork\u673a\u5236\uff08\u5b50\u8fdb\u7a0b\u7ee7\u627f\u7236\u8fdb\u7a0b\u7684file descriptor\u548cfile tableentry\uff09\uff0c\u5404\u79cdIO\u64cd\u4f5c\uff08\u6bd4\u5982dup\uff0cread\uff0cwrite\uff09\u7b49\u7b49\u90fd\u5bfc\u81f4\u4e86\u95ee\u9898\u7684\u590d\u6742\u6027\uff1b \u6bd4\u5982\u5b58\u5728\u7740\u8fd9\u4e9b\u53ef\u80fd\u7684\u60c5\u51b5\uff1a dup \uff0c\u540c\u4e00\u8fdb\u7a0b\u4e2d\uff0c\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry fork \u540e\uff0c\u7236\u8fdb\u7a0b\uff0c\u5b50\u8fdb\u7a0b\u7684\u540c\u4e00\u4e2afile descriptor\u5171\u4eab\u540c\u4e00\u4e2afile table entry\uff08\u56e0\u4e3afile descriptor table\u662f\u6bcf\u4e2a\u8fdb\u7a0b\u79c1\u6709\u7684\uff0c\u6240\u4ee5\u8fd9\u79cd\u60c5\u51b5\u5176\u5b9e\u7c7b\u4f3c\u4e8e\u7b2c\u4e00\u79cd\u60c5\u51b5\uff0c\u5373\u591a\u4e2afile descriptor\u6307\u5411\u4e86\u540c\u4e00\u4e2afile table entry\uff09 \u4e0a\u9762\u63cf\u8ff0\u4e86file descriptor\u548cfile table entry\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e0b\u9762\u63cf\u8ff0file table entry\u548ciNode\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a \u662f\u6709\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u4e0d\u540c\u7684file table entry\u6307\u5411\u4e86\u540c\u4e00\u4e2aiNode\u7684\uff1b \u663e\u7136OS\u7684\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u5c31\u5bfc\u81f4\u5f53\u4e00\u4e2a\u6587\u4ef6\u88ab\u591a\u4e2a\u4e0d\u540c\u7684process\u8fdb\u884cshare\u7684\u65f6\u5019\uff0c\u800c\u6bcf\u4e2aprocess\u90fd\u53ef\u4ee5\u6267\u884c\u4e00\u7cfb\u5217\u7684IO\u64cd\u4f5c\uff0c\u8fd9\u5c31\u5bfc\u81f4\u4e86\u53ef\u80fd\u5b58\u5728\u7684\u6570\u636e\u51b2\u7a81\u95ee\u9898\uff1b \u603b\u7684\u6765\u8bf4\uff0c\u6309\u7167OS\u7684\u8fd9\u603b\u7ed3\u6784\u8bbe\u8ba1\uff0c\u4ee5\u53caOS\u63d0\u4f9b\u7684\u5404\u79cd\u64cd\u4f5c\uff0c\u662f\u53ef\u4ee5\u603b\u7ed3\u51fa\u53ef\u80fd\u7684\u6240\u6709\u60c5\u5f62\u7684\uff1b","title":"Overview"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-file-descriptors","text":"The following lists typical operations on file descriptors on modern Unix-like systems. Most of these functions are declared in the <unistd.h> header, but some are in the <fcntl.h> header instead.","title":"Operations on file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#creating-file-descriptors","text":"open () creat() [ 4] socket() accept() socketpair () pipe() opendir() open_by_handle_at() (Linux) signalfd() (Linux) eventfd() (Linux) timerfd_create() (Linux) memfd_create() (Linux) userfaultfd() (Linux)","title":"Creating file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#deriving-file-descriptors","text":"dirfd() fileno()","title":"Deriving file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-a-single-file-descriptor","text":"read (), write () readv(), writev() pread(), pwrite() recv(), send() recvmsg(), sendmsg() (including allowing sending FDs) sendfile() lseek() fstat() fchmod() fchown() fdopen() ftruncate() fsync() fdatasync() fstatvfs() dprintf() vmsplice() (Linux)","title":"Operations on a single file descriptor"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-multiple-file-descriptors","text":"select() , pselect() poll() epoll() (for Linux) kqueue() (for BSD-based systems). sendfile() splice() , tee() (for Linux)","title":"Operations on multiple file descriptors"},{"location":"Programming/01-philosophy/File-descriptor/#operations-on-the-file-descriptor-table","text":"The fcntl() function is used to perform various operations on a file descriptor, depending on the command argument passed to it. There are commands to get and set attributes associated with a file descriptor, including F_GETFD , F_SETFD , F_GETFL and F_SETFL . close() closefrom() (BSD and Solaris only; deletes all file descriptors greater than or equal to specified number) dup() (duplicates an existing file descriptor guaranteeing to be the lowest number available file descriptor) dup2() (the new file descriptor will have the value passed as an argument) fcntl ( F_DUPFD )","title":"Operations on the file descriptor table"},{"location":"Programming/01-philosophy/File-descriptor/#operations-that-modify-process-state","text":"fchdir() (sets the process's current working directory based on a directory file descriptor) mmap() (maps ranges of a file into the process's address space)","title":"Operations that modify process state"},{"location":"Programming/01-philosophy/File-descriptor/#file-locking","text":"flock() fcntl() ( F_GETLK , F_SETLK ) and F_SETLKW lockf()","title":"File locking"},{"location":"Programming/01-philosophy/File-descriptor/#sockets","text":"connect() bind() listen() accept() (creates a new file descriptor for an incoming connection) getsockname() getpeername() getsockopt() setsockopt() shutdown() (shuts down one or both halves of a full duplex connection)","title":"Sockets"},{"location":"Programming/01-philosophy/File-descriptor/#miscellaneous","text":"ioctl() (a large collection of miscellaneous operations on a single file descriptor, often associated with a device)","title":"Miscellaneous"},{"location":"Programming/01-philosophy/File-descriptor/#upcoming-operations","text":"A series of new operations on file descriptors has been added to many modern Unix-like systems, as well as numerous C libraries, to be standardized in a future version of POSIX .[ 5] The at suffix signifies that the function takes an additional first argument supplying a file descriptor from which relative paths are resolved, the forms lacking the at suffix thus becoming equivalent to passing a file descriptor corresponding to the current working directory . The purpose of these new operations is to defend against a certain class of TOCTTOU attacks. openat() faccessat() fchmodat() fchownat() fstatat() futimesat() linkat() mkdirat() mknodat() readlinkat() renameat() symlinkat() unlinkat() mkfifoat() fdopendir()","title":"Upcoming operations"},{"location":"Programming/01-philosophy/File-descriptor/#file-descriptors-as-capabilities","text":"Unix file descriptors behave in many ways as capabilities . They can be passed between processes across Unix domain sockets using the sendmsg() system call. Note, however, that what is actually passed is a reference to an \"open file description\" that has mutable state (the file offset, and the file status and access flags). This complicates the secure use of file descriptors as capabilities, since when programs share access to the same open file description, they can interfere with each other's use of it by changing its offset or whether it is blocking or non-blocking, for example.[ 6] [ 7] In operating systems that are specifically designed as capability systems, there is very rarely any mutable state associated with a capability itself. A Unix process' file descriptor table is an example of a C-list .","title":"File descriptors as capabilities"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/","text":"Everything is a file Why is \u201cEverything is a file\u201d unique to the Unix operating systems? A Event loop Device file Beej's Guide to Network Programming APUE chapter 16 Network IPC: Sockets 20190523 why everything in Unix is a file everything in Unix is file \u548c file API \u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u5f85everything in Unix is file Everything is a file # \"Everything is a file\" describes one of the defining(\u6700\u5178\u578b\u7684) features of Unix , and its derivatives \u2014 that a wide range of input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some inter-process and network communications are simple streams of bytes exposed through the filesystem name space .[ 1] SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u5bf9 \"Everything is a file\" \u542b\u4e49\u7684\u89e3\u91ca\uff1a\u5373\u5c06\u8fd9\u4e9bresource\u90fd\u770b\u505a\u662ffile\uff08 streams of bytes \uff09 SUMMARY : \u4e0a\u8ff0 hard-drives\uff0cmodems\uff0ckeyboards\uff0c\u7b49\u90fd\u662fdevice\uff0c\u663e\u7136\u5728Unix\u4e2d\uff0c\u5b83\u4eec\u90fd\u88ab\u770b\u505a\u6210\u4e86file\uff08 streams of bytes \uff09 The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources. There are a number of file types . When a file is opened, a file descriptor is created. The file path becoming the addressing system and the file descriptor being the byte stream I/O interface. But file descriptors are also created for things like anonymous pipes and network sockets via different methods. So it is more accurate to say \"Everything is a file descriptor\" .[ 2] [ 3] Additionally, a range of pseudo and virtual filesystems exists which exposes information about processes and other system information in a hierarchical file-like structure. These are mounted into the single file hierarchy . An example of this purely virtual filesystem is under /proc that exposes many system properties as files. All of these \"files\" have standard Unix file attributes such as an owner and access permissions , and can be queried by the same classic Unix tools and filters . However, this is not universally considered a fast or portable approach. Some operating systems do not even mount /proc by default due to security or speed concerns.[ 4] It is, though, used heavily by both the widely installed BusyBox [ 5] on embedded systems and by procps, which is used on most Linux systems. In both cases it is used in implementations of process-related POSIX shell commands. It is similarly used on Android systems in the operating system's Toolbox program.[ 6] Unix's successor Plan 9 took this concept into distributed computing with the 9P protocol. Why is \u201cEverything is a file\u201d unique to the Unix operating systems? # A # So, why is this unique to Unix? Typical operating systems, prior to Unix, treated files one way and treated each peripheral device(\u5916\u8bbe) according to the characteristics of that device. That is, if the output of a program was written to a file on disk, that was the only place the output could go; you could not send it to the printer or the tape drive. Each program had to be aware of each device used for input and output, and have command options to deal with alternate I/O devices. Unix treats all devices as files , but with special attributes. To simplify programs, standard input and standard output are the default input and output devices of a program(\u8fd9\u53e5\u8bdd\u89e3\u91ca\u4e86 standard input \uff0c standard output \u7684\u539f\u56e0 ). So program output normally intended for the console screen could go anywhere, to a disk file or a printer or a serial port. This is called I/O redirection . Does other operating systems such as Windows and Macs not operate on files? Of course all modern OSes support various filesystems and can \"operate on files\", but the distinction is how are devices handled? Don't know about Mac, but Windows does offer some I/O redirection. And, compared to what other operating systems is it unique? Not really any more. Linux has the same feature. Of course, if an OS adopts I/O redirection, then it tends to use other Unix features and ends up Unix-like in the end. Event loop # \u5728\u8fd9\u7bc7\u6587\u7ae0\u7684 File_interface \u7ae0\u8282\u5bf9every thing is a file\u8fdb\u884c\u9610\u91ca\uff1b Device file # \u5c06device\u62bd\u8c61\u4e3afile\uff0c\u8fd9\u5c31\u662feverything is a file\u6700\u597d\u7684\u4f53\u73b0\uff1b Beej's Guide to Network Programming # \u5728\u8fd9\u672c\u4e66\u7684\u7b2c\u4e8c\u7ae0 2. What is a socket? \u4e2d\u5bf9everything is a file\u8fdb\u884c\u4e86\u9610\u8ff0\uff1b APUE chapter 16 Network IPC: Sockets # 20190523 # \u6628\u5929\u5728\u9605\u8bfbAPUE\u7684\u7684chapter 16 Network IPC: Sockets\u65f6\uff0c\u6240\u60f3\uff1a everything in Unix is a file\uff0c\u6240\u4ee5\u548c\u6211\u5e94\u8be5\u91c7\u7528\u770b\u5f85\u666e\u901a\u6587\u4ef6\u7684\u65b9\u5f0f\u6765\u770b\u5f85Unix\u7684socket\u3002socket\u548cfile\u4e00\u6837\uff0c\u90fd\u662f\u901a\u8fc7 file descriptor \u6765\u8fdb\u884c\u8bbf\u95ee\u3002POSIX\u4e2d\u63d0\u4f9b\u7684\u64cd\u4f5csocket\u7684\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u90fd\u662f fd \uff0c\u8868\u793a\u8fd9\u4e2asocket\u7684file descriptor\uff0c\u8fd9\u79cd\u505a\u6cd5\u548cfile\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\u3002 socket() \u51fd\u6570\u5c31\u597d\u6bd4 create() \u51fd\u6570\u3002\u5176\u5b9eAPUE\u7684\u4f5c\u8005\u572816.2\u4e2d\u5c31\u5bf9\u6bd4\u4e86Unix\u7684\u9488\u5bf9file\u7684API\u548c\u9488\u5bf9socket\u7684API\u3002 \u5982\u679c\u4ece\u9762\u5411\u5bf9\u8c61\u7684\u89d2\u5ea6\u6765\u6784\u9020POSIX\u7684\u6587\u4ef6api\u548csocket api\u7684\u8bdd\uff0c\u63a5\u53d7file descriptor\u7684api\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u6210\u5458\u51fd\u6570\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u6709\u4e00\u4e2afile descriptor\u3002 \u5728 Beej's Guide to Network Programming \u76842. What is a socket?\u7ae0\u8282\u4e5f\u662f\u4ecefile descriptor\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0socket\u7684\uff1b why everything in Unix is a file # Unix\u662f\u5178\u578b\u7684 Monolithic kernel \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u5c06\u5f88\u591a\u4e1c\u897f\u5c01\u88c5\u597d\u800c\u53ea\u63d0\u4f9b\u4e00\u4e2adescriptor\u6765\u4f9b\u7528\u6237\u4f7f\u7528\uff0c\u8fd9\u4e2adescriptor\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\u5c31\u662ffile descriptor\u3002\u663e\u7136\uff0ceverything in Unix is a file\u662f\u4e00\u79cd\u7b80\u5316\u7684\u62bd\u8c61\uff0c\u5b83\u8ba9\u7528\u6237\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u3002 \u5f53\u7136\uff0c\u4ece\u5185\u6838\u7684\u5b9e\u73b0\u4e0a\u662f\u5426\u771f\u7684\u662f\u5982\u6b64\u6211\u76ee\u524d\u8fd8\u4e0d\u5f97\u800c\u77e5\uff0c\u4f46\u662f\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002 everything in Unix is file \u548c file API # \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0ceverything in Unix is file\u662f\u4e00\u4e2a\u4e2aphilosophy\uff0c\u5b83\u662f\u6982\u5ff5\u4e0a\u7684\uff0c\u5b83\u66f4\u591a\u7684\u662f\u6307\uff1a\u5c06\u5b83\u770b\u505a\u662f\u4e00\u4e2afile\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884cIO\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u662f\u6307\u6211\u4eec\u53ef\u4ee5\u5bf9everything in Unix\u90fd\u4f7f\u7528Unix file\u7684API\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0cAPUE\u768416.2 Socket Descriptors\u8fdb\u884c\u4e86\u4e00\u4e9b\u63cf\u8ff0\uff1b \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 pipe(7) - Linux man page \u7684I/O on pipes and FIFOs\u7ae0\u8282\u4e2d\u63d0\u53ca\uff1a It is not possible to apply lseek(2) to a pipe. \u663e\u7136\uff0c\u6211\u4eec\u53ef\u4ee5 \u8ba4\u4e3a \uff08\u4ece\u903b\u8f91\u4e0a\uff09pipe\u662f\u4e00\u4e2afile\uff0c\u4f46\u662f\u5b83\u5b9e\u9645\u4e0a\u5e76\u4e0d\u662ffile\uff0c\u6240\u4ee5\uff0c\u5e76\u4e0d\u80fd\u591f\u5bf9\u5176\u4f7f\u7528lseek\u7cfb\u7edf\u8c03\u7528\u3002 \u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u5f85everything in Unix is file # \u5f15\u7528\u81ea File descriptor : In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b \u5bf9\u8fd9\u4e2a\u89c2\u70b9\u7684\u9a8c\u8bc1\u5305\u62ec\uff1a EPOLL instance","title":"Unix-philosophy-everything-is-a-file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#everything-is-a-file","text":"\"Everything is a file\" describes one of the defining(\u6700\u5178\u578b\u7684) features of Unix , and its derivatives \u2014 that a wide range of input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some inter-process and network communications are simple streams of bytes exposed through the filesystem name space .[ 1] SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u5bf9 \"Everything is a file\" \u542b\u4e49\u7684\u89e3\u91ca\uff1a\u5373\u5c06\u8fd9\u4e9bresource\u90fd\u770b\u505a\u662ffile\uff08 streams of bytes \uff09 SUMMARY : \u4e0a\u8ff0 hard-drives\uff0cmodems\uff0ckeyboards\uff0c\u7b49\u90fd\u662fdevice\uff0c\u663e\u7136\u5728Unix\u4e2d\uff0c\u5b83\u4eec\u90fd\u88ab\u770b\u505a\u6210\u4e86file\uff08 streams of bytes \uff09 The advantage of this approach is that the same set of tools, utilities and APIs can be used on a wide range of resources. There are a number of file types . When a file is opened, a file descriptor is created. The file path becoming the addressing system and the file descriptor being the byte stream I/O interface. But file descriptors are also created for things like anonymous pipes and network sockets via different methods. So it is more accurate to say \"Everything is a file descriptor\" .[ 2] [ 3] Additionally, a range of pseudo and virtual filesystems exists which exposes information about processes and other system information in a hierarchical file-like structure. These are mounted into the single file hierarchy . An example of this purely virtual filesystem is under /proc that exposes many system properties as files. All of these \"files\" have standard Unix file attributes such as an owner and access permissions , and can be queried by the same classic Unix tools and filters . However, this is not universally considered a fast or portable approach. Some operating systems do not even mount /proc by default due to security or speed concerns.[ 4] It is, though, used heavily by both the widely installed BusyBox [ 5] on embedded systems and by procps, which is used on most Linux systems. In both cases it is used in implementations of process-related POSIX shell commands. It is similarly used on Android systems in the operating system's Toolbox program.[ 6] Unix's successor Plan 9 took this concept into distributed computing with the 9P protocol.","title":"Everything is a file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#why-is-everything-is-a-file-unique-to-the-unix-operating-systems","text":"","title":"Why is \u201cEverything is a file\u201d unique to the Unix operating systems?"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#a","text":"So, why is this unique to Unix? Typical operating systems, prior to Unix, treated files one way and treated each peripheral device(\u5916\u8bbe) according to the characteristics of that device. That is, if the output of a program was written to a file on disk, that was the only place the output could go; you could not send it to the printer or the tape drive. Each program had to be aware of each device used for input and output, and have command options to deal with alternate I/O devices. Unix treats all devices as files , but with special attributes. To simplify programs, standard input and standard output are the default input and output devices of a program(\u8fd9\u53e5\u8bdd\u89e3\u91ca\u4e86 standard input \uff0c standard output \u7684\u539f\u56e0 ). So program output normally intended for the console screen could go anywhere, to a disk file or a printer or a serial port. This is called I/O redirection . Does other operating systems such as Windows and Macs not operate on files? Of course all modern OSes support various filesystems and can \"operate on files\", but the distinction is how are devices handled? Don't know about Mac, but Windows does offer some I/O redirection. And, compared to what other operating systems is it unique? Not really any more. Linux has the same feature. Of course, if an OS adopts I/O redirection, then it tends to use other Unix features and ends up Unix-like in the end.","title":"A"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#event-loop","text":"\u5728\u8fd9\u7bc7\u6587\u7ae0\u7684 File_interface \u7ae0\u8282\u5bf9every thing is a file\u8fdb\u884c\u9610\u91ca\uff1b","title":"Event loop"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#device-file","text":"\u5c06device\u62bd\u8c61\u4e3afile\uff0c\u8fd9\u5c31\u662feverything is a file\u6700\u597d\u7684\u4f53\u73b0\uff1b","title":"Device file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#beejs-guide-to-network-programming","text":"\u5728\u8fd9\u672c\u4e66\u7684\u7b2c\u4e8c\u7ae0 2. What is a socket? \u4e2d\u5bf9everything is a file\u8fdb\u884c\u4e86\u9610\u8ff0\uff1b","title":"Beej's Guide to Network Programming"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#apue-chapter-16-network-ipc-sockets","text":"","title":"APUE chapter 16 Network IPC: Sockets"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#20190523","text":"\u6628\u5929\u5728\u9605\u8bfbAPUE\u7684\u7684chapter 16 Network IPC: Sockets\u65f6\uff0c\u6240\u60f3\uff1a everything in Unix is a file\uff0c\u6240\u4ee5\u548c\u6211\u5e94\u8be5\u91c7\u7528\u770b\u5f85\u666e\u901a\u6587\u4ef6\u7684\u65b9\u5f0f\u6765\u770b\u5f85Unix\u7684socket\u3002socket\u548cfile\u4e00\u6837\uff0c\u90fd\u662f\u901a\u8fc7 file descriptor \u6765\u8fdb\u884c\u8bbf\u95ee\u3002POSIX\u4e2d\u63d0\u4f9b\u7684\u64cd\u4f5csocket\u7684\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u90fd\u662f fd \uff0c\u8868\u793a\u8fd9\u4e2asocket\u7684file descriptor\uff0c\u8fd9\u79cd\u505a\u6cd5\u548cfile\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\u3002 socket() \u51fd\u6570\u5c31\u597d\u6bd4 create() \u51fd\u6570\u3002\u5176\u5b9eAPUE\u7684\u4f5c\u8005\u572816.2\u4e2d\u5c31\u5bf9\u6bd4\u4e86Unix\u7684\u9488\u5bf9file\u7684API\u548c\u9488\u5bf9socket\u7684API\u3002 \u5982\u679c\u4ece\u9762\u5411\u5bf9\u8c61\u7684\u89d2\u5ea6\u6765\u6784\u9020POSIX\u7684\u6587\u4ef6api\u548csocket api\u7684\u8bdd\uff0c\u63a5\u53d7file descriptor\u7684api\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u6210\u5458\u51fd\u6570\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u6709\u4e00\u4e2afile descriptor\u3002 \u5728 Beej's Guide to Network Programming \u76842. What is a socket?\u7ae0\u8282\u4e5f\u662f\u4ecefile descriptor\u7684\u89d2\u5ea6\u6765\u63cf\u8ff0socket\u7684\uff1b","title":"20190523"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#why-everything-in-unix-is-a-file","text":"Unix\u662f\u5178\u578b\u7684 Monolithic kernel \uff0c\u6240\u4ee5\u5b83\u9700\u8981\u5c06\u5f88\u591a\u4e1c\u897f\u5c01\u88c5\u597d\u800c\u53ea\u63d0\u4f9b\u4e00\u4e2adescriptor\u6765\u4f9b\u7528\u6237\u4f7f\u7528\uff0c\u8fd9\u4e2adescriptor\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\u5c31\u662ffile descriptor\u3002\u663e\u7136\uff0ceverything in Unix is a file\u662f\u4e00\u79cd\u7b80\u5316\u7684\u62bd\u8c61\uff0c\u5b83\u8ba9\u7528\u6237\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\u3002 \u5f53\u7136\uff0c\u4ece\u5185\u6838\u7684\u5b9e\u73b0\u4e0a\u662f\u5426\u771f\u7684\u662f\u5982\u6b64\u6211\u76ee\u524d\u8fd8\u4e0d\u5f97\u800c\u77e5\uff0c\u4f46\u662f\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002","title":"why everything in Unix is a file"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#everything-in-unix-is-file-file-api","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0ceverything in Unix is file\u662f\u4e00\u4e2a\u4e2aphilosophy\uff0c\u5b83\u662f\u6982\u5ff5\u4e0a\u7684\uff0c\u5b83\u66f4\u591a\u7684\u662f\u6307\uff1a\u5c06\u5b83\u770b\u505a\u662f\u4e00\u4e2afile\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884cIO\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u662f\u6307\u6211\u4eec\u53ef\u4ee5\u5bf9everything in Unix\u90fd\u4f7f\u7528Unix file\u7684API\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0cAPUE\u768416.2 Socket Descriptors\u8fdb\u884c\u4e86\u4e00\u4e9b\u63cf\u8ff0\uff1b \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 pipe(7) - Linux man page \u7684I/O on pipes and FIFOs\u7ae0\u8282\u4e2d\u63d0\u53ca\uff1a It is not possible to apply lseek(2) to a pipe. \u663e\u7136\uff0c\u6211\u4eec\u53ef\u4ee5 \u8ba4\u4e3a \uff08\u4ece\u903b\u8f91\u4e0a\uff09pipe\u662f\u4e00\u4e2afile\uff0c\u4f46\u662f\u5b83\u5b9e\u9645\u4e0a\u5e76\u4e0d\u662ffile\uff0c\u6240\u4ee5\uff0c\u5e76\u4e0d\u80fd\u591f\u5bf9\u5176\u4f7f\u7528lseek\u7cfb\u7edf\u8c03\u7528\u3002","title":"everything in Unix is file \u548c file API"},{"location":"Programming/01-philosophy/Unix-philosophy-Everything-is-a-file/#kerneleverything-in-unix-is-file","text":"\u5f15\u7528\u81ea File descriptor : In Unix-like systems, file descriptors can refer to any Unix file type named in a file system. As well as regular files, this includes directories , block and character devices (also called \"special files\"), Unix domain sockets , and named pipes . File descriptors can also refer to other objects that do not normally exist in the file system, such as anonymous pipes and network sockets . SUMMARY : Everything is a file \uff1b\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u770b\u5f85everything in Unix is file\uff0cUnix-like system\u662f monolithic kernel \uff0c\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u4e9bdevice\u6216\u8005file\u90fd\u662f\u7531kernel\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u5b83\u4eec\u90fd\u6709\u5bf9\u5e94\u7684kernel structure\uff1b\u6211\u4eec\u901a\u8fc7file descriptor\u6765\u5f15\u7528\u8fd9\u4e9bkernel structure\uff0c\u6211\u4eec\u53ea\u80fd\u591f\u901a\u8fc7system call\u6765\u5bf9\u8fd9\u4e9bkernel structure\u8fdb\u884c\u64cd\u4f5c\uff1b \u5bf9\u8fd9\u4e2a\u89c2\u70b9\u7684\u9a8c\u8bc1\u5305\u62ec\uff1a EPOLL instance","title":"\u4ecekernel\u5b9e\u73b0\u7684\u89d2\u5ea6\u6765\u770b\u5f85everything in Unix is file"},{"location":"Programming/01-philosophy/Unix-philosophy/","text":"Unix philosophy Origin Unix philosophy # The Unix philosophy , originated by Ken Thompson , is a set of cultural norms and philosophical approaches to minimalist , modular software development . It is based on the experience of leading developers of the Unix operating system . Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a \" software tools \" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the \"Unix philosophy.\" The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design . SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u5f3a\u8c03\u4e86Unix philosophy\u7684\u6838\u5fc3\u6240\u5728\uff0c\u5373composability\u3002 Origin # The UNIX philosophy is documented by Doug McIlroy [ 1] in the Bell System Technical Journal from 1978:[ 2] Make each program do one thing well. To do a new job, build a fresh rather than complicate old programs by adding new \"features\". Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently\uff08\u4e25\u683c\u7684\uff09 columnar or binary input formats. Don't insist on interactive input. Design and build software, even\uff08\u751a\u81f3\u662f\uff09 operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them. \u8bbe\u8ba1\u548c\u6784\u5efa\u8f6f\u4ef6\uff0c\u751a\u81f3\u662f\u64cd\u4f5c\u7cfb\u7edf\uff0c\u8981\u53ca\u65e9\u5c1d\u8bd5\uff0c\u6700\u597d\u5728\u51e0\u5468\u5185\u5b8c\u6210\u3002 \u4e0d\u8981\u72b9\u8c6b\u6254\u6389\u7b28\u62d9\u7684\u90e8\u5206\u5e76\u91cd\u5efa\u5b83\u4eec\u3002 Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them. \u4f7f\u7528\u5de5\u5177\u4f18\u5148\u4e8e\u4e0d\u719f\u7ec3\u7684\u5e2e\u52a9\u6765\u51cf\u8f7b\u7f16\u7a0b\u4efb\u52a1\uff0c\u5373\u4f7f\u4f60\u4e0d\u5f97\u4e0d\u7ed5\u9053\u53bb\u6784\u5efa\u5de5\u5177\u5e76\u671f\u671b\u5728\u4f60\u4f7f\u7528\u5b83\u4eec\u4e4b\u540e\u629b\u51fa\u4e00\u4e9b\u5de5\u5177\u3002 It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):[ 1] Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. In their award-winning Unix paper of 1974, Ritchie and Thompson quote the following design considerations:[ 3] Make it easy to write, test, and run programs. Interactive use instead of batch processing . Economy and elegance of design due to size constraints (\"salvation through suffering\"). Self-supporting system: all Unix software is maintained under Unix. The whole philosophy of UNIX seems to stay out of assembler . \u2014\u2009 Michael Sean Mahoney [ 4]","title":"Unix-philosophy"},{"location":"Programming/01-philosophy/Unix-philosophy/#unix-philosophy","text":"The Unix philosophy , originated by Ken Thompson , is a set of cultural norms and philosophical approaches to minimalist , modular software development . It is based on the experience of leading developers of the Unix operating system . Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a \" software tools \" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the \"Unix philosophy.\" The Unix philosophy emphasizes building simple, short, clear, modular, and extensible code that can be easily maintained and repurposed by developers other than its creators. The Unix philosophy favors composability as opposed to monolithic design . SUMMARY : \u6700\u540e\u4e00\u6bb5\u8bdd\u5f3a\u8c03\u4e86Unix philosophy\u7684\u6838\u5fc3\u6240\u5728\uff0c\u5373composability\u3002","title":"Unix philosophy"},{"location":"Programming/01-philosophy/Unix-philosophy/#origin","text":"The UNIX philosophy is documented by Doug McIlroy [ 1] in the Bell System Technical Journal from 1978:[ 2] Make each program do one thing well. To do a new job, build a fresh rather than complicate old programs by adding new \"features\". Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently\uff08\u4e25\u683c\u7684\uff09 columnar or binary input formats. Don't insist on interactive input. Design and build software, even\uff08\u751a\u81f3\u662f\uff09 operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them. \u8bbe\u8ba1\u548c\u6784\u5efa\u8f6f\u4ef6\uff0c\u751a\u81f3\u662f\u64cd\u4f5c\u7cfb\u7edf\uff0c\u8981\u53ca\u65e9\u5c1d\u8bd5\uff0c\u6700\u597d\u5728\u51e0\u5468\u5185\u5b8c\u6210\u3002 \u4e0d\u8981\u72b9\u8c6b\u6254\u6389\u7b28\u62d9\u7684\u90e8\u5206\u5e76\u91cd\u5efa\u5b83\u4eec\u3002 Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them. \u4f7f\u7528\u5de5\u5177\u4f18\u5148\u4e8e\u4e0d\u719f\u7ec3\u7684\u5e2e\u52a9\u6765\u51cf\u8f7b\u7f16\u7a0b\u4efb\u52a1\uff0c\u5373\u4f7f\u4f60\u4e0d\u5f97\u4e0d\u7ed5\u9053\u53bb\u6784\u5efa\u5de5\u5177\u5e76\u671f\u671b\u5728\u4f60\u4f7f\u7528\u5b83\u4eec\u4e4b\u540e\u629b\u51fa\u4e00\u4e9b\u5de5\u5177\u3002 It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):[ 1] Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. In their award-winning Unix paper of 1974, Ritchie and Thompson quote the following design considerations:[ 3] Make it easy to write, test, and run programs. Interactive use instead of batch processing . Economy and elegance of design due to size constraints (\"salvation through suffering\"). Self-supporting system: all Unix software is maintained under Unix. The whole philosophy of UNIX seems to stay out of assembler . \u2014\u2009 Michael Sean Mahoney [ 4]","title":"Origin"},{"location":"Programming/02-Process/","text":"\u524d\u8a00 # \u4e0eprocess\u76f8\u5173\u7684\u4e3b\u9898\u3002","title":"Introduction"},{"location":"Programming/02-Process/#_1","text":"\u4e0eprocess\u76f8\u5173\u7684\u4e3b\u9898\u3002","title":"\u524d\u8a00"},{"location":"Programming/02-Process/IPC/POSIX-message-queues/","text":"MQ_OVERVIEW(7) #","title":"POSIX-message-queues"},{"location":"Programming/02-Process/IPC/POSIX-message-queues/#mq_overview7","text":"","title":"MQ_OVERVIEW(7)"},{"location":"Programming/02-Process/IPC/POSIX-semaphores/","text":"SEM_OVERVIEW(7) #","title":"POSIX-semaphores"},{"location":"Programming/02-Process/IPC/POSIX-semaphores/#sem_overview7","text":"","title":"SEM_OVERVIEW(7)"},{"location":"Programming/02-Process/IPC/System-V-IPC/System-V-IPC/","text":"SVIPC(7) #","title":"System-V-IPC"},{"location":"Programming/02-Process/IPC/System-V-IPC/System-V-IPC/#svipc7","text":"","title":"SVIPC(7)"},{"location":"Programming/02-Process/Thread/pthreads/","text":"PTHREADS(7) # NAME # pthreads - POSIX threads DESCRIPTION # POSIX.1 specifies a set of interfaces (functions, header files) for threaded programming commonly known as POSIX threads, or Pthreads. A single process can contain multiple threads, all of which are executing the same program. These threads share the same global memory (data and heap segments), but each thread has its own stack (automatic variables). POSIX.1 also requires that threads share a range of other attributes (i.e., these attributes are process-wide rather than per-thread): process ID parent process ID process group ID and session ID controlling terminal user and group IDs open file descriptors record locks (see fcntl(2) ) signal dispositions file mode creation mask ( umask(2) ) current directory ( chdir(2) ) and root directory ( chroot(2) ) interval timers ( setitimer(2) ) and POSIX timers ( timer_create(2) ) nice value ( setpriority(2) ) resource limits ( setrlimit(2) ) measurements of the consumption of CPU time ( times(2) ) and resources ( getrusage(2) ) SUMMARY : \u5982\u4f55\u7edf\u8ba1the consumption of CPU of a thread\uff1f\u53c2\u89c1 PTHREAD_GETCPUCLOCKID(3) As well as the stack, POSIX.1 specifies that various other attributes are distinct for each thread, including: thread ID (the pthread_t data type) signal mask ( pthread_sigmask(3) ) the errno variable alternate signal stack ( sigaltstack(2) ) real-time scheduling policy and priority ( sched(7) ) The following Linux-specific features are also per-thread: capabilities (see capabilities(7) ) CPU affinity ( sched_setaffinity(2) ) Pthreads function return values # Most pthreads functions return 0 on success, and an error number on failure. Note that the pthreads functions do not set errno . For each of the pthreads functions that can return an error, POSIX.1-2001 specifies that the function can never fail with the error EINTR . SUMMARY : linux system call\u662f\u4f1a\u8bbe\u7f6e errno \u7684\u3002 Thread IDs # Each of the threads in a process has a unique thread identifier (stored in the type pthread_t). This identifier is returned to the caller of pthread_create(3) , and a thread can obtain its own thread identifier using pthread_self(3) . Thread IDs are guaranteed to be unique only within a process. (In all pthreads functions that accept a thread ID as an argument, that ID by definition refers to a thread in the same process as the caller.) The system may reuse a thread ID after a terminated thread has been joined, or a detached thread has terminated. POSIX says: \"If an application attempts to use a thread ID whose lifetime has ended, the behavior is undefined.\" Thread-safe functions # Async-cancel-safe functions # An async-cancel-safe function is one that can be safely called in an application where asynchronous cancelability is enabled (see pthread_setcancelstate(3) ). Only the following functions are required to be async-cancel-safe by POSIX.1-2001 and POSIX.1-2008: pthread_cancel() pthread_setcancelstate() pthread_setcanceltype() Cancellation points # POSIX.1 specifies that certain functions must, and certain other functions may, be cancellation points . If a thread is cancelable, its cancelability type is deferred , and a cancellation request is pending for the thread, then the thread is canceled when it calls a function that is a cancellation point . The following functions are required to be cancellation points by POSIX.1-2001 and/or POSIX.1-2008: accept() aio_suspend() clock_nanosleep() close() connect() creat() fcntl() F_SETLKW fdatasync() fsync() getmsg() getpmsg() lockf() F_LOCK mq_receive() mq_send() mq_timedreceive() mq_timedsend() msgrcv() msgsnd() msync() nanosleep() open() openat() [Added in POSIX.1-2008] pause() poll() pread() pselect() pthread_cond_timedwait() pthread_cond_wait() pthread_join() pthread_testcancel() putmsg() putpmsg() pwrite() read() readv() recv() recvfrom() recvmsg() select() sem_timedwait() sem_wait() send() sendmsg() sendto() sigpause() [POSIX.1-2001 only (moves to \"may\" list in POSIX.1-2008)] sigsuspend() sigtimedwait() sigwait() sigwaitinfo() sleep() system() tcdrain() usleep() [POSIX.1-2001 only (function removed in POSIX.1-2008)] wait() waitid() waitpid() write() writev() The following functions may be cancellation points according to POSIX.1-2001 and/or POSIX.1-2008: access() asctime() asctime_r() catclose() catgets() catopen() chmod() [Added in POSIX.1-2008] chown() [Added in POSIX.1-2008] closedir() closelog() ctermid() ctime() ctime_r() dbm_close() dbm_delete() dbm_fetch() dbm_nextkey() dbm_open() dbm_store() dlclose() dlopen() dprintf() [Added in POSIX.1-2008] endgrent() endhostent() endnetent() endprotoent() endpwent() endservent() endutxent() faccessat() [Added in POSIX.1-2008] fchmod() [Added in POSIX.1-2008] fchmodat() [Added in POSIX.1-2008] fchown() [Added in POSIX.1-2008] fchownat() [Added in POSIX.1-2008] fclose() fcntl() (for any value of cmd argument) fflush() fgetc() fgetpos() fgets() fgetwc() fgetws() fmtmsg() fopen() fpathconf() fprintf() fputc() fputs() fputwc() fputws() fread() freopen() fscanf() fseek() fseeko() fsetpos() fstat() fstatat() [Added in POSIX.1-2008] ftell() ftello() ftw() futimens() [Added in POSIX.1-2008] fwprintf() fwrite() fwscanf() getaddrinfo() getc() getc_unlocked() getchar() getchar_unlocked() getcwd() getdate() getdelim() [Added in POSIX.1-2008] getgrent() getgrgid() getgrgid_r() getgrnam() getgrnam_r() gethostbyaddr() [SUSv3 only (function removed in POSIX.1-2008)] gethostbyname() [SUSv3 only (function removed in POSIX.1-2008)] gethostent() gethostid() gethostname() getline() [Added in POSIX.1-2008] getlogin() getlogin_r() getnameinfo() getnetbyaddr() getnetbyname() getnetent() getopt() (if opterr is nonzero) getprotobyname() getprotobynumber() getprotoent() getpwent() getpwnam() getpwnam_r() getpwuid() getpwuid_r() gets() getservbyname() getservbyport() getservent() getutxent() getutxid() getutxline() getwc() getwchar() getwd() [SUSv3 only (function removed in POSIX.1-2008)] glob() iconv_close() iconv_open() ioctl() link() linkat() [Added in POSIX.1-2008] lio_listio() [Added in POSIX.1-2008] localtime() localtime_r() lockf() [Added in POSIX.1-2008] lseek() lstat() mkdir() [Added in POSIX.1-2008] mkdirat() [Added in POSIX.1-2008] mkdtemp() [Added in POSIX.1-2008] mkfifo() [Added in POSIX.1-2008] mkfifoat() [Added in POSIX.1-2008] mknod() [Added in POSIX.1-2008] mknodat() [Added in POSIX.1-2008] mkstemp() mktime() nftw() opendir() openlog() pathconf() pclose() perror() popen() posix_fadvise() posix_fallocate() posix_madvise() posix_openpt() posix_spawn() posix_spawnp() posix_trace_clear() posix_trace_close() posix_trace_create() posix_trace_create_withlog() posix_trace_eventtypelist_getnext_id() posix_trace_eventtypelist_rewind() posix_trace_flush() posix_trace_get_attr() posix_trace_get_filter() posix_trace_get_status() posix_trace_getnext_event() posix_trace_open() posix_trace_rewind() posix_trace_set_filter() posix_trace_shutdown() posix_trace_timedgetnext_event() posix_typed_mem_open() printf() psiginfo() [Added in POSIX.1-2008] psignal() [Added in POSIX.1-2008] pthread_rwlock_rdlock() pthread_rwlock_timedrdlock() pthread_rwlock_timedwrlock() pthread_rwlock_wrlock() putc() putc_unlocked() putchar() putchar_unlocked() puts() pututxline() putwc() putwchar() readdir() readdir_r() readlink() [Added in POSIX.1-2008] readlinkat() [Added in POSIX.1-2008] remove() rename() renameat() [Added in POSIX.1-2008] rewind() rewinddir() scandir() [Added in POSIX.1-2008] scanf() seekdir() semop() setgrent() sethostent() setnetent() setprotoent() setpwent() setservent() setutxent() sigpause() [Added in POSIX.1-2008] stat() strerror() strerror_r() strftime() symlink() symlinkat() [Added in POSIX.1-2008] sync() syslog() tmpfile() tmpnam() ttyname() ttyname_r() tzset() ungetc() ungetwc() unlink() unlinkat() [Added in POSIX.1-2008] utime() [Added in POSIX.1-2008] utimensat() [Added in POSIX.1-2008] utimes() [Added in POSIX.1-2008] vdprintf() [Added in POSIX.1-2008] vfprintf() vfwprintf() vprintf() vwprintf() wcsftime() wordexp() wprintf() wscanf() An implementation may also mark other functions not specified in the standard as cancellation points . In particular, an implementation is likely to mark any nonstandard function that may block as a cancellation point . (This includes most functions that can touch files.) Linux implementations of POSIX threads # Over time, two threading implementations have been provided by the GNU C library on Linux: LinuxThreads This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. NPTL (Native POSIX Threads Library) This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity . Both threading implementations employ the Linux clone(2) system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex(2) system call.","title":"pthreads"},{"location":"Programming/02-Process/Thread/pthreads/#pthreads7","text":"","title":"PTHREADS(7)"},{"location":"Programming/02-Process/Thread/pthreads/#name","text":"pthreads - POSIX threads","title":"NAME"},{"location":"Programming/02-Process/Thread/pthreads/#description","text":"POSIX.1 specifies a set of interfaces (functions, header files) for threaded programming commonly known as POSIX threads, or Pthreads. A single process can contain multiple threads, all of which are executing the same program. These threads share the same global memory (data and heap segments), but each thread has its own stack (automatic variables). POSIX.1 also requires that threads share a range of other attributes (i.e., these attributes are process-wide rather than per-thread): process ID parent process ID process group ID and session ID controlling terminal user and group IDs open file descriptors record locks (see fcntl(2) ) signal dispositions file mode creation mask ( umask(2) ) current directory ( chdir(2) ) and root directory ( chroot(2) ) interval timers ( setitimer(2) ) and POSIX timers ( timer_create(2) ) nice value ( setpriority(2) ) resource limits ( setrlimit(2) ) measurements of the consumption of CPU time ( times(2) ) and resources ( getrusage(2) ) SUMMARY : \u5982\u4f55\u7edf\u8ba1the consumption of CPU of a thread\uff1f\u53c2\u89c1 PTHREAD_GETCPUCLOCKID(3) As well as the stack, POSIX.1 specifies that various other attributes are distinct for each thread, including: thread ID (the pthread_t data type) signal mask ( pthread_sigmask(3) ) the errno variable alternate signal stack ( sigaltstack(2) ) real-time scheduling policy and priority ( sched(7) ) The following Linux-specific features are also per-thread: capabilities (see capabilities(7) ) CPU affinity ( sched_setaffinity(2) )","title":"DESCRIPTION"},{"location":"Programming/02-Process/Thread/pthreads/#pthreads-function-return-values","text":"Most pthreads functions return 0 on success, and an error number on failure. Note that the pthreads functions do not set errno . For each of the pthreads functions that can return an error, POSIX.1-2001 specifies that the function can never fail with the error EINTR . SUMMARY : linux system call\u662f\u4f1a\u8bbe\u7f6e errno \u7684\u3002","title":"Pthreads function return values"},{"location":"Programming/02-Process/Thread/pthreads/#thread-ids","text":"Each of the threads in a process has a unique thread identifier (stored in the type pthread_t). This identifier is returned to the caller of pthread_create(3) , and a thread can obtain its own thread identifier using pthread_self(3) . Thread IDs are guaranteed to be unique only within a process. (In all pthreads functions that accept a thread ID as an argument, that ID by definition refers to a thread in the same process as the caller.) The system may reuse a thread ID after a terminated thread has been joined, or a detached thread has terminated. POSIX says: \"If an application attempts to use a thread ID whose lifetime has ended, the behavior is undefined.\"","title":"Thread IDs"},{"location":"Programming/02-Process/Thread/pthreads/#thread-safe-functions","text":"","title":"Thread-safe functions"},{"location":"Programming/02-Process/Thread/pthreads/#async-cancel-safe-functions","text":"An async-cancel-safe function is one that can be safely called in an application where asynchronous cancelability is enabled (see pthread_setcancelstate(3) ). Only the following functions are required to be async-cancel-safe by POSIX.1-2001 and POSIX.1-2008: pthread_cancel() pthread_setcancelstate() pthread_setcanceltype()","title":"Async-cancel-safe functions"},{"location":"Programming/02-Process/Thread/pthreads/#cancellation-points","text":"POSIX.1 specifies that certain functions must, and certain other functions may, be cancellation points . If a thread is cancelable, its cancelability type is deferred , and a cancellation request is pending for the thread, then the thread is canceled when it calls a function that is a cancellation point . The following functions are required to be cancellation points by POSIX.1-2001 and/or POSIX.1-2008: accept() aio_suspend() clock_nanosleep() close() connect() creat() fcntl() F_SETLKW fdatasync() fsync() getmsg() getpmsg() lockf() F_LOCK mq_receive() mq_send() mq_timedreceive() mq_timedsend() msgrcv() msgsnd() msync() nanosleep() open() openat() [Added in POSIX.1-2008] pause() poll() pread() pselect() pthread_cond_timedwait() pthread_cond_wait() pthread_join() pthread_testcancel() putmsg() putpmsg() pwrite() read() readv() recv() recvfrom() recvmsg() select() sem_timedwait() sem_wait() send() sendmsg() sendto() sigpause() [POSIX.1-2001 only (moves to \"may\" list in POSIX.1-2008)] sigsuspend() sigtimedwait() sigwait() sigwaitinfo() sleep() system() tcdrain() usleep() [POSIX.1-2001 only (function removed in POSIX.1-2008)] wait() waitid() waitpid() write() writev() The following functions may be cancellation points according to POSIX.1-2001 and/or POSIX.1-2008: access() asctime() asctime_r() catclose() catgets() catopen() chmod() [Added in POSIX.1-2008] chown() [Added in POSIX.1-2008] closedir() closelog() ctermid() ctime() ctime_r() dbm_close() dbm_delete() dbm_fetch() dbm_nextkey() dbm_open() dbm_store() dlclose() dlopen() dprintf() [Added in POSIX.1-2008] endgrent() endhostent() endnetent() endprotoent() endpwent() endservent() endutxent() faccessat() [Added in POSIX.1-2008] fchmod() [Added in POSIX.1-2008] fchmodat() [Added in POSIX.1-2008] fchown() [Added in POSIX.1-2008] fchownat() [Added in POSIX.1-2008] fclose() fcntl() (for any value of cmd argument) fflush() fgetc() fgetpos() fgets() fgetwc() fgetws() fmtmsg() fopen() fpathconf() fprintf() fputc() fputs() fputwc() fputws() fread() freopen() fscanf() fseek() fseeko() fsetpos() fstat() fstatat() [Added in POSIX.1-2008] ftell() ftello() ftw() futimens() [Added in POSIX.1-2008] fwprintf() fwrite() fwscanf() getaddrinfo() getc() getc_unlocked() getchar() getchar_unlocked() getcwd() getdate() getdelim() [Added in POSIX.1-2008] getgrent() getgrgid() getgrgid_r() getgrnam() getgrnam_r() gethostbyaddr() [SUSv3 only (function removed in POSIX.1-2008)] gethostbyname() [SUSv3 only (function removed in POSIX.1-2008)] gethostent() gethostid() gethostname() getline() [Added in POSIX.1-2008] getlogin() getlogin_r() getnameinfo() getnetbyaddr() getnetbyname() getnetent() getopt() (if opterr is nonzero) getprotobyname() getprotobynumber() getprotoent() getpwent() getpwnam() getpwnam_r() getpwuid() getpwuid_r() gets() getservbyname() getservbyport() getservent() getutxent() getutxid() getutxline() getwc() getwchar() getwd() [SUSv3 only (function removed in POSIX.1-2008)] glob() iconv_close() iconv_open() ioctl() link() linkat() [Added in POSIX.1-2008] lio_listio() [Added in POSIX.1-2008] localtime() localtime_r() lockf() [Added in POSIX.1-2008] lseek() lstat() mkdir() [Added in POSIX.1-2008] mkdirat() [Added in POSIX.1-2008] mkdtemp() [Added in POSIX.1-2008] mkfifo() [Added in POSIX.1-2008] mkfifoat() [Added in POSIX.1-2008] mknod() [Added in POSIX.1-2008] mknodat() [Added in POSIX.1-2008] mkstemp() mktime() nftw() opendir() openlog() pathconf() pclose() perror() popen() posix_fadvise() posix_fallocate() posix_madvise() posix_openpt() posix_spawn() posix_spawnp() posix_trace_clear() posix_trace_close() posix_trace_create() posix_trace_create_withlog() posix_trace_eventtypelist_getnext_id() posix_trace_eventtypelist_rewind() posix_trace_flush() posix_trace_get_attr() posix_trace_get_filter() posix_trace_get_status() posix_trace_getnext_event() posix_trace_open() posix_trace_rewind() posix_trace_set_filter() posix_trace_shutdown() posix_trace_timedgetnext_event() posix_typed_mem_open() printf() psiginfo() [Added in POSIX.1-2008] psignal() [Added in POSIX.1-2008] pthread_rwlock_rdlock() pthread_rwlock_timedrdlock() pthread_rwlock_timedwrlock() pthread_rwlock_wrlock() putc() putc_unlocked() putchar() putchar_unlocked() puts() pututxline() putwc() putwchar() readdir() readdir_r() readlink() [Added in POSIX.1-2008] readlinkat() [Added in POSIX.1-2008] remove() rename() renameat() [Added in POSIX.1-2008] rewind() rewinddir() scandir() [Added in POSIX.1-2008] scanf() seekdir() semop() setgrent() sethostent() setnetent() setprotoent() setpwent() setservent() setutxent() sigpause() [Added in POSIX.1-2008] stat() strerror() strerror_r() strftime() symlink() symlinkat() [Added in POSIX.1-2008] sync() syslog() tmpfile() tmpnam() ttyname() ttyname_r() tzset() ungetc() ungetwc() unlink() unlinkat() [Added in POSIX.1-2008] utime() [Added in POSIX.1-2008] utimensat() [Added in POSIX.1-2008] utimes() [Added in POSIX.1-2008] vdprintf() [Added in POSIX.1-2008] vfprintf() vfwprintf() vprintf() vwprintf() wcsftime() wordexp() wprintf() wscanf() An implementation may also mark other functions not specified in the standard as cancellation points . In particular, an implementation is likely to mark any nonstandard function that may block as a cancellation point . (This includes most functions that can touch files.)","title":"Cancellation points"},{"location":"Programming/02-Process/Thread/pthreads/#linux-implementations-of-posix-threads","text":"Over time, two threading implementations have been provided by the GNU C library on Linux: LinuxThreads This is the original Pthreads implementation. Since glibc 2.4, this implementation is no longer supported. NPTL (Native POSIX Threads Library) This is the modern Pthreads implementation. By comparison with LinuxThreads, NPTL provides closer conformance to the requirements of the POSIX.1 specification and better performance when creating large numbers of threads. NPTL is available since glibc 2.3.2, and requires features that are present in the Linux 2.6 kernel. Both of these are so-called 1:1 implementations, meaning that each thread maps to a kernel scheduling entity . Both threading implementations employ the Linux clone(2) system call. In NPTL, thread synchronization primitives (mutexes, thread joining, and so on) are implemented using the Linux futex(2) system call.","title":"Linux implementations of POSIX threads"},{"location":"Programming/03-IO/","text":"","title":"Introduction"},{"location":"Programming/03-Time/","text":"","title":"Introduction"},{"location":"Programming/04-Signal/","text":"Signal # \u5185\u5bb9\uff1a APUE # Chapter 10 Signals # signal\u662f\u4ec0\u4e48\uff1f 10.2 Signal Concepts\uff1aSignals are software interrupts. \u4fe1\u53f7\u7684\u6765\u6e90\uff1f 10.2 Signal Concepts \u8fd9\u4e00\u8282\u5bf9\u4fe1\u53f7\u6765\u6e90\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002\u53ef\u4ee5\u770b\u5230signal\u7684\u6765\u6e90\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\u3002\u5728\u9605\u8bfb Understanding.The.Linux.kernel.3rd.Edition \u7684Chapter 4. Interrupts and Exceptions\u65f6\uff0c\u6211\u601d\u8003\u4e86\u5982\u4e0b\u95ee\u9898\uff1aUnix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u663e\u7136\u4e0d\u662f\u7684\uff0c\u6bd4\u5982 SIGINT \u5c31\u4e0d\u662f\u6e90\u81ea\u4e8eexception\u3002\u5728\u672c\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u6240\u4ee5\u6211\u4eec\u5e94\u8be5\u6e05\u695a\uff0cexception\u662f\u4fe1\u53f7\u7684\u4f17\u591a\u6765\u6e90\u4e2d\u7684\u4e00\u79cd\u3002 Understanding.The.Linux.kernel.3rd.Edition # Chapter 4. Interrupts and Exceptions # Chapter 11. Signals # SIGNAL(7) #","title":"Introduction"},{"location":"Programming/04-Signal/#signal","text":"\u5185\u5bb9\uff1a","title":"Signal"},{"location":"Programming/04-Signal/#apue","text":"","title":"APUE"},{"location":"Programming/04-Signal/#chapter-10-signals","text":"signal\u662f\u4ec0\u4e48\uff1f 10.2 Signal Concepts\uff1aSignals are software interrupts. \u4fe1\u53f7\u7684\u6765\u6e90\uff1f 10.2 Signal Concepts \u8fd9\u4e00\u8282\u5bf9\u4fe1\u53f7\u6765\u6e90\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002\u53ef\u4ee5\u770b\u5230signal\u7684\u6765\u6e90\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\u3002\u5728\u9605\u8bfb Understanding.The.Linux.kernel.3rd.Edition \u7684Chapter 4. Interrupts and Exceptions\u65f6\uff0c\u6211\u601d\u8003\u4e86\u5982\u4e0b\u95ee\u9898\uff1aUnix signal\u90fd\u5bf9\u5e94\u7684\u662fexceptions\uff1f\u663e\u7136\u4e0d\u662f\u7684\uff0c\u6bd4\u5982 SIGINT \u5c31\u4e0d\u662f\u6e90\u81ea\u4e8eexception\u3002\u5728\u672c\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u6240\u4ee5\u6211\u4eec\u5e94\u8be5\u6e05\u695a\uff0cexception\u662f\u4fe1\u53f7\u7684\u4f17\u591a\u6765\u6e90\u4e2d\u7684\u4e00\u79cd\u3002","title":"Chapter 10 Signals"},{"location":"Programming/04-Signal/#understandingthelinuxkernel3rdedition","text":"","title":"Understanding.The.Linux.kernel.3rd.Edition"},{"location":"Programming/04-Signal/#chapter-4-interrupts-and-exceptions","text":"","title":"Chapter 4. Interrupts and Exceptions"},{"location":"Programming/04-Signal/#chapter-11-signals","text":"","title":"Chapter 11. Signals"},{"location":"Programming/04-Signal/#signal7","text":"","title":"SIGNAL(7)"},{"location":"Programming/05-Linux-Virtualization/","text":"\u524d\u8a00 # \u672c\u7ae0\u4e3b\u8981\u4ecb\u7ecd virtualization \u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8linux OS-level virtualization \u3002 \u7136\u540e\u91cd\u70b9\u4ecb\u7ecdlinux kernel\u662f\u5982\u4f55\u5b9e\u73b0 OS-level virtualization \u7684\u3002","title":"Introduction"},{"location":"Programming/05-Linux-Virtualization/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u4ecb\u7ecd virtualization \u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8linux OS-level virtualization \u3002 \u7136\u540e\u91cd\u70b9\u4ecb\u7ecdlinux kernel\u662f\u5982\u4f55\u5b9e\u73b0 OS-level virtualization \u7684\u3002","title":"\u524d\u8a00"},{"location":"Programming/05-Linux-Virtualization/Linux-containers/","text":"Linux containers # LXC #","title":"Linux-containers"},{"location":"Programming/05-Linux-Virtualization/Linux-containers/#linux-containers","text":"","title":"Linux containers"},{"location":"Programming/05-Linux-Virtualization/Linux-containers/#lxc","text":"","title":"LXC"},{"location":"Programming/05-Linux-Virtualization/Virtualization/","text":"Virtualization # NOTE: \u4e0b\u9762\u4ecb\u7ecd\u5404\u4e2a\u5c42\u7ea7\u7684\u7684virtualization Hardware virtualization # Main article: Hardware virtualization See also: Mobile virtualization Desktop virtualization # Main article: Desktop virtualization Containerization # Main article: Operating-system-level virtualization NOTE: \u8fd9\u662f\u672c\u7ae0\u4e3b\u8981\u5173\u6ce8\u7684 OS-level virtualization # OS-level virtualization refers to an operating system paradigm in which the kernel allows the existence of multiple isolated user space instances. Such instances, called containers ( Solaris , Docker ), Zones ( Solaris ), virtual private servers ( OpenVZ ), partitions , virtual environments (VEs), virtual kernel ( DragonFly BSD ), or jails ( FreeBSD jail or chroot jail ),[ 1] may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares , CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container. NOTE: \u5e38\u5e38\u542c\u5230\u7684 Docker \uff0c container \u6240\u4f7f\u7528\u7684\u5c31\u662f OS-level virtualization On Unix-like operating systems, this feature can be seen as an advanced implementation of the standard chroot mechanism, which changes the apparent root folder for the current running process and its children. In addition to isolation mechanisms, the kernel often provides resource-management features to limit the impact of one container's activities on other containers. NOTE: linux kernel\u7279\u6027 Linux namespaces \u7528\u4e8e\u652f\u6301isolation\uff1b linux kernel\u7279\u6027 Linux control groups \u7528\u4e8e\u652f\u6301 resource-management \uff1b The term \"container,\" while most popularly referring to OS-level virtualization systems, is sometimes ambiguously used to refer to fuller virtual machine environments operating in varying degrees of concert with the host OS, e.g. Microsoft's \" Hyper-V Containers.\" List of Linux containers #","title":"Virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#virtualization","text":"NOTE: \u4e0b\u9762\u4ecb\u7ecd\u5404\u4e2a\u5c42\u7ea7\u7684\u7684virtualization","title":"Virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#hardware-virtualization","text":"Main article: Hardware virtualization See also: Mobile virtualization","title":"Hardware virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#desktop-virtualization","text":"Main article: Desktop virtualization","title":"Desktop virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#containerization","text":"Main article: Operating-system-level virtualization NOTE: \u8fd9\u662f\u672c\u7ae0\u4e3b\u8981\u5173\u6ce8\u7684","title":"Containerization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#os-level-virtualization","text":"OS-level virtualization refers to an operating system paradigm in which the kernel allows the existence of multiple isolated user space instances. Such instances, called containers ( Solaris , Docker ), Zones ( Solaris ), virtual private servers ( OpenVZ ), partitions , virtual environments (VEs), virtual kernel ( DragonFly BSD ), or jails ( FreeBSD jail or chroot jail ),[ 1] may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares , CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container. NOTE: \u5e38\u5e38\u542c\u5230\u7684 Docker \uff0c container \u6240\u4f7f\u7528\u7684\u5c31\u662f OS-level virtualization On Unix-like operating systems, this feature can be seen as an advanced implementation of the standard chroot mechanism, which changes the apparent root folder for the current running process and its children. In addition to isolation mechanisms, the kernel often provides resource-management features to limit the impact of one container's activities on other containers. NOTE: linux kernel\u7279\u6027 Linux namespaces \u7528\u4e8e\u652f\u6301isolation\uff1b linux kernel\u7279\u6027 Linux control groups \u7528\u4e8e\u652f\u6301 resource-management \uff1b The term \"container,\" while most popularly referring to OS-level virtualization systems, is sometimes ambiguously used to refer to fuller virtual machine environments operating in varying degrees of concert with the host OS, e.g. Microsoft's \" Hyper-V Containers.\"","title":"OS-level virtualization"},{"location":"Programming/05-Linux-Virtualization/Virtualization/#list-of-linux-containers","text":"","title":"List of Linux containers"},{"location":"Programming/05-Linux-Virtualization/Linux-control-groups/Control-groups/","text":"NAMESPACES(7) # cgroups #","title":"Introduction"},{"location":"Programming/05-Linux-Virtualization/Linux-control-groups/Control-groups/#namespaces7","text":"","title":"NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-control-groups/Control-groups/#cgroups","text":"","title":"cgroups"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/","text":"\u524d\u8a00 # \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7c7b\u6bd4\u7684\u65b9\u6cd5\u6765\u7406\u89e3linux namespace\uff0c\u5373\u4ece\u5176\u4ed6\u4f7f\u7528\u4e86namespace\u7684\u9886\u57df\u6765\u7c7b\u6bd4\u7406\u89e3linux namespace\uff0c\u6bd4\u5982 c++ \u4e2d\u7684 namespace \u3002\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684 namespace \u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u597d\u3002\u663e\u7136\uff0c\u65e0\u8bba\u5728\u54ea\u4e2a\u5c42\u7ea7\uff08programming language\u3001operating system\uff09\uff0c\u4f7f\u7528namespace\u7684\u76ee\u7684\u662f\uff1a separation \u4ee5Hierarchy\u7684\u7ed3\u6784\u6765\u7ec4\u7ec7\u6570\u636e \u5728\u7406\u89e3\u4e86\u4f7f\u7528namespace\u7684\u76ee\u7684\u540e\uff0c\u63a8\u8350\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u8f93\u5165\u6d45\u51fa\uff0c\u9605\u8bfb\u5b8c\u6210\u540e\uff0c\u57fa\u672c\u4e0a\u80fd\u591f\u77e5\u9053linux namespace\u6240\u89e3\u51b3\u7684\u5b9e\u9645\u95ee\u9898\u548c\u5b83\u7684\u4ef7\u503c\u4e86\u3002 \u638c\u63e1\u4e86\u8fd9\u4e9b\u540e\uff0c\u518d\u53bb\u9605\u8bfbman\u4e2d\u5bf9\u5b83\u7684\u89e3\u91ca\u5c31\u4f1a\u975e\u5e38\u5bb9\u6613\u4e86\u3002","title":"Introduction"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/#_1","text":"\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7c7b\u6bd4\u7684\u65b9\u6cd5\u6765\u7406\u89e3linux namespace\uff0c\u5373\u4ece\u5176\u4ed6\u4f7f\u7528\u4e86namespace\u7684\u9886\u57df\u6765\u7c7b\u6bd4\u7406\u89e3linux namespace\uff0c\u6bd4\u5982 c++ \u4e2d\u7684 namespace \u3002\u7ef4\u57fa\u767e\u79d1\u7684 Namespace \u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684 namespace \u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u597d\u3002\u663e\u7136\uff0c\u65e0\u8bba\u5728\u54ea\u4e2a\u5c42\u7ea7\uff08programming language\u3001operating system\uff09\uff0c\u4f7f\u7528namespace\u7684\u76ee\u7684\u662f\uff1a separation \u4ee5Hierarchy\u7684\u7ed3\u6784\u6765\u7ec4\u7ec7\u6570\u636e \u5728\u7406\u89e3\u4e86\u4f7f\u7528namespace\u7684\u76ee\u7684\u540e\uff0c\u63a8\u8350\u9605\u8bfb Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces \uff0c\u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u8f93\u5165\u6d45\u51fa\uff0c\u9605\u8bfb\u5b8c\u6210\u540e\uff0c\u57fa\u672c\u4e0a\u80fd\u591f\u77e5\u9053linux namespace\u6240\u89e3\u51b3\u7684\u5b9e\u9645\u95ee\u9898\u548c\u5b83\u7684\u4ef7\u503c\u4e86\u3002 \u638c\u63e1\u4e86\u8fd9\u4e9b\u540e\uff0c\u518d\u53bb\u9605\u8bfbman\u4e2d\u5bf9\u5b83\u7684\u89e3\u91ca\u5c31\u4f1a\u975e\u5e38\u5bb9\u6613\u4e86\u3002","title":"\u524d\u8a00"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/","text":"Linux namespaces # Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces # With the advent of tools like Docker , Linux Containers , and others, it has become super easy to isolate Linux processes into their own little system environments. This makes it possible to run a whole range of applications on a single real Linux machine and ensure no two of them can interfere with each other, without having to resort to using virtual machines . These tools have been a huge boon to PaaS providers. But what exactly happens under the hood? These tools rely on a number of features and components of the Linux kernel. Some of these features were introduced fairly recently, while others still require you to patch the kernel itself. But one of the key components, using Linux namespaces , has been a feature of Linux since version 2.6.24 was released in 2008. Anyone familiar with chroot already has a basic idea of what Linux namespaces can do and how to use namespace generally. Just as chroot allows processes to see any arbitrary directory as the root of the system (independent of the rest of the processes), Linux namespaces allow other aspects of the operating system to be independently modified as well. This includes the process tree , networking interfaces, mount points , inter-process communication resources and more. Why Use Namespaces for Process Isolation? # In a single-user computer, a single system environment may be fine. But on a server, where you want to run multiple services, it is essential to security and stability that the services are as isolated from each other as possible. Imagine a server running multiple services, one of which gets compromised by an intruder. In such a case, the intruder may be able to exploit that service and work his way to the other services, and may even be able compromise the entire server. Namespace isolation can provide a secure environment to eliminate this risk. For example, using namespacing, it is possible to safely execute arbitrary or unknown programs on your server. Recently, there has been a growing number of programming contest and \u201chackathon\u201d platforms, such as HackerRank , TopCoder , Codeforces , and many more. A lot of them utilize automated pipelines to run and validate programs that are submitted by the contestants. It is often impossible to know in advance the true nature of contestants\u2019 programs, and some may even contain malicious elements. By running these programs namespaced in complete isolation from the rest of the system, the software can be tested and validated without putting the rest of the machine at risk. Similarly, online continuous integration services, such as Drone.io , automatically fetch your code repository and execute the test scripts on their own servers. Again, namespace isolation is what makes it possible to provide these services safely. Namespacing tools like Docker also allow better control over processes\u2019 use of system resources, making such tools extremely popular for use by PaaS providers. Services like Heroku and Google App Engine use such tools to isolate and run multiple web server applications on the same real hardware. These tools allow them to run each application (which may have been deployed by any of a number of different users) without worrying about one of them using too many system resources, or interfering and/or conflicting with other deployed services on the same machine. With such process isolation, it is even possible to have entirely different stacks of dependency softwares (and versions) for each isolated environment! If you\u2019ve used tools like Docker, you already know that these tools are capable of isolating processes in small \u201ccontainers\u201d. Running processes in Docker containers is like running them in virtual machines, only these containers are significantly lighter than virtual machines. Process Namespace # Historically, the Linux kernel has maintained a single process tree . The tree contains a reference to every process currently running in a parent-child hierarchy . A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. With the introduction of Linux namespaces , it became possible to have multiple \u201cnested\u201d process trees. Each process tree can have an entirely isolated set of processes. This can ensure that processes belonging to one process tree cannot inspect or kill - in fact cannot even know of the existence of - processes in other sibling or parent process trees. Every time a computer with Linux boots up, it starts with just one process, with process identifier (PID) 1. This process is the root of the process tree , and it initiates the rest of the system by performing the appropriate maintenance work and starting the correct daemons/services. All the other processes start below this process in the tree. The PID namespace allows one to spin off a new tree, with its own PID 1 process . The process that does this remains in the parent namespace , in the original tree, but makes the child the root of its own process tree . With PID namespace isolation , processes in the child namespace have no way of knowing of the parent process\u2019s existence. However, processes in the parent namespace have a complete view of processes in the child namespace , as if they were any other process in the parent namespace . It is possible to create a nested set of child namespaces : one process starts a child process in a new PID namespace, and that child process spawns yet another process in a new PID namespace, and so on. With the introduction of PID namespaces , a single process can now have multiple PIDs associated with it, one for each namespace it falls under. In the Linux source code, we can see that a struct named pid , which used to keep track of just a single PID, now tracks multiple PIDs through the use of a struct named upid : struct upid { int nr; // the PID value struct pid_namespace *ns; // namespace where this PID is relevant // ... }; struct pid { // ... int level; // number of upids struct upid numbers[0]; // array of upids }; To create a new PID namespace , one must call the clone() system call with a special flag CLONE_NEWPID . (C provides a wrapper to expose this system call, and so do many other popular languages.) Whereas the other namespaces discussed below can also be created using the unshare() system call, a PID namespace can only be created at the time a new process is spawned using clone() . Once clone() is called with this flag, the new process immediately starts in a new PID namespace, under a new process tree. This can be demonstrated with a simple C program: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"PID: %ld\\n\", (long)getpid()); return 0; } int main() { pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); printf(\"clone() = %ld\\n\", (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } Compile and run this program with root privileges and you will notice an output that resembles this: clone() = 5304 PID: 1 The PID, as printed from within the child_fn , will be 1 . Try replacing the static int child_fn() function with the following, to print the parent PID from the isolated process\u2019s perspective: static int child_fn() { printf(\"Parent PID: %ld\\n\", (long)getppid()); return 0; } Running the program this time yields the following output: clone() = 11449 Parent PID: 0 Notice how the parent PID from the isolated process\u2019s perspective is 0, indicating no parent. Try running the same program again, but this time, remove the CLONE_NEWPID flag from within the clone() function call: pid_t child_pid = clone(child_fn, child_stack+1048576, SIGCHLD, NULL); This time, you will notice that the parent PID is no longer 0: clone() = 11561 Parent PID: 11560 However, this is just the first step in our tutorial. These processes still have unrestricted access to other common or shared resources. For example, the networking interface: if the child process created above were to listen on port 80, it would prevent every other process on the system from being able to listen on it. Linux Network Namespace # This is where a network namespace becomes useful. A network namespace allows each of these processes to see an entirely different set of networking interfaces . Even the loopback interface is different for each network namespace. Isolating a process into its own network namespace involves introducing another flag to the clone() function call: CLONE_NEWNET ; #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } Output: Original `net` Namespace: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:24:8c:a1:ac:e7 brd ff:ff:ff:ff:ff:ff New `net` Namespace: 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 What\u2019s going on here? The physical ethernet device enp4s0 belongs to the global network namespace , as indicated by the \u201c ip \u201d tool run from this namespace. However, the physical interface is not available in the new network namespace . Moreover, the loopback device is active in the original network namespace , but is \u201cdown\u201d in the child network namespace . In order to provide a usable network interface in the child namespace , it is necessary to set up additional \u201cvirtual\u201d network interfaces which span multiple namespaces. Once that is done, it is then possible to create Ethernet bridges , and even route packets between the namespaces. Finally, to make the whole thing work, a \u201crouting process\u201d must be running in the global network namespace to receive traffic from the physical interface, and route it through the appropriate virtual interfaces to to the correct child network namespaces . Maybe you can see why tools like Docker, which do all this heavy lifting for you, are so popular! To do this by hand, you can create a pair of virtual Ethernet connections between a parent and a child namespace by running a single command from the parent namespace : ip link add name veth0 type veth peer name veth1 netns <pid> Here, <pid> should be replaced by the process ID of the process in the child namespace as observed by the parent. Running this command establishes a pipe-like connection between these two namespaces. The parent namespace retains the veth0 device, and passes the veth1 device to the child namespace. Anything that enters one of the ends, comes out through the other end, just as you would expect from a real Ethernet connection between two real nodes. Accordingly, both sides of this virtual Ethernet connection must be assigned IP addresses. Mount Namespace # Linux also maintains a data structure for all the mountpoints of the system. It includes information like what disk partitions are mounted, where they are mounted, whether they are readonly, et cetera. With Linux namespaces, one can have this data structure cloned, so that processes under different namespaces can change the mountpoints without affecting each other. Creating separate mount namespace has an effect similar to doing a chroot() . chroot() is good, but it does not provide complete isolation, and its effects are restricted to the root mountpoint only. Creating a separate mount namespace allows each of these isolated processes to have a completely different view of the entire system\u2019s mountpoint structure from the original one. This allows you to have a different root for each isolated process, as well as other mountpoints that are specific to those processes. Used with care per this tutorial, you can avoid exposing any information about the underlying system. The clone() flag required to achieve this is CLONE_NEWNS : clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | CLONE_NEWNS | SIGCHLD, NULL) Initially, the child process sees the exact same mountpoints as its parent process would. However, being under a new mount namespace , the child process can mount or unmount whatever endpoints it wants to, and the change will affect neither its parent\u2019s namespace, nor any other mount namespace in the entire system. For example, if the parent process has a particular disk partition mounted at root, the isolated process will see the exact same disk partition mounted at the root in the beginning. But the benefit of isolating the mount namespace is apparent when the isolated process tries to change the root partition to something else, as the change will only affect the isolated mount namespace. Interestingly, this actually makes it a bad idea to spawn the target child process directly with the CLONE_NEWNS flag. A better approach is to start a special \u201cinit\u201d process with the CLONE_NEWNS flag, have that \u201cinit\u201d process change the \u201c/\u201d, \u201c/proc\u201d, \u201c/dev\u201d or other mountpoints as desired, and then start the target process. This is discussed in a little more detail near the end of this namespace tutorial. Other Namespaces # There are other namespaces that these processes can be isolated into, namely user, IPC, and UTS. The user namespace allows a process to have root privileges within the namespace , without giving it that access to processes outside of the namespace. Isolating a process by the IPC namespace gives it its own interprocess communication resources, for example, System V IPC and POSIX messages. The UTS namespace isolates two specific identifiers of the system: nodename and domainname . A quick example to show how UTS namespace is isolated is shown below: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/utsname.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static void print_nodename() { struct utsname utsname; uname(&utsname); printf(\"%s\\n\", utsname.nodename); } static int child_fn() { printf(\"New UTS namespace nodename: \"); print_nodename(); printf(\"Changing nodename inside new UTS namespace\\n\"); sethostname(\"GLaDOS\", 6); printf(\"New UTS namespace nodename: \"); print_nodename(); return 0; } int main() { printf(\"Original UTS namespace nodename: \"); print_nodename(); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWUTS | SIGCHLD, NULL); sleep(1); printf(\"Original UTS namespace nodename: \"); print_nodename(); waitpid(child_pid, NULL, 0); return 0; } This program yields the following output: Original UTS namespace nodename: XT New UTS namespace nodename: XT Changing nodename inside new UTS namespace New UTS namespace nodename: GLaDOS Original UTS namespace nodename: XT Here, child_fn() prints the nodename , changes it to something else, and prints it again. Naturally, the change happens only inside the new UTS namespace. More information on what all of the namespaces provide and isolate can be found in the tutorial here Cross-Namespace Communication # Often it is necessary to establish some sort of communication between the parent and the child namespace. This might be for doing configuration work within an isolated environment, or it can simply be to retain the ability to peek into the condition of that environment from outside. One way of doing that is to keep an SSH daemon running within that environment. You can have a separate SSH daemon inside each network namespace. However, having multiple SSH daemons running uses a lot of valuable resources like memory. This is where having a special \u201cinit\u201d process proves to be a good idea again. The \u201cinit\u201d process can establish a communication channel between the parent namespace and the child namespace. This channel can be based on UNIX sockets or can even use TCP. To create a UNIX socket that spans two different mount namespaces, you need to first create the child process, then create the UNIX socket, and then isolate the child into a separate mount namespace. But how can we create the process first, and isolate it later? Linux provides unshare() . This special system call allows a process to isolate itself from the original namespace, instead of having the parent isolate the child in the first place. For example, the following code has the exact same effect as the code previously mentioned in the network namespace section: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { // calling unshare() from inside the init process lets you create a new namespace after a new process has been spawned unshare(CLONE_NEWNET); printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } And since the \u201cinit\u201d process is something you have devised, you can make it do all the necessary work first, and then isolate itself from the rest of the system before executing the target child. Conclusion # This tutorial is just an overview of how to use namespaces in Linux. It should give you a basic idea of how a Linux developer might start to implement system isolation, an integral part of the architecture of tools like Docker or Linux Containers. In most cases, it would be best to simply use one of these existing tools, which are already well-known and tested. But in some cases, it might make sense to have your very own, customized process isolation mechanism, and in that case, this namespace tutorial will help you out tremendously. There is a lot more going on under the hood than I\u2019ve covered in this article, and there are more ways you might want to limit your target processes for added safety and isolation. But, hopefully, this can serve as a useful starting point for someone who is interested in knowing more about how namespace isolation with Linux really works. Tags Sandboxing SystemIsolation Linux Namespaces","title":"Linux-Namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#linux-namespaces","text":"","title":"Linux namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#separation-anxiety-a-tutorial-for-isolating-your-system-with-linux-namespaces","text":"With the advent of tools like Docker , Linux Containers , and others, it has become super easy to isolate Linux processes into their own little system environments. This makes it possible to run a whole range of applications on a single real Linux machine and ensure no two of them can interfere with each other, without having to resort to using virtual machines . These tools have been a huge boon to PaaS providers. But what exactly happens under the hood? These tools rely on a number of features and components of the Linux kernel. Some of these features were introduced fairly recently, while others still require you to patch the kernel itself. But one of the key components, using Linux namespaces , has been a feature of Linux since version 2.6.24 was released in 2008. Anyone familiar with chroot already has a basic idea of what Linux namespaces can do and how to use namespace generally. Just as chroot allows processes to see any arbitrary directory as the root of the system (independent of the rest of the processes), Linux namespaces allow other aspects of the operating system to be independently modified as well. This includes the process tree , networking interfaces, mount points , inter-process communication resources and more.","title":"Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#why-use-namespaces-for-process-isolation","text":"In a single-user computer, a single system environment may be fine. But on a server, where you want to run multiple services, it is essential to security and stability that the services are as isolated from each other as possible. Imagine a server running multiple services, one of which gets compromised by an intruder. In such a case, the intruder may be able to exploit that service and work his way to the other services, and may even be able compromise the entire server. Namespace isolation can provide a secure environment to eliminate this risk. For example, using namespacing, it is possible to safely execute arbitrary or unknown programs on your server. Recently, there has been a growing number of programming contest and \u201chackathon\u201d platforms, such as HackerRank , TopCoder , Codeforces , and many more. A lot of them utilize automated pipelines to run and validate programs that are submitted by the contestants. It is often impossible to know in advance the true nature of contestants\u2019 programs, and some may even contain malicious elements. By running these programs namespaced in complete isolation from the rest of the system, the software can be tested and validated without putting the rest of the machine at risk. Similarly, online continuous integration services, such as Drone.io , automatically fetch your code repository and execute the test scripts on their own servers. Again, namespace isolation is what makes it possible to provide these services safely. Namespacing tools like Docker also allow better control over processes\u2019 use of system resources, making such tools extremely popular for use by PaaS providers. Services like Heroku and Google App Engine use such tools to isolate and run multiple web server applications on the same real hardware. These tools allow them to run each application (which may have been deployed by any of a number of different users) without worrying about one of them using too many system resources, or interfering and/or conflicting with other deployed services on the same machine. With such process isolation, it is even possible to have entirely different stacks of dependency softwares (and versions) for each isolated environment! If you\u2019ve used tools like Docker, you already know that these tools are capable of isolating processes in small \u201ccontainers\u201d. Running processes in Docker containers is like running them in virtual machines, only these containers are significantly lighter than virtual machines.","title":"Why Use Namespaces for Process Isolation?"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#process-namespace","text":"Historically, the Linux kernel has maintained a single process tree . The tree contains a reference to every process currently running in a parent-child hierarchy . A process, given it has sufficient privileges and satisfies certain conditions, can inspect another process by attaching a tracer to it or may even be able to kill it. With the introduction of Linux namespaces , it became possible to have multiple \u201cnested\u201d process trees. Each process tree can have an entirely isolated set of processes. This can ensure that processes belonging to one process tree cannot inspect or kill - in fact cannot even know of the existence of - processes in other sibling or parent process trees. Every time a computer with Linux boots up, it starts with just one process, with process identifier (PID) 1. This process is the root of the process tree , and it initiates the rest of the system by performing the appropriate maintenance work and starting the correct daemons/services. All the other processes start below this process in the tree. The PID namespace allows one to spin off a new tree, with its own PID 1 process . The process that does this remains in the parent namespace , in the original tree, but makes the child the root of its own process tree . With PID namespace isolation , processes in the child namespace have no way of knowing of the parent process\u2019s existence. However, processes in the parent namespace have a complete view of processes in the child namespace , as if they were any other process in the parent namespace . It is possible to create a nested set of child namespaces : one process starts a child process in a new PID namespace, and that child process spawns yet another process in a new PID namespace, and so on. With the introduction of PID namespaces , a single process can now have multiple PIDs associated with it, one for each namespace it falls under. In the Linux source code, we can see that a struct named pid , which used to keep track of just a single PID, now tracks multiple PIDs through the use of a struct named upid : struct upid { int nr; // the PID value struct pid_namespace *ns; // namespace where this PID is relevant // ... }; struct pid { // ... int level; // number of upids struct upid numbers[0]; // array of upids }; To create a new PID namespace , one must call the clone() system call with a special flag CLONE_NEWPID . (C provides a wrapper to expose this system call, and so do many other popular languages.) Whereas the other namespaces discussed below can also be created using the unshare() system call, a PID namespace can only be created at the time a new process is spawned using clone() . Once clone() is called with this flag, the new process immediately starts in a new PID namespace, under a new process tree. This can be demonstrated with a simple C program: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"PID: %ld\\n\", (long)getpid()); return 0; } int main() { pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); printf(\"clone() = %ld\\n\", (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } Compile and run this program with root privileges and you will notice an output that resembles this: clone() = 5304 PID: 1 The PID, as printed from within the child_fn , will be 1 . Try replacing the static int child_fn() function with the following, to print the parent PID from the isolated process\u2019s perspective: static int child_fn() { printf(\"Parent PID: %ld\\n\", (long)getppid()); return 0; } Running the program this time yields the following output: clone() = 11449 Parent PID: 0 Notice how the parent PID from the isolated process\u2019s perspective is 0, indicating no parent. Try running the same program again, but this time, remove the CLONE_NEWPID flag from within the clone() function call: pid_t child_pid = clone(child_fn, child_stack+1048576, SIGCHLD, NULL); This time, you will notice that the parent PID is no longer 0: clone() = 11561 Parent PID: 11560 However, this is just the first step in our tutorial. These processes still have unrestricted access to other common or shared resources. For example, the networking interface: if the child process created above were to listen on port 80, it would prevent every other process on the system from being able to listen on it.","title":"Process Namespace"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#linux-network-namespace","text":"This is where a network namespace becomes useful. A network namespace allows each of these processes to see an entirely different set of networking interfaces . Even the loopback interface is different for each network namespace. Isolating a process into its own network namespace involves introducing another flag to the clone() function call: CLONE_NEWNET ; #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } Output: Original `net` Namespace: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:24:8c:a1:ac:e7 brd ff:ff:ff:ff:ff:ff New `net` Namespace: 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 What\u2019s going on here? The physical ethernet device enp4s0 belongs to the global network namespace , as indicated by the \u201c ip \u201d tool run from this namespace. However, the physical interface is not available in the new network namespace . Moreover, the loopback device is active in the original network namespace , but is \u201cdown\u201d in the child network namespace . In order to provide a usable network interface in the child namespace , it is necessary to set up additional \u201cvirtual\u201d network interfaces which span multiple namespaces. Once that is done, it is then possible to create Ethernet bridges , and even route packets between the namespaces. Finally, to make the whole thing work, a \u201crouting process\u201d must be running in the global network namespace to receive traffic from the physical interface, and route it through the appropriate virtual interfaces to to the correct child network namespaces . Maybe you can see why tools like Docker, which do all this heavy lifting for you, are so popular! To do this by hand, you can create a pair of virtual Ethernet connections between a parent and a child namespace by running a single command from the parent namespace : ip link add name veth0 type veth peer name veth1 netns <pid> Here, <pid> should be replaced by the process ID of the process in the child namespace as observed by the parent. Running this command establishes a pipe-like connection between these two namespaces. The parent namespace retains the veth0 device, and passes the veth1 device to the child namespace. Anything that enters one of the ends, comes out through the other end, just as you would expect from a real Ethernet connection between two real nodes. Accordingly, both sides of this virtual Ethernet connection must be assigned IP addresses.","title":"Linux Network Namespace"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#mount-namespace","text":"Linux also maintains a data structure for all the mountpoints of the system. It includes information like what disk partitions are mounted, where they are mounted, whether they are readonly, et cetera. With Linux namespaces, one can have this data structure cloned, so that processes under different namespaces can change the mountpoints without affecting each other. Creating separate mount namespace has an effect similar to doing a chroot() . chroot() is good, but it does not provide complete isolation, and its effects are restricted to the root mountpoint only. Creating a separate mount namespace allows each of these isolated processes to have a completely different view of the entire system\u2019s mountpoint structure from the original one. This allows you to have a different root for each isolated process, as well as other mountpoints that are specific to those processes. Used with care per this tutorial, you can avoid exposing any information about the underlying system. The clone() flag required to achieve this is CLONE_NEWNS : clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | CLONE_NEWNS | SIGCHLD, NULL) Initially, the child process sees the exact same mountpoints as its parent process would. However, being under a new mount namespace , the child process can mount or unmount whatever endpoints it wants to, and the change will affect neither its parent\u2019s namespace, nor any other mount namespace in the entire system. For example, if the parent process has a particular disk partition mounted at root, the isolated process will see the exact same disk partition mounted at the root in the beginning. But the benefit of isolating the mount namespace is apparent when the isolated process tries to change the root partition to something else, as the change will only affect the isolated mount namespace. Interestingly, this actually makes it a bad idea to spawn the target child process directly with the CLONE_NEWNS flag. A better approach is to start a special \u201cinit\u201d process with the CLONE_NEWNS flag, have that \u201cinit\u201d process change the \u201c/\u201d, \u201c/proc\u201d, \u201c/dev\u201d or other mountpoints as desired, and then start the target process. This is discussed in a little more detail near the end of this namespace tutorial.","title":"Mount Namespace"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#other-namespaces","text":"There are other namespaces that these processes can be isolated into, namely user, IPC, and UTS. The user namespace allows a process to have root privileges within the namespace , without giving it that access to processes outside of the namespace. Isolating a process by the IPC namespace gives it its own interprocess communication resources, for example, System V IPC and POSIX messages. The UTS namespace isolates two specific identifiers of the system: nodename and domainname . A quick example to show how UTS namespace is isolated is shown below: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/utsname.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static void print_nodename() { struct utsname utsname; uname(&utsname); printf(\"%s\\n\", utsname.nodename); } static int child_fn() { printf(\"New UTS namespace nodename: \"); print_nodename(); printf(\"Changing nodename inside new UTS namespace\\n\"); sethostname(\"GLaDOS\", 6); printf(\"New UTS namespace nodename: \"); print_nodename(); return 0; } int main() { printf(\"Original UTS namespace nodename: \"); print_nodename(); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWUTS | SIGCHLD, NULL); sleep(1); printf(\"Original UTS namespace nodename: \"); print_nodename(); waitpid(child_pid, NULL, 0); return 0; } This program yields the following output: Original UTS namespace nodename: XT New UTS namespace nodename: XT Changing nodename inside new UTS namespace New UTS namespace nodename: GLaDOS Original UTS namespace nodename: XT Here, child_fn() prints the nodename , changes it to something else, and prints it again. Naturally, the change happens only inside the new UTS namespace. More information on what all of the namespaces provide and isolate can be found in the tutorial here","title":"Other Namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#cross-namespace-communication","text":"Often it is necessary to establish some sort of communication between the parent and the child namespace. This might be for doing configuration work within an isolated environment, or it can simply be to retain the ability to peek into the condition of that environment from outside. One way of doing that is to keep an SSH daemon running within that environment. You can have a separate SSH daemon inside each network namespace. However, having multiple SSH daemons running uses a lot of valuable resources like memory. This is where having a special \u201cinit\u201d process proves to be a good idea again. The \u201cinit\u201d process can establish a communication channel between the parent namespace and the child namespace. This channel can be based on UNIX sockets or can even use TCP. To create a UNIX socket that spans two different mount namespaces, you need to first create the child process, then create the UNIX socket, and then isolate the child into a separate mount namespace. But how can we create the process first, and isolate it later? Linux provides unshare() . This special system call allows a process to isolate itself from the original namespace, instead of having the parent isolate the child in the first place. For example, the following code has the exact same effect as the code previously mentioned in the network namespace section: #define _GNU_SOURCE #include <sched.h> #include <stdio.h> #include <stdlib.h> #include <sys/wait.h> #include <unistd.h> static char child_stack[1048576]; static int child_fn() { // calling unshare() from inside the init process lets you create a new namespace after a new process has been spawned unshare(CLONE_NEWNET); printf(\"New `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); return 0; } int main() { printf(\"Original `net` Namespace:\\n\"); system(\"ip link\"); printf(\"\\n\\n\"); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } And since the \u201cinit\u201d process is something you have devised, you can make it do all the necessary work first, and then isolate itself from the rest of the system before executing the target child.","title":"Cross-Namespace Communication"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/Linux-Namespaces/#conclusion","text":"This tutorial is just an overview of how to use namespaces in Linux. It should give you a basic idea of how a Linux developer might start to implement system isolation, an integral part of the architecture of tools like Docker or Linux Containers. In most cases, it would be best to simply use one of these existing tools, which are already well-known and tested. But in some cases, it might make sense to have your very own, customized process isolation mechanism, and in that case, this namespace tutorial will help you out tremendously. There is a lot more going on under the hood than I\u2019ve covered in this article, and there are more ways you might want to limit your target processes for added safety and isolation. But, hopefully, this can serve as a useful starting point for someone who is interested in knowing more about how namespace isolation with Linux really works. Tags Sandboxing SystemIsolation Linux Namespaces","title":"Conclusion"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/","text":"NAMESPACES(7) CGROUP_NAMESPACES(7) IPC_NAMESPACES(7) NETWORK_NAMESPACES(7) MOUNT_NAMESPACES(7) PID_NAMESPACES(7) USER_NAMESPACES(7) UTS_NAMESPACES(7) NAMESPACES(7) # CGROUP_NAMESPACES(7) # IPC_NAMESPACES(7) # NETWORK_NAMESPACES(7) # MOUNT_NAMESPACES(7) # PID_NAMESPACES(7) # USER_NAMESPACES(7) # UTS_NAMESPACES(7) #","title":"man-7-namespaces"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#namespaces7","text":"","title":"NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#cgroup_namespaces7","text":"","title":"CGROUP_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#ipc_namespaces7","text":"","title":"IPC_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#network_namespaces7","text":"","title":"NETWORK_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#mount_namespaces7","text":"","title":"MOUNT_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#pid_namespaces7","text":"","title":"PID_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#user_namespaces7","text":"","title":"USER_NAMESPACES(7)"},{"location":"Programming/05-Linux-Virtualization/Linux-namespaces/man-7-namespaces/#uts_namespaces7","text":"","title":"UTS_NAMESPACES(7)"},{"location":"Programming/06-Util/","text":"","title":"Introduction"},{"location":"Programming/06-Util/Log/Linux-log-files/","text":"LinuxLogFiles #","title":"Linux-log-files"},{"location":"Programming/06-Util/Log/Linux-log-files/#linuxlogfiles","text":"","title":"LinuxLogFiles"},{"location":"Programming/06-Util/Log/Syslog/Syslog/","text":"syslog # Syslog Tutorial: How It Works, Examples, Best Practices, and More 10.5 The UNIX System Log (syslog) Facility","title":"Syslog"},{"location":"Programming/06-Util/Log/Syslog/Syslog/#syslog","text":"Syslog Tutorial: How It Works, Examples, Best Practices, and More 10.5 The UNIX System Log (syslog) Facility","title":"syslog"},{"location":"Programming/07-Linux-Kernel-module/Loadable-kernel-module/","text":"Loadable kernel module # The Linux Kernel Module Programming Guide #","title":"Loadable-kernel-module"},{"location":"Programming/07-Linux-Kernel-module/Loadable-kernel-module/#loadable-kernel-module","text":"","title":"Loadable kernel module"},{"location":"Programming/07-Linux-Kernel-module/Loadable-kernel-module/#the-linux-kernel-module-programming-guide","text":"","title":"The Linux Kernel Module Programming Guide"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/","text":"How do I read the output of dmesg to determine how much memory a process is using when oom-killer is invoked? How to translate kernel's trap divide error rsp:2b6d2ea40450 to a source location? A How do you read a segfault kernel log message A When the report points to a program, not a shared library If it's a shared library What the error means A A How do I read the output of dmesg to determine how much memory a process is using when oom-killer is invoked? # How to translate kernel's trap divide error rsp:2b6d2ea40450 to a source location? # Customer reported an error in one of our programs caused by division by zero. We have only this VLM line: kernel: myprog[16122] trap divide error rip:79dd99 rsp:2b6d2ea40450 error:0 I do not believe there is core file for that. I searched through the Internet to find how I can tell the line of the program that caused this division by zero, but so far I am failing. I understand that 16122 is pid of the program, so that will not help me. I suspect that rsp:2b6d2ea40450 has something to do with the address of the line that caused the error ( 0x2b6d2ea40450 ) but is that true? If it is then how can I translate it to a physical approximate location in the source assuming I can load debug version of myprog into gdb, and then request to show the context around this address... Any, any help will be greatly appreciated! A # ip is the instruction pointer , rsp is the stack pointer . The stack pointer is not too useful unless you have a core image or a running process. You can use either addr2line or the disassemble command in gdb to see the line that got the error, based on the ip . $ cat divtest.c main() { int a, b; a = 1; b = a/0; } $ ./divtest Floating point exception (core dumped) $ dmesg|tail -1 [ 6827.463256] traps: divtest[3255] trap divide error ip:400504 sp:7fff54e81330 error:0 in divtest[400000+1000] $ addr2line -e divtest 400504 ./divtest.c:5 $ gdb divtest (gdb) disass /m 0x400504 Dump of assembler code for function main: 2 { 0x00000000004004f0 : push %rbp 0x00000000004004f1 : mov %rsp,%rbp 3 int a, b; 4 5 a = 1; b = a/0; 0x00000000004004f4 : movl $0x1,-0x4(%rbp) 0x00000000004004fb : mov -0x4(%rbp),%eax 0x00000000004004fe : mov $0x0,%ecx 0x0000000000400503 : cltd 0x0000000000400504 : idiv %ecx 0x0000000000400506 : mov %eax,-0x8(%rbp) 6 } 0x0000000000400509 : pop %rbp 0x000000000040050a : retq End of assembler dump. How do you read a segfault kernel log message # This can be a very simple question, I'm am attempting to debug an application which generates the following segfault error in the kern.log kernel: myapp[15514]: segfault at 794ef0 ip 080513b sp 794ef0 error 6 in myapp[8048000+24000] Here are my questions: Is there any documentation as to what are the diff error numbers on segfault, in this instance it is error 6, but i've seen error 4, 5 What is the meaning of the information at bf794ef0 ip 0805130b sp bf794ef0 and myapp[8048000+24000] ? So far i was able to compile with symbols, and when i do a x 0x8048000+24000 it returns a symbol, is that the correct way of doing it? My assumptions thus far are the following: sp = stack pointer? ip = instruction pointer at = ???? myapp[8048000+24000] = address of symbol? A # When the report points to a program, not a shared library # Run addr2line -e myapp 080513b (and repeat for the other instruction pointer values given) to see where the error is happening. Better, get a debug-instrumented build , and reproduce the problem under a debugger such as gdb. If it's a shared library # In the libfoo.so[NNNNNN+YYYY] part, the NNNNNN is where the library was loaded. Subtract this from the instruction pointer ( ip ) and you'll get the offset into the .so of the offending instruction. Then you can use objdump -DCgl libfoo.so and search for the instruction at that offset. You should easily be able to figure out which function it is from the asm labels. If the .so doesn't have optimizations you can also try using addr2line -e libfoo.so <offset> . What the error means # Here's the breakdown of the fields: address - the location in memory the code is trying to access (it's likely that 10 and 11 are offsets from a pointer we expect to be set to a valid value but which is instead pointing to 0 ) ip - instruction pointer, ie. where the code which is trying to do this lives sp - stack pointer error - Architecture-specific flags; see arch/*/mm/fault.c for your platform. COMMENTS : you gotta be wrong about error \u2013 Dima Tisnek Sep 12 '12 at 12:25 much better now \u2013 Dima Tisnek Sep 30 '12 at 9:07 Event for a shared lib, the \"[8048000+24000]\" part should give a hint where the crashing segment of the lib was mapped in memory. \"readelf --segments mylib.so\" lists these segments, and then you can calculate the EIP offset into the crashing segment and feed that to addr2line (or view it in \"objdump -dgS\"). \u2013 oliver Jun 13 '13 at 17:18 I believe 0x8048000 is (probably) the address where the text segment was mapped, so you will want to pass -j .text to the objdump command. (At least, that is what I needed when diagnosing one of these just now.) \u2013 Nemo Jun 5 '14 at 16:30 @Charles Duffy If I ever see you I will hug like I never hugged a living soul. \u2013 Baroudi Safwen Jan 11 '18 at 18:47 A # Based on my limited knowledge, your assumptions are correct. sp = stack pointer ip = instruction pointer myapp[8048000+24000] = address If I were debugging the problem I would modify the code to produce a core dump or log a stack backtrace on the crash. You might also run the program under (or attach) GDB. The error code is just the architectural error code for page faults and seems to be architecture specific. They are often documented in arch/*/mm/fault.c in the kernel source. My copy of Linux/arch/i386/mm/fault.c has the following definition for error_code: bit 0 == 0 means no page found, 1 means protection fault bit 1 == 0 means read, 1 means write bit 2 == 0 means kernel, 1 means user-mode My copy of Linux/arch/x86_64/mm/fault.c adds the following: bit 3 == 1 means fault was an instruction fetch Beat me to it :) \u2013 David Titarenco Feb 1 '10 at 19:34 The issue i have is that: 1) The application is segfaulting in a production environment, where symbols are stripped, all i have is just the logs 2) I'm trying to find that memory location in the development env, so at least i can see where it is crashing. \u2013 Sullenx Feb 1 '10 at 19:42 1 If you have the pre-stripped binary, try running it through nm or objdump. \u2013 jschmier Feb 1 '10 at 19:52 nm is pretty helpful, at least I have an idea where the crash happened. One last thing, what is an error 6? ... is there any table out there? \u2013 Sullenx Feb 1 '10 at 20:07 I updated my answer to include the error code. \u2013 jschmier Feb 1 '10 at 20:47 3 segfault at 794ef0 ... sp 794ef0 - stack is obviously corrupted. \u2013 Nikolai Fetissov Feb 1 '10 at 20:54 Thank you, this is very helpful \u2013 Sullenx Feb 1 '10 at 20:56 A # If it's a shared library You're hosed, unfortunately; it's not possible to know where the libraries were placed in memory by the dynamic linker after-the-fact . Well, there is still a possibility to retrieve the information, not from the binary, but from the object. But you need the base address of the object. And this information still is within the coredump, in the link_map structure. So first you want to import the struct link_map into GDB. So lets compile a program with it with debug symbol and add it to the GDB. link.c #include <link.h> toto(){struct link_map * s = 0x400;} get_baseaddr_from_coredump.sh #!/bin/bash BINARY=$(which myapplication) IsBinPIE () { readelf -h $1|grep 'Type' |grep \"EXEC\">/dev/null || return 0 return 1 } Hex2Decimal () { export number=\"`echo \"$1\" | sed -e 's:^0[xX]::' | tr '[a-f]' '[A-F]'`\" export number=`echo \"ibase=16; $number\" | bc` } GetBinaryLength () { if [ $# != 1 ]; then echo \"Error, no argument provided\" fi IsBinPIE $1 || (echo \"ET_EXEC file, need a base_address\"; exit 0) export totalsize=0 # Get PT_LOAD's size segment out of Program Header Table (ELF format) export sizes=\"$(readelf -l $1 |grep LOAD |awk '{print $6}'|tr '\\n' ' ')\" for size in $sizes do Hex2Decimal \"$size\"; export totalsize=$(expr $number + $totalsize); export totalsize=$(expr $number + $totalsize) done return $totalsize } if [ $# = 1 ]; then echo \"Using binary $1\" IsBinPIE $1 && (echo \"NOT ET_EXEC, need a base_address...\"; exit 0) BINARY=$1 fi gcc -g3 -fPIC -shared link.c -o link.so GOTADDR=$(readelf -S $BINARY|grep -E '\\.got.plt[ \\t]'|awk '{print $4}') echo \"First do the following command :\" echo file $BINARY echo add-symbol-file ./link.so 0x0 read echo \"Now copy/paste the following into your gdb session with attached coredump\" cat <<EOF set \\$linkmapaddr = *(0x$GOTADDR + 4) set \\$mylinkmap = (struct link_map *) \\$linkmapaddr while (\\$mylinkmap != 0) if (\\$mylinkmap->l_addr) printf \"add-symbol-file .%s %#.08x\\n\", \\$mylinkmap->l_name, \\$mylinkmap->l_addr end set \\$mylinkmap = \\$mylinkmap->l_next end it will print you the whole link_map content, within a set of GDB command. It itself it might seems unnesseray but with the base_addr of the shared object we are about, you might get some more information out of an address by debuging directly the involved shared object in another GDB instance. Keep the first gdb to have an idee of the symbol. NOTE : the script is rather incomplete i suspect you may add to the second parameter of add-symbol-file printed the sum with this value : readelf -S $SO_PATH|grep -E '\\.text[ \\t]'|awk '{print $5}' where $SO_PATH is the first argument of the add-symbol-file Hope it helps","title":"Dmesg-format"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#how-do-i-read-the-output-of-dmesg-to-determine-how-much-memory-a-process-is-using-when-oom-killer-is-invoked","text":"","title":"How do I read the output of dmesg to determine how much memory a process is using when oom-killer is invoked?"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#how-to-translate-kernels-trap-divide-error-rsp2b6d2ea40450-to-a-source-location","text":"Customer reported an error in one of our programs caused by division by zero. We have only this VLM line: kernel: myprog[16122] trap divide error rip:79dd99 rsp:2b6d2ea40450 error:0 I do not believe there is core file for that. I searched through the Internet to find how I can tell the line of the program that caused this division by zero, but so far I am failing. I understand that 16122 is pid of the program, so that will not help me. I suspect that rsp:2b6d2ea40450 has something to do with the address of the line that caused the error ( 0x2b6d2ea40450 ) but is that true? If it is then how can I translate it to a physical approximate location in the source assuming I can load debug version of myprog into gdb, and then request to show the context around this address... Any, any help will be greatly appreciated!","title":"How to translate kernel's trap divide error rsp:2b6d2ea40450 to a source location?"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a","text":"ip is the instruction pointer , rsp is the stack pointer . The stack pointer is not too useful unless you have a core image or a running process. You can use either addr2line or the disassemble command in gdb to see the line that got the error, based on the ip . $ cat divtest.c main() { int a, b; a = 1; b = a/0; } $ ./divtest Floating point exception (core dumped) $ dmesg|tail -1 [ 6827.463256] traps: divtest[3255] trap divide error ip:400504 sp:7fff54e81330 error:0 in divtest[400000+1000] $ addr2line -e divtest 400504 ./divtest.c:5 $ gdb divtest (gdb) disass /m 0x400504 Dump of assembler code for function main: 2 { 0x00000000004004f0 : push %rbp 0x00000000004004f1 : mov %rsp,%rbp 3 int a, b; 4 5 a = 1; b = a/0; 0x00000000004004f4 : movl $0x1,-0x4(%rbp) 0x00000000004004fb : mov -0x4(%rbp),%eax 0x00000000004004fe : mov $0x0,%ecx 0x0000000000400503 : cltd 0x0000000000400504 : idiv %ecx 0x0000000000400506 : mov %eax,-0x8(%rbp) 6 } 0x0000000000400509 : pop %rbp 0x000000000040050a : retq End of assembler dump.","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#how-do-you-read-a-segfault-kernel-log-message","text":"This can be a very simple question, I'm am attempting to debug an application which generates the following segfault error in the kern.log kernel: myapp[15514]: segfault at 794ef0 ip 080513b sp 794ef0 error 6 in myapp[8048000+24000] Here are my questions: Is there any documentation as to what are the diff error numbers on segfault, in this instance it is error 6, but i've seen error 4, 5 What is the meaning of the information at bf794ef0 ip 0805130b sp bf794ef0 and myapp[8048000+24000] ? So far i was able to compile with symbols, and when i do a x 0x8048000+24000 it returns a symbol, is that the correct way of doing it? My assumptions thus far are the following: sp = stack pointer? ip = instruction pointer at = ???? myapp[8048000+24000] = address of symbol?","title":"How do you read a segfault kernel log message"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a_1","text":"","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#when-the-report-points-to-a-program-not-a-shared-library","text":"Run addr2line -e myapp 080513b (and repeat for the other instruction pointer values given) to see where the error is happening. Better, get a debug-instrumented build , and reproduce the problem under a debugger such as gdb.","title":"When the report points to a program, not a shared library"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#if-its-a-shared-library","text":"In the libfoo.so[NNNNNN+YYYY] part, the NNNNNN is where the library was loaded. Subtract this from the instruction pointer ( ip ) and you'll get the offset into the .so of the offending instruction. Then you can use objdump -DCgl libfoo.so and search for the instruction at that offset. You should easily be able to figure out which function it is from the asm labels. If the .so doesn't have optimizations you can also try using addr2line -e libfoo.so <offset> .","title":"If it's a shared library"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#what-the-error-means","text":"Here's the breakdown of the fields: address - the location in memory the code is trying to access (it's likely that 10 and 11 are offsets from a pointer we expect to be set to a valid value but which is instead pointing to 0 ) ip - instruction pointer, ie. where the code which is trying to do this lives sp - stack pointer error - Architecture-specific flags; see arch/*/mm/fault.c for your platform. COMMENTS : you gotta be wrong about error \u2013 Dima Tisnek Sep 12 '12 at 12:25 much better now \u2013 Dima Tisnek Sep 30 '12 at 9:07 Event for a shared lib, the \"[8048000+24000]\" part should give a hint where the crashing segment of the lib was mapped in memory. \"readelf --segments mylib.so\" lists these segments, and then you can calculate the EIP offset into the crashing segment and feed that to addr2line (or view it in \"objdump -dgS\"). \u2013 oliver Jun 13 '13 at 17:18 I believe 0x8048000 is (probably) the address where the text segment was mapped, so you will want to pass -j .text to the objdump command. (At least, that is what I needed when diagnosing one of these just now.) \u2013 Nemo Jun 5 '14 at 16:30 @Charles Duffy If I ever see you I will hug like I never hugged a living soul. \u2013 Baroudi Safwen Jan 11 '18 at 18:47","title":"What the error means"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a_2","text":"Based on my limited knowledge, your assumptions are correct. sp = stack pointer ip = instruction pointer myapp[8048000+24000] = address If I were debugging the problem I would modify the code to produce a core dump or log a stack backtrace on the crash. You might also run the program under (or attach) GDB. The error code is just the architectural error code for page faults and seems to be architecture specific. They are often documented in arch/*/mm/fault.c in the kernel source. My copy of Linux/arch/i386/mm/fault.c has the following definition for error_code: bit 0 == 0 means no page found, 1 means protection fault bit 1 == 0 means read, 1 means write bit 2 == 0 means kernel, 1 means user-mode My copy of Linux/arch/x86_64/mm/fault.c adds the following: bit 3 == 1 means fault was an instruction fetch Beat me to it :) \u2013 David Titarenco Feb 1 '10 at 19:34 The issue i have is that: 1) The application is segfaulting in a production environment, where symbols are stripped, all i have is just the logs 2) I'm trying to find that memory location in the development env, so at least i can see where it is crashing. \u2013 Sullenx Feb 1 '10 at 19:42 1 If you have the pre-stripped binary, try running it through nm or objdump. \u2013 jschmier Feb 1 '10 at 19:52 nm is pretty helpful, at least I have an idea where the crash happened. One last thing, what is an error 6? ... is there any table out there? \u2013 Sullenx Feb 1 '10 at 20:07 I updated my answer to include the error code. \u2013 jschmier Feb 1 '10 at 20:47 3 segfault at 794ef0 ... sp 794ef0 - stack is obviously corrupted. \u2013 Nikolai Fetissov Feb 1 '10 at 20:54 Thank you, this is very helpful \u2013 Sullenx Feb 1 '10 at 20:56","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-format/#a_3","text":"If it's a shared library You're hosed, unfortunately; it's not possible to know where the libraries were placed in memory by the dynamic linker after-the-fact . Well, there is still a possibility to retrieve the information, not from the binary, but from the object. But you need the base address of the object. And this information still is within the coredump, in the link_map structure. So first you want to import the struct link_map into GDB. So lets compile a program with it with debug symbol and add it to the GDB. link.c #include <link.h> toto(){struct link_map * s = 0x400;} get_baseaddr_from_coredump.sh #!/bin/bash BINARY=$(which myapplication) IsBinPIE () { readelf -h $1|grep 'Type' |grep \"EXEC\">/dev/null || return 0 return 1 } Hex2Decimal () { export number=\"`echo \"$1\" | sed -e 's:^0[xX]::' | tr '[a-f]' '[A-F]'`\" export number=`echo \"ibase=16; $number\" | bc` } GetBinaryLength () { if [ $# != 1 ]; then echo \"Error, no argument provided\" fi IsBinPIE $1 || (echo \"ET_EXEC file, need a base_address\"; exit 0) export totalsize=0 # Get PT_LOAD's size segment out of Program Header Table (ELF format) export sizes=\"$(readelf -l $1 |grep LOAD |awk '{print $6}'|tr '\\n' ' ')\" for size in $sizes do Hex2Decimal \"$size\"; export totalsize=$(expr $number + $totalsize); export totalsize=$(expr $number + $totalsize) done return $totalsize } if [ $# = 1 ]; then echo \"Using binary $1\" IsBinPIE $1 && (echo \"NOT ET_EXEC, need a base_address...\"; exit 0) BINARY=$1 fi gcc -g3 -fPIC -shared link.c -o link.so GOTADDR=$(readelf -S $BINARY|grep -E '\\.got.plt[ \\t]'|awk '{print $4}') echo \"First do the following command :\" echo file $BINARY echo add-symbol-file ./link.so 0x0 read echo \"Now copy/paste the following into your gdb session with attached coredump\" cat <<EOF set \\$linkmapaddr = *(0x$GOTADDR + 4) set \\$mylinkmap = (struct link_map *) \\$linkmapaddr while (\\$mylinkmap != 0) if (\\$mylinkmap->l_addr) printf \"add-symbol-file .%s %#.08x\\n\", \\$mylinkmap->l_name, \\$mylinkmap->l_addr end set \\$mylinkmap = \\$mylinkmap->l_next end it will print you the whole link_map content, within a set of GDB command. It itself it might seems unnesseray but with the base_addr of the shared object we are about, you might get some more information out of an address by debuging directly the involved shared object in another GDB instance. Keep the first gdb to have an idee of the symbol. NOTE : the script is rather incomplete i suspect you may add to the second parameter of add-symbol-file printed the sum with this value : readelf -S $SO_PATH|grep -E '\\.text[ \\t]'|awk '{print $5}' where $SO_PATH is the first argument of the add-symbol-file Hope it helps","title":"A"},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/","text":"What the Linux kernel's messages about segfaulting programs mean on 64-bit x86 # For quite a while the Linux kernel has had an option to log a kernel message about every faulting user program , and it probably defaults to on in your Linux distribution. I've seen these messages fly by for years, but for reasons beyond the scope of this entry I've recently wanted to understand what they mean in some moderate amount of detail. I'll start with a straightforward and typical example, one that I see every time I build and test Go (as this is a test case that is supposed to crash): testp[19288]: segfault at 0 ip 0000000000401271 sp 00007fff2ce4d210 error 4 in testp[400000+98000] The meaning of this is: ' testp[19288] ' is the faulting program and its PID ' segfault at 0 ' tells us the memory address (in hex) that caused the segfault when the program tried to access it. Here the address is 0, so we have a null dereference of some sort. ' ip 0000000000401271 ' is the value of the instruction pointer at the time of the fault. This should be the instruction that attempted to do the invalid memory access. In 64-bit x86, this will be register %rip (useful for inspecting things in GDB and elsewhere). ' sp 00007fff2ce4d210 ' is the value of the stack pointer. In 64-bit x86, this will be %rsp . ' error 4 ' is the page fault error code bits from traps.h in hex, as usual, and will almost always be at least 4 (which means 'user-mode access'). A value of 4 means it was a read of an unmapped area , such as address 0, while a value of 6 (4+2) means it was a write of an unmapped area . ' in testp[400000+98000] ' tells us the specific virtual memory area that the instruction pointer is in, specifying which file it is (here it's the executable), the starting address that VMA is mapped at ( 0x400000 ), and the size of the mapping ( 0x98000 ). With a faulting address of 0 and an error code of 4, we know this particular segfault is a read of a null pointer . Here's two more error messages: bash[12235]: segfault at 1054808 ip 000000000041d989 sp 00007ffec1f1cbd8 error 6 in bash[400000+f4000] 'Error 6' means a write to an unmapped user address , here 0x1054808 . bash[11909]: segfault at 0 ip 00007f83c03db746 sp 00007ffccbeda010 error 4 in libc-2.23.so[7f83c0350000+1c0000] Error 4 and address 0 is a null pointer read but this time it's in some libc function, not in bash's own code, since it's reported as 'in libc-2.23.so [...]'. Since I looked at the core dump, I can tell you that this was in strlen() . On 64-bit x86 Linux, you'll get a somewhat different message if the problem is actually with the instruction being executed, not the address it's referencing. For example: bash[2848] trap invalid opcode ip:48db90 sp:7ffddc8879e8 error:0 in bash[400000+f4000] There are a number of such trap types set up in traps.c . Two notable additional ones are 'divide error', which you get if you do an integer division by zero, and 'general protection', which you can get for certain extremely wild pointers (one case I know of is when your 64-bit x86 address is not in 'canonical form' ). Although these fields are formatted slightly differently, most of them mean the same thing as in segfaults. The exception is ' error:0 ', which is not a page fault error code. I don't understand the relevant kernel code enough to know what it means, but if I'm reading between the lines correctly in entry_64.txt , then it's either 0 (the usual case) or an error code from the CPU. Here is one possible list of exceptions that get error codes. Sometimes these messages can be a little bit unusual and surprising. Here is a silly sample program and the error it produces when run. The code: ``` include # int main(int argc, char argv) { int ( p)(); p = 0x0; return printf(\"%d\\n\", ( p)()); } ``` If compiled (without optimization is best) and run, this generates the kernel message: a.out[3714]: segfault at 0 ip (null) sp 00007ffe872aa418 error 14 in a.out[400000+1000] The ' (null) ' bit turns out to be expected; it's what the general kernel printf() function generates when asked to print something as a pointer and it's null (as seen here ). In our case the instruction pointer is 0 (null) because we've made a subroutine call through a null pointer and thus we're trying to execute code at address 0. I don't know why the 'in ...' portion says that we're in the executable (although in this case the call actually was there). The error code of 14 is in hex, which means that as bits it's 010100. This is a user mode read of an unmapped area (our usual '4' case), but it's an instruction fetch, not a normal data read or write. Any error 14s are a sign of some form of mangled function call or a return to a mangled address because the stack has been mashed. (These bits turn out to come straight from the CPU's page fault IDT .) For 64-bit x86 Linux kernels (and possibly for 32-bit x86 ones as well), the code you want to look at is show_signal_msg in fault.c , which prints the general 'segfault at ..' message, do_trap and do_general_protection in traps.c , which print the 'trap ...' messages, and print_vma_addr in memory.c , which prints the 'in ...' portion for all of these messages. Sidebar: The various error code bits as numbers # +1 protection fault in a mapped area (eg writing to a read-only mapping) +2 write (instead of a read) +4 user mode access (instead of kernel mode access) +8 use of reserved bits in the page table entry detected (the kernel will panic if this happens) +16 (+0x10) fault was an instruction fetch, not data read or write +32 (+0x20) 'protection keys block access' (don't ask me) Hex 0x14 is 0x10 + 4; (hex) 6 is 4 + 2. Error code 7 (0x7) is 4 + 2 + 1, a user-mode write to a read-only mapping, and is what you get if you attempt to write to a string constant in C: char *ex = \"example\"; int main(int argc, char **argv) { *ex = 'E'; } Compile and run this and you will get: a.out[8832]: segfault at 400540 ip 0000000000400499 sp 00007ffce6831490 error 7 in a.out[400000+1000] It appears that the program code always gets loaded at 0x400000 for ordinary programs, although I believe that shared libraries can have their location randomized. PS: Per a comment in the kernel source, all accesses to addresses above the end of user space will be labeled as 'protection fault in a mapped area' whether or not there are actual page table entries there. The kernel does this so you can't work out where its memory pages are by looking at the error code. (I believe that user space normally ends around 0x07fffffffffff, per mm.txt , although see the comments about TASK_SIZE_MAX in processor.h and also page_64_types.h .)","title":"Dmesg-segfaulting"},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/#what-the-linux-kernels-messages-about-segfaulting-programs-mean-on-64-bit-x86","text":"For quite a while the Linux kernel has had an option to log a kernel message about every faulting user program , and it probably defaults to on in your Linux distribution. I've seen these messages fly by for years, but for reasons beyond the scope of this entry I've recently wanted to understand what they mean in some moderate amount of detail. I'll start with a straightforward and typical example, one that I see every time I build and test Go (as this is a test case that is supposed to crash): testp[19288]: segfault at 0 ip 0000000000401271 sp 00007fff2ce4d210 error 4 in testp[400000+98000] The meaning of this is: ' testp[19288] ' is the faulting program and its PID ' segfault at 0 ' tells us the memory address (in hex) that caused the segfault when the program tried to access it. Here the address is 0, so we have a null dereference of some sort. ' ip 0000000000401271 ' is the value of the instruction pointer at the time of the fault. This should be the instruction that attempted to do the invalid memory access. In 64-bit x86, this will be register %rip (useful for inspecting things in GDB and elsewhere). ' sp 00007fff2ce4d210 ' is the value of the stack pointer. In 64-bit x86, this will be %rsp . ' error 4 ' is the page fault error code bits from traps.h in hex, as usual, and will almost always be at least 4 (which means 'user-mode access'). A value of 4 means it was a read of an unmapped area , such as address 0, while a value of 6 (4+2) means it was a write of an unmapped area . ' in testp[400000+98000] ' tells us the specific virtual memory area that the instruction pointer is in, specifying which file it is (here it's the executable), the starting address that VMA is mapped at ( 0x400000 ), and the size of the mapping ( 0x98000 ). With a faulting address of 0 and an error code of 4, we know this particular segfault is a read of a null pointer . Here's two more error messages: bash[12235]: segfault at 1054808 ip 000000000041d989 sp 00007ffec1f1cbd8 error 6 in bash[400000+f4000] 'Error 6' means a write to an unmapped user address , here 0x1054808 . bash[11909]: segfault at 0 ip 00007f83c03db746 sp 00007ffccbeda010 error 4 in libc-2.23.so[7f83c0350000+1c0000] Error 4 and address 0 is a null pointer read but this time it's in some libc function, not in bash's own code, since it's reported as 'in libc-2.23.so [...]'. Since I looked at the core dump, I can tell you that this was in strlen() . On 64-bit x86 Linux, you'll get a somewhat different message if the problem is actually with the instruction being executed, not the address it's referencing. For example: bash[2848] trap invalid opcode ip:48db90 sp:7ffddc8879e8 error:0 in bash[400000+f4000] There are a number of such trap types set up in traps.c . Two notable additional ones are 'divide error', which you get if you do an integer division by zero, and 'general protection', which you can get for certain extremely wild pointers (one case I know of is when your 64-bit x86 address is not in 'canonical form' ). Although these fields are formatted slightly differently, most of them mean the same thing as in segfaults. The exception is ' error:0 ', which is not a page fault error code. I don't understand the relevant kernel code enough to know what it means, but if I'm reading between the lines correctly in entry_64.txt , then it's either 0 (the usual case) or an error code from the CPU. Here is one possible list of exceptions that get error codes. Sometimes these messages can be a little bit unusual and surprising. Here is a silly sample program and the error it produces when run. The code: ```","title":"What the Linux kernel's messages about segfaulting programs mean on 64-bit x86"},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/#include","text":"int main(int argc, char argv) { int ( p)(); p = 0x0; return printf(\"%d\\n\", ( p)()); } ``` If compiled (without optimization is best) and run, this generates the kernel message: a.out[3714]: segfault at 0 ip (null) sp 00007ffe872aa418 error 14 in a.out[400000+1000] The ' (null) ' bit turns out to be expected; it's what the general kernel printf() function generates when asked to print something as a pointer and it's null (as seen here ). In our case the instruction pointer is 0 (null) because we've made a subroutine call through a null pointer and thus we're trying to execute code at address 0. I don't know why the 'in ...' portion says that we're in the executable (although in this case the call actually was there). The error code of 14 is in hex, which means that as bits it's 010100. This is a user mode read of an unmapped area (our usual '4' case), but it's an instruction fetch, not a normal data read or write. Any error 14s are a sign of some form of mangled function call or a return to a mangled address because the stack has been mashed. (These bits turn out to come straight from the CPU's page fault IDT .) For 64-bit x86 Linux kernels (and possibly for 32-bit x86 ones as well), the code you want to look at is show_signal_msg in fault.c , which prints the general 'segfault at ..' message, do_trap and do_general_protection in traps.c , which print the 'trap ...' messages, and print_vma_addr in memory.c , which prints the 'in ...' portion for all of these messages.","title":"include "},{"location":"Programming/08-Shell/Dmesg/Dmesg-segfaulting/#sidebar-the-various-error-code-bits-as-numbers","text":"+1 protection fault in a mapped area (eg writing to a read-only mapping) +2 write (instead of a read) +4 user mode access (instead of kernel mode access) +8 use of reserved bits in the page table entry detected (the kernel will panic if this happens) +16 (+0x10) fault was an instruction fetch, not data read or write +32 (+0x20) 'protection keys block access' (don't ask me) Hex 0x14 is 0x10 + 4; (hex) 6 is 4 + 2. Error code 7 (0x7) is 4 + 2 + 1, a user-mode write to a read-only mapping, and is what you get if you attempt to write to a string constant in C: char *ex = \"example\"; int main(int argc, char **argv) { *ex = 'E'; } Compile and run this and you will get: a.out[8832]: segfault at 400540 ip 0000000000400499 sp 00007ffce6831490 error 7 in a.out[400000+1000] It appears that the program code always gets loaded at 0x400000 for ordinary programs, although I believe that shared libraries can have their location randomized. PS: Per a comment in the kernel source, all accesses to addresses above the end of user space will be labeled as 'protection fault in a mapped area' whether or not there are actual page table entries there. The kernel does this so you can't work out where its memory pages are by looking at the error code. (I believe that user space normally ends around 0x07fffffffffff, per mm.txt , although see the comments about TASK_SIZE_MAX in processor.h and also page_64_types.h .)","title":"Sidebar: The various error code bits as numbers"},{"location":"Programming/08-Shell/Dmesg/Dmesg/","text":"How to find out why process was killed on server QuotePredict [8\u6708 1 09:20] traps: hsserver[194024] trap divide error ip:7f9090ab8885 sp:7f8e67ffe8c8 error:0 [ +0.000007] traps: hsserver[194014] trap divide error ip:7f9090ab8885 sp:7f8e83ffe8c8 error:0 [8\u6708 2 09:19] traps: hsserver[106631] trap divide error ip:7f94437d4885 sp:7f925e7fb8c8 error:0 in libfsc_quote_predict.so[7f9443771000+8a000]","title":"Dmesg"},{"location":"Programming/08-Shell/Linux-Kernel-module/","text":"MODINFO(8) # LSMOD(8) # MODPROBE(8) #","title":"Introduction"},{"location":"Programming/08-Shell/Linux-Kernel-module/#modinfo8","text":"","title":"MODINFO(8)"},{"location":"Programming/08-Shell/Linux-Kernel-module/#lsmod8","text":"","title":"LSMOD(8)"},{"location":"Programming/08-Shell/Linux-Kernel-module/#modprobe8","text":"","title":"MODPROBE(8)"}]}